{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavlos01232/Match_Outcome_Prediction/blob/main/Copy_of_game_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzVKKLMBRPxZ",
        "outputId": "5c289927-6477-4d00-879e-87d58b27f35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         0.0119392853, -0.0026036678])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.3222656250, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8265503049), tensor(1.7994883060), tensor(1.1036907434), tensor(1.4757459164), tensor(0.8564109802), tensor(0.7092776895), tensor(1.6848160028), tensor(1.1613157988), tensor(1.3372441530), tensor(1.0906388760), tensor(1.0172271729), tensor(1.0802649260), tensor(1.8331731558), tensor(1.2373331785), tensor(1.6385632753), tensor(1.8251466751), tensor(1.6076239347), tensor(1.1280952692), tensor(0.8818843365), tensor(1.1014093161)]\n",
            "b:  [tensor(1.6650396585), tensor(0.7210913897), tensor(1.4754663706), tensor(1.2917177677), tensor(1.0120211840), tensor(2.0127961636), tensor(1.2772898674), tensor(1.3992855549), tensor(1.2825865746), tensor(1.0631135702), tensor(1.3680591583), tensor(1.2882391214), tensor(0.7621220946), tensor(1.2600783110), tensor(1.2257406712), tensor(0.8891686797), tensor(1.0841168165), tensor(1.3933502436), tensor(1.3821190596), tensor(1.5466895103)]\n",
            "c:  [tensor(0.0005956963), tensor(0.0005956963), tensor(0.0005956963), tensor(0.0001042592), tensor(-2.5559190817e-06), tensor(5.4917829402e-05), tensor(-2.7082080123e-05), tensor(6.0032543843e-06), tensor(0.0001042592), tensor(-2.5559190817e-06), tensor(5.4917829402e-05), tensor(-2.7082080123e-05), tensor(6.0032543843e-06), tensor(0.0001042592), tensor(-2.5559190817e-06), tensor(5.4917829402e-05), tensor(-2.7082080123e-05), tensor(6.0032543843e-06)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.2946479917,  0.3384072483, -0.2513119578,  0.4481681585,\n",
            "        -0.0337487459, -0.0844672918, -0.2376220226, -0.1702057123,\n",
            "        -0.1933637559,  0.1296342015, -0.1234755665, -0.1466802806,\n",
            "         0.3039904237, -0.1297250986, -0.0662493259,  0.5869022608,\n",
            "         0.3277940750,  0.0258959532, -0.0271571279,  0.1129089594])\n",
            "btensor.grad: tensor([ 0.8730577826, -0.1970743239,  0.4355230927, -0.4085541666,\n",
            "         0.2366471440,  0.4504147172,  0.1137902141,  0.1203997731,\n",
            "         0.0560476780,  0.0558525324,  0.2011656463,  0.0157862306,\n",
            "        -0.0344979167, -0.0354329944, -0.3727207780, -0.3222306371,\n",
            "        -0.4828188419, -0.1343892515,  0.0151062608, -0.0877486467])\n",
            "ctensor.grad: tensor([-0.1296142936, -0.1296142936, -0.1296142936, -0.0539750420,\n",
            "         0.0013170039, -0.0289748814,  0.0136460839, -0.0029864155,\n",
            "        -0.0539750420,  0.0013170039, -0.0289748814,  0.0136460839,\n",
            "        -0.0029864155, -0.0539750420,  0.0013170039, -0.0289748814,\n",
            "         0.0136460839, -0.0029864155])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.3073120117, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8277243972), tensor(1.7979924679), tensor(1.1047813892), tensor(1.4737752676), tensor(0.8565338850), tensor(0.7095830441), tensor(1.6858996153), tensor(1.1620658636), tensor(1.3381023407), tensor(1.0900809765), tensor(1.0177731514), tensor(1.0809329748), tensor(1.8318388462), tensor(1.2379103899), tensor(1.6388986111), tensor(1.8225305080), tensor(1.6062223911), tensor(1.1280196905), tensor(0.8820222616), tensor(1.1009624004)]\n",
            "b:  [tensor(1.6611701250), tensor(0.7218714952), tensor(1.4735341072), tensor(1.2935183048), tensor(1.0110613108), tensor(2.0107820034), tensor(1.2768406868), tensor(1.3987710476), tensor(1.2823820114), tensor(1.0628951788), tensor(1.3671889305), tensor(1.2881894112), tensor(0.7622495890), tensor(1.2602577209), tensor(1.2273881435), tensor(0.8904969096), tensor(1.0861848593), tensor(1.3939752579), tensor(1.3820790052), tensor(1.5471010208)]\n",
            "c:  [tensor(0.0006548617), tensor(0.0006548617), tensor(0.0006548617), tensor(0.0001346169), tensor(-3.2949237720e-06), tensor(7.1344635217e-05), tensor(-3.4669290471e-05), tensor(7.6743663158e-06), tensor(0.0001346169), tensor(-3.2949237720e-06), tensor(7.1344635217e-05), tensor(-3.4669290471e-05), tensor(7.6743663158e-06), tensor(0.0001346169), tensor(-3.2949237720e-06), tensor(7.1344635217e-05), tensor(-3.4669290471e-05), tensor(7.6743663158e-06)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.2348172069,  0.2991573215, -0.2181220055,  0.3941324949,\n",
            "        -0.0245771408, -0.0610668659, -0.2167180777, -0.1500106454,\n",
            "        -0.1716454625,  0.1115913093, -0.1091997921, -0.1336091459,\n",
            "         0.2668587267, -0.1154340506, -0.0670732558,  0.5232324600,\n",
            "         0.2803010941,  0.0151122808, -0.0275839567,  0.0893719196])\n",
            "btensor.grad: tensor([ 0.7738993764, -0.1560191512,  0.3864570260, -0.3601135612,\n",
            "         0.1919784546,  0.4028334022,  0.0898302794,  0.1028970703,\n",
            "         0.0409023762,  0.0436694622,  0.1740517467,  0.0099425912,\n",
            "        -0.0255003236, -0.0358852744, -0.3295050859, -0.2656480670,\n",
            "        -0.4136040211, -0.1250002682,  0.0080114603, -0.0823078156])\n",
            "ctensor.grad: tensor([-0.1183309183, -0.1183309183, -0.1183309183, -0.0607154630,\n",
            "         0.0014780090, -0.0328536034,  0.0151744178, -0.0033422238,\n",
            "        -0.0607154630,  0.0014780090, -0.0328536034,  0.0151744178,\n",
            "        -0.0033422238, -0.0607154630,  0.0014780090, -0.0328536034,\n",
            "         0.0151744178, -0.0033422238])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2955322266, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8286592960), tensor(1.7966694832), tensor(1.1057277918), tensor(1.4720424414), tensor(0.8566219807), tensor(0.7098002434), tensor(1.6868886948), tensor(1.1627268791), tensor(1.3388645649), tensor(1.0896005630), tensor(1.0182559490), tensor(1.0815407038), tensor(1.8306677341), tensor(1.2384231091), tensor(1.6392326355), tensor(1.8201969862), tensor(1.6050250530), tensor(1.1279867887), tensor(0.8821588159), tensor(1.1006119251)]\n",
            "b:  [tensor(1.6577407122), tensor(0.7224900126), tensor(1.4718195200), tensor(1.2951056957), tensor(1.0102846622), tensor(2.0089793205), tensor(1.2764905691), tensor(1.3983316422), tensor(1.2822387218), tensor(1.0627255440), tensor(1.3664361238), tensor(1.2881637812), tensor(0.7623431087), tensor(1.2604360580), tensor(1.2288455963), tensor(0.8915924430), tensor(1.0879584551), tensor(1.3945558071), tensor(1.3820677996), tensor(1.5474870205)]\n",
            "c:  [tensor(0.0007092018), tensor(0.0007092018), tensor(0.0007092018), tensor(0.0001680652), tensor(-4.1074031287e-06), tensor(8.9570959972e-05), tensor(-4.2943371227e-05), tensor(9.5112727649e-06), tensor(0.0001680652), tensor(-4.1074031287e-06), tensor(8.9570959972e-05), tensor(-4.2943371227e-05), tensor(9.5112727649e-06), tensor(0.0001680652), tensor(-4.1074031287e-06), tensor(8.9570959972e-05), tensor(-4.2943371227e-05), tensor(9.5112727649e-06)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.1869836450,  0.2645860314, -0.1892884076,  0.3465682268,\n",
            "        -0.0176147223, -0.0434371233, -0.1978223324, -0.1321974993,\n",
            "        -0.1524417102,  0.0960749686, -0.0965681076, -0.1215561703,\n",
            "         0.2342155576, -0.1025508642, -0.0668153167,  0.4667019844,\n",
            "         0.2394567728,  0.0065814853, -0.0273137093,  0.0700958371])\n",
            "btensor.grad: tensor([ 0.6858824492, -0.1237080395,  0.3429234624, -0.3174762726,\n",
            "         0.1553290188,  0.3605381250,  0.0700227022,  0.0878766924,\n",
            "         0.0286607742,  0.0339201689,  0.1505591720,  0.0051233768,\n",
            "        -0.0187096503, -0.0356761217, -0.2914863229, -0.2191045880,\n",
            "        -0.3547215462, -0.1161196828,  0.0022491813, -0.0772050619])\n",
            "ctensor.grad: tensor([-0.1086800918, -0.1086800918, -0.1086800918, -0.0668967366,\n",
            "         0.0016249586, -0.0364526436,  0.0165481605, -0.0036738121,\n",
            "        -0.0668967366,  0.0016249586, -0.0364526436,  0.0165481605,\n",
            "        -0.0036738121, -0.0668967366,  0.0016249586, -0.0364526436,\n",
            "         0.0165481605, -0.0036738121])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2866210938, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8294028640), tensor(1.7954987288), tensor(1.1065489054), tensor(1.4705188274), tensor(0.8566837311), tensor(0.7099512815), tensor(1.6877921820), tensor(1.1633092165), tensor(1.3395416737), tensor(1.0891867876), tensor(1.0186828375), tensor(1.0820928812), tensor(1.8296400309), tensor(1.2388778925), tensor(1.6395611763), tensor(1.8181145191), tensor(1.6040031910), tensor(1.1279871464), tensor(0.8822914958), tensor(1.1003400087)]\n",
            "b:  [tensor(1.6547017097), tensor(0.7229811549), tensor(1.4702979326), tensor(1.2965052128), tensor(1.0096580982), tensor(2.0073647499), tensor(1.2762219906), tensor(1.3979566097), tensor(1.2821445465), tensor(1.0625947714), tensor(1.3657850027), tensor(1.2881577015), tensor(0.7624110579), tensor(1.2606108189), tensor(1.2301355600), tensor(0.8924962282), tensor(1.0894811153), tensor(1.3950943947), tensor(1.3820796013), tensor(1.5478489399)]\n",
            "c:  [tensor(0.0007596071), tensor(0.0007596071), tensor(0.0007596071), tensor(0.0002043632), tensor(-4.9873478929e-06), tensor(0.0001094720), tensor(-5.1840481319e-05), tensor(1.1503614587e-05), tensor(0.0002043632), tensor(-4.9873478929e-06), tensor(0.0001094720), tensor(-5.1840481319e-05), tensor(1.1503614587e-05), tensor(0.0002043632), tensor(-4.9873478929e-06), tensor(0.0001094720), tensor(-5.1840481319e-05), tensor(1.1503614587e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.4870959520e-01,  2.3414140940e-01, -1.6422620416e-01,\n",
            "         3.0473279953e-01, -1.2347579002e-02, -3.0206441879e-02,\n",
            "        -1.8068957329e-01, -1.1647248268e-01, -1.3542249799e-01,\n",
            "         8.2749664783e-02, -8.5371121764e-02, -1.1044548452e-01,\n",
            "         2.0554727316e-01, -9.0956807137e-02, -6.5706729889e-02,\n",
            "         4.1650074720e-01,  2.0436435938e-01, -7.0869922638e-05,\n",
            "        -2.6534259319e-02,  5.4376304150e-02])\n",
            "btensor.grad: tensor([ 0.6078022718, -0.0982260406,  0.3043233156, -0.2799091935,\n",
            "         0.1253238320,  0.3229208291,  0.0537173152,  0.0750025511,\n",
            "         0.0188415051,  0.0261595249,  0.1302201301,  0.0012100935,\n",
            "        -0.0135909300, -0.0349520147, -0.2579903603, -0.1807527542,\n",
            "        -0.3045258522, -0.1077197790, -0.0023568273, -0.0723799467])\n",
            "ctensor.grad: tensor([-0.1008107066, -0.1008107066, -0.1008107066, -0.0725959539,\n",
            "         0.0017598895, -0.0398021042,  0.0177942179, -0.0039846832,\n",
            "        -0.0725959539,  0.0017598895, -0.0398021042,  0.0177942179,\n",
            "        -0.0039846832, -0.0725959539,  0.0017598895, -0.0398021042,\n",
            "         0.0177942179, -0.0039846832])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2798461914, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8299932480), tensor(1.7944620848), tensor(1.1072610617), tensor(1.4691790342), tensor(0.8567256927), tensor(0.7100529671), tensor(1.6886178255), tensor(1.1638221741), tensor(1.3401433229), tensor(1.0888302326), tensor(1.0190600157), tensor(1.0825940371), tensor(1.8287380934), tensor(1.2392805815), tensor(1.6398808956), tensor(1.8162549734), tensor(1.6031320095), tensor(1.1280130148), tensor(0.8824185133), tensor(1.1001319885)]\n",
            "b:  [tensor(1.6520088911), tensor(0.7233715653), tensor(1.4689474106), tensor(1.2977391481), tensor(1.0091540813), tensor(2.0059175491), tensor(1.2760201693), tensor(1.3976367712), tensor(1.2820893526), tensor(1.0624947548), tensor(1.3652218580), tensor(1.2881673574), tensor(0.7624598145), tensor(1.2607799768), tensor(1.2312778234), tensor(0.8932418227), tensor(1.0907894373), tensor(1.3955934048), tensor(1.3821095228), tensor(1.5481879711)]\n",
            "c:  [tensor(0.0008069731), tensor(0.0008069731), tensor(0.0008069731), tensor(0.0002433131), tensor(-5.9298608903e-06), tensor(0.0001309426), tensor(-6.1310151068e-05), tensor(1.3643085367e-05), tensor(0.0002433131), tensor(-5.9298608903e-06), tensor(0.0001309426), tensor(-6.1310151068e-05), tensor(1.3643085367e-05), tensor(0.0002433131), tensor(-5.9298608903e-06), tensor(0.0001309426), tensor(-6.1310151068e-05), tensor(1.3643085367e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.1180776954,  0.2073203325, -0.1424304247,  0.2679537535,\n",
            "        -0.0083905458, -0.0203341246, -0.1651183367, -0.1025858819,\n",
            "        -0.1203231290,  0.0713157654, -0.0754410774, -0.1002262756,\n",
            "         0.1803861856, -0.0805490017, -0.0639525056,  0.3719038367,\n",
            "         0.1742373109, -0.0051737428, -0.0254019499,  0.0416079760])\n",
            "btensor.grad: tensor([ 0.5385597348, -0.0780862570,  0.2701057196, -0.2467962205,\n",
            "         0.1008105129,  0.2894400656,  0.0403572321,  0.0639759600,\n",
            "         0.0110301971,  0.0200102329,  0.1126212925, -0.0019263625,\n",
            "        -0.0097468216, -0.0338368118, -0.2284535617, -0.1491151452,\n",
            "        -0.2616579533, -0.0997919142, -0.0059849620, -0.0677977800])\n",
            "ctensor.grad: tensor([-0.0947320089, -0.0947320089, -0.0947320089, -0.0778996646,\n",
            "         0.0018850258, -0.0429411940,  0.0189393386, -0.0042789420,\n",
            "        -0.0778996646,  0.0018850258, -0.0429411940,  0.0189393386,\n",
            "        -0.0042789420, -0.0778996646,  0.0018850258, -0.0429411940,\n",
            "         0.0189393386, -0.0042789420])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2742919922, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8304611444), tensor(1.7935436964), tensor(1.1078784466), tensor(1.4680008888), tensor(0.8567529321), tensor(0.7101181149), tensor(1.6893725395), tensor(1.1642738581), tensor(1.3406779766), tensor(1.0885227919), tensor(1.0193932056), tensor(1.0830482244), tensor(1.8279465437), tensor(1.2396367788), tensor(1.6401895285), tensor(1.8145936728), tensor(1.6023900509), tensor(1.1280580759), tensor(0.8825387955), tensor(1.0999755859)]\n",
            "b:  [tensor(1.6496230364), tensor(0.7236823440), tensor(1.4677485228), tensor(1.2988271713), tensor(1.0087499619), tensor(2.0046195984), tensor(1.2758729458), tensor(1.3973641396), tensor(1.2820650339), tensor(1.0624189377), tensor(1.3647348881), tensor(1.2881892920), tensor(0.7624942064), tensor(1.2609422207), tensor(1.2322897911), tensor(0.8938568234), tensor(1.0919144154), tensor(1.3960551023), tensor(1.3821535110), tensor(1.5485051870)]\n",
            "c:  [tensor(0.0008521415), tensor(0.0008521415), tensor(0.0008521415), tensor(0.0002847614), tensor(-6.9311618063e-06), tensor(0.0001538994), tensor(-7.1314498200e-05), tensor(1.5923644241e-05), tensor(0.0002847614), tensor(-6.9311618063e-06), tensor(0.0001538994), tensor(-7.1314498200e-05), tensor(1.5923644241e-05), tensor(0.0002847614), tensor(-6.9311618063e-06), tensor(0.0001538994), tensor(-7.1314498200e-05), tensor(1.5923644241e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0935797095,  0.1836872995, -0.1234793663,  0.2356231213,\n",
            "        -0.0054531097, -0.0130293369, -0.1509475708, -0.0903269649,\n",
            "        -0.1069191843,  0.0614986122, -0.0666464865, -0.0908439606,\n",
            "         0.1583034396, -0.0712289810, -0.0617255718,  0.3322620988,\n",
            "         0.1483871341, -0.0090186000, -0.0240514874,  0.0312730074])\n",
            "btensor.grad: tensor([ 0.4771678150, -0.0621564388,  0.2397715598, -0.2175985873,\n",
            "         0.0808179006,  0.2596032023,  0.0294505954,  0.0545343757,\n",
            "         0.0048664808,  0.0151540041,  0.0973901451, -0.0043968558,\n",
            "        -0.0068724547, -0.0324519873, -0.2023895681, -0.1229953766,\n",
            "        -0.2250006199, -0.0923278928, -0.0087886453, -0.0634418726])\n",
            "ctensor.grad: tensor([-0.0903367773, -0.0903367773, -0.0903367773, -0.0828966424,\n",
            "         0.0020026017, -0.0459134728,  0.0200086925, -0.0045611155,\n",
            "        -0.0828966424,  0.0020026017, -0.0459134728,  0.0200086925,\n",
            "        -0.0045611155, -0.0828966424,  0.0020026017, -0.0459134728,\n",
            "         0.0200086925, -0.0045611155])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2702026367, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8308311701), tensor(1.7927294970), tensor(1.1084134579), tensor(1.4669648409), tensor(0.8567694426), tensor(0.7101565599), tensor(1.6900627613), tensor(1.1646714211), tensor(1.3411530256), tensor(1.0882575512), tensor(1.0196875334), tensor(1.0834594965), tensor(1.8272519112), tensor(1.2399512529), tensor(1.6404854059), tensor(1.8131086826), tensor(1.6017589569), tensor(1.1281173229), tensor(0.8826516867), tensor(1.0998609066)]\n",
            "b:  [tensor(1.6475093365), tensor(0.7239300609), tensor(1.4666841030), tensor(1.2997864485), tensor(1.0084272623), tensor(2.0034546852), tensor(1.2757699490), tensor(1.3971319199), tensor(1.2820647955), tensor(1.0623623133), tensor(1.3643138409), tensor(1.2882208824), tensor(0.7625179291), tensor(1.2610965967), tensor(1.2331867218), tensor(0.8943639398), tensor(1.0928825140), tensor(1.3964817524), tensor(1.3822079897), tensor(1.5488017797)]\n",
            "c:  [tensor(0.0008958922), tensor(0.0008958922), tensor(0.0008958922), tensor(0.0003285971), tensor(-7.9885203377e-06), tensor(0.0001782805), tensor(-8.1826969108e-05), tensor(1.8341403120e-05), tensor(0.0003285971), tensor(-7.9885203377e-06), tensor(0.0001782805), tensor(-8.1826969108e-05), tensor(1.8341403120e-05), tensor(0.0003285971), tensor(-7.9885203377e-06), tensor(0.0001782805), tensor(-8.1826969108e-05), tensor(1.8341403120e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0740035772,  0.1628467292, -0.1070116162,  0.2072039843,\n",
            "        -0.0033069849, -0.0076923370, -0.1380395889, -0.0795113146,\n",
            "        -0.0950172991,  0.0530599058, -0.0588636547, -0.0822583362,\n",
            "         0.1389251947, -0.0629062653, -0.0591675639,  0.2970014215,\n",
            "         0.1262159944, -0.0118452907, -0.0225840211,  0.0229400992])\n",
            "btensor.grad: tensor([ 4.2274022102e-01, -4.9544274807e-02,  2.1287275851e-01,\n",
            "        -1.9185653329e-01,  6.4532145858e-02,  2.3299363256e-01,\n",
            "         2.0595014095e-02,  4.6445250511e-02,  5.3048133850e-05,\n",
            "         1.1329054832e-02,  8.4206014872e-02, -6.3164234161e-03,\n",
            "        -4.7489870340e-03, -3.0881166458e-02, -1.7938169837e-01,\n",
            "        -1.0142302513e-01, -1.9362306595e-01, -8.5327297449e-02,\n",
            "        -1.0906279087e-02, -5.9308171272e-02])\n",
            "ctensor.grad: tensor([-0.0875013098, -0.0875013098, -0.0875013098, -0.0876714811,\n",
            "         0.0021147174, -0.0487622656,  0.0210249349, -0.0048355176,\n",
            "        -0.0876714811,  0.0021147174, -0.0487622656,  0.0210249349,\n",
            "        -0.0048355176, -0.0876714811,  0.0021147174, -0.0487622656,\n",
            "         0.0210249349, -0.0048355176])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2670898438, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8311231136), tensor(1.7920072079), tensor(1.1088770628), tensor(1.4660537243), tensor(0.8567783237), tensor(0.7101758718), tensor(1.6906940937), tensor(1.1650213003), tensor(1.3415752649), tensor(1.0880285501), tensor(1.0199475288), tensor(1.0838316679), tensor(1.8266422749), tensor(1.2402287722), tensor(1.6407673359), tensor(1.8117805719), tensor(1.6012229919), tensor(1.1281865835), tensor(0.8827570677), tensor(1.0997797251)]\n",
            "b:  [tensor(1.6456369162), tensor(0.7241278291), tensor(1.4657390118), tensor(1.3006322384), tensor(1.0081708431), tensor(2.0024085045), tensor(1.2757027149), tensor(1.3969343901), tensor(1.2820831537), tensor(1.0623207092), tensor(1.3639498949), tensor(1.2882597446), tensor(0.7625339627), tensor(1.2612426281), tensor(1.2339820862), tensor(0.8947819471), tensor(1.0937162638), tensor(1.3968756199), tensor(1.3822703362), tensor(1.5490787029)]\n",
            "c:  [tensor(0.0009389285), tensor(0.0009389285), tensor(0.0009389285), tensor(0.0003747488), tensor(-9.1001793407e-06), tensor(0.0002040456), tensor(-9.2831091024e-05), tensor(2.0894769477e-05), tensor(0.0003747488), tensor(-9.1001793407e-06), tensor(0.0002040456), tensor(-9.2831091024e-05), tensor(2.0894769477e-05), tensor(0.0003747488), tensor(-9.1001793407e-06), tensor(0.0002040456), tensor(-9.2831091024e-05), tensor(2.0894769477e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0583897829,  0.1444528997, -0.0927136838,  0.1822193861,\n",
            "        -0.0017772913, -0.0038591623, -0.1262725592, -0.0699836612,\n",
            "        -0.0844541788,  0.0457968712, -0.0519888401, -0.0744287074,\n",
            "         0.1219182611, -0.0555036068, -0.0563945919,  0.2656167448,\n",
            "         0.1072037816, -0.0138568282, -0.0210785866,  0.0162443519])\n",
            "btensor.grad: tensor([ 0.3744893074, -0.0395576954,  0.1890151203, -0.1691654027,\n",
            "         0.0512860976,  0.2092291713,  0.0134356618,  0.0395069569,\n",
            "        -0.0036612749,  0.0083297491,  0.0727836788, -0.0077741742,\n",
            "        -0.0032030437, -0.0292091668, -0.1590658128, -0.0836068988,\n",
            "        -0.1667401791, -0.0787816644, -0.0124682188, -0.0553907156])\n",
            "ctensor.grad: tensor([-0.0860724524, -0.0860724524, -0.0860724524, -0.0923034474,\n",
            "         0.0022233180, -0.0515301302,  0.0220082346, -0.0051067318,\n",
            "        -0.0923034474,  0.0022233180, -0.0515301302,  0.0220082346,\n",
            "        -0.0051067318, -0.0923034474,  0.0022233180, -0.0515301302,\n",
            "         0.0220082346, -0.0051067318])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2640380859, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8313529491), tensor(1.7913662195), tensor(1.1092785597), tensor(1.4652525187), tensor(0.8567819595), tensor(0.7101817727), tensor(1.6912717819), tensor(1.1653293371), tensor(1.3419506550), tensor(1.0878309011), tensor(1.0201771259), tensor(1.0841681957), tensor(1.8261073828), tensor(1.2404733896), tensor(1.6410348415), tensor(1.8105922937), tensor(1.6007684469), tensor(1.1282627583), tensor(0.8828550577), tensor(1.0997252464)]\n",
            "b:  [tensor(1.6439783573), tensor(0.7242860794), tensor(1.4648997784), tensor(1.3013781309), tensor(1.0079681873), tensor(2.0014686584), tensor(1.2756643295), tensor(1.3967666626), tensor(1.2821155787), tensor(1.0622907877), tensor(1.3636355400), tensor(1.2883039713), tensor(0.7625444531), tensor(1.2613800764), tensor(1.2346876860), tensor(0.8951264620), tensor(1.0944347382), tensor(1.3972390890), tensor(1.3823381662), tensor(1.5493371487)]\n",
            "c:  [tensor(0.0009818857), tensor(0.0009818857), tensor(0.0009818857), tensor(0.0004231812), tensor(-1.0265257515e-05), tensor(0.0002311738), tensor(-0.0001043192), tensor(2.3584054361e-05), tensor(0.0004231812), tensor(-1.0265257515e-05), tensor(0.0002311738), tensor(-0.0001043192), tensor(2.3584054361e-05), tensor(0.0004231812), tensor(-1.0265257515e-05), tensor(0.0002311738), tensor(-0.0001043192), tensor(2.3584054361e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0459650755,  0.1282025874, -0.0803096294,  0.1602469683,\n",
            "        -0.0007287264, -0.0011743307, -0.1155412197, -0.0616021454,\n",
            "        -0.0750842467,  0.0395283699, -0.0459306389, -0.0673034191,\n",
            "         0.1069893837, -0.0489341021, -0.0535012335,  0.2376585305,\n",
            "         0.0909071565, -0.0152252316, -0.0195955038,  0.0108884275])\n",
            "btensor.grad: tensor([ 0.3317100704, -0.0316514075,  0.1678460538, -0.1491693854,\n",
            "         0.0405265279,  0.1879818738,  0.0076801777,  0.0335506350,\n",
            "        -0.0064888000,  0.0059775114,  0.0628807247, -0.0088564754,\n",
            "        -0.0020961147, -0.0274900198, -0.1411267817, -0.0688999891,\n",
            "        -0.1436917782, -0.0726839900, -0.0135707259, -0.0516924858])\n",
            "ctensor.grad: tensor([-0.0859144703, -0.0859144703, -0.0859144703, -0.0968646258,\n",
            "         0.0023301556, -0.0542564318,  0.0229762997, -0.0053785699,\n",
            "        -0.0968646258,  0.0023301556, -0.0542564318,  0.0229762997,\n",
            "        -0.0053785699, -0.0968646258,  0.0023301556, -0.0542564318,\n",
            "         0.0229762997, -0.0053785699])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2621459961, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8315334916), tensor(1.7907969952), tensor(1.1096264124), tensor(1.4645478725), tensor(0.8567822576), tensor(0.7101786137), tensor(1.6918005943), tensor(1.1656005383), tensor(1.3422845602), tensor(1.0876604319), tensor(1.0203801394), tensor(1.0844724178), tensor(1.8256379366), tensor(1.2406890392), tensor(1.6412876844), tensor(1.8095285892), tensor(1.6003837585), tensor(1.1283432245), tensor(0.8829458952), tensor(1.0996921062)]\n",
            "b:  [tensor(1.6425094604), tensor(0.7244130373), tensor(1.4641544819), tensor(1.3020359278), tensor(1.0078091621), tensor(2.0006239414), tensor(1.2756489515), tensor(1.3966245651), tensor(1.2821586132), tensor(1.0622701645), tensor(1.3633641005), tensor(1.2883521318), tensor(0.7625511289), tensor(1.2615089417), tensor(1.2353141308), tensor(0.8954102397), tensor(1.0950543880), tensor(1.3975741863), tensor(1.3824096918), tensor(1.5495781898)]\n",
            "c:  [tensor(0.0010253318), tensor(0.0010253318), tensor(0.0010253318), tensor(0.0004738916), tensor(-1.1483664821e-05), tensor(0.0002596628), tensor(-0.0001162916), tensor(2.6411655199e-05), tensor(0.0004738916), tensor(-1.1483664821e-05), tensor(0.0002596628), tensor(-0.0001162916), tensor(2.6411655199e-05), tensor(0.0004738916), tensor(-1.1483664821e-05), tensor(0.0002596628), tensor(-0.0001162916), tensor(2.6411655199e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-3.6110997200e-02,  1.1383675039e-01, -6.9563478231e-02,\n",
            "         1.4091825485e-01, -5.6624412537e-05,  6.3240528107e-04,\n",
            "        -1.0575258732e-01, -5.4236859083e-02, -6.6773578525e-02,\n",
            "         3.4103542566e-02, -4.0608495474e-02, -6.0843139887e-02,\n",
            "         9.3879163265e-02, -4.3130874634e-02, -5.0562024117e-02,\n",
            "         2.1273398399e-01,  7.6938152313e-02, -1.6085445881e-02,\n",
            "        -1.8172442913e-02,  6.6169500351e-03])\n",
            "btensor.grad: tensor([ 0.2937771678, -0.0253907442,  0.1490510404, -0.1315572560,\n",
            "         0.0317941830,  0.1689607799,  0.0030829310,  0.0284289867,\n",
            "        -0.0086051226,  0.0041323900,  0.0542881489, -0.0096368790,\n",
            "        -0.0013311766, -0.0257739425, -0.1252878606, -0.0567595363,\n",
            "        -0.1239271164, -0.0670178235, -0.0143076181, -0.0482079983])\n",
            "ctensor.grad: tensor([-0.0868922472, -0.0868922472, -0.0868922472, -0.1014208868,\n",
            "         0.0024368148, -0.0569780171,  0.0239447076, -0.0056552012,\n",
            "        -0.1014208868,  0.0024368148, -0.0569780171,  0.0239447076,\n",
            "        -0.0056552012, -0.1014208868,  0.0024368148, -0.0569780171,\n",
            "         0.0239447076, -0.0056552012])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2607421875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8316751122), tensor(1.7902914286), tensor(1.1099277735), tensor(1.4639283419), tensor(0.8567806482), tensor(0.7101697326), tensor(1.6922847033), tensor(1.1658394337), tensor(1.3425816298), tensor(1.0875134468), tensor(1.0205597878), tensor(1.0847474337), tensor(1.8252261877), tensor(1.2408790588), tensor(1.6415258646), tensor(1.8085761070), tensor(1.6000589132), tensor(1.1284260750), tensor(0.8830301166), tensor(1.0996760130)]\n",
            "b:  [tensor(1.6412087679), tensor(0.7245152593), tensor(1.4634927511), tensor(1.3026161194), tensor(1.0076856613), tensor(1.9998643398), tensor(1.2756518126), tensor(1.3965045214), tensor(1.2822093964), tensor(1.0622566938), tensor(1.3631299734), tensor(1.2884030342), tensor(0.7625552416), tensor(1.2616294622), tensor(1.2358705997), tensor(0.8956440091), tensor(1.0955892801), tensor(1.3978830576), tensor(1.3824834824), tensor(1.5498028994)]\n",
            "c:  [tensor(0.0010697803), tensor(0.0010697803), tensor(0.0010697803), tensor(0.0005269072), tensor(-1.2756019714e-05), tensor(0.0002895272), tensor(-0.0001287551), tensor(2.9381706554e-05), tensor(0.0005269072), tensor(-1.2756019714e-05), tensor(0.0002895272), tensor(-0.0001287551), tensor(2.9381706554e-05), tensor(0.0005269072), tensor(-1.2756019714e-05), tensor(0.0002895272), tensor(-0.0001287551), tensor(2.9381706554e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0283290148,  0.1011212617, -0.0602639914,  0.1239035130,\n",
            "         0.0003260374,  0.0017725229, -0.0968246460, -0.0477767289,\n",
            "        -0.0594126321,  0.0294026434, -0.0359412879, -0.0550006106,\n",
            "         0.0823580623, -0.0380110741, -0.0476312190,  0.1904938966,\n",
            "         0.0649642944, -0.0165588856, -0.0168401599,  0.0032284260])\n",
            "btensor.grad: tensor([ 0.2601333261, -0.0204427242,  0.1323543787, -0.1160502136,\n",
            "         0.0247120857,  0.1519104987, -0.0005605817,  0.0240191370,\n",
            "        -0.0101481676,  0.0026881695,  0.0468182564, -0.0101718307,\n",
            "        -0.0008242428, -0.0240952075, -0.1113018692, -0.0467549562,\n",
            "        -0.1069712639, -0.0617698133, -0.0147539377, -0.0449361801])\n",
            "ctensor.grad: tensor([-0.0888970420, -0.0888970420, -0.0888970420, -0.1060310900,\n",
            "         0.0025447088, -0.0597287901,  0.0249270722, -0.0059401011,\n",
            "        -0.1060310900,  0.0025447088, -0.0597287901,  0.0249270722,\n",
            "        -0.0059401011, -0.1060310900,  0.0025447088, -0.0597287901,\n",
            "         0.0249270722, -0.0059401011])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2593383789, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8317862153), tensor(1.7898421288), tensor(1.1101889610), tensor(1.4633837938), tensor(0.8567782640), tensor(0.7101576924), tensor(1.6927281618), tensor(1.1660500765), tensor(1.3428461552), tensor(1.0873869658), tensor(1.0207190514), tensor(1.0849961042), tensor(1.8248649836), tensor(1.2410466671), tensor(1.6417496204), tensor(1.8077229261), tensor(1.5997854471), tensor(1.1285097599), tensor(0.8831081986), tensor(1.0996732712)]\n",
            "b:  [tensor(1.6400573254), tensor(0.7245979309), tensor(1.4629051685), tensor(1.3031281233), tensor(1.0075907707), tensor(1.9991812706), tensor(1.2756689787), tensor(1.3964034319), tensor(1.2822655439), tensor(1.0622489452), tensor(1.3629283905), tensor(1.2884556055), tensor(0.7625578642), tensor(1.2617418766), tensor(1.2363654375), tensor(0.8958365917), tensor(1.0960514545), tensor(1.3981677294), tensor(1.3825583458), tensor(1.5500122309)]\n",
            "c:  [tensor(0.0011156952), tensor(0.0011156952), tensor(0.0011156952), tensor(0.0005822818), tensor(-1.4083579117e-05), tensor(0.0003207972), tensor(-0.0001417229), tensor(3.2500200177e-05), tensor(0.0005822818), tensor(-1.4083579117e-05), tensor(0.0003207972), tensor(-0.0001417229), tensor(3.2500200177e-05), tensor(0.0005822818), tensor(-1.4083579117e-05), tensor(0.0003207972), tensor(-0.0001417229), tensor(3.2500200177e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0222156644,  0.0898526758, -0.0522313714,  0.1089214087,\n",
            "         0.0004774332,  0.0024044514, -0.0886825323, -0.0421239734,\n",
            "        -0.0528984666,  0.0253078341, -0.0318627656, -0.0497361943,\n",
            "         0.0722304583, -0.0335191488, -0.0447559655,  0.1706357300,\n",
            "         0.0547016859, -0.0167372227, -0.0156134963,  0.0005493462])\n",
            "btensor.grad: tensor([ 0.2302891165, -0.0165381432,  0.1175099611, -0.1024104357,\n",
            "         0.0189783722,  0.1366052479, -0.0034266710,  0.0202119201,\n",
            "        -0.0112407207,  0.0015475750,  0.0403183997, -0.0105125904,\n",
            "        -0.0005192868, -0.0224830806, -0.0989609659, -0.0385188460,\n",
            "        -0.0924239159, -0.0569253266, -0.0149698257, -0.0418747663])\n",
            "ctensor.grad: tensor([-0.0918298289, -0.0918298289, -0.0918298289, -0.1107492372,\n",
            "         0.0026551192, -0.0625400171,  0.0259354785, -0.0062369891,\n",
            "        -0.1107492372,  0.0026551192, -0.0625400171,  0.0259354785,\n",
            "        -0.0062369891, -0.1107492372,  0.0026551192, -0.0625400171,\n",
            "         0.0259354785, -0.0062369891])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2584228516, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8318734765), tensor(1.7894427776), tensor(1.1104154587), tensor(1.4629051685), tensor(0.8567759991), tensor(0.7101444006), tensor(1.6931344271), tensor(1.1662360430), tensor(1.3430818319), tensor(1.0872782469), tensor(1.0208605528), tensor(1.0852211714), tensor(1.8245483637), tensor(1.2411946058), tensor(1.6419594288), tensor(1.8069584370), tensor(1.5995559692), tensor(1.1285932064), tensor(0.8831807375), tensor(1.0996811390)]\n",
            "b:  [tensor(1.6390383244), tensor(0.7246652246), tensor(1.4623836279), tensor(1.3035801649), tensor(1.0075191259), tensor(1.9985669851), tensor(1.2756972313), tensor(1.3963187933), tensor(1.2823255062), tensor(1.0622457266), tensor(1.3627551794), tensor(1.2885091305), tensor(0.7625597119), tensor(1.2618466616), tensor(1.2368057966), tensor(0.8959953189), tensor(1.0964511633), tensor(1.3984299898), tensor(1.3826334476), tensor(1.5502073765)]\n",
            "c:  [tensor(0.0011634936), tensor(0.0011634936), tensor(0.0011634936), tensor(0.0006400939), tensor(-1.5468180209e-05), tensor(0.0003535174), tensor(-0.0001552132), tensor(3.5774755816e-05), tensor(0.0006400939), tensor(-1.5468180209e-05), tensor(0.0003535174), tensor(-0.0001552132), tensor(3.5774755816e-05), tensor(0.0006400939), tensor(-1.5468180209e-05), tensor(0.0003535174), tensor(-0.0001552132), tensor(3.5774755816e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0174463987,  0.0798588842, -0.0453075767,  0.0957199335,\n",
            "         0.0004531145,  0.0026549101, -0.0812586546, -0.0371860564,\n",
            "        -0.0471408218,  0.0217329264, -0.0283091515, -0.0450051501,\n",
            "         0.0633197427, -0.0295885801, -0.0419699103,  0.1528860182,\n",
            "         0.0459056497, -0.0166969299, -0.0145067573, -0.0015618205])\n",
            "btensor.grad: tensor([ 0.2038076222, -0.0134638250,  0.1043045819, -0.0904181600,\n",
            "         0.0143334568,  0.1228501797, -0.0056604743,  0.0169179142,\n",
            "        -0.0119820833,  0.0006469488,  0.0346538126, -0.0107036829,\n",
            "        -0.0003665108, -0.0209548175, -0.0880663991, -0.0317479968,\n",
            "        -0.0799427032, -0.0524635315, -0.0150123835, -0.0390206575])\n",
            "ctensor.grad: tensor([-0.0955968797, -0.0955968797, -0.0955968797, -0.1156243160,\n",
            "         0.0027692004, -0.0654403865,  0.0269806813, -0.0065491078,\n",
            "        -0.1156243160,  0.0027692004, -0.0654403865,  0.0269806813,\n",
            "        -0.0065491078, -0.1156243160,  0.0027692004, -0.0654403865,\n",
            "         0.0269806813, -0.0065491078])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2578125000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8319422603), tensor(1.7890878916), tensor(1.1106122732), tensor(1.4624848366), tensor(0.8567745090), tensor(0.7101312876), tensor(1.6935068369), tensor(1.1664004326), tensor(1.3432921171), tensor(1.0871852636), tensor(1.0209866762), tensor(1.0854250193), tensor(1.8242709637), tensor(1.2413253784), tensor(1.6421558857), tensor(1.8062733412), tensor(1.5993641615), tensor(1.1286756992), tensor(0.8832483888), tensor(1.0996972322)]\n",
            "b:  [tensor(1.6381368637), tensor(0.7247204781), tensor(1.4619208574), tensor(1.3039796352), tensor(1.0074663162), tensor(1.9980145693), tensor(1.2757341862), tensor(1.3962484598), tensor(1.2823877335), tensor(1.0622460842), tensor(1.3626066446), tensor(1.2885630131), tensor(0.7625613809), tensor(1.2619442940), tensor(1.2371981144), tensor(0.8961263299), tensor(1.0967973471), tensor(1.3986718655), tensor(1.3827080727), tensor(1.5503891706)]\n",
            "c:  [tensor(0.0012135631), tensor(0.0012135631), tensor(0.0012135631), tensor(0.0007004443), tensor(-1.6912177671e-05), tensor(0.0003877455), tensor(-0.0001692493), tensor(3.9214610297e-05), tensor(0.0007004443), tensor(-1.6912177671e-05), tensor(0.0003877455), tensor(-0.0001692493), tensor(3.9214610297e-05), tensor(0.0007004443), tensor(-1.6912177671e-05), tensor(0.0003877455), tensor(-0.0001692493), tensor(3.9214610297e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0137582421,  0.0709814876, -0.0393530726,  0.0840746164,\n",
            "         0.0002924204,  0.0026189089, -0.0744916201, -0.0328841507,\n",
            "        -0.0420616046,  0.0185953975, -0.0252226293, -0.0407633558,\n",
            "         0.0554755330, -0.0261631012, -0.0392955840,  0.1370077133,\n",
            "         0.0383647084, -0.0164988637, -0.0135244131, -0.0032125115])\n",
            "btensor.grad: tensor([ 1.8030042946e-01, -1.1053919792e-02,  9.2542469501e-02,\n",
            "        -7.9883426428e-02,  1.0570362210e-02,  1.1047181487e-01,\n",
            "        -7.3816776276e-03,  1.4058023691e-02, -1.2443542480e-02,\n",
            "        -7.2956085205e-05,  2.9708772898e-02, -1.0781645775e-02,\n",
            "        -3.2914429903e-04, -1.9522398710e-02, -7.8457891941e-02,\n",
            "        -2.6197075844e-02, -6.9240808487e-02, -4.8365414143e-02,\n",
            "        -1.4925599098e-02, -3.6368012428e-02])\n",
            "ctensor.grad: tensor([-0.1001390666, -0.1001390666, -0.1001390666, -0.1207007468,\n",
            "         0.0028879941, -0.0684563294,  0.0280722752, -0.0068797125,\n",
            "        -0.1207007468,  0.0028879941, -0.0684563294,  0.0280722752,\n",
            "        -0.0068797125, -0.1207007468,  0.0028879941, -0.0684563294,\n",
            "         0.0280722752, -0.0068797125])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2573852539, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8319969773), tensor(1.7887724638), tensor(1.1107834578), tensor(1.4621158838), tensor(0.8567743897), tensor(0.7101194263), tensor(1.6938484907), tensor(1.1665461063), tensor(1.3434799910), tensor(1.0871061087), tensor(1.0210994482), tensor(1.0856099129), tensor(1.8240281343), tensor(1.2414413691), tensor(1.6423395872), tensor(1.8056594133), tensor(1.5992046595), tensor(1.1287566423), tensor(0.8833116889), tensor(1.0997197628)]\n",
            "b:  [tensor(1.6373397112), tensor(0.7247663140), tensor(1.4615105391), tensor(1.3043328524), tensor(1.0074286461), tensor(1.9975179434), tensor(1.2757775784), tensor(1.3961906433), tensor(1.2824512720), tensor(1.0622494221), tensor(1.3624798059), tensor(1.2886168957), tensor(0.7625632882), tensor(1.2620352507), tensor(1.2375479937), tensor(0.8962346315), tensor(1.0970976353), tensor(1.3988949060), tensor(1.3827817440), tensor(1.5505586863)]\n",
            "c:  [tensor(0.0012662647), tensor(0.0012662647), tensor(0.0012662647), tensor(0.0007634543), tensor(-1.8418410036e-05), tensor(0.0004235521), tensor(-0.0001838589), tensor(4.2830513848e-05), tensor(0.0007634543), tensor(-1.8418410036e-05), tensor(0.0004235521), tensor(-0.0001838589), tensor(4.2830513848e-05), tensor(0.0007634543), tensor(-1.8418410036e-05), tensor(0.0004235521), tensor(-0.0001838589), tensor(4.2830513848e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.0942578316e-02,  6.3088238239e-02, -3.4241229296e-02,\n",
            "         7.3799490929e-02,  2.3722648621e-05,  2.3685693741e-03,\n",
            "        -6.8326115608e-02, -2.9145926237e-02, -3.7584353238e-02,\n",
            "         1.5831649303e-02, -2.2554725409e-02, -3.6972746253e-02,\n",
            "         4.8563301563e-02, -2.3189544678e-02, -3.6748856306e-02,\n",
            "         1.2279196829e-01,  3.1898915768e-02, -1.6191720963e-02,\n",
            "        -1.2661159039e-02, -4.5053958893e-03])\n",
            "btensor.grad: tensor([ 0.1594317853, -0.0091693997,  0.0820601881, -0.0706351101,\n",
            "         0.0075272471,  0.0993172824, -0.0086889863,  0.0115698576,\n",
            "        -0.0126972198, -0.0006572008,  0.0253785849, -0.0107774734,\n",
            "        -0.0003782082, -0.0181942284, -0.0699847043, -0.0216547847,\n",
            "        -0.0600655079, -0.0446130037, -0.0147413611, -0.0339125395])\n",
            "ctensor.grad: tensor([-0.1054032296, -0.1054032296, -0.1054032296, -0.1260199249,\n",
            "         0.0030124662, -0.0716130584,  0.0292190108, -0.0072318092,\n",
            "        -0.1260199249,  0.0030124662, -0.0716130584,  0.0292190108,\n",
            "        -0.0072318092, -0.1260199249,  0.0030124662, -0.0716130584,\n",
            "         0.0292190108, -0.0072318092])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2564697266, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8320410848), tensor(1.7884922028), tensor(1.1109328270), tensor(1.4617922306), tensor(0.8567759991), tensor(0.7101095915), tensor(1.6941620111), tensor(1.1666756868), tensor(1.3436481953), tensor(1.0870391130), tensor(1.0212007761), tensor(1.0857778788), tensor(1.8238158226), tensor(1.2415444851), tensor(1.6425112486), tensor(1.8051091433), tensor(1.5990729332), tensor(1.1288356781), tensor(0.8833712935), tensor(1.0997473001)]\n",
            "b:  [tensor(1.6366353035), tensor(0.7248048186), tensor(1.4611469507), tensor(1.3046454191), tensor(1.0074033737), tensor(1.9970717430), tensor(1.2758258581), tensor(1.3961436749), tensor(1.2825151682), tensor(1.0622551441), tensor(1.3623719215), tensor(1.2886704206), tensor(0.7625657320), tensor(1.2621201277), tensor(1.2378605604), tensor(0.8963243961), tensor(1.0973587036), tensor(1.3991007805), tensor(1.3828542233), tensor(1.5507168770)]\n",
            "c:  [tensor(0.0013219385), tensor(0.0013219385), tensor(0.0013219385), tensor(0.0008292648), tensor(-1.9990169676e-05), tensor(0.0004610190), tensor(-0.0001990733), tensor(4.6634839237e-05), tensor(0.0008292648), tensor(-1.9990169676e-05), tensor(0.0004610190), tensor(-0.0001990733), tensor(4.6634839237e-05), tensor(0.0008292648), tensor(-1.9990169676e-05), tensor(0.0004610190), tensor(-0.0001990733), tensor(4.6634839237e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0088263750,  0.0560596138, -0.0298688710,  0.0647240877,\n",
            "        -0.0003218651,  0.0019619465, -0.0627104044, -0.0259091854,\n",
            "        -0.0336483121,  0.0133885741, -0.0202589929, -0.0335966349,\n",
            "         0.0424681902, -0.0206172466, -0.0343429446,  0.1100567877,\n",
            "         0.0263528228, -0.0158151388, -0.0119188428, -0.0055136979])\n",
            "btensor.grad: tensor([ 0.1408907771, -0.0077066123,  0.0727135539, -0.0625250340,\n",
            "         0.0050544739,  0.0892514884, -0.0096629858,  0.0093991756,\n",
            "        -0.0127868652, -0.0011386871,  0.0215841234, -0.0107110739,\n",
            "        -0.0004946105, -0.0169737637, -0.0625205040, -0.0179542303,\n",
            "        -0.0522046089, -0.0411831737, -0.0144943595, -0.0316481590])\n",
            "ctensor.grad: tensor([-0.1113475934, -0.1113475934, -0.1113475934, -0.1316208839,\n",
            "         0.0031435208, -0.0749338269,  0.0304290000, -0.0076086503,\n",
            "        -0.1316208839,  0.0031435208, -0.0749338269,  0.0304290000,\n",
            "        -0.0076086503, -0.1316208839,  0.0031435208, -0.0749338269,\n",
            "         0.0304290000, -0.0076086503])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2562255859, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8320774436), tensor(1.7882431746), tensor(1.1110634804), tensor(1.4615087509), tensor(0.8567796350), tensor(0.7101023793), tensor(1.6944500208), tensor(1.1667912006), tensor(1.3437991142), tensor(1.0869830847), tensor(1.0212922096), tensor(1.0859308243), tensor(1.8236303329), tensor(1.2416365147), tensor(1.6426717043), tensor(1.8046159744), tensor(1.5989649296), tensor(1.1289126873), tensor(0.8834277391), tensor(1.0997787714)]\n",
            "b:  [tensor(1.6360132694), tensor(0.7248377204), tensor(1.4608250856), tensor(1.3049225807), tensor(1.0073881149), tensor(1.9966709614), tensor(1.2758777142), tensor(1.3961062431), tensor(1.2825789452), tensor(1.0622628927), tensor(1.3622807264), tensor(1.2887234688), tensor(0.7625690699), tensor(1.2621994019), tensor(1.2381403446), tensor(0.8963991404), tensor(1.0975860357), tensor(1.3992910385), tensor(1.3829252720), tensor(1.5508646965)]\n",
            "c:  [tensor(0.0013809095), tensor(0.0013809095), tensor(0.0013809095), tensor(0.0008980351), tensor(-2.1631172785e-05), tensor(0.0005002397), tensor(-0.0002149283), tensor(5.0641465350e-05), tensor(0.0008980351), tensor(-2.1631172785e-05), tensor(0.0005002397), tensor(-0.0002149283), tensor(5.0641465350e-05), tensor(0.0008980351), tensor(-2.1631172785e-05), tensor(0.0005002397), tensor(-0.0002149283), tensor(5.0641465350e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0072720051,  0.0497951806, -0.0261375010,  0.0566971302,\n",
            "        -0.0007311106,  0.0014413595, -0.0575954914, -0.0231128931,\n",
            "        -0.0301934574,  0.0112115145, -0.0182926953, -0.0306002647,\n",
            "         0.0370880961, -0.0184075832, -0.0320821553,  0.0986324251,\n",
            "         0.0215951800, -0.0153986216, -0.0112946630, -0.0063001513])\n",
            "btensor.grad: tensor([ 0.1244142354, -0.0065835118,  0.0643640757, -0.0554253459,\n",
            "         0.0030496195,  0.0801589489, -0.0103747249,  0.0074952543,\n",
            "        -0.0127671957, -0.0015453100,  0.0182479322, -0.0106062293,\n",
            "        -0.0006617941, -0.0158664584, -0.0559471548, -0.0149527788,\n",
            "        -0.0454676151, -0.0380611420, -0.0142010450, -0.0295649767])\n",
            "ctensor.grad: tensor([-0.1179421023, -0.1179421023, -0.1179421023, -0.1375406086,\n",
            "         0.0032820066, -0.0784415454,  0.0317098312, -0.0080132540,\n",
            "        -0.1375406086,  0.0032820066, -0.0784415454,  0.0317098312,\n",
            "        -0.0080132540, -0.1375406086,  0.0032820066, -0.0784415454,\n",
            "         0.0317098312, -0.0080132540])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2557373047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8321083188), tensor(1.7880221605), tensor(1.1111783981), tensor(1.4612607956), tensor(0.8567855954), tensor(0.7100982070), tensor(1.6947147846), tensor(1.1668947935), tensor(1.3439348936), tensor(1.0869367123), tensor(1.0213752985), tensor(1.0860705376), tensor(1.8234686852), tensor(1.2417191267), tensor(1.6428215504), tensor(1.8041740656), tensor(1.5988774300), tensor(1.1289875507), tensor(0.8834816217), tensor(1.0998133421)]\n",
            "b:  [tensor(1.6354644299), tensor(0.7248663902), tensor(1.4605406523), tensor(1.3051686287), tensor(1.0073810816), tensor(1.9963113070), tensor(1.2759320736), tensor(1.3960771561), tensor(1.2826422453), tensor(1.0622724295), tensor(1.3622041941), tensor(1.2887758017), tensor(0.7625733614), tensor(1.2622737885), tensor(1.2383911610), tensor(0.8964617848), tensor(1.0977846384), tensor(1.3994671106), tensor(1.3829946518), tensor(1.5510029793)]\n",
            "c:  [tensor(0.0014434925), tensor(0.0014434925), tensor(0.0014434925), tensor(0.0009699424), tensor(-2.3345541194e-05), tensor(0.0005413188), tensor(-0.0002314626), tensor(5.4866071878e-05), tensor(0.0009699424), tensor(-2.3345541194e-05), tensor(0.0005413188), tensor(-0.0002314626), tensor(5.4866071878e-05), tensor(0.0009699424), tensor(-2.3345541194e-05), tensor(0.0005413188), tensor(-0.0002314626), tensor(5.4866071878e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0061726570,  0.0441997498, -0.0229717195,  0.0495908260,\n",
            "        -0.0011888742,  0.0008349419, -0.0529446602, -0.0207113624,\n",
            "        -0.0271663480,  0.0092676580, -0.0166252106, -0.0279507563,\n",
            "         0.0323303938, -0.0165177584, -0.0299692750,  0.0883779600,\n",
            "         0.0175063610, -0.0149669051, -0.0107815266, -0.0069143772])\n",
            "btensor.grad: tensor([ 0.1097618639, -0.0057311654,  0.0568984151, -0.0492149293,\n",
            "         0.0014166608,  0.0719285607, -0.0108822584,  0.0058165789,\n",
            "        -0.0126626492, -0.0019018650,  0.0153070688, -0.0104764700,\n",
            "        -0.0008638129, -0.0148692131, -0.0501613021, -0.0125308633,\n",
            "        -0.0397107601, -0.0352223516, -0.0138868690, -0.0276578665])\n",
            "ctensor.grad: tensor([-0.1251661181, -0.1251661181, -0.1251661181, -0.1438145936,\n",
            "         0.0034287360, -0.0821581706,  0.0330686234, -0.0084492126,\n",
            "        -0.1438145936,  0.0034287360, -0.0821581706,  0.0330686234,\n",
            "        -0.0084492126, -0.1438145936,  0.0034287360, -0.0821581706,\n",
            "         0.0330686234, -0.0084492126])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2553100586, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8321354389), tensor(1.7878261805), tensor(1.1112798452), tensor(1.4610443115), tensor(0.8567939997), tensor(0.7100973725), tensor(1.6949583292), tensor(1.1669880152), tensor(1.3440575600), tensor(1.0868991613), tensor(1.0214513540), tensor(1.0861985683), tensor(1.8233280182), tensor(1.2417937517), tensor(1.6429616213), tensor(1.8037782907), tensor(1.5988074541), tensor(1.1290602684), tensor(0.8835334778), tensor(1.0998502970)]\n",
            "b:  [tensor(1.6349807978), tensor(0.7248918414), tensor(1.4602895975), tensor(1.3053876162), tensor(1.0073806047), tensor(1.9959889650), tensor(1.2759882212), tensor(1.3960554600), tensor(1.2827048302), tensor(1.0622835159), tensor(1.3621406555), tensor(1.2888274193), tensor(0.7625788450), tensor(1.2623436451), tensor(1.2386165857), tensor(0.8965147138), tensor(1.0979585648), tensor(1.3996303082), tensor(1.3830624819), tensor(1.5511325598)]\n",
            "c:  [tensor(0.0015100006), tensor(0.0015100006), tensor(0.0015100006), tensor(0.0010451812), tensor(-2.5137793273e-05), tensor(0.0005843714), tensor(-0.0002487188), tensor(5.9325775510e-05), tensor(0.0010451812), tensor(-2.5137793273e-05), tensor(0.0005843714), tensor(-0.0002487188), tensor(5.9325775510e-05), tensor(0.0010451812), tensor(-2.5137793273e-05), tensor(0.0005843714), tensor(-0.0002487188), tensor(5.9325775510e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0054284930,  0.0391969681, -0.0202915668,  0.0432946682,\n",
            "        -0.0016829967,  0.0001709461, -0.0487132072, -0.0186542869,\n",
            "        -0.0245256014,  0.0075191855, -0.0152179748, -0.0256151333,\n",
            "         0.0281235576, -0.0149139166, -0.0280058235,  0.0791647062,\n",
            "         0.0139952302, -0.0145361423, -0.0103710294, -0.0073999465])\n",
            "btensor.grad: tensor([ 9.6725076437e-02, -5.0913393497e-03,  5.0220459700e-02,\n",
            "        -4.3793767691e-02,  8.5078179836e-05,  6.4468890429e-02,\n",
            "        -1.1224329472e-02,  4.3286532164e-03, -1.2505531311e-02,\n",
            "        -2.2202730179e-03,  1.2706100941e-02, -1.0335028172e-02,\n",
            "        -1.0941252112e-03, -1.3974636793e-02, -4.5075863600e-02,\n",
            "        -1.0589957237e-02, -3.4788846970e-02, -3.2648772001e-02,\n",
            "        -1.3565957546e-02, -2.5915622711e-02])\n",
            "ctensor.grad: tensor([-0.1330161989, -0.1330161989, -0.1330161989, -0.1504775882,\n",
            "         0.0035845027, -0.0861052126,  0.0345124081, -0.0089194067,\n",
            "        -0.1504775882,  0.0035845027, -0.0861052126,  0.0345124081,\n",
            "        -0.0089194067, -0.1504775882,  0.0035845027, -0.0861052126,\n",
            "         0.0345124081, -0.0089194067])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2551269531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8321603537), tensor(1.7876526117), tensor(1.1113700867), tensor(1.4608557224), tensor(0.8568050265), tensor(0.7101000547), tensor(1.6951826811), tensor(1.1670725346), tensor(1.3441686630), tensor(1.0868694782), tensor(1.0215215683), tensor(1.0863163471), tensor(1.8232060671), tensor(1.2418615818), tensor(1.6430926323), tensor(1.8034238815), tensor(1.5987526178), tensor(1.1291308403), tensor(0.8835837841), tensor(1.0998892784)]\n",
            "b:  [tensor(1.6345552206), tensor(0.7249149680), tensor(1.4600684643), tensor(1.3055828810), tensor(1.0073857307), tensor(1.9957004786), tensor(1.2760454416), tensor(1.3960404396), tensor(1.2827663422), tensor(1.0622961521), tensor(1.3620886803), tensor(1.2888784409), tensor(0.7625855803), tensor(1.2624095678), tensor(1.2388195992), tensor(0.8965599537), tensor(1.0981115103), tensor(1.3997819424), tensor(1.3831287622), tensor(1.5512542725)]\n",
            "c:  [tensor(0.0015807435), tensor(0.0015807435), tensor(0.0015807435), tensor(0.0011239634), tensor(-2.7012838473e-05), tensor(0.0006295237), tensor(-0.0002667428), tensor(6.4039602876e-05), tensor(0.0011239634), tensor(-2.7012838473e-05), tensor(0.0006295237), tensor(-0.0002667428), tensor(6.4039602876e-05), tensor(0.0011239634), tensor(-2.7012838473e-05), tensor(0.0006295237), tensor(-0.0002667428), tensor(6.4039602876e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0049807429,  0.0347167104, -0.0180407763,  0.0377063751,\n",
            "        -0.0022087097, -0.0005348921, -0.0448722839, -0.0169071257,\n",
            "        -0.0222251564,  0.0059401393, -0.0140438378, -0.0235661119,\n",
            "         0.0243934989, -0.0135636330, -0.0261905640,  0.0708790570,\n",
            "         0.0109706521, -0.0141244531, -0.0100562572, -0.0077879429])\n",
            "btensor.grad: tensor([ 0.0851151496, -0.0046285987,  0.0442314446, -0.0390630960,\n",
            "        -0.0010142699,  0.0576996505, -0.0114459395,  0.0030039698,\n",
            "        -0.0123138428, -0.0025168657,  0.0103969872, -0.0101948380,\n",
            "        -0.0013432726, -0.0131874681, -0.0406103432, -0.0090496540,\n",
            "        -0.0305960178, -0.0303235352, -0.0132520795, -0.0243334770])\n",
            "ctensor.grad: tensor([-0.1414858103, -0.1414858103, -0.1414858103, -0.1575643271,\n",
            "         0.0037500903, -0.0903045312,  0.0360479765, -0.0094276471,\n",
            "        -0.1575643271,  0.0037500903, -0.0903045312,  0.0360479765,\n",
            "        -0.0094276471, -0.1575643271,  0.0037500903, -0.0903045312,\n",
            "         0.0360479765, -0.0094276471])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2549438477, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8321841955), tensor(1.7874990702), tensor(1.1114509106), tensor(1.4606920481), tensor(0.8568187952), tensor(0.7101064324), tensor(1.6953896284), tensor(1.1671496630), tensor(1.3442698717), tensor(1.0868469477), tensor(1.0215868950), tensor(1.0864251852), tensor(1.8231006861), tensor(1.2419238091), tensor(1.6432151794), tensor(1.8031067848), tensor(1.5987107754), tensor(1.1291996241), tensor(0.8836329579), tensor(1.0999298096)]\n",
            "b:  [tensor(1.6341813803), tensor(0.7249364853), tensor(1.4598741531), tensor(1.3057576418), tensor(1.0073953867), tensor(1.9954427481), tensor(1.2761033773), tensor(1.3960313797), tensor(1.2828269005), tensor(1.0623100996), tensor(1.3620469570), tensor(1.2889287472), tensor(0.7625936270), tensor(1.2624720335), tensor(1.2390030622), tensor(0.8965991735), tensor(1.0982466936), tensor(1.3999230862), tensor(1.3831934929), tensor(1.5513688326)]\n",
            "c:  [tensor(0.0016560318), tensor(0.0016560318), tensor(0.0016560318), tensor(0.0012065182), tensor(-2.8975977330e-05), tensor(0.0006769125), tensor(-0.0002855838), tensor(6.9028414146e-05), tensor(0.0012065182), tensor(-2.8975977330e-05), tensor(0.0006769125), tensor(-0.0002855838), tensor(6.9028414146e-05), tensor(0.0012065182), tensor(-2.8975977330e-05), tensor(0.0006769125), tensor(-0.0002855838), tensor(6.9028414146e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0047642589,  0.0306995511, -0.0161606669,  0.0327403545,\n",
            "        -0.0027524233, -0.0012718439, -0.0413867235, -0.0154320598,\n",
            "        -0.0202312209,  0.0045003891, -0.0130760670, -0.0217789114,\n",
            "         0.0210842490, -0.0124391317, -0.0245189369,  0.0634183958,\n",
            "         0.0083662868, -0.0137448907, -0.0098320246, -0.0081061423])\n",
            "btensor.grad: tensor([ 0.0747726336, -0.0043062568,  0.0388556421, -0.0349527299,\n",
            "        -0.0019219965,  0.0515431166, -0.0115772486,  0.0018147677,\n",
            "        -0.0121086836, -0.0027970076,  0.0083380342, -0.0100607276,\n",
            "        -0.0016076006, -0.0124974549, -0.0366973579, -0.0078402162,\n",
            "        -0.0270271301, -0.0282270014, -0.0129477978, -0.0229049921])\n",
            "ctensor.grad: tensor([-0.1505767107, -0.1505767107, -0.1505767107, -0.1651095599,\n",
            "         0.0039262795, -0.0947778150,  0.0376820415, -0.0099776275,\n",
            "        -0.1651095599,  0.0039262795, -0.0947778150,  0.0376820415,\n",
            "        -0.0099776275, -0.1651095599,  0.0039262795, -0.0947778150,\n",
            "         0.0376820415, -0.0099776275])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2543945312, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8322078586), tensor(1.7873636484), tensor(1.1115239859), tensor(1.4605504274), tensor(0.8568353653), tensor(0.7101165652), tensor(1.6955807209), tensor(1.1672207117), tensor(1.3443623781), tensor(1.0868310928), tensor(1.0216484070), tensor(1.0865262747), tensor(1.8230099678), tensor(1.2419813871), tensor(1.6433300972), tensor(1.8028233051), tensor(1.5986801386), tensor(1.1292666197), tensor(0.8836814165), tensor(1.0999716520)]\n",
            "b:  [tensor(1.6338536739), tensor(0.7249569893), tensor(1.4597040415), tensor(1.3059145212), tensor(1.0074087381), tensor(1.9952130318), tensor(1.2761615515), tensor(1.3960276842), tensor(1.2828863859), tensor(1.0623254776), tensor(1.3620144129), tensor(1.2889784575), tensor(0.7626030445), tensor(1.2625315189), tensor(1.2391694784), tensor(0.8966337442), tensor(1.0983667374), tensor(1.4000548124), tensor(1.3832569122), tensor(1.5514769554)]\n",
            "c:  [tensor(0.0017361799), tensor(0.0017361799), tensor(0.0017361799), tensor(0.0012930923), tensor(-3.1032901461e-05), tensor(0.0007266863), tensor(-0.0003052945), tensor(7.4315059464e-05), tensor(0.0012930923), tensor(-3.1032901461e-05), tensor(0.0007266863), tensor(-0.0003052945), tensor(7.4315059464e-05), tensor(0.0012930923), tensor(-3.1032901461e-05), tensor(0.0007266863), tensor(-0.0003052945), tensor(7.4315059464e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0047353506,  0.0270872861, -0.0146088600,  0.0283191204,\n",
            "        -0.0033171177, -0.0020294189, -0.0382263660, -0.0141983330,\n",
            "        -0.0185108334,  0.0031778216, -0.0122947842, -0.0202289596,\n",
            "         0.0181422234, -0.0115143061, -0.0229888856,  0.0566941351,\n",
            "         0.0061157346, -0.0134006143, -0.0096937418, -0.0083754063])\n",
            "btensor.grad: tensor([ 0.0655476898, -0.0040980875,  0.0340179503, -0.0313857496,\n",
            "        -0.0026820004,  0.0459363759, -0.0116394162,  0.0007417351,\n",
            "        -0.0119035244, -0.0030746460,  0.0064974725, -0.0099441409,\n",
            "        -0.0018818490, -0.0119053423, -0.0332733989, -0.0069096684,\n",
            "        -0.0239987373, -0.0263476074, -0.0126724243, -0.0216202736])\n",
            "ctensor.grad: tensor([-0.1602961123, -0.1602961123, -0.1602961123, -0.1731482297,\n",
            "         0.0041138511, -0.0995476097,  0.0394213572, -0.0105732847,\n",
            "        -0.1731482297,  0.0041138511, -0.0995476097,  0.0394213572,\n",
            "        -0.0105732847, -0.1731482297,  0.0041138511, -0.0995476097,\n",
            "         0.0394213572, -0.0105732847])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2543945312, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8322321773), tensor(1.7872444391), tensor(1.1115906239), tensor(1.4604285955), tensor(0.8568548560), tensor(0.7101305723), tensor(1.6957575083), tensor(1.1672866344), tensor(1.3444476128), tensor(1.0868213177), tensor(1.0217068195), tensor(1.0866208076), tensor(1.8229323626), tensor(1.2420352697), tensor(1.6434381008), tensor(1.8025702238), tensor(1.5986592770), tensor(1.1293320656), tensor(0.8837295771), tensor(1.1000146866)]\n",
            "b:  [tensor(1.6335670948), tensor(0.7249768972), tensor(1.4595557451), tensor(1.3060560226), tensor(1.0074254274), tensor(1.9950089455), tensor(1.2762198448), tensor(1.3960288763), tensor(1.2829449177), tensor(1.0623422861), tensor(1.3619902134), tensor(1.2890276909), tensor(0.7626138926), tensor(1.2625885010), tensor(1.2393208742), tensor(0.8966647983), tensor(1.0984739065), tensor(1.4001781940), tensor(1.3833190203), tensor(1.5515793562)]\n",
            "c:  [tensor(0.0018215168), tensor(0.0018215168), tensor(0.0018215168), tensor(0.0013839502), tensor(-3.3189702663e-05), tensor(0.0007790047), tensor(-0.0003259308), tensor(7.9924604506e-05), tensor(0.0013839502), tensor(-3.3189702663e-05), tensor(0.0007790047), tensor(-0.0003259308), tensor(7.9924604506e-05), tensor(0.0013839502), tensor(-3.3189702663e-05), tensor(0.0007790047), tensor(-0.0003259308), tensor(7.9924604506e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0048581362,  0.0238363743, -0.0133380890,  0.0243715048,\n",
            "        -0.0038961172, -0.0028048754, -0.0353673697, -0.0131773353,\n",
            "        -0.0170376040,  0.0019581914, -0.0116759837, -0.0188953951,\n",
            "         0.0155215263, -0.0107676983, -0.0215935707,  0.0506277345,\n",
            "         0.0041638613, -0.0131008029, -0.0096347928, -0.0086177886])\n",
            "btensor.grad: tensor([ 0.0573125929, -0.0039827526,  0.0296601951, -0.0282994807,\n",
            "        -0.0033309832,  0.0408181846, -0.0116621256, -0.0002365559,\n",
            "        -0.0117079020, -0.0033508539,  0.0048400462, -0.0098490119,\n",
            "        -0.0021690913, -0.0114032328, -0.0302835107, -0.0062087178,\n",
            "        -0.0214366913, -0.0246669650, -0.0124266148, -0.0204714537])\n",
            "ctensor.grad: tensor([-0.1706737578, -0.1706737578, -0.1706737578, -0.1817160398,\n",
            "         0.0043136016, -0.1046366319,  0.0412726924, -0.0112190954,\n",
            "        -0.1817160398,  0.0043136016, -0.1046366319,  0.0412726924,\n",
            "        -0.0112190954, -0.1817160398,  0.0043136016, -0.1046366319,\n",
            "         0.0412726924, -0.0112190954])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2545166016, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8322576880), tensor(1.7871398926), tensor(1.1116522551), tensor(1.4603244066), tensor(0.8568773270), tensor(0.7101485133), tensor(1.6959214211), tensor(1.1673483849), tensor(1.3445265293), tensor(1.0868171453), tensor(1.0217628479), tensor(1.0867096186), tensor(1.8228664398), tensor(1.2420861721), tensor(1.6435397863), tensor(1.8023444414), tensor(1.5986468792), tensor(1.1293963194), tensor(0.8837778568), tensor(1.1000589132)]\n",
            "b:  [tensor(1.6333173513), tensor(0.7249966264), tensor(1.4594271183), tensor(1.3061841726), tensor(1.0074448586), tensor(1.9948282242), tensor(1.2762781382), tensor(1.3960345984), tensor(1.2830026150), tensor(1.0623604059), tensor(1.3619735241), tensor(1.2890765667), tensor(0.7626262307), tensor(1.2626434565), tensor(1.2394592762), tensor(0.8966932893), tensor(1.0985703468), tensor(1.4002940655), tensor(1.3833800554), tensor(1.5516766310)]\n",
            "c:  [tensor(0.0019123757), tensor(0.0019123757), tensor(0.0019123757), tensor(0.0014793756), tensor(-3.5452882003e-05), tensor(0.0008340396), tensor(-0.0003475524), tensor(8.5884414148e-05), tensor(0.0014793756), tensor(-3.5452882003e-05), tensor(0.0008340396), tensor(-0.0003475524), tensor(8.5884414148e-05), tensor(0.0014793756), tensor(-3.5452882003e-05), tensor(0.0008340396), tensor(-0.0003475524), tensor(8.5884414148e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0051033497,  0.0208985656, -0.0123214126,  0.0208468437,\n",
            "        -0.0044924021, -0.0035907030, -0.0327827930, -0.0123464763,\n",
            "        -0.0157821514,  0.0008252263, -0.0112022907, -0.0177555978,\n",
            "         0.0131810308, -0.0101799965, -0.0203270614,  0.0451491885,\n",
            "         0.0024711490, -0.0128520727, -0.0096504092, -0.0088459849])\n",
            "btensor.grad: tensor([ 0.0499497950, -0.0039436221,  0.0257268250, -0.0256396830,\n",
            "        -0.0038889423,  0.0361375213, -0.0116584897, -0.0011361837,\n",
            "        -0.0115320683, -0.0036301613,  0.0033432841, -0.0097794533,\n",
            "        -0.0024622679, -0.0109883547, -0.0276795030, -0.0057002902,\n",
            "        -0.0192804337, -0.0231717825, -0.0122131705, -0.0194547176])\n",
            "ctensor.grad: tensor([-0.1817177683, -0.1817177683, -0.1817177683, -0.1908507794,\n",
            "         0.0045263623, -0.1100698262,  0.0432430953, -0.0119196214,\n",
            "        -0.1908507794,  0.0045263623, -0.1100698262,  0.0432430953,\n",
            "        -0.0119196214, -0.1908507794,  0.0045263623, -0.1100698262,\n",
            "         0.0432430953, -0.0119196214])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2542724609, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8322849274), tensor(1.7870486975), tensor(1.1117098331), tensor(1.4602359533), tensor(0.8569028378), tensor(0.7101704478), tensor(1.6960736513), tensor(1.1674067974), tensor(1.3446002007), tensor(1.0868183374), tensor(1.0218172073), tensor(1.0867935419), tensor(1.8228110075), tensor(1.2421348095), tensor(1.6436357498), tensor(1.8021434546), tensor(1.5986418724), tensor(1.1294596195), tensor(0.8838265538), tensor(1.1001042128)]\n",
            "b:  [tensor(1.6331005096), tensor(0.7250164747), tensor(1.4593162537), tensor(1.3063009977), tensor(1.0074667931), tensor(1.9946689606), tensor(1.2763363123), tensor(1.3960444927), tensor(1.2830594778), tensor(1.0623799562), tensor(1.3619636297), tensor(1.2891252041), tensor(0.7626400590), tensor(1.2626967430), tensor(1.2395863533), tensor(0.8967200518), tensor(1.0986577272), tensor(1.4004033804), tensor(1.3834402561), tensor(1.5517694950)]\n",
            "c:  [tensor(0.0020091108), tensor(0.0020091108), tensor(0.0020091108), tensor(0.0015796712), tensor(-3.7829369830e-05), tensor(0.0008919750), tensor(-0.0003702222), tensor(9.2224399850e-05), tensor(0.0015796712), tensor(-3.7829369830e-05), tensor(0.0008919750), tensor(-0.0003702222), tensor(9.2224399850e-05), tensor(0.0015796712), tensor(-3.7829369830e-05), tensor(0.0008919750), tensor(-0.0003702222), tensor(9.2224399850e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0054495335,  0.0182438940, -0.0115182698,  0.0176881552,\n",
            "        -0.0051000118, -0.0043880939, -0.0304483175, -0.0116861761,\n",
            "        -0.0147253480, -0.0002391934, -0.0108649135, -0.0167953148,\n",
            "         0.0110870004, -0.0097342730, -0.0191826522,  0.0401928723,\n",
            "         0.0009899139, -0.0126516223, -0.0097334981, -0.0090692341])\n",
            "btensor.grad: tensor([ 0.0433601290, -0.0039672256,  0.0221672654, -0.0233592987,\n",
            "        -0.0043830276,  0.0318494141, -0.0116428137, -0.0019689649,\n",
            "        -0.0113787651, -0.0039193630,  0.0019805729, -0.0097374916,\n",
            "        -0.0027613230, -0.0106535554, -0.0254216194, -0.0053554773,\n",
            "        -0.0174741745, -0.0218527317, -0.0120388865, -0.0185608864])\n",
            "ctensor.grad: tensor([-0.1934700906, -0.1934700906, -0.1934700906, -0.2005909532,\n",
            "         0.0047529768, -0.1158707589,  0.0453396849, -0.0126799690,\n",
            "        -0.2005909532,  0.0047529768, -0.1158707589,  0.0453396849,\n",
            "        -0.0126799690, -0.2005909532,  0.0047529768, -0.1158707589,\n",
            "         0.0453396849, -0.0126799690])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2542114258, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8323143125), tensor(1.7869695425), tensor(1.1117644310), tensor(1.4601616859), tensor(0.8569314480), tensor(0.7101964355), tensor(1.6962153912), tensor(1.1674627066), tensor(1.3446694613), tensor(1.0868245363), tensor(1.0218703747), tensor(1.0868735313), tensor(1.8227649927), tensor(1.2421818972), tensor(1.6437264681), tensor(1.8019648790), tensor(1.5986434221), tensor(1.1295220852), tensor(0.8838759661), tensor(1.1001507044)]\n",
            "b:  [tensor(1.6329132318), tensor(0.7250366807), tensor(1.4592216015), tensor(1.3064080477), tensor(1.0074908733), tensor(1.9945293665), tensor(1.2763944864), tensor(1.3960582018), tensor(1.2831157446), tensor(1.0624010563), tensor(1.3619599342), tensor(1.2891738415), tensor(0.7626553774), tensor(1.2627487183), tensor(1.2397036552), tensor(0.8967458010), tensor(1.0987375975), tensor(1.4005068541), tensor(1.3834997416), tensor(1.5518584251)]\n",
            "c:  [tensor(0.0021120864), tensor(0.0021120864), tensor(0.0021120864), tensor(0.0016851600), tensor(-4.0326543967e-05), tensor(0.0009530087), tensor(-0.0003940072), tensor(9.8977376183e-05), tensor(0.0016851600), tensor(-4.0326543967e-05), tensor(0.0009530087), tensor(-0.0003940072), tensor(9.8977376183e-05), tensor(0.0016851600), tensor(-4.0326543967e-05), tensor(0.0009530087), tensor(-0.0003940072), tensor(9.8977376183e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0058780909,  0.0158361346, -0.0109077692,  0.0148460865,\n",
            "        -0.0057209730, -0.0051925182, -0.0283484459, -0.0111787915,\n",
            "        -0.0138446093, -0.0012439191, -0.0106442869, -0.0160047263,\n",
            "         0.0092042089, -0.0094166994, -0.0181554705,  0.0357033573,\n",
            "        -0.0003079176, -0.0125022531, -0.0098810792, -0.0092974305])\n",
            "btensor.grad: tensor([ 0.0374516584, -0.0040442348,  0.0189344585, -0.0214133859,\n",
            "        -0.0048258677,  0.0279090405, -0.0116271973, -0.0027504265,\n",
            "        -0.0112612247, -0.0042212009,  0.0007299483, -0.0097289681,\n",
            "        -0.0030683316, -0.0103957951, -0.0234714746, -0.0051444769,\n",
            "        -0.0159754753, -0.0206913650, -0.0119054317, -0.0177839994])\n",
            "ctensor.grad: tensor([-0.2059514970, -0.2059514970, -0.2059514970, -0.2109777182,\n",
            "         0.0049943486, -0.1220674738,  0.0475699455, -0.0135059468,\n",
            "        -0.2109777182,  0.0049943486, -0.1220674738,  0.0475699455,\n",
            "        -0.0135059468, -0.2109777182,  0.0049943486, -0.1220674738,\n",
            "         0.0475699455, -0.0135059468])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2536621094, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8323462009), tensor(1.7869013548), tensor(1.1118167639), tensor(1.4601002932), tensor(0.8569632173), tensor(0.7102264762), tensor(1.6963477135), tensor(1.1675167084), tensor(1.3447350264), tensor(1.0868355036), tensor(1.0219230652), tensor(1.0869503021), tensor(1.8227274418), tensor(1.2422279119), tensor(1.6438126564), tensor(1.8018066883), tensor(1.5986506939), tensor(1.1295841932), tensor(0.8839264512), tensor(1.1001983881)]\n",
            "b:  [tensor(1.6327525377), tensor(0.7250575423), tensor(1.4591416121), tensor(1.3065068722), tensor(1.0075170994), tensor(1.9944080114), tensor(1.2764525414), tensor(1.3960756063), tensor(1.2831716537), tensor(1.0624237061), tensor(1.3619620800), tensor(1.2892225981), tensor(0.7626723051), tensor(1.2627997398), tensor(1.2398126125), tensor(0.8967710137), tensor(1.0988112688), tensor(1.4006052017), tensor(1.3835588694), tensor(1.5519440174)]\n",
            "c:  [tensor(0.0022216854), tensor(0.0022216854), tensor(0.0022216854), tensor(0.0017961868), tensor(-4.2952244257e-05), tensor(0.0010173524), tensor(-0.0004189780), tensor(0.0001061792), tensor(0.0017961868), tensor(-4.2952244257e-05), tensor(0.0010173524), tensor(-0.0004189780), tensor(0.0001061792), tensor(0.0017961868), tensor(-4.2952244257e-05), tensor(0.0010173524), tensor(-0.0004189780), tensor(0.0001061792)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0063757896,  0.0136480182, -0.0104623735,  0.0122841597,\n",
            "        -0.0063586235, -0.0060079098, -0.0264636278, -0.0108059347,\n",
            "        -0.0131237209, -0.0022002459, -0.0105351806, -0.0153593048,\n",
            "         0.0075100660, -0.0092089176, -0.0172446370,  0.0316305161,\n",
            "        -0.0014495850, -0.0124137998, -0.0100924373, -0.0095386505])\n",
            "btensor.grad: tensor([ 0.0321453772, -0.0041674972,  0.0159919858, -0.0197647512,\n",
            "        -0.0052349791,  0.0242796540, -0.0116174817, -0.0034894794,\n",
            "        -0.0111774206, -0.0045347214, -0.0004234910, -0.0097509623,\n",
            "        -0.0033825114, -0.0102123916, -0.0217964053, -0.0050467253,\n",
            "        -0.0147445202, -0.0196803510, -0.0118169785, -0.0171197653])\n",
            "ctensor.grad: tensor([-0.2191980332, -0.2191980332, -0.2191980332, -0.2220536619,\n",
            "         0.0052513978, -0.1286874712,  0.0499416068, -0.0144035611,\n",
            "        -0.2220536619,  0.0052513978, -0.1286874712,  0.0499416068,\n",
            "        -0.0144035611, -0.2220536619,  0.0052513978, -0.1286874712,\n",
            "         0.0499416068, -0.0144035611])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2535400391, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8323808312), tensor(1.7868430614), tensor(1.1118675470), tensor(1.4600504637), tensor(0.8569982648), tensor(0.7102606297), tensor(1.6964715719), tensor(1.1675695181), tensor(1.3447977304), tensor(1.0868511200), tensor(1.0219756365), tensor(1.0870245695), tensor(1.8226975203), tensor(1.2422734499), tensor(1.6438947916), tensor(1.8016669750), tensor(1.5986629725), tensor(1.1296460629), tensor(0.8839782476), tensor(1.1002473831)]\n",
            "b:  [tensor(1.6326156855), tensor(0.7250791788), tensor(1.4590750933), tensor(1.3065987825), tensor(1.0075452328), tensor(1.9943033457), tensor(1.2765107155), tensor(1.3960965872), tensor(1.2832273245), tensor(1.0624480247), tensor(1.3619695902), tensor(1.2892715931), tensor(0.7626908422), tensor(1.2628502846), tensor(1.2399144173), tensor(0.8967962265), tensor(1.0988799334), tensor(1.4006992579), tensor(1.3836177588), tensor(1.5520268679)]\n",
            "c:  [tensor(0.0023383142), tensor(0.0023383142), tensor(0.0023383142), tensor(0.0019131188), tensor(-4.5714798034e-05), tensor(0.0010852328), tensor(-0.0004452093), tensor(0.0001138688), tensor(0.0019131188), tensor(-4.5714798034e-05), tensor(0.0010852328), tensor(-0.0004452093), tensor(0.0001138688), tensor(0.0019131188), tensor(-4.5714798034e-05), tensor(0.0010852328), tensor(-0.0004452093), tensor(0.0001138688)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0069313049,  0.0116507560, -0.0101667047,  0.0099693537,\n",
            "        -0.0070085526, -0.0068327188, -0.0247735977, -0.0105550289,\n",
            "        -0.0125492867, -0.0031184256, -0.0105211437, -0.0148519576,\n",
            "         0.0059795976, -0.0091061592, -0.0164372474,  0.0279328749,\n",
            "        -0.0024657845, -0.0123746991, -0.0103612542, -0.0097982883])\n",
            "btensor.grad: tensor([ 0.0273713581, -0.0043281615,  0.0133057237, -0.0183815360,\n",
            "        -0.0056208745,  0.0209276080, -0.0116261840, -0.0041948557,\n",
            "        -0.0111289024, -0.0048604012, -0.0014943480, -0.0098097324,\n",
            "        -0.0037029758, -0.0101000667, -0.0203670263, -0.0050462484,\n",
            "        -0.0137445927, -0.0188133419, -0.0117695928, -0.0165584087])\n",
            "ctensor.grad: tensor([-0.2332577258, -0.2332577258, -0.2332577258, -0.2338639498,\n",
            "         0.0055251098, -0.1357607841,  0.0524626970, -0.0153792594,\n",
            "        -0.2338639498,  0.0055251098, -0.1357607841,  0.0524626970,\n",
            "        -0.0153792594, -0.2338639498,  0.0055251098, -0.1357607841,\n",
            "         0.0524626970, -0.0153792594])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2532958984, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8324185014), tensor(1.7867939472), tensor(1.1119176149), tensor(1.4600111246), tensor(0.8570366502), tensor(0.7102990150), tensor(1.6965879202), tensor(1.1676216125), tensor(1.3448582888), tensor(1.0868711472), tensor(1.0220285654), tensor(1.0870969296), tensor(1.8226746321), tensor(1.2423188686), tensor(1.6439734697), tensor(1.8015441895), tensor(1.5986799002), tensor(1.1297080517), tensor(0.8840317130), tensor(1.1002978086)]\n",
            "b:  [tensor(1.6325004101), tensor(0.7251018286), tensor(1.4590208530), tensor(1.3066849709), tensor(1.0075751543), tensor(1.9942142963), tensor(1.2765690088), tensor(1.3961210251), tensor(1.2832829952), tensor(1.0624740124), tensor(1.3619821072), tensor(1.2893210649), tensor(0.7627109885), tensor(1.2629005909), tensor(1.2400102615), tensor(0.8968218565), tensor(1.0989446640), tensor(1.4007896185), tensor(1.3836766481), tensor(1.5521073341)]\n",
            "c:  [tensor(0.0024624062), tensor(0.0024624062), tensor(0.0024624062), tensor(0.0020363473), tensor(-4.8623056500e-05), tensor(0.0011568926), tensor(-0.0004727802), tensor(0.0001220892), tensor(0.0020363473), tensor(-4.8623056500e-05), tensor(0.0011568926), tensor(-0.0004727802), tensor(0.0001220892), tensor(0.0020363473), tensor(-4.8623056500e-05), tensor(0.0011568926), tensor(-0.0004727802), tensor(0.0001220892)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0075374842,  0.0098251551, -0.0100051165,  0.0078648329,\n",
            "        -0.0076764822, -0.0076715946, -0.0232689381, -0.0104115307,\n",
            "        -0.0121045131, -0.0040062368, -0.0105976760, -0.0144701824,\n",
            "         0.0045893192, -0.0090937614, -0.0157320797,  0.0245651305,\n",
            "        -0.0033786297, -0.0123929977, -0.0106873512, -0.0100829899])\n",
            "btensor.grad: tensor([ 0.0230635330, -0.0045269430,  0.0108437240, -0.0172351599,\n",
            "        -0.0059954226,  0.0178214908, -0.0116552114, -0.0048772246,\n",
            "        -0.0111231804, -0.0052045584, -0.0025012791, -0.0099058151,\n",
            "        -0.0040330887, -0.0100500286, -0.0191577375, -0.0051283240,\n",
            "        -0.0129499435, -0.0180730224, -0.0117690563, -0.0160998106])\n",
            "ctensor.grad: tensor([-0.2481837571, -0.2481837571, -0.2481837571, -0.2464569509,\n",
            "         0.0058165155, -0.1433196366,  0.0551416725, -0.0164408647,\n",
            "        -0.2464569509,  0.0058165155, -0.1433196366,  0.0551416725,\n",
            "        -0.0164408647, -0.2464569509,  0.0058165155, -0.1433196366,\n",
            "         0.0551416725, -0.0164408647])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2525634766, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8324594498), tensor(1.7867531776), tensor(1.1119674444), tensor(1.4599814415), tensor(0.8570784330), tensor(0.7103416324), tensor(1.6966975927), tensor(1.1676734686), tensor(1.3449171782), tensor(1.0868954659), tensor(1.0220823288), tensor(1.0871679783), tensor(1.8226580620), tensor(1.2423646450), tensor(1.6440490484), tensor(1.8014366627), tensor(1.5987008810), tensor(1.1297703981), tensor(0.8840870261), tensor(1.1003497839)]\n",
            "b:  [tensor(1.6324045658), tensor(0.7251256108), tensor(1.4589779377), tensor(1.3067665100), tensor(1.0076069832), tensor(1.9941396713), tensor(1.2766275406), tensor(1.3961486816), tensor(1.2833387852), tensor(1.0625017881), tensor(1.3619993925), tensor(1.2893712521), tensor(0.7627328634), tensor(1.2629508972), tensor(1.2401009798), tensor(0.8968482614), tensor(1.0990062952), tensor(1.4008768797), tensor(1.3837357759), tensor(1.5521860123)]\n",
            "c:  [tensor(0.0025944125), tensor(0.0025944125), tensor(0.0025944125), tensor(0.0021662894), tensor(-5.1686420193e-05), tensor(0.0012325920), tensor(-0.0005017740), tensor(0.0001308874), tensor(0.0021662894), tensor(-5.1686420193e-05), tensor(0.0012325920), tensor(-0.0005017740), tensor(0.0001308874), tensor(0.0021662894), tensor(-5.1686420193e-05), tensor(0.0012325920), tensor(-0.0005017740), tensor(0.0001308874)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0081877708,  0.0081459135, -0.0099575520,  0.0059466362,\n",
            "        -0.0083619356, -0.0085234642, -0.0219271183, -0.0103693008,\n",
            "        -0.0117747467, -0.0048707128, -0.0107571483, -0.0142042711,\n",
            "         0.0033234358, -0.0091648102, -0.0151193440,  0.0214972161,\n",
            "        -0.0042038560, -0.0124695897, -0.0110664964, -0.0103937089])\n",
            "btensor.grad: tensor([ 0.0191689506, -0.0047509968,  0.0085775852, -0.0163007379,\n",
            "        -0.0063648224,  0.0149365067, -0.0117139220, -0.0055394024,\n",
            "        -0.0111567974, -0.0055668354, -0.0034530759, -0.0100395083,\n",
            "        -0.0043707676, -0.0100623369, -0.0181452930, -0.0052814484,\n",
            "        -0.0123283863, -0.0174553096, -0.0118160844, -0.0157333612])\n",
            "ctensor.grad: tensor([-0.2640126348, -0.2640126348, -0.2640126348, -0.2598843873,\n",
            "         0.0061267288, -0.1513988078,  0.0579875559, -0.0175963957,\n",
            "        -0.2598843873,  0.0061267288, -0.1513988078,  0.0579875559,\n",
            "        -0.0175963957, -0.2598843873,  0.0061267288, -0.1513988078,\n",
            "         0.0579875559, -0.0175963957])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2523193359, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8325038552), tensor(1.7867201567), tensor(1.1120175123), tensor(1.4599604607), tensor(0.8571237922), tensor(0.7103886008), tensor(1.6968013048), tensor(1.1677255630), tensor(1.3449749947), tensor(1.0869240761), tensor(1.0221372843), tensor(1.0872381926), tensor(1.8226472139), tensor(1.2424112558), tensor(1.6441220045), tensor(1.8013432026), tensor(1.5987256765), tensor(1.1298334599), tensor(0.8841445446), tensor(1.1004034281)]\n",
            "b:  [tensor(1.6323263645), tensor(0.7251506448), tensor(1.4589455128), tensor(1.3068442345), tensor(1.0076406002), tensor(1.9940783978), tensor(1.2766865492), tensor(1.3961796761), tensor(1.2833949327), tensor(1.0625315905), tensor(1.3620212078), tensor(1.2894222736), tensor(0.7627564669), tensor(1.2630015612), tensor(1.2401875257), tensor(0.8968757391), tensor(1.0990656614), tensor(1.4009616375), tensor(1.3837953806), tensor(1.5522632599)]\n",
            "c:  [tensor(0.0027348201), tensor(0.0027348201), tensor(0.0027348201), tensor(0.0023033898), tensor(-5.4914875363e-05), tensor(0.0013126094), tensor(-0.0005322787), tensor(0.0001403149), tensor(0.0023033898), tensor(-5.4914875363e-05), tensor(0.0013126094), tensor(-0.0005322787), tensor(0.0001403149), tensor(0.0023033898), tensor(-5.4914875363e-05), tensor(0.0013126094), tensor(-0.0005322787), tensor(0.0001403149)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0088785291,  0.0065993220, -0.0100146532,  0.0041886568,\n",
            "        -0.0090681314, -0.0093953609, -0.0207450390, -0.0104158819,\n",
            "        -0.0115558077, -0.0057174563, -0.0109929889, -0.0140493363,\n",
            "         0.0021643639, -0.0093129873, -0.0145969838,  0.0186917707,\n",
            "        -0.0049595237, -0.0126028657, -0.0115001798, -0.0107377768])\n",
            "btensor.grad: tensor([ 0.0156331994, -0.0050056875,  0.0064811707, -0.0155517161,\n",
            "        -0.0067323670,  0.0122446716, -0.0118016005, -0.0061933547,\n",
            "        -0.0112344027, -0.0059490204, -0.0043631494, -0.0102127790,\n",
            "        -0.0047199763, -0.0101384819, -0.0173150599, -0.0054961443,\n",
            "        -0.0118694305, -0.0169516504, -0.0119096637, -0.0154600143])\n",
            "ctensor.grad: tensor([-0.2808153033, -0.2808153033, -0.2808153033, -0.2742009461,\n",
            "         0.0064569092, -0.1600348055,  0.0610095859, -0.0188550055,\n",
            "        -0.2742009461,  0.0064569092, -0.1600348055,  0.0610095859,\n",
            "        -0.0188550055, -0.2742009461,  0.0064569092, -0.1600348055,\n",
            "         0.0610095859, -0.0188550055])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2526245117, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8325518966), tensor(1.7866942883), tensor(1.1120684147), tensor(1.4599475861), tensor(0.8571727872), tensor(0.7104400396), tensor(1.6968997717), tensor(1.1677782536), tensor(1.3450322151), tensor(1.0869568586), tensor(1.0221937895), tensor(1.0873081684), tensor(1.8226417303), tensor(1.2424589396), tensor(1.6441928148), tensor(1.8012626171), tensor(1.5987539291), tensor(1.1298973560), tensor(0.8842044473), tensor(1.1004589796)]\n",
            "b:  [tensor(1.6322642565), tensor(0.7251771092), tensor(1.4589228630), tensor(1.3069190979), tensor(1.0076761246), tensor(1.9940297604), tensor(1.2767461538), tensor(1.3962138891), tensor(1.2834516764), tensor(1.0625633001), tensor(1.3620474339), tensor(1.2894743681), tensor(0.7627818584), tensor(1.2630529404), tensor(1.2402707338), tensor(0.8969045877), tensor(1.0991233587), tensor(1.4010443687), tensor(1.3838557005), tensor(1.5523396730)]\n",
            "c:  [tensor(0.0028841398), tensor(0.0028841398), tensor(0.0028841398), tensor(0.0024481229), tensor(-5.8319026721e-05), tensor(0.0013972438), tensor(-0.0005643876), tensor(0.0001504284), tensor(0.0024481229), tensor(-5.8319026721e-05), tensor(0.0013972438), tensor(-0.0005643876), tensor(0.0001504284), tensor(0.0024481229), tensor(-5.8319026721e-05), tensor(0.0013972438), tensor(-0.0005643876), tensor(0.0001504284)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0096102953,  0.0051662326, -0.0101695061,  0.0025681257,\n",
            "        -0.0097947121, -0.0102871656, -0.0197050571, -0.0105468631,\n",
            "        -0.0114340894, -0.0065570176, -0.0112988204, -0.0139933303,\n",
            "         0.0010941625, -0.0095309019, -0.0141618103,  0.0161226131,\n",
            "        -0.0056611300, -0.0127895474, -0.0119848847, -0.0111182928])\n",
            "btensor.grad: tensor([ 0.0124124177, -0.0052874386,  0.0045371652, -0.0149787664,\n",
            "        -0.0071102008,  0.0097214282, -0.0119259953, -0.0068415999,\n",
            "        -0.0113589764, -0.0063457489, -0.0052390695, -0.0104241967,\n",
            "        -0.0050805844, -0.0102726519, -0.0166464746, -0.0057646036,\n",
            "        -0.0115478039, -0.0165546536, -0.0120546818, -0.0152734518])\n",
            "ctensor.grad: tensor([-0.2986392379, -0.2986392379, -0.2986392379, -0.2894660532,\n",
            "         0.0068083042, -0.1692687571,  0.0642176643, -0.0202268623,\n",
            "        -0.2894660532,  0.0068083042, -0.1692687571,  0.0642176643,\n",
            "        -0.0202268623, -0.2894660532,  0.0068083042, -0.1692687571,\n",
            "         0.0642176643, -0.0202268623])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2520141602, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8326038122), tensor(1.7866750956), tensor(1.1121203899), tensor(1.4599422216), tensor(0.8572255373), tensor(0.7104960680), tensor(1.6969938278), tensor(1.1678320169), tensor(1.3450891972), tensor(1.0869938135), tensor(1.0222522020), tensor(1.0873783827), tensor(1.8226412535), tensor(1.2425080538), tensor(1.6442618370), tensor(1.8011938334), tensor(1.5987855196), tensor(1.1299625635), tensor(0.8842670918), tensor(1.1005166769)]\n",
            "b:  [tensor(1.6322169304), tensor(0.7252050638), tensor(1.4589092731), tensor(1.3069919348), tensor(1.0077136755), tensor(1.9939930439), tensor(1.2768065929), tensor(1.3962513208), tensor(1.2835093737), tensor(1.0625971556), tensor(1.3620778322), tensor(1.2895277739), tensor(0.7628091574), tensor(1.2631052732), tensor(1.2403513193), tensor(0.8969349861), tensor(1.0991801023), tensor(1.4011256695), tensor(1.3839169741), tensor(1.5524154902)]\n",
            "c:  [tensor(0.0030429245), tensor(0.0030429245), tensor(0.0030429245), tensor(0.0026009942), tensor(-6.1910148361e-05), tensor(0.0014868152), tensor(-0.0005981987), tensor(0.0001612899), tensor(0.0026009942), tensor(-6.1910148361e-05), tensor(0.0014868152), tensor(-0.0005981987), tensor(0.0001612899), tensor(0.0026009942), tensor(-6.1910148361e-05), tensor(0.0014868152), tensor(-0.0005981987), tensor(0.0001612899)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0103780031,  0.0038356334, -0.0104054809,  0.0010688305,\n",
            "        -0.0105454922, -0.0112005472, -0.0187993050, -0.0107556880,\n",
            "        -0.0114047304, -0.0073907971, -0.0116734207, -0.0140317231,\n",
            "         0.0001040101, -0.0098153353, -0.0138067007,  0.0137642808,\n",
            "        -0.0063176155, -0.0130371451, -0.0125247240, -0.0115366578])\n",
            "btensor.grad: tensor([ 0.0094689466, -0.0055963695,  0.0027225912, -0.0145566761,\n",
            "        -0.0075000376,  0.0073491037, -0.0120869875, -0.0074900687,\n",
            "        -0.0115278959, -0.0067681074, -0.0060890913, -0.0106756687,\n",
            "        -0.0054547712, -0.0104607940, -0.0161272287, -0.0060826540,\n",
            "        -0.0113544464, -0.0162618458, -0.0122448802, -0.0151684284])\n",
            "ctensor.grad: tensor([-0.3175692558, -0.3175692558, -0.3175692558, -0.3057425320,\n",
            "         0.0071822363, -0.1791427881,  0.0676221177, -0.0217231680,\n",
            "        -0.3057425320,  0.0071822363, -0.1791427881,  0.0676221177,\n",
            "        -0.0217231680, -0.3057425320,  0.0071822363, -0.1791427881,\n",
            "         0.0676221177, -0.0217231680])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2512817383, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8326597214), tensor(1.7866621017), tensor(1.1121740341), tensor(1.4599438906), tensor(0.8572821617), tensor(0.7105567455), tensor(1.6970839500), tensor(1.1678872108), tensor(1.3451465368), tensor(1.0870349407), tensor(1.0223127604), tensor(1.0874491930), tensor(1.8226453066), tensor(1.2425588369), tensor(1.6443294287), tensor(1.8011358976), tensor(1.5988202095), tensor(1.1300292015), tensor(0.8843326569), tensor(1.1005766392)]\n",
            "b:  [tensor(1.6321830750), tensor(0.7252346873), tensor(1.4589041471), tensor(1.3070633411), tensor(1.0077531338), tensor(1.9939675331), tensor(1.2768679857), tensor(1.3962920904), tensor(1.2835680246), tensor(1.0626331568), tensor(1.3621124029), tensor(1.2895826101), tensor(0.7628383636), tensor(1.2631587982), tensor(1.2404299974), tensor(0.8969672322), tensor(1.0992364883), tensor(1.4012060165), tensor(1.3839794397), tensor(1.5524911880)]\n",
            "c:  [tensor(0.0032117607), tensor(0.0032117607), tensor(0.0032117607), tensor(0.0027625435), tensor(-6.5700216510e-05), tensor(0.0015816675), tensor(-0.0006338159), tensor(0.0001729679), tensor(0.0027625435), tensor(-6.5700216510e-05), tensor(0.0015816675), tensor(-0.0006338159), tensor(0.0001729679), tensor(0.0027625435), tensor(-6.5700216510e-05), tensor(0.0015816675), tensor(-0.0006338159), tensor(0.0001729679)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0111808777,  0.0025913417, -0.0107238889, -0.0003322363,\n",
            "        -0.0113219023, -0.0121399164, -0.0180162191, -0.0110342205,\n",
            "        -0.0114594083, -0.0082247853, -0.0121115297, -0.0141583011,\n",
            "        -0.0008149743, -0.0101593733, -0.0135278255,  0.0115917698,\n",
            "        -0.0069409013, -0.0133357644, -0.0131112933, -0.0119894743])\n",
            "btensor.grad: tensor([ 0.0067677945, -0.0059292912,  0.0010169446, -0.0142733157,\n",
            "        -0.0079011470,  0.0051116645, -0.0122880936, -0.0081442595,\n",
            "        -0.0117404461, -0.0072093010, -0.0069243014, -0.0109658241,\n",
            "        -0.0058410801, -0.0107024312, -0.0157425106, -0.0064435601,\n",
            "        -0.0112698078, -0.0160621107, -0.0124867558, -0.0151420832])\n",
            "ctensor.grad: tensor([-0.3376723528, -0.3376723528, -0.3376723528, -0.3230985105,\n",
            "         0.0075801341, -0.1897045821,  0.0712343678, -0.0233559385,\n",
            "        -0.3230985105,  0.0075801341, -0.1897045821,  0.0712343678,\n",
            "        -0.0233559385, -0.3230985105,  0.0075801341, -0.1897045821,\n",
            "         0.0712343678, -0.0233559385])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2509765625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8327198625), tensor(1.7866549492), tensor(1.1122295856), tensor(1.4599521160), tensor(0.8573427796), tensor(0.7106223106), tensor(1.6971707344), tensor(1.1679440737), tensor(1.3452044725), tensor(1.0870802402), tensor(1.0223758221), tensor(1.0875210762), tensor(1.8226536512), tensor(1.2426116467), tensor(1.6443960667), tensor(1.8010879755), tensor(1.5988578796), tensor(1.1300976276), tensor(0.8844013810), tensor(1.1006391048)]\n",
            "b:  [tensor(1.6321617365), tensor(0.7252660990), tensor(1.4589071274), tensor(1.3071339130), tensor(1.0077947378), tensor(1.9939526320), tensor(1.2769306898), tensor(1.3963360786), tensor(1.2836281061), tensor(1.0626715422), tensor(1.3621511459), tensor(1.2896391153), tensor(0.7628695965), tensor(1.2632137537), tensor(1.2405073643), tensor(0.8970014453), tensor(1.0992928743), tensor(1.4012857676), tensor(1.3840433359), tensor(1.5525671244)]\n",
            "c:  [tensor(0.0033912733), tensor(0.0033912733), tensor(0.0033912733), tensor(0.0029333476), tensor(-6.9701971370e-05), tensor(0.0016821699), tensor(-0.0006713488), tensor(0.0001855376), tensor(0.0029333476), tensor(-6.9701971370e-05), tensor(0.0016821699), tensor(-0.0006713488), tensor(0.0001855376), tensor(0.0029333476), tensor(-6.9701971370e-05), tensor(0.0016821699), tensor(-0.0006713488), tensor(0.0001855376)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0120235085,  0.0014252067, -0.0111129284, -0.0016442537,\n",
            "        -0.0121272802, -0.0131140947, -0.0173481703, -0.0113812983,\n",
            "        -0.0115926098, -0.0090609491, -0.0126124918, -0.0143722370,\n",
            "        -0.0016790628, -0.0105644464, -0.0133237243,  0.0095857829,\n",
            "        -0.0075394511, -0.0136949420, -0.0137506723, -0.0124853849])\n",
            "btensor.grad: tensor([ 0.0042740181, -0.0062880516, -0.0005941689, -0.0141179562,\n",
            "        -0.0083239153,  0.0029881597, -0.0125312805, -0.0088054836,\n",
            "        -0.0120050907, -0.0076767206, -0.0077511370, -0.0112988353,\n",
            "        -0.0062461197, -0.0109966397, -0.0154817998, -0.0068480968,\n",
            "        -0.0112829208, -0.0159536600, -0.0127778649, -0.0151958466])\n",
            "ctensor.grad: tensor([-0.3590250611, -0.3590250611, -0.3590250611, -0.3416080773,\n",
            "         0.0080035133, -0.2010047585,  0.0750657246, -0.0251393765,\n",
            "        -0.3416080773,  0.0080035133, -0.2010047585,  0.0750657246,\n",
            "        -0.0251393765, -0.3416080773,  0.0080035133, -0.2010047585,\n",
            "         0.0750657246, -0.0251393765])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2503662109, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8327843547), tensor(1.7866533995), tensor(1.1122874022), tensor(1.4599665403), tensor(0.8574075699), tensor(0.7106928825), tensor(1.6972546577), tensor(1.1680030823), tensor(1.3452634811), tensor(1.0871298313), tensor(1.0224417448), tensor(1.0875943899), tensor(1.8226661682), tensor(1.2426667213), tensor(1.6444619894), tensor(1.8010493517), tensor(1.5988985300), tensor(1.1301681995), tensor(0.8844736218), tensor(1.1007041931)]\n",
            "b:  [tensor(1.6321519613), tensor(0.7252994776), tensor(1.4589177370), tensor(1.3072043657), tensor(1.0078386068), tensor(1.9939478636), tensor(1.2769948244), tensor(1.3963835239), tensor(1.2836896181), tensor(1.0627124310), tensor(1.3621940613), tensor(1.2896975279), tensor(0.7629029155), tensor(1.2632704973), tensor(1.2405840158), tensor(0.8970379233), tensor(1.0993498564), tensor(1.4013653994), tensor(1.3841089010), tensor(1.5526437759)]\n",
            "c:  [tensor(0.0035821360), tensor(0.0035821360), tensor(0.0035821360), tensor(0.0031140223), tensor(-7.3928975326e-05), tensor(0.0017887192), tensor(-0.0007109132), tensor(0.0001990819), tensor(0.0031140223), tensor(-7.3928975326e-05), tensor(0.0017887192), tensor(-0.0007109132), tensor(0.0001990819), tensor(0.0031140223), tensor(-7.3928975326e-05), tensor(0.0017887192), tensor(-0.0007109132), tensor(0.0001990819)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0129020810,  0.0003205687, -0.0115695000, -0.0028812885,\n",
            "        -0.0129630566, -0.0141184330, -0.0167882442, -0.0117905140,\n",
            "        -0.0117991026, -0.0099103153, -0.0131734163, -0.0146620721,\n",
            "        -0.0024953485, -0.0110236406, -0.0131878853,  0.0077277087,\n",
            "        -0.0081241727, -0.0141115785, -0.0144434571, -0.0130229592])\n",
            "btensor.grad: tensor([ 0.0019631274, -0.0066738129, -0.0021252334, -0.0140790939,\n",
            "        -0.0087666214,  0.0009636581, -0.0128154755, -0.0094843060,\n",
            "        -0.0123096704, -0.0081664324, -0.0085730553, -0.0116754770,\n",
            "        -0.0066656694, -0.0113459229, -0.0153363049, -0.0072904229,\n",
            "        -0.0113947392, -0.0159316957, -0.0131197572, -0.0153223276])\n",
            "ctensor.grad: tensor([-0.3817254305, -0.3817254305, -0.3817254305, -0.3613491952,\n",
            "         0.0084540099, -0.2130986601,  0.0791287422, -0.0270886943,\n",
            "        -0.3613491952,  0.0084540099, -0.2130986601,  0.0791287422,\n",
            "        -0.0270886943, -0.3613491952,  0.0084540099, -0.2130986601,\n",
            "         0.0791287422, -0.0270886943])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2505493164, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8328534365), tensor(1.7866569757), tensor(1.1123478413), tensor(1.4599868059), tensor(0.8574767709), tensor(0.7107686996), tensor(1.6973363161), tensor(1.1680643559), tensor(1.3453238010), tensor(1.0871837139), tensor(1.0225106478), tensor(1.0876694918), tensor(1.8226824999), tensor(1.2427244186), tensor(1.6445275545), tensor(1.8010193110), tensor(1.5989420414), tensor(1.1302411556), tensor(0.8845495582), tensor(1.1007722616)]\n",
            "b:  [tensor(1.6321529150), tensor(0.7253348827), tensor(1.4589356184), tensor(1.3072750568), tensor(1.0078847408), tensor(1.9939527512), tensor(1.2770605087), tensor(1.3964344263), tensor(1.2837529182), tensor(1.0627558231), tensor(1.3622410297), tensor(1.2897579670), tensor(0.7629384398), tensor(1.2633292675), tensor(1.2406605482), tensor(0.8970767856), tensor(1.0994077921), tensor(1.4014453888), tensor(1.3841764927), tensor(1.5527213812)]\n",
            "c:  [tensor(0.0037850658), tensor(0.0037850658), tensor(0.0037850658), tensor(0.0033052263), tensor(-7.8395656601e-05), tensor(0.0019017424), tensor(-0.0007526315), tensor(0.0002136924), tensor(0.0033052263), tensor(-7.8395656601e-05), tensor(0.0019017424), tensor(-0.0007526315), tensor(0.0002136924), tensor(0.0033052263), tensor(-7.8395656601e-05), tensor(0.0019017424), tensor(-0.0007526315), tensor(0.0002136924)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0138205290, -0.0007219017, -0.0120898485, -0.0040581226,\n",
            "        -0.0138348341, -0.0151597261, -0.0163283348, -0.0122577846,\n",
            "        -0.0120751560, -0.0107711256, -0.0137920678, -0.0150319636,\n",
            "        -0.0032677650, -0.0115383863, -0.0131188035,  0.0059989318,\n",
            "        -0.0086967349, -0.0145802498, -0.0151873231, -0.0136068463])\n",
            "btensor.grad: tensor([-0.0001922995, -0.0070827603, -0.0035879612, -0.0141478479,\n",
            "        -0.0092348829, -0.0009750426, -0.0131427050, -0.0101766586,\n",
            "        -0.0126658678, -0.0086793900, -0.0093970299, -0.0120933652,\n",
            "        -0.0071074739, -0.0117430091, -0.0152947903, -0.0077711940,\n",
            "        -0.0115873814, -0.0159927011, -0.0135164261, -0.0155190229])\n",
            "ctensor.grad: tensor([-0.4058597684, -0.4058597684, -0.4058597684, -0.3824082315,\n",
            "         0.0089333681, -0.2260463834,  0.0834366232, -0.0292210001,\n",
            "        -0.3824082315,  0.0089333681, -0.2260463834,  0.0834366232,\n",
            "        -0.0292210001, -0.3824082315,  0.0089333681, -0.2260463834,\n",
            "         0.0834366232, -0.0292210001])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2498779297, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8329273462), tensor(1.7866655588), tensor(1.1124111414), tensor(1.4600126743), tensor(0.8575504422), tensor(0.7108498812), tensor(1.6974161863), tensor(1.1681282520), tensor(1.3453859091), tensor(1.0872420073), tensor(1.0225830078), tensor(1.0877468586), tensor(1.8227025270), tensor(1.2427849770), tensor(1.6445931196), tensor(1.8009973764), tensor(1.5989884138), tensor(1.1303167343), tensor(0.8846294880), tensor(1.1008434296)]\n",
            "b:  [tensor(1.6321640015), tensor(0.7253724933), tensor(1.4589606524), tensor(1.3073467016), tensor(1.0079333782), tensor(1.9939669371), tensor(1.2771281004), tensor(1.3964889050), tensor(1.2838182449), tensor(1.0628019571), tensor(1.3622921705), tensor(1.2898207903), tensor(0.7629762888), tensor(1.2633901834), tensor(1.2407373190), tensor(0.8971182108), tensor(1.0994670391), tensor(1.4015260935), tensor(1.3842463493), tensor(1.5528002977)]\n",
            "c:  [tensor(0.0040008351), tensor(0.0040008351), tensor(0.0040008351), tensor(0.0035076651), tensor(-8.3117389295e-05), tensor(0.0020216997), tensor(-0.0007966331), tensor(0.0002294701), tensor(0.0035076651), tensor(-8.3117389295e-05), tensor(0.0020216997), tensor(-0.0007966331), tensor(0.0002294701), tensor(0.0035076651), tensor(-8.3117389295e-05), tensor(0.0020216997), tensor(-0.0007966331), tensor(0.0002294701)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0147812963, -0.0017198473, -0.0126708746, -0.0051829815,\n",
            "        -0.0147355795, -0.0162409544, -0.0159636736, -0.0127853751,\n",
            "        -0.0124187842, -0.0116485059, -0.0144741833, -0.0154773816,\n",
            "        -0.0040064454, -0.0121040344, -0.0131144673,  0.0043853819,\n",
            "        -0.0092701316, -0.0151062608, -0.0159852505, -0.0142362118])\n",
            "btensor.grad: tensor([-0.0022165366, -0.0075161755, -0.0050007999, -0.0143192410,\n",
            "        -0.0097306147, -0.0028436482, -0.0135176182, -0.0108919889,\n",
            "        -0.0130672455, -0.0092204809, -0.0102295280, -0.0125555396,\n",
            "        -0.0075700209, -0.0121901035, -0.0153528452, -0.0082885027,\n",
            "        -0.0118608475, -0.0161325037, -0.0139610171, -0.0157872438])\n",
            "ctensor.grad: tensor([-0.4315385818, -0.4315385818, -0.4315385818, -0.4048775136,\n",
            "         0.0094434703, -0.2399144173,  0.0880033746, -0.0315552540,\n",
            "        -0.4048775136,  0.0094434703, -0.2399144173,  0.0880033746,\n",
            "        -0.0315552540, -0.4048775136,  0.0094434703, -0.2399144173,\n",
            "         0.0880033746, -0.0315552540])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2491455078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8330062628), tensor(1.7866789103), tensor(1.1124776602), tensor(1.4600440264), tensor(0.8576288223), tensor(0.7109367251), tensor(1.6974946260), tensor(1.1681951284), tensor(1.3454500437), tensor(1.0873047113), tensor(1.0226590633), tensor(1.0878268480), tensor(1.8227261305), tensor(1.2428486347), tensor(1.6446589231), tensor(1.8009829521), tensor(1.5990376472), tensor(1.1303951740), tensor(0.8847137094), tensor(1.1009180546)]\n",
            "b:  [tensor(1.6321846247), tensor(0.7254123688), tensor(1.4589924812), tensor(1.3074196577), tensor(1.0079846382), tensor(1.9939901829), tensor(1.2771978378), tensor(1.3965470791), tensor(1.2838858366), tensor(1.0628509521), tensor(1.3623476028), tensor(1.2898861170), tensor(0.7630165815), tensor(1.2634536028), tensor(1.2408148050), tensor(0.8971624374), tensor(1.0995280743), tensor(1.4016078711), tensor(1.3843185902), tensor(1.5528808832)]\n",
            "c:  [tensor(0.0042302716), tensor(0.0042302716), tensor(0.0042302716), tensor(0.0037220940), tensor(-8.8110558863e-05), tensor(0.0021490860), tensor(-0.0008430550), tensor(0.0002465266), tensor(0.0037220940), tensor(-8.8110558863e-05), tensor(0.0021490860), tensor(-0.0008430550), tensor(0.0002465266), tensor(0.0037220940), tensor(-8.8110558863e-05), tensor(0.0021490860), tensor(-0.0008430550), tensor(0.0002465266)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0157845020, -0.0026758462, -0.0133090019, -0.0062704086,\n",
            "        -0.0156799555, -0.0173678398, -0.0156847239, -0.0133696198,\n",
            "        -0.0128216408, -0.0125488043, -0.0152106136, -0.0159910917,\n",
            "        -0.0047166944, -0.0127218962, -0.0131703764,  0.0028745905,\n",
            "        -0.0098457932, -0.0156936049, -0.0168389082, -0.0149165392])\n",
            "btensor.grad: tensor([-0.0041302890, -0.0079806745, -0.0063661635, -0.0145810843,\n",
            "        -0.0102540702, -0.0046527684, -0.0139387846, -0.0116320103,\n",
            "        -0.0135202408, -0.0097870827, -0.0110769272, -0.0130619407,\n",
            "        -0.0080548897, -0.0126887560, -0.0155048072, -0.0088413358,\n",
            "        -0.0122077465, -0.0163445771, -0.0144582987, -0.0161241293])\n",
            "ctensor.grad: tensor([-0.4588724375, -0.4588724375, -0.4588724375, -0.4288576245,\n",
            "         0.0099863438, -0.2547727525,  0.0928436071, -0.0341130644,\n",
            "        -0.4288576245,  0.0099863438, -0.2547727525,  0.0928436071,\n",
            "        -0.0341130644, -0.4288576245,  0.0099863438, -0.2547727525,\n",
            "         0.0928436071, -0.0341130644])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2483520508, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8330904245), tensor(1.7866969109), tensor(1.1125476360), tensor(1.4600806236), tensor(0.8577121496), tensor(0.7110294104), tensor(1.6975721121), tensor(1.1682651043), tensor(1.3455164433), tensor(1.0873720646), tensor(1.0227390528), tensor(1.0879096985), tensor(1.8227531910), tensor(1.2429155111), tensor(1.6447253227), tensor(1.8009756804), tensor(1.5990897417), tensor(1.1304768324), tensor(0.8848024607), tensor(1.1009962559)]\n",
            "b:  [tensor(1.6322143078), tensor(0.7254547477), tensor(1.4590309858), tensor(1.3074942827), tensor(1.0080386400), tensor(1.9940222502), tensor(1.2772698402), tensor(1.3966090679), tensor(1.2839559317), tensor(1.0629029274), tensor(1.3624073267), tensor(1.2899541855), tensor(0.7630593777), tensor(1.2635197639), tensor(1.2408934832), tensor(0.8972095847), tensor(1.0995912552), tensor(1.4016910791), tensor(1.3843936920), tensor(1.5529634953)]\n",
            "c:  [tensor(0.0044742650), tensor(0.0044742650), tensor(0.0044742650), tensor(0.0039493227), tensor(-9.3392642157e-05), tensor(0.0022844367), tensor(-0.0008920417), tensor(0.0002649851), tensor(0.0039493227), tensor(-9.3392642157e-05), tensor(0.0022844367), tensor(-0.0008920417), tensor(0.0002649851), tensor(0.0039493227), tensor(-9.3392642157e-05), tensor(0.0022844367), tensor(-0.0008920417), tensor(0.0002649851)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0168336034, -0.0035969913, -0.0140035748, -0.0073249340,\n",
            "        -0.0166636705, -0.0185394287, -0.0154912472, -0.0140059292,\n",
            "        -0.0132849440, -0.0134713352, -0.0160071403, -0.0165772587,\n",
            "        -0.0054033399, -0.0133867264, -0.0132863969,  0.0014521405,\n",
            "        -0.0104274154, -0.0163384676, -0.0177475214, -0.0156444609])\n",
            "btensor.grad: tensor([-0.0059475414, -0.0084741414, -0.0076992512, -0.0149311423,\n",
            "        -0.0108088702, -0.0064090490, -0.0144076943, -0.0123981535,\n",
            "        -0.0140203238, -0.0103836060, -0.0119401813, -0.0136143565,\n",
            "        -0.0085613988, -0.0132359564, -0.0157455504, -0.0094323754,\n",
            "        -0.0126249790, -0.0166324377, -0.0150110722, -0.0165272951])\n",
            "ctensor.grad: tensor([-0.4879866838, -0.4879866838, -0.4879866838, -0.4544569850,\n",
            "         0.0105641708, -0.2707012594,  0.0979734212, -0.0369171314,\n",
            "        -0.4544569850,  0.0105641708, -0.2707012594,  0.0979734212,\n",
            "        -0.0369171314, -0.4544569850,  0.0105641708, -0.2707012594,\n",
            "         0.0979734212, -0.0369171314])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2473144531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8331800699), tensor(1.7867193222), tensor(1.1126214266), tensor(1.4601223469), tensor(0.8578006029), tensor(0.7111282349), tensor(1.6976490021), tensor(1.1683386564), tensor(1.3455854654), tensor(1.0874441862), tensor(1.0228233337), tensor(1.0879958868), tensor(1.8227835894), tensor(1.2429860830), tensor(1.6447925568), tensor(1.8009750843), tensor(1.5991448164), tensor(1.1305620670), tensor(0.8848960400), tensor(1.1010783911)]\n",
            "b:  [tensor(1.6322528124), tensor(0.7254997492), tensor(1.4590760469), tensor(1.3075711727), tensor(1.0080956221), tensor(1.9940629005), tensor(1.2773444653), tensor(1.3966749907), tensor(1.2840287685), tensor(1.0629580021), tensor(1.3624714613), tensor(1.2900252342), tensor(0.7631048560), tensor(1.2635889053), tensor(1.2409738302), tensor(0.8972598910), tensor(1.0996568203), tensor(1.4017760754), tensor(1.3844717741), tensor(1.5530484915)]\n",
            "c:  [tensor(0.0047337692), tensor(0.0047337692), tensor(0.0047337692), tensor(0.0041902200), tensor(-9.8982294730e-05), tensor(0.0024283291), tensor(-0.0009437464), tensor(0.0002849828), tensor(0.0041902200), tensor(-9.8982294730e-05), tensor(0.0024283291), tensor(-0.0009437464), tensor(0.0002849828), tensor(0.0041902200), tensor(-9.8982294730e-05), tensor(0.0024283291), tensor(-0.0009437464), tensor(0.0002849828)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0179304481, -0.0044925362, -0.0147519112, -0.0083502531,\n",
            "        -0.0176875591, -0.0197650194, -0.0153758526, -0.0146992207,\n",
            "        -0.0138092563, -0.0144235194, -0.0168634951, -0.0172336102,\n",
            "        -0.0060715675, -0.0141086578, -0.0134568512,  0.0001095459,\n",
            "        -0.0110239983, -0.0170445442, -0.0187176466, -0.0164273977])\n",
            "btensor.grad: tensor([-0.0076909438, -0.0089980364, -0.0090082586, -0.0153696239,\n",
            "        -0.0113994330, -0.0081275702, -0.0149280429, -0.0131950378,\n",
            "        -0.0145695210, -0.0110104084, -0.0128287971, -0.0142143965,\n",
            "        -0.0090958737, -0.0138356388, -0.0160728693, -0.0100635886,\n",
            "        -0.0131065845, -0.0169917643, -0.0156161189, -0.0169959068])\n",
            "ctensor.grad: tensor([-0.5190084577, -0.5190084577, -0.5190084577, -0.4817944765,\n",
            "         0.0111792991, -0.2877848148,  0.1034093723, -0.0399953164,\n",
            "        -0.4817944765,  0.0111792991, -0.2877848148,  0.1034093723,\n",
            "        -0.0399953164, -0.4817944765,  0.0111792991, -0.2877848148,\n",
            "         0.1034093723, -0.0399953164])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2465209961, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8332754374), tensor(1.7867461443), tensor(1.1126991510), tensor(1.4601691961), tensor(0.8578944206), tensor(0.7112334967), tensor(1.6977256536), tensor(1.1684159040), tensor(1.3456574678), tensor(1.0875211954), tensor(1.0229122639), tensor(1.0880856514), tensor(1.8228172064), tensor(1.2430604696), tensor(1.6448609829), tensor(1.8009809256), tensor(1.5992029905), tensor(1.1306511164), tensor(0.8849947453), tensor(1.1011646986)]\n",
            "b:  [tensor(1.6322996616), tensor(0.7255474925), tensor(1.4591275454), tensor(1.3076505661), tensor(1.0081557035), tensor(1.9941120148), tensor(1.2774219513), tensor(1.3967450857), tensor(1.2841045856), tensor(1.0630162954), tensor(1.3625401258), tensor(1.2900995016), tensor(0.7631531358), tensor(1.2636613846), tensor(1.2410562038), tensor(0.8973135352), tensor(1.0997251272), tensor(1.4018632174), tensor(1.3845531940), tensor(1.5531361103)]\n",
            "c:  [tensor(0.0050098184), tensor(0.0050098184), tensor(0.0050098184), tensor(0.0044457186), tensor(-0.0001048994), tensor(0.0025813878), tensor(-0.0009983312), tensor(0.0003066710), tensor(0.0044457186), tensor(-0.0001048994), tensor(0.0025813878), tensor(-0.0009983312), tensor(0.0003066710), tensor(0.0044457186), tensor(-0.0001048994), tensor(0.0025813878), tensor(-0.0009983312), tensor(0.0003066710)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0190789104, -0.0053626597, -0.0155529380, -0.0093630552,\n",
            "        -0.0187586546, -0.0210487843, -0.0153299570, -0.0154428482,\n",
            "        -0.0143893342, -0.0154053867, -0.0177795142, -0.0179568455,\n",
            "        -0.0067226887, -0.0148785114, -0.0136840194, -0.0011639036,\n",
            "        -0.0116338134, -0.0178092718, -0.0197464824, -0.0172621310])\n",
            "btensor.grad: tensor([-0.0093669854, -0.0095544159, -0.0102978647, -0.0158854127,\n",
            "        -0.0120237917, -0.0098146796, -0.0154948235, -0.0140268654,\n",
            "        -0.0151702166, -0.0116699934, -0.0137436092, -0.0148614049,\n",
            "        -0.0096586235, -0.0144857466, -0.0164783299, -0.0107325912,\n",
            "        -0.0136525631, -0.0174207687, -0.0162809491, -0.0175291300])\n",
            "ctensor.grad: tensor([-0.5520980358, -0.5520980358, -0.5520980358, -0.5109973550,\n",
            "         0.0118342694, -0.3061171174,  0.1091696247, -0.0433763601,\n",
            "        -0.5109973550,  0.0118342694, -0.3061171174,  0.1091696247,\n",
            "        -0.0433763601, -0.5109973550,  0.0118342694, -0.3061171174,\n",
            "         0.1091696247, -0.0433763601])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2452392578, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8333768249), tensor(1.7867772579), tensor(1.1127811670), tensor(1.4602210522), tensor(0.8579937816), tensor(0.7113454342), tensor(1.6978024244), tensor(1.1684970856), tensor(1.3457325697), tensor(1.0876033306), tensor(1.0230060816), tensor(1.0881793499), tensor(1.8228540421), tensor(1.2431390285), tensor(1.6449308395), tensor(1.8009928465), tensor(1.5992642641), tensor(1.1307443380), tensor(0.8850989342), tensor(1.1012554169)]\n",
            "b:  [tensor(1.6323546171), tensor(0.7255982161), tensor(1.4591854811), tensor(1.3077329397), tensor(1.0082191229), tensor(1.9941693544), tensor(1.2775025368), tensor(1.3968195915), tensor(1.2841837406), tensor(1.0630781651), tensor(1.3626135588), tensor(1.2901773453), tensor(0.7632043958), tensor(1.2637373209), tensor(1.2411409616), tensor(0.8973707557), tensor(1.0997964144), tensor(1.4019527435), tensor(1.3846381903), tensor(1.5532267094)]\n",
            "c:  [tensor(0.0053035230), tensor(0.0053035230), tensor(0.0053035230), tensor(0.0047168224), tensor(-0.0001111653), tensor(0.0027442884), tensor(-0.0010559678), tensor(0.0003302176), tensor(0.0047168224), tensor(-0.0001111653), tensor(0.0027442884), tensor(-0.0010559678), tensor(0.0003302176), tensor(0.0047168224), tensor(-0.0001111653), tensor(0.0027442884), tensor(-0.0010559678), tensor(0.0003302176)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0202819705, -0.0062164515, -0.0164107084, -0.0103609562,\n",
            "        -0.0198743343, -0.0223866701, -0.0153563023, -0.0162399411,\n",
            "        -0.0150232613, -0.0164207816, -0.0187549144, -0.0187495798,\n",
            "        -0.0073630214, -0.0156998634, -0.0139621198, -0.0023778789,\n",
            "        -0.0122613907, -0.0186395645, -0.0208411217, -0.0181536674])\n",
            "btensor.grad: tensor([-0.0109957438, -0.0101460516, -0.0115852654, -0.0164816380,\n",
            "        -0.0126864091, -0.0114793181, -0.0161130428, -0.0148956925,\n",
            "        -0.0158201456, -0.0123629570, -0.0146918595, -0.0155574083,\n",
            "        -0.0102517717, -0.0151842833, -0.0169618428, -0.0114439130,\n",
            "        -0.0142641068, -0.0179156661, -0.0169982314, -0.0181264877])\n",
            "ctensor.grad: tensor([-0.5874091983, -0.5874091983, -0.5874091983, -0.5422071815,\n",
            "         0.0125318300, -0.3258013427,  0.1152730584, -0.0470931754,\n",
            "        -0.5422071815,  0.0125318300, -0.3258013427,  0.1152730584,\n",
            "        -0.0470931754, -0.5422071815,  0.0125318300, -0.3258013427,\n",
            "         0.1152730584, -0.0470931754])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2446899414, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8334845304), tensor(1.7868125439), tensor(1.1128677130), tensor(1.4602777958), tensor(0.8580989838), tensor(0.7114644051), tensor(1.6978796721), tensor(1.1685825586), tensor(1.3458111286), tensor(1.0876907110), tensor(1.0231050253), tensor(1.0882774591), tensor(1.8228939772), tensor(1.2432218790), tensor(1.6450022459), tensor(1.8010104895), tensor(1.5993288755), tensor(1.1308419704), tensor(0.8852089047), tensor(1.1013509035)]\n",
            "b:  [tensor(1.6324175596), tensor(0.7256520987), tensor(1.4592498541), tensor(1.3078186512), tensor(1.0082859993), tensor(1.9942350388), tensor(1.2775864601), tensor(1.3968986273), tensor(1.2842663527), tensor(1.0631436110), tensor(1.3626918793), tensor(1.2902588844), tensor(0.7632587552), tensor(1.2638169527), tensor(1.2412285805), tensor(0.8974317312), tensor(1.0998711586), tensor(1.4020451307), tensor(1.3847271204), tensor(1.5533206463)]\n",
            "c:  [tensor(0.0056160800), tensor(0.0056160800), tensor(0.0056160800), tensor(0.0050046104), tensor(-0.0001178028), tensor(0.0029177638), tensor(-0.0011168378), tensor(0.0003558093), tensor(0.0050046104), tensor(-0.0001178028), tensor(0.0029177638), tensor(-0.0011168378), tensor(0.0003558093), tensor(0.0050046104), tensor(-0.0001178028), tensor(0.0029177638), tensor(-0.0011168378), tensor(0.0003558093)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0215425491, -0.0070511550, -0.0173188448, -0.0113526583,\n",
            "        -0.0210427046, -0.0237921476, -0.0154492855, -0.0170924664,\n",
            "        -0.0157147478, -0.0174668431, -0.0197918862, -0.0196120888,\n",
            "        -0.0079951286, -0.0165722370, -0.0142905563, -0.0035378151,\n",
            "        -0.0129120946, -0.0195311308, -0.0219982266, -0.0191038549])\n",
            "btensor.grad: tensor([-0.0125876311, -0.0107728839, -0.0128713548, -0.0171521902,\n",
            "        -0.0133853108, -0.0131316185, -0.0167802572, -0.0158013254,\n",
            "        -0.0165234804, -0.0130872726, -0.0156744719, -0.0163016319,\n",
            "        -0.0108767375, -0.0159369707, -0.0175243914, -0.0121960044,\n",
            "        -0.0149388313, -0.0184788108, -0.0177775621, -0.0187869072])\n",
            "ctensor.grad: tensor([-0.6251142621, -0.6251142621, -0.6251142621, -0.5755762458,\n",
            "         0.0132749612, -0.3469506800,  0.1217402071, -0.0511834063,\n",
            "        -0.5755762458,  0.0132749612, -0.3469506800,  0.1217402071,\n",
            "        -0.0511834063, -0.5755762458,  0.0132749612, -0.3469506800,\n",
            "         0.1217402071, -0.0511834063])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2429809570, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8335988522), tensor(1.7868518829), tensor(1.1129591465), tensor(1.4603395462), tensor(0.8582102656), tensor(0.7115907073), tensor(1.6979576349), tensor(1.1686725616), tensor(1.3458933830), tensor(1.0877834558), tensor(1.0232094526), tensor(1.0883800983), tensor(1.8229370117), tensor(1.2433093786), tensor(1.6450755596), tensor(1.8010337353), tensor(1.5993968248), tensor(1.1309443712), tensor(0.8853250146), tensor(1.1014515162)]\n",
            "b:  [tensor(1.6324883699), tensor(0.7257093191), tensor(1.4593206644), tensor(1.3079081774), tensor(1.0083566904), tensor(1.9943089485), tensor(1.2776739597), tensor(1.3969823122), tensor(1.2843527794), tensor(1.0632128716), tensor(1.3627753258), tensor(1.2903443575), tensor(0.7633164525), tensor(1.2639006376), tensor(1.2413194180), tensor(0.8974967003), tensor(1.0999494791), tensor(1.4021406174), tensor(1.3848202229), tensor(1.5534181595)]\n",
            "c:  [tensor(0.0059487852), tensor(0.0059487852), tensor(0.0059487852), tensor(0.0053102458), tensor(-0.0001248363), tensor(0.0031026080), tensor(-0.0011811344), tensor(0.0003836532), tensor(0.0053102458), tensor(-0.0001248363), tensor(0.0031026080), tensor(-0.0011811344), tensor(0.0003836532), tensor(0.0053102458), tensor(-0.0001248363), tensor(0.0031026080), tensor(-0.0011811344), tensor(0.0003836532)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0228621960, -0.0078786314, -0.0182806849, -0.0123405457,\n",
            "        -0.0222616196, -0.0252625942, -0.0156019926, -0.0179959238,\n",
            "        -0.0164590664, -0.0185512006, -0.0208961219, -0.0205384046,\n",
            "        -0.0086165667, -0.0175014734, -0.0146696568, -0.0046517514,\n",
            "        -0.0135826468, -0.0204895735, -0.0232247114, -0.0201122761])\n",
            "btensor.grad: tensor([-0.0141537338, -0.0114385188, -0.0141614974, -0.0179009736,\n",
            "        -0.0141273439, -0.0147758126, -0.0175021887, -0.0167481899,\n",
            "        -0.0172811747, -0.0138455629, -0.0166970789, -0.0170965791,\n",
            "        -0.0115351789, -0.0167416930, -0.0181577802, -0.0129941702,\n",
            "        -0.0156729221, -0.0191062391, -0.0186164975, -0.0195093155])\n",
            "ctensor.grad: tensor([-0.6654100418, -0.6654100418, -0.6654100418, -0.6112709641,\n",
            "         0.0140668862, -0.3696884811,  0.1285930872, -0.0556879602,\n",
            "        -0.6112709641,  0.0140668862, -0.3696884811,  0.1285930872,\n",
            "        -0.0556879602, -0.6112709641,  0.0140668862, -0.3696884811,\n",
            "         0.1285930872, -0.0556879602])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2410278320, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8337200880), tensor(1.7868953943), tensor(1.1130555868), tensor(1.4604061842), tensor(0.8583279252), tensor(0.7117247581), tensor(1.6980366707), tensor(1.1687673330), tensor(1.3459796906), tensor(1.0878818035), tensor(1.0233198404), tensor(1.0884877443), tensor(1.8229831457), tensor(1.2434017658), tensor(1.6451510191), tensor(1.8010623455), tensor(1.5994682312), tensor(1.1310520172), tensor(0.8854476213), tensor(1.1015573740)]\n",
            "b:  [tensor(1.6325669289), tensor(0.7257700562), tensor(1.4593980312), tensor(1.3080017567), tensor(1.0084311962), tensor(1.9943910837), tensor(1.2777653933), tensor(1.3970710039), tensor(1.2844432592), tensor(1.0632860661), tensor(1.3628641367), tensor(1.2904341221), tensor(0.7633776069), tensor(1.2639886141), tensor(1.2414137125), tensor(0.8975659013), tensor(1.1000318527), tensor(1.4022395611), tensor(1.3849178553), tensor(1.5535196066)]\n",
            "c:  [tensor(0.0063030347), tensor(0.0063030347), tensor(0.0063030347), tensor(0.0056349831), tensor(-0.0001322918), tensor(0.0032996843), tensor(-0.0012490618), tensor(0.0004139801), tensor(0.0056349831), tensor(-0.0001322918), tensor(0.0032996843), tensor(-0.0012490618), tensor(0.0004139801), tensor(0.0056349831), tensor(-0.0001322918), tensor(0.0032996843), tensor(-0.0012490618), tensor(0.0004139801)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0242463946, -0.0086980462, -0.0192945600, -0.0133306980,\n",
            "        -0.0235316753, -0.0268079042, -0.0158127546, -0.0189538300,\n",
            "        -0.0172585808, -0.0196791291, -0.0220663249, -0.0215377361,\n",
            "        -0.0092343688, -0.0184831619, -0.0150955319, -0.0057266466,\n",
            "        -0.0142799020, -0.0215183496, -0.0245184302, -0.0211800337])\n",
            "btensor.grad: tensor([-0.0157032702, -0.0121487379, -0.0154642165, -0.0187194645,\n",
            "        -0.0149116442, -0.0164159834, -0.0182788968, -0.0177411437,\n",
            "        -0.0180897713, -0.0146411657, -0.0177634060, -0.0179461837,\n",
            "        -0.0122317001, -0.0176042616, -0.0188641846, -0.0138409734,\n",
            "        -0.0164742470, -0.0197975338, -0.0195172429, -0.0202934742])\n",
            "ctensor.grad: tensor([-0.7084993124, -0.7084993124, -0.7084993124, -0.6494745016,\n",
            "         0.0149111059, -0.3941522539,  0.1358548105, -0.0606536716,\n",
            "        -0.6494745016,  0.0149111059, -0.3941522539,  0.1358548105,\n",
            "        -0.0606536716, -0.6494745016,  0.0149111059, -0.3941522539,\n",
            "         0.1358548105, -0.0606536716])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2393188477, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8338485956), tensor(1.7869429588), tensor(1.1131573915), tensor(1.4604778290), tensor(0.8584522009), tensor(0.7118669152), tensor(1.6981170177), tensor(1.1688671112), tensor(1.3460702896), tensor(1.0879861116), tensor(1.0234363079), tensor(1.0886007547), tensor(1.8230323792), tensor(1.2434993982), tensor(1.6452288628), tensor(1.8010962009), tensor(1.5995432138), tensor(1.1311650276), tensor(0.8855770826), tensor(1.1016689539)]\n",
            "b:  [tensor(1.6326531172), tensor(0.7258345485), tensor(1.4594819546), tensor(1.3080998659), tensor(1.0085098743), tensor(1.9944814444), tensor(1.2778608799), tensor(1.3971649408), tensor(1.2845380306), tensor(1.0633634329), tensor(1.3629585505), tensor(1.2905284166), tensor(0.7634424567), tensor(1.2640812397), tensor(1.2415119410), tensor(0.8976395726), tensor(1.1001185179), tensor(1.4023423195), tensor(1.3850202560), tensor(1.5536253452)]\n",
            "c:  [tensor(0.0066803470), tensor(0.0066803470), tensor(0.0066803470), tensor(0.0059801759), tensor(-0.0001401975), tensor(0.0035099301), tensor(-0.0013208369), tensor(0.0004470464), tensor(0.0059801759), tensor(-0.0001401975), tensor(0.0035099301), tensor(-0.0013208369), tensor(0.0004470464), tensor(0.0059801759), tensor(-0.0001401975), tensor(0.0035099301), tensor(-0.0013208369), tensor(0.0004470464)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0256967545, -0.0095109940, -0.0203614831, -0.0143280029,\n",
            "        -0.0248588324, -0.0284283161, -0.0160793066, -0.0199647546,\n",
            "        -0.0181109961, -0.0208500326, -0.0233033299, -0.0226060972,\n",
            "        -0.0098471642, -0.0195212364, -0.0155679286, -0.0067647807,\n",
            "        -0.0150044560, -0.0226103663, -0.0258864164, -0.0223146677])\n",
            "btensor.grad: tensor([-0.0172443856, -0.0129013956, -0.0167845190, -0.0196117163,\n",
            "        -0.0157432333, -0.0180623829, -0.0191071033, -0.0187825114,\n",
            "        -0.0189553499, -0.0154765844, -0.0188740194, -0.0188477635,\n",
            "        -0.0129669122, -0.0185220540, -0.0196464658, -0.0147393942,\n",
            "        -0.0173366070, -0.0205521286, -0.0204816461, -0.0211399794])\n",
            "ctensor.grad: tensor([-0.7546241283, -0.7546241283, -0.7546241283, -0.6903859973,\n",
            "         0.0158114266, -0.4204916060,  0.1435503960, -0.0661325604,\n",
            "        -0.6903859973,  0.0158114266, -0.4204916060,  0.1435503960,\n",
            "        -0.0661325604, -0.6903859973,  0.0158114266, -0.4204916060,\n",
            "         0.1435503960, -0.0661325604])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2376708984, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8339846730), tensor(1.7869945765), tensor(1.1132647991), tensor(1.4605544806), tensor(0.8585833907), tensor(0.7120175362), tensor(1.6981990337), tensor(1.1689722538), tensor(1.3461652994), tensor(1.0880963802), tensor(1.0235593319), tensor(1.0887194872), tensor(1.8230845928), tensor(1.2436025143), tensor(1.6453093290), tensor(1.8011350632), tensor(1.5996220112), tensor(1.1312838793), tensor(0.8857136965), tensor(1.1017864943)]\n",
            "b:  [tensor(1.6327470541), tensor(0.7259030938), tensor(1.4595725536), tensor(1.3082027435), tensor(1.0085929632), tensor(1.9945800304), tensor(1.2779608965), tensor(1.3972642422), tensor(1.2846374512), tensor(1.0634452105), tensor(1.3630586863), tensor(1.2906274796), tensor(0.7635111809), tensor(1.2641787529), tensor(1.2416144609), tensor(0.8977180123), tensor(1.1002098322), tensor(1.4024491310), tensor(1.3851277828), tensor(1.5537356138)]\n",
            "c:  [tensor(0.0070823655), tensor(0.0070823655), tensor(0.0070823655), tensor(0.0063472884), tensor(-0.0001485835), tensor(0.0037343665), tensor(-0.0013966906), tensor(0.0004831374), tensor(0.0063472884), tensor(-0.0001485835), tensor(0.0037343665), tensor(-0.0013966906), tensor(0.0004831374), tensor(0.0063472884), tensor(-0.0001485835), tensor(0.0037343665), tensor(-0.0013966906), tensor(0.0004831374)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0272150040, -0.0103219301, -0.0214816332, -0.0153295994,\n",
            "        -0.0262398720, -0.0301272869, -0.0163946152, -0.0210302174,\n",
            "        -0.0190127622, -0.0220571458, -0.0246133953, -0.0237450674,\n",
            "        -0.0104513168, -0.0206156969, -0.0160827637, -0.0077706315,\n",
            "        -0.0157597065, -0.0237775445, -0.0273281336, -0.0235161483])\n",
            "btensor.grad: tensor([-0.0187867973, -0.0137032270, -0.0181278884, -0.0205781758,\n",
            "        -0.0166184679, -0.0197158158, -0.0199929476, -0.0198708922,\n",
            "        -0.0198776722, -0.0163477659, -0.0200344324, -0.0198044777,\n",
            "        -0.0137433968, -0.0194919109, -0.0204978883, -0.0156934261,\n",
            "        -0.0182621479, -0.0213668942, -0.0215114355, -0.0220441818])\n",
            "ctensor.grad: tensor([-0.8040367365, -0.8040367365, -0.8040367365, -0.7342250943,\n",
            "         0.0167719964, -0.4488727152,  0.1517073363, -0.0721820891,\n",
            "        -0.7342250943,  0.0167719964, -0.4488727152,  0.1517073363,\n",
            "        -0.0721820891, -0.7342250943,  0.0167719964, -0.4488727152,\n",
            "         0.1517073363, -0.0721820891])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2354125977, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8341287374), tensor(1.7870502472), tensor(1.1133780479), tensor(1.4606361389), tensor(0.8587217927), tensor(0.7121770978), tensor(1.6982828379), tensor(1.1690829992), tensor(1.3462651968), tensor(1.0882129669), tensor(1.0236892700), tensor(1.0888442993), tensor(1.8231397867), tensor(1.2437113523), tensor(1.6453925371), tensor(1.8011788130), tensor(1.5997047424), tensor(1.1314089298), tensor(0.8858579397), tensor(1.1019103527)]\n",
            "b:  [tensor(1.6328487396), tensor(0.7259758711), tensor(1.4596700668), tensor(1.3083108664), tensor(1.0086807013), tensor(1.9946869612), tensor(1.2780655622), tensor(1.3973692656), tensor(1.2847417593), tensor(1.0635315180), tensor(1.3631649017), tensor(1.2907315493), tensor(0.7635840178), tensor(1.2642813921), tensor(1.2417215109), tensor(0.8978015184), tensor(1.1003061533), tensor(1.4025603533), tensor(1.3852407932), tensor(1.5538506508)]\n",
            "c:  [tensor(0.0075108684), tensor(0.0075108684), tensor(0.0075108684), tensor(0.0067379051), tensor(-0.0001574822), tensor(0.0039741057), tensor(-0.0014768679), tensor(0.0005225716), tensor(0.0067379051), tensor(-0.0001574822), tensor(0.0039741057), tensor(-0.0014768679), tensor(0.0005225716), tensor(0.0067379051), tensor(-0.0001574822), tensor(0.0039741057), tensor(-0.0014768679), tensor(0.0005225716)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0288092494, -0.0111287236, -0.0226546526, -0.0163402557,\n",
            "        -0.0276769400, -0.0319112539, -0.0167589188, -0.0221498609,\n",
            "        -0.0199700743, -0.0233097076, -0.0259958953, -0.0249528363,\n",
            "        -0.0110487342, -0.0217655897, -0.0166375339, -0.0087475367,\n",
            "        -0.0165414810, -0.0250172615, -0.0288488269, -0.0247824788])\n",
            "btensor.grad: tensor([-0.0203367062, -0.0145529509, -0.0195002854, -0.0216168761,\n",
            "        -0.0175448209, -0.0213833749, -0.0209321976, -0.0210100114,\n",
            "        -0.0208547115, -0.0172573328, -0.0212493539, -0.0208159089,\n",
            "        -0.0145630203, -0.0205168426, -0.0214210153, -0.0167042613,\n",
            "        -0.0192530155, -0.0222373903, -0.0226129293, -0.0230079889])\n",
            "ctensor.grad: tensor([-0.8570059538, -0.8570059538, -0.8570059538, -0.7812330723,\n",
            "         0.0177973434, -0.4794787467,  0.1603545845, -0.0788683295,\n",
            "        -0.7812330723,  0.0177973434, -0.4794787467,  0.1603545845,\n",
            "        -0.0788683295, -0.7812330723,  0.0177973434, -0.4794787467,\n",
            "         0.1603545845, -0.0788683295])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2324829102, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8342811465), tensor(1.7871099710), tensor(1.1134974957), tensor(1.4607229233), tensor(0.8588676453), tensor(0.7123460174), tensor(1.6983686686), tensor(1.1691995859), tensor(1.3463701010), tensor(1.0883359909), tensor(1.0238264799), tensor(1.0889754295), tensor(1.8231979609), tensor(1.2438262701), tensor(1.6454787254), tensor(1.8012273312), tensor(1.5997915268), tensor(1.1315405369), tensor(0.8860102296), tensor(1.1020410061)]\n",
            "b:  [tensor(1.6329581738), tensor(0.7260531783), tensor(1.4597746134), tensor(1.3084244728), tensor(1.0087733269), tensor(1.9948022366), tensor(1.2781752348), tensor(1.3974802494), tensor(1.2848511934), tensor(1.0636225939), tensor(1.3632775545), tensor(1.2908409834), tensor(0.7636611462), tensor(1.2643893957), tensor(1.2418335676), tensor(0.8978903890), tensor(1.1004077196), tensor(1.4026762247), tensor(1.3853596449), tensor(1.5539708138)]\n",
            "c:  [tensor(0.0079677925), tensor(0.0079677925), tensor(0.0079677925), tensor(0.0071537415), tensor(-0.0001669284), tensor(0.0042303614), tensor(-0.0015616295), tensor(0.0005657038), tensor(0.0071537415), tensor(-0.0001669284), tensor(0.0042303614), tensor(-0.0015616295), tensor(0.0005657038), tensor(0.0071537415), tensor(-0.0001669284), tensor(0.0042303614), tensor(-0.0015616295), tensor(0.0005657038)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0304799080, -0.0119336396, -0.0238811970, -0.0173635483,\n",
            "        -0.0291734934, -0.0337827206, -0.0171616077, -0.0233219564,\n",
            "        -0.0209790543, -0.0246062279, -0.0274531096, -0.0262299255,\n",
            "        -0.0116371512, -0.0229767561, -0.0172325373, -0.0096985288,\n",
            "        -0.0173513889, -0.0263324380, -0.0304520726, -0.0261203349])\n",
            "btensor.grad: tensor([-0.0218975823, -0.0154580474, -0.0209023654, -0.0227321088,\n",
            "        -0.0185210854, -0.0230660439, -0.0219266415, -0.0222042203,\n",
            "        -0.0218864679, -0.0182070732, -0.0225234628, -0.0218855739,\n",
            "        -0.0154278502, -0.0216054618, -0.0224179626, -0.0177791715,\n",
            "        -0.0203084946, -0.0231667161, -0.0237812400, -0.0240315199])\n",
            "ctensor.grad: tensor([-0.9138477445, -0.9138477445, -0.9138477445, -0.8316729665,\n",
            "         0.0188924000, -0.5125116110,  0.1695231497, -0.0862644240,\n",
            "        -0.8316729665,  0.0188924000, -0.5125116110,  0.1695231497,\n",
            "        -0.0862644240, -0.8316729665,  0.0188924000, -0.5125116110,\n",
            "         0.1695231497, -0.0862644240])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2297363281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8344423175), tensor(1.7871736288), tensor(1.1136232615), tensor(1.4608148336), tensor(0.8590212464), tensor(0.7125247717), tensor(1.6984566450), tensor(1.1693223715), tensor(1.3464802504), tensor(1.0884656906), tensor(1.0239714384), tensor(1.0891133547), tensor(1.8232589960), tensor(1.2439475060), tensor(1.6455680132), tensor(1.8012804985), tensor(1.5998824835), tensor(1.1316791773), tensor(0.8861709237), tensor(1.1021786928)]\n",
            "b:  [tensor(1.6330755949), tensor(0.7261352539), tensor(1.4598863125), tensor(1.3085440397), tensor(1.0088710785), tensor(1.9949260950), tensor(1.2782901525), tensor(1.3975975513), tensor(1.2849661112), tensor(1.0637185574), tensor(1.3633968830), tensor(1.2909560204), tensor(0.7637428641), tensor(1.2645031214), tensor(1.2419509888), tensor(0.8979849815), tensor(1.1005148888), tensor(1.4027969837), tensor(1.3854848146), tensor(1.5540963411)]\n",
            "c:  [tensor(0.0084552448), tensor(0.0084552448), tensor(0.0084552448), tensor(0.0075966604), tensor(-0.0001769597), tensor(0.0045044585), tensor(-0.0016512534), tensor(0.0006129292), tensor(0.0075966604), tensor(-0.0001769597), tensor(0.0045044585), tensor(-0.0016512534), tensor(0.0006129292), tensor(0.0075966604), tensor(-0.0001769597), tensor(0.0045044585), tensor(-0.0016512534), tensor(0.0006129292)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0322295427, -0.0127335638, -0.0251575708, -0.0183919668,\n",
            "        -0.0307245255, -0.0357458591, -0.0176047087, -0.0245487094,\n",
            "        -0.0220353473, -0.0259498656, -0.0289881825, -0.0275831148,\n",
            "        -0.0122151375, -0.0242506266, -0.0178660005, -0.0106227882,\n",
            "        -0.0181905031, -0.0277266502, -0.0321361423, -0.0275276303])\n",
            "btensor.grad: tensor([-0.0234757662, -0.0164210796, -0.0223401189, -0.0239228308,\n",
            "        -0.0195486769, -0.0247669518, -0.0229733586, -0.0234539062,\n",
            "        -0.0229780674, -0.0191972256, -0.0238548219, -0.0230131745,\n",
            "        -0.0163441077, -0.0227520168, -0.0234834254, -0.0189193487,\n",
            "        -0.0214343071, -0.0241516232, -0.0250220895, -0.0251104832])\n",
            "ctensor.grad: tensor([-0.9749047756, -0.9749047756, -0.9749047756, -0.8858373165,\n",
            "         0.0200625695, -0.5481945276,  0.1792478710, -0.0944507644,\n",
            "        -0.8858373165,  0.0200625695, -0.5481945276,  0.1792478710,\n",
            "        -0.0944507644, -0.8858373165,  0.0200625695, -0.5481945276,\n",
            "         0.1792478710, -0.0944507644])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2263793945, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8346126080), tensor(1.7872413397), tensor(1.1137557030), tensor(1.4609119892), tensor(0.8591828942), tensor(0.7127137780), tensor(1.6985470057), tensor(1.1694514751), tensor(1.3465960026), tensor(1.0886023045), tensor(1.0241243839), tensor(1.0892584324), tensor(1.8233228922), tensor(1.2440754175), tensor(1.6456606388), tensor(1.8013380766), tensor(1.5999777317), tensor(1.1318252087), tensor(0.8863404393), tensor(1.1023237705)]\n",
            "b:  [tensor(1.6332010031), tensor(0.7262224555), tensor(1.4600054026), tensor(1.3086700439), tensor(1.0089741945), tensor(1.9950585365), tensor(1.2784105539), tensor(1.3977212906), tensor(1.2850867510), tensor(1.0638196468), tensor(1.3635231256), tensor(1.2910770178), tensor(0.7638294101), tensor(1.2646229267), tensor(1.2420741320), tensor(0.8980856538), tensor(1.1006280184), tensor(1.4029229879), tensor(1.3856165409), tensor(1.5542275906)]\n",
            "c:  [tensor(0.0089755245), tensor(0.0089755245), tensor(0.0089755245), tensor(0.0080686836), tensor(-0.0001876166), tensor(0.0047978451), tensor(-0.0017460362), tensor(0.0006646895), tensor(0.0080686836), tensor(-0.0001876166), tensor(0.0047978451), tensor(-0.0017460362), tensor(0.0006646895), tensor(0.0080686836), tensor(-0.0001876166), tensor(0.0047978451), tensor(-0.0017460362), tensor(0.0006646895)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0340592861, -0.0135331601, -0.0264886618, -0.0194317102,\n",
            "        -0.0323321819, -0.0378026962, -0.0180838108, -0.0258245766,\n",
            "        -0.0231462829, -0.0273338258, -0.0305979103, -0.0290063024,\n",
            "        -0.0127726197, -0.0255837440, -0.0185314566, -0.0115218125,\n",
            "        -0.0190567374, -0.0291947126, -0.0339028239, -0.0290062129])\n",
            "btensor.grad: tensor([-0.0250758268, -0.0174458325, -0.0238156617, -0.0251903832,\n",
            "        -0.0206324160, -0.0264847577, -0.0240789056, -0.0247566402,\n",
            "        -0.0241231918, -0.0202270746, -0.0252501965, -0.0242011547,\n",
            "        -0.0173100159, -0.0239606500, -0.0246211588, -0.0201321840,\n",
            "        -0.0226309299, -0.0251904726, -0.0263347626, -0.0262451172])\n",
            "ctensor.grad: tensor([-1.0405600071, -1.0405600071, -1.0405600071, -0.9440466166,\n",
            "         0.0213137735, -0.5867735744,  0.1895654947, -0.1035207435,\n",
            "        -0.9440466166,  0.0213137735, -0.5867735744,  0.1895654947,\n",
            "        -0.1035207435, -0.9440466166,  0.0213137735, -0.5867735744,\n",
            "         0.1895654947, -0.1035207435])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2225341797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8347924948), tensor(1.7873129845), tensor(1.1138950586), tensor(1.4610143900), tensor(0.8593528271), tensor(0.7129135728), tensor(1.6986399889), tensor(1.1695872545), tensor(1.3467174768), tensor(1.0887460709), tensor(1.0242857933), tensor(1.0894109011), tensor(1.8233894110), tensor(1.2442103624), tensor(1.6457567215), tensor(1.8014000654), tensor(1.6000775099), tensor(1.1319788694), tensor(0.8865191936), tensor(1.1024765968)]\n",
            "b:  [tensor(1.6333345175), tensor(0.7263151407), tensor(1.4601321220), tensor(1.3088027239), tensor(1.0090830326), tensor(1.9951996803), tensor(1.2785366774), tensor(1.3978518248), tensor(1.2852133512), tensor(1.0639261007), tensor(1.3636566401), tensor(1.2912042141), tensor(0.7639210820), tensor(1.2647490501), tensor(1.2422032356), tensor(0.8981927633), tensor(1.1007474661), tensor(1.4030543566), tensor(1.3857551813), tensor(1.5543646812)]\n",
            "c:  [tensor(0.0095311375), tensor(0.0095311375), tensor(0.0095311375), tensor(0.0085720103), tensor(-0.0001989429), tensor(0.0051121064), tensor(-0.0018462949), tensor(0.0007214772), tensor(0.0085720103), tensor(-0.0001989429), tensor(0.0051121064), tensor(-0.0018462949), tensor(0.0007214772), tensor(0.0085720103), tensor(-0.0001989429), tensor(0.0051121064), tensor(-0.0018462949), tensor(0.0007214772)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0359727740, -0.0143218338, -0.0278649926, -0.0204737186,\n",
            "        -0.0339925289, -0.0399560928, -0.0185879469, -0.0271537602,\n",
            "        -0.0243018251, -0.0287572742, -0.0322903097, -0.0305022299,\n",
            "        -0.0133087635, -0.0269781351, -0.0192254037, -0.0123928748,\n",
            "        -0.0199482441, -0.0307419300, -0.0357566476, -0.0305597782])\n",
            "btensor.grad: tensor([-0.0266956855, -0.0185387731, -0.0253326297, -0.0265299976,\n",
            "        -0.0217722580, -0.0282200873, -0.0252342224, -0.0261178464,\n",
            "        -0.0253274441, -0.0212950706, -0.0267105401, -0.0254426599,\n",
            "        -0.0183317363, -0.0252287686, -0.0258266628, -0.0214179158,\n",
            "        -0.0239000320, -0.0262764990, -0.0277195573, -0.0274291039])\n",
            "ctensor.grad: tensor([-1.1112260818, -1.1112260818, -1.1112260818, -1.0066541433,\n",
            "         0.0226525422, -0.6285226941,  0.2005171925, -0.1135754064,\n",
            "        -1.0066541433,  0.0226525422, -0.6285226941,  0.2005171925,\n",
            "        -0.1135754064, -1.0066541433,  0.0226525422, -0.6285226941,\n",
            "         0.2005171925, -0.1135754064])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2187500000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8349823356), tensor(1.7873884439), tensor(1.1140414476), tensor(1.4611220360), tensor(0.8595312834), tensor(0.7131246328), tensor(1.6987355947), tensor(1.1697299480), tensor(1.3468450308), tensor(1.0888971090), tensor(1.0244561434), tensor(1.0895712376), tensor(1.8234585524), tensor(1.2443525791), tensor(1.6458564997), tensor(1.8014662266), tensor(1.6001818180), tensor(1.1321407557), tensor(0.8867076635), tensor(1.1026375294)]\n",
            "b:  [tensor(1.6334762573), tensor(0.7264136672), tensor(1.4602665901), tensor(1.3089424372), tensor(1.0091978312), tensor(1.9953495264), tensor(1.2786688805), tensor(1.3979895115), tensor(1.2853462696), tensor(1.0640381575), tensor(1.3637977839), tensor(1.2913379669), tensor(0.7640181184), tensor(1.2648818493), tensor(1.2423387766), tensor(0.8983067274), tensor(1.1008737087), tensor(1.4031914473), tensor(1.3859010935), tensor(1.5545079708)]\n",
            "c:  [tensor(0.0101248184), tensor(0.0101248184), tensor(0.0101248184), tensor(0.0091090370), tensor(-0.0002109859), tensor(0.0054489770), tensor(-0.0019523688), tensor(0.0007838423), tensor(0.0091090370), tensor(-0.0002109859), tensor(0.0054489770), tensor(-0.0019523688), tensor(0.0007838423), tensor(0.0091090370), tensor(-0.0002109859), tensor(0.0054489770), tensor(-0.0019523688), tensor(0.0007838423)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0379696488, -0.0151007921, -0.0292878747, -0.0215173960,\n",
            "        -0.0356971025, -0.0422096252, -0.0191119909, -0.0285280645,\n",
            "        -0.0255038235, -0.0302183628, -0.0340650380, -0.0320716947,\n",
            "        -0.0138173103, -0.0284329653, -0.0199445784, -0.0132316537,\n",
            "        -0.0208630562, -0.0323752165, -0.0376995802, -0.0321888626])\n",
            "btensor.grad: tensor([-0.0283432771, -0.0197006762, -0.0268885791, -0.0279511809,\n",
            "        -0.0229672119, -0.0299706459, -0.0264462233, -0.0275369436,\n",
            "        -0.0265904665, -0.0224007368, -0.0282403827, -0.0267426968,\n",
            "        -0.0194062144, -0.0265571177, -0.0271030664, -0.0227873921,\n",
            "        -0.0252432823, -0.0274070203, -0.0291809440, -0.0286608934])\n",
            "ctensor.grad: tensor([-1.1873620749, -1.1873620749, -1.1873620749, -1.0740528107,\n",
            "         0.0240860283, -0.6737408638,  0.2121477127, -0.1247302294,\n",
            "        -1.0740528107,  0.0240860283, -0.6737408638,  0.2121477127,\n",
            "        -0.1247302294, -1.0740528107,  0.0240860283, -0.6737408638,\n",
            "         0.2121477127, -0.1247302294])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2133789062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8351826072), tensor(1.7874677181), tensor(1.1141952276), tensor(1.4612348080), tensor(0.8597185016), tensor(0.7133474350), tensor(1.6988338232), tensor(1.1698796749), tensor(1.3469787836), tensor(1.0890556574), tensor(1.0246357918), tensor(1.0897397995), tensor(1.8235299587), tensor(1.2445023060), tensor(1.6459598541), tensor(1.8015364408), tensor(1.6002907753), tensor(1.1323112249), tensor(0.8869063258), tensor(1.1028069258)]\n",
            "b:  [tensor(1.6336263418), tensor(0.7265183330), tensor(1.4604090452), tensor(1.3090896606), tensor(1.0093189478), tensor(1.9955081940), tensor(1.2788074017), tensor(1.3981345892), tensor(1.2854857445), tensor(1.0641558170), tensor(1.3639469147), tensor(1.2914785147), tensor(0.7641208172), tensor(1.2650215626), tensor(1.2424809933), tensor(0.8984279633), tensor(1.1010069847), tensor(1.4033343792), tensor(1.3860547543), tensor(1.5546576977)]\n",
            "c:  [tensor(0.0107595660), tensor(0.0107595660), tensor(0.0107595660), tensor(0.0096823741), tensor(-0.0002237969), tensor(0.0058103581), tensor(-0.0020646222), tensor(0.0008523972), tensor(0.0096823741), tensor(-0.0002237969), tensor(0.0058103581), tensor(-0.0020646222), tensor(0.0008523972), tensor(0.0096823741), tensor(-0.0002237969), tensor(0.0058103581), tensor(-0.0020646222), tensor(0.0008523972)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0400501490, -0.0158660263, -0.0307539105, -0.0225577354,\n",
            "        -0.0374462605, -0.0445635319, -0.0196483135, -0.0299478173,\n",
            "        -0.0267461445, -0.0317098200, -0.0359184742, -0.0337087810,\n",
            "        -0.0142825246, -0.0299469233, -0.0206761956, -0.0140334591,\n",
            "        -0.0217953324, -0.0340867639, -0.0397273898, -0.0338910520])\n",
            "btensor.grad: tensor([-0.0300119817, -0.0209352076, -0.0284872055, -0.0294488072,\n",
            "        -0.0242177024, -0.0317333639, -0.0277048945, -0.0290111005,\n",
            "        -0.0279023647, -0.0235369205, -0.0298356712, -0.0280984640,\n",
            "        -0.0205393992, -0.0279426277, -0.0284481347, -0.0242437720,\n",
            "        -0.0266590118, -0.0285751224, -0.0307223201, -0.0299340487])\n",
            "ctensor.grad: tensor([-1.2694956064, -1.2694956064, -1.2694956064, -1.1466743946,\n",
            "         0.0256221406, -0.7227625251,  0.2245067805, -0.1371098161,\n",
            "        -1.1466743946,  0.0256221406, -0.7227625251,  0.2245067805,\n",
            "        -0.1371098161, -1.1466743946,  0.0256221406, -0.7227625251,\n",
            "         0.2245067805, -0.1371098161])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2078857422, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8353936672), tensor(1.7875508070), tensor(1.1143565178), tensor(1.4613527060), tensor(0.8599146605), tensor(0.7135825157), tensor(1.6989347935), tensor(1.1700366735), tensor(1.3471188545), tensor(1.0892218351), tensor(1.0248250961), tensor(1.0899168253), tensor(1.8236035109), tensor(1.2446599007), tensor(1.6460669041), tensor(1.8016103506), tensor(1.6004045010), tensor(1.1324906349), tensor(0.8871155381), tensor(1.1029852629)]\n",
            "b:  [tensor(1.6337848902), tensor(0.7266296148), tensor(1.4605597258), tensor(1.3092447519), tensor(1.0094466209), tensor(1.9956756830), tensor(1.2789524794), tensor(1.3982872963), tensor(1.2856320143), tensor(1.0642793179), tensor(1.3641043901), tensor(1.2916259766), tensor(0.7642294765), tensor(1.2651685476), tensor(1.2426302433), tensor(0.8985569477), tensor(1.1011477709), tensor(1.4034832716), tensor(1.3862164021), tensor(1.5548138618)]\n",
            "c:  [tensor(0.0114386640), tensor(0.0114386640), tensor(0.0114386640), tensor(0.0102948733), tensor(-0.0002374317), tensor(0.0061983354), tensor(-0.0021834471), tensor(0.0009278258), tensor(0.0102948733), tensor(-0.0002374317), tensor(0.0061983354), tensor(-0.0021834471), tensor(0.0009278258), tensor(0.0102948733), tensor(-0.0002374317), tensor(0.0061983354), tensor(-0.0021834471), tensor(0.0009278258)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0422128439, -0.0166065842, -0.0322515368, -0.0235844851,\n",
            "        -0.0392295122, -0.0470190048, -0.0201880932, -0.0314060748,\n",
            "        -0.0280233044, -0.0332236588, -0.0378514081, -0.0354118049,\n",
            "        -0.0147000551, -0.0315219164, -0.0214215666, -0.0147913247,\n",
            "        -0.0227385759, -0.0358795524, -0.0418385267, -0.0356670916])\n",
            "btensor.grad: tensor([-0.0317014270, -0.0222508907, -0.0301275551, -0.0310236216,\n",
            "        -0.0255260542, -0.0334997773, -0.0290074348, -0.0305409580,\n",
            "        -0.0292656422, -0.0247068405, -0.0315053165, -0.0295038819,\n",
            "        -0.0217326321, -0.0293861628, -0.0298599601, -0.0257925987,\n",
            "        -0.0281515121, -0.0297760665, -0.0323354602, -0.0312405825])\n",
            "ctensor.grad: tensor([-1.3581957817, -1.3581957817, -1.3581957817, -1.2249982357,\n",
            "         0.0272695925, -0.7759549022,  0.2376499325, -0.1508571804,\n",
            "        -1.2249982357,  0.0272695925, -0.7759549022,  0.2376499325,\n",
            "        -0.1508571804, -1.2249982357,  0.0272695925, -0.7759549022,\n",
            "         0.2376499325, -0.1508571804])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.2011718750, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8356159329), tensor(1.7876373529), tensor(1.1145254374), tensor(1.4614756107), tensor(0.8601198196), tensor(0.7138303518), tensor(1.6990383863), tensor(1.1702011824), tensor(1.3472654819), tensor(1.0893956423), tensor(1.0250244141), tensor(1.0901026726), tensor(1.8236787319), tensor(1.2448256016), tensor(1.6461777687), tensor(1.8016878366), tensor(1.6005229950), tensor(1.1326793432), tensor(0.8873357177), tensor(1.1031727791)]\n",
            "b:  [tensor(1.6339519024), tensor(0.7267478704), tensor(1.4607187510), tensor(1.3094080687), tensor(1.0095810890), tensor(1.9958519936), tensor(1.2791042328), tensor(1.3984478712), tensor(1.2857854366), tensor(1.0644087791), tensor(1.3642705679), tensor(1.2917808294), tensor(0.7643444538), tensor(1.2653229237), tensor(1.2427868843), tensor(0.8986941576), tensor(1.1012964249), tensor(1.4036382437), tensor(1.3863865137), tensor(1.5549767017)]\n",
            "c:  [tensor(0.0121657187), tensor(0.0121657187), tensor(0.0121657187), tensor(0.0109496508), tensor(-0.0002519508), tensor(0.0066151968), tensor(-0.0023092669), tensor(0.0010108901), tensor(0.0109496508), tensor(-0.0002519508), tensor(0.0066151968), tensor(-0.0023092669), tensor(0.0010108901), tensor(0.0109496508), tensor(-0.0002519508), tensor(0.0066151968), tensor(-0.0023092669), tensor(0.0010108901)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0444576144, -0.0173145682, -0.0337767601, -0.0245912075,\n",
            "        -0.0410341024, -0.0495710373, -0.0207173824, -0.0328972340,\n",
            "        -0.0293339137, -0.0347514749, -0.0398622900, -0.0371783078,\n",
            "        -0.0150474310, -0.0331507921, -0.0221649855, -0.0154959895,\n",
            "        -0.0236873031, -0.0377496481, -0.0440326929, -0.0375065506])\n",
            "btensor.grad: tensor([-0.0334073715, -0.0236502290, -0.0318032503, -0.0326739550,\n",
            "        -0.0268868804, -0.0352634192, -0.0303437710, -0.0321205109,\n",
            "        -0.0306727886, -0.0258978605, -0.0332418978, -0.0309631824,\n",
            "        -0.0229904614, -0.0308831334, -0.0313351452, -0.0274389386,\n",
            "        -0.0297205448, -0.0309989154, -0.0340272188, -0.0325738192])\n",
            "ctensor.grad: tensor([-1.4541087151, -1.4541087151, -1.4541087151, -1.3095549345,\n",
            "         0.0290380698, -0.8337225914,  0.2516394258, -0.1661286503,\n",
            "        -1.3095549345,  0.0290380698, -0.8337225914,  0.2516394258,\n",
            "        -0.1661286503, -1.3095549345,  0.0290380698, -0.8337225914,\n",
            "         0.2516394258, -0.1661286503])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.1938476562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8358498216), tensor(1.7877272367), tensor(1.1147019863), tensor(1.4616034031), tensor(0.8603340983), tensor(0.7140914202), tensor(1.6991444826), tensor(1.1703732014), tensor(1.3474187851), tensor(1.0895770788), tensor(1.0252342224), tensor(1.0902976990), tensor(1.8237552643), tensor(1.2449997663), tensor(1.6462922096), tensor(1.8017685413), tensor(1.6006461382), tensor(1.1328778267), tensor(0.8875672221), tensor(1.1033698320)]\n",
            "b:  [tensor(1.6341274977), tensor(0.7268735766), tensor(1.4608863592), tensor(1.3095800877), tensor(1.0097225904), tensor(1.9960370064), tensor(1.2792627811), tensor(1.3986166716), tensor(1.2859460115), tensor(1.0645443201), tensor(1.3644458055), tensor(1.2919431925), tensor(0.7644659877), tensor(1.2654850483), tensor(1.2429512739), tensor(0.8988401294), tensor(1.1014533043), tensor(1.4037994146), tensor(1.3865654469), tensor(1.5551463366)]\n",
            "c:  [tensor(0.0129446918), tensor(0.0129446918), tensor(0.0129446918), tensor(0.0116501171), tensor(-0.0002674199), tensor(0.0070634512), tensor(-0.0024425399), tensor(0.0011024384), tensor(0.0116501171), tensor(-0.0002674199), tensor(0.0070634512), tensor(-0.0024425399), tensor(0.0011024384), tensor(0.0116501171), tensor(-0.0002674199), tensor(0.0070634512), tensor(-0.0024425399), tensor(0.0011024384)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0467766523, -0.0179778486, -0.0353201032, -0.0255635977,\n",
            "        -0.0428498983, -0.0522185564, -0.0212225914, -0.0344092250,\n",
            "        -0.0306650940, -0.0362810493, -0.0419531912, -0.0390013605,\n",
            "        -0.0153128505, -0.0348308086, -0.0228953511, -0.0161321051,\n",
            "        -0.0246301889, -0.0397007465, -0.0463040471, -0.0394153595])\n",
            "btensor.grad: tensor([-0.0351255722, -0.0251411796, -0.0335139930, -0.0344003737,\n",
            "        -0.0283012316, -0.0370078385, -0.0317146778, -0.0337505788,\n",
            "        -0.0321155787, -0.0271106958, -0.0350459218, -0.0324644446,\n",
            "        -0.0243071243, -0.0324346125, -0.0328667462, -0.0291945338,\n",
            "        -0.0313682556, -0.0322322547, -0.0357938409, -0.0339206457])\n",
            "ctensor.grad: tensor([-1.5579456091, -1.5579456091, -1.5579456091, -1.4009318352,\n",
            "         0.0309382919, -0.8965089321,  0.2665457129, -0.1830965579,\n",
            "        -1.4009318352,  0.0309382919, -0.8965089321,  0.2665457129,\n",
            "        -0.1830965579, -1.4009318352,  0.0309382919, -0.8965089321,\n",
            "         0.2665457129, -0.1830965579])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.1856079102, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8360956311), tensor(1.7878201008), tensor(1.1148862839), tensor(1.4617358446), tensor(0.8605573773), tensor(0.7143661976), tensor(1.6992529631), tensor(1.1705528498), tensor(1.3475787640), tensor(1.0897660255), tensor(1.0254547596), tensor(1.0905020237), tensor(1.8238326311), tensor(1.2451825142), tensor(1.6464102268), tensor(1.8018519878), tensor(1.6007739305), tensor(1.1330864429), tensor(0.8878104687), tensor(1.1035767794)]\n",
            "b:  [tensor(1.6343116760), tensor(0.7270072103), tensor(1.4610626698), tensor(1.3097610474), tensor(1.0098714828), tensor(1.9962306023), tensor(1.2794283628), tensor(1.3987936974), tensor(1.2861139774), tensor(1.0646859407), tensor(1.3646303415), tensor(1.2921131849), tensor(0.7645944357), tensor(1.2656551600), tensor(1.2431235313), tensor(0.8989954591), tensor(1.1016187668), tensor(1.4039666653), tensor(1.3867535591), tensor(1.5553226471)]\n",
            "c:  [tensor(0.0137799466), tensor(0.0137799466), tensor(0.0137799466), tensor(0.0124000059), tensor(-0.0002839110), tensor(0.0075458526), tensor(-0.0025837647), tensor(0.0012034122), tensor(0.0124000059), tensor(-0.0002839110), tensor(0.0075458526), tensor(-0.0025837647), tensor(0.0012034122), tensor(0.0124000059), tensor(-0.0002839110), tensor(0.0075458526), tensor(-0.0025837647), tensor(0.0012034122)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0491644740, -0.0185792744, -0.0368647575, -0.0264788866,\n",
            "        -0.0446521044, -0.0549550056, -0.0216867924, -0.0359320343,\n",
            "        -0.0320031308, -0.0377964675, -0.0441116691, -0.0408707038,\n",
            "        -0.0154689550, -0.0365518332, -0.0235987902, -0.0166844539,\n",
            "        -0.0255545378, -0.0417207479, -0.0486469269, -0.0413798392])\n",
            "btensor.grad: tensor([-0.0368425921, -0.0267266929, -0.0352503359, -0.0361918509,\n",
            "        -0.0297682881, -0.0387192369, -0.0331072211, -0.0354168564,\n",
            "        -0.0335882902, -0.0283269882, -0.0369148552, -0.0339981914,\n",
            "        -0.0256876834, -0.0340266228, -0.0344484150, -0.0310600996,\n",
            "        -0.0330922604, -0.0334568620, -0.0376279354, -0.0352647305])\n",
            "ctensor.grad: tensor([-1.6705093384, -1.6705093384, -1.6705093384, -1.4997770786,\n",
            "         0.0329821929, -0.9648022652,  0.2824496925, -0.2019474059,\n",
            "        -1.4997770786,  0.0329821929, -0.9648022652,  0.2824496925,\n",
            "        -0.2019474059, -1.4997770786,  0.0329821929, -0.9648022652,\n",
            "         0.2824496925, -0.2019474059])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.1752929688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8363536596), tensor(1.7879155874), tensor(1.1150782108), tensor(1.4618724585), tensor(0.8607894778), tensor(0.7146550417), tensor(1.6993633509), tensor(1.1707401276), tensor(1.3477454185), tensor(1.0899623632), tensor(1.0256863832), tensor(1.0907158852), tensor(1.8239101171), tensor(1.2453740835), tensor(1.6465314627), tensor(1.8019376993), tensor(1.6009061337), tensor(1.1333054304), tensor(0.8880656958), tensor(1.1037937403)]\n",
            "b:  [tensor(1.6345044374), tensor(0.7271493077), tensor(1.4612476826), tensor(1.3099513054), tensor(1.0100278854), tensor(1.9964324236), tensor(1.2796008587), tensor(1.3989793062), tensor(1.2862893343), tensor(1.0648336411), tensor(1.3648245335), tensor(1.2922909260), tensor(0.7647300959), tensor(1.2658333778), tensor(1.2433038950), tensor(0.8991606832), tensor(1.1017931700), tensor(1.4041399956), tensor(1.3869512081), tensor(1.5555056334)]\n",
            "c:  [tensor(0.0146762868), tensor(0.0146762868), tensor(0.0146762868), tensor(0.0132034132), tensor(-0.0003015026), tensor(0.0080654183), tensor(-0.0027334874), tensor(0.0013148562), tensor(0.0132034132), tensor(-0.0003015026), tensor(0.0080654183), tensor(-0.0027334874), tensor(0.0013148562), tensor(0.0132034132), tensor(-0.0003015026), tensor(0.0080654183), tensor(-0.0027334874), tensor(0.0013148562)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0516096950, -0.0191030949, -0.0383942127, -0.0273166895,\n",
            "        -0.0464208126, -0.0577716827, -0.0220874548, -0.0374461114,\n",
            "        -0.0333387107, -0.0392765403, -0.0463330746, -0.0427809656,\n",
            "        -0.0154905319, -0.0383073092, -0.0242531300, -0.0171324685,\n",
            "        -0.0264459848, -0.0438045263, -0.0510481000, -0.0433916748])\n",
            "btensor.grad: tensor([-0.0385405943, -0.0284140706, -0.0370030999, -0.0380533934,\n",
            "        -0.0312764123, -0.0403736532, -0.0345047116, -0.0371180773,\n",
            "        -0.0350787640, -0.0295404196, -0.0388445556, -0.0355581641,\n",
            "        -0.0271318033, -0.0356521904, -0.0360743105, -0.0330449939,\n",
            "        -0.0348911285, -0.0346603692, -0.0395238996, -0.0365892649])\n",
            "ctensor.grad: tensor([-1.7926806211, -1.7926806211, -1.7926806211, -1.6068139076,\n",
            "         0.0351830758, -1.0391311646,  0.2994451821, -0.2228881121,\n",
            "        -1.6068139076,  0.0351830758, -1.0391311646,  0.2994451821,\n",
            "        -0.2228881121, -1.6068139076,  0.0351830758, -1.0391311646,\n",
            "         0.2994451821, -0.2228881121])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.1643676758, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8366241455), tensor(1.7880132198), tensor(1.1152776480), tensor(1.4620127678), tensor(0.8610301018), tensor(0.7149583101), tensor(1.6994754076), tensor(1.1709347963), tensor(1.3479186296), tensor(1.0901658535), tensor(1.0259294510), tensor(1.0909394026), tensor(1.8239867687), tensor(1.2455744743), tensor(1.6466556787), tensor(1.8020249605), tensor(1.6010425091), tensor(1.1335351467), tensor(0.8883332014), tensor(1.1040209532)]\n",
            "b:  [tensor(1.6347054243), tensor(0.7273003459), tensor(1.4614415169), tensor(1.3101511002), tensor(1.0101920366), tensor(1.9966421127), tensor(1.2797802687), tensor(1.3991734982), tensor(1.2864722013), tensor(1.0649873018), tensor(1.3650286198), tensor(1.2924766541), tensor(0.7648732662), tensor(1.2660199404), tensor(1.2434926033), tensor(0.8993364573), tensor(1.1019769907), tensor(1.4043190479), tensor(1.3871586323), tensor(1.5556949377)]\n",
            "c:  [tensor(0.0156390201), tensor(0.0156390201), tensor(0.0156390201), tensor(0.0140648307), tensor(-0.0003202805), tensor(0.0086254543), tensor(-0.0028923077), tensor(0.0014379235), tensor(0.0140648307), tensor(-0.0003202805), tensor(0.0086254543), tensor(-0.0028923077), tensor(0.0014379235), tensor(0.0140648307), tensor(-0.0003202805), tensor(0.0086254543), tensor(-0.0028923077), tensor(0.0014379235)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0540973544, -0.0195207596, -0.0398856997, -0.0280517340,\n",
            "        -0.0481234789, -0.0606490374, -0.0224027634, -0.0389355123,\n",
            "        -0.0346513614, -0.0406945050, -0.0486066490, -0.0447122306,\n",
            "        -0.0153396726, -0.0400798321, -0.0248394608, -0.0174480006,\n",
            "        -0.0272814631, -0.0459437370, -0.0534958839, -0.0454415977])\n",
            "btensor.grad: tensor([-0.0402045101, -0.0302032828, -0.0387600362, -0.0399700999,\n",
            "        -0.0328232422, -0.0419445336, -0.0358917713, -0.0388339460,\n",
            "        -0.0365719795, -0.0307291746, -0.0408273935, -0.0371347070,\n",
            "        -0.0286341757, -0.0373012125, -0.0377314389, -0.0351565480,\n",
            "        -0.0367603302, -0.0358101428, -0.0414759517, -0.0378689766])\n",
            "ctensor.grad: tensor([-1.9254678488, -1.9254678488, -1.9254678488, -1.7228342295,\n",
            "         0.0375557691, -1.1200720072,  0.3176404834, -0.2461346686,\n",
            "        -1.7228342295,  0.0375557691, -1.1200720072,  0.3176404834,\n",
            "        -0.2461346686, -1.7228342295,  0.0375557691, -1.1200720072,\n",
            "         0.3176404834, -0.2461346686])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.1516113281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8369072080), tensor(1.7881122828), tensor(1.1154842377), tensor(1.4621560574), tensor(0.8612787127), tensor(0.7152761817), tensor(1.6995884180), tensor(1.1711366177), tensor(1.3480981588), tensor(1.0903759003), tensor(1.0261839628), tensor(1.0911725760), tensor(1.8240616322), tensor(1.2457838058), tensor(1.6467822790), tensor(1.8021129370), tensor(1.6011826992), tensor(1.1337757111), tensor(0.8886130452), tensor(1.1042584181)]\n",
            "b:  [tensor(1.6349145174), tensor(0.7274608612), tensor(1.4616440535), tensor(1.3103607893), tensor(1.0103640556), tensor(1.9968590736), tensor(1.2799664736), tensor(1.3993762732), tensor(1.2866624594), tensor(1.0651466846), tensor(1.3652428389), tensor(1.2926701307), tensor(0.7650242448), tensor(1.2662147284), tensor(1.2436896563), tensor(0.8995234370), tensor(1.1021704674), tensor(1.4045034647), tensor(1.3873759508), tensor(1.5558903217)]\n",
            "c:  [tensor(0.0166740082), tensor(0.0166740082), tensor(0.0166740082), tensor(0.0149891898), tensor(-0.0003403389), tensor(0.0092295762), tensor(-0.0030608906), tensor(0.0015738840), tensor(0.0149891898), tensor(-0.0003403389), tensor(0.0092295762), tensor(-0.0030608906), tensor(0.0015738840), tensor(0.0149891898), tensor(-0.0003403389), tensor(0.0092295762), tensor(-0.0030608906), tensor(0.0015738840)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0566077232, -0.0198024809, -0.0413075686, -0.0286473036,\n",
            "        -0.0497204065, -0.0635749102, -0.0225911140, -0.0403707027,\n",
            "        -0.0359139517, -0.0420131981, -0.0509117693, -0.0466432795,\n",
            "        -0.0149771571, -0.0418547392, -0.0253257006, -0.0176024586,\n",
            "        -0.0280386209, -0.0481238961, -0.0559667349, -0.0475037694])\n",
            "btensor.grad: tensor([-0.0418086834, -0.0321042240, -0.0404996872, -0.0419364870,\n",
            "        -0.0343947485, -0.0433925986, -0.0372471213, -0.0405500084,\n",
            "        -0.0380448103, -0.0318754911, -0.0428494513, -0.0386980772,\n",
            "        -0.0301959459, -0.0389542878, -0.0394009650, -0.0373993516,\n",
            "        -0.0386927128, -0.0368806720, -0.0434684157, -0.0390739441])\n",
            "ctensor.grad: tensor([-2.0699779987, -2.0699779987, -2.0699779987, -1.8487185240,\n",
            "         0.0401169062, -1.2082430124,  0.3371657133, -0.2719210982,\n",
            "        -1.8487185240,  0.0401169062, -1.2082430124,  0.3371657133,\n",
            "        -0.2719210982, -1.8487185240,  0.0401169062, -1.2082430124,\n",
            "         0.3371657133, -0.2719210982])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.1366577148, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8372027874), tensor(1.7882118225), tensor(1.1156973839), tensor(1.4623013735), tensor(0.8615345359), tensor(0.7156087756), tensor(1.6997015476), tensor(1.1713452339), tensor(1.3482836485), tensor(1.0905917883), tensor(1.0264501572), tensor(1.0914152861), tensor(1.8241333961), tensor(1.2460018396), tensor(1.6469106674), tensor(1.8022006750), tensor(1.6013261080), tensor(1.1340273619), tensor(0.8889052272), tensor(1.1045062542)]\n",
            "b:  [tensor(1.6351311207), tensor(0.7276314497), tensor(1.4618550539), tensor(1.3105804920), tensor(1.0105439425), tensor(1.9970824718), tensor(1.2801592350), tensor(1.3995875120), tensor(1.2868598700), tensor(1.0653114319), tensor(1.3654673100), tensor(1.2928713560), tensor(0.7651832700), tensor(1.2664177418), tensor(1.2438950539), tensor(0.8997223377), tensor(1.1023738384), tensor(1.4046926498), tensor(1.3876034021), tensor(1.5560911894)]\n",
            "c:  [tensor(0.0177877285), tensor(0.0177877285), tensor(0.0177877285), tensor(0.0159819033), tensor(-0.0003617815), tensor(0.0098817246), tensor(-0.0032399774), tensor(0.0017241266), tensor(0.0159819033), tensor(-0.0003617815), tensor(0.0098817246), tensor(-0.0032399774), tensor(0.0017241266), tensor(0.0159819033), tensor(-0.0003617815), tensor(0.0098817246), tensor(-0.0032399774), tensor(0.0017241266)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0591139793, -0.0199131668, -0.0426238775, -0.0290611982,\n",
            "        -0.0511605740, -0.0665189028, -0.0226206779, -0.0417191684,\n",
            "        -0.0371013209, -0.0431885123, -0.0532337725, -0.0485506952,\n",
            "        -0.0143509507, -0.0436094999, -0.0256801099, -0.0175518394,\n",
            "        -0.0286788344, -0.0503262281, -0.0584393144, -0.0495640039])\n",
            "btensor.grad: tensor([-0.0433193222, -0.0341188908, -0.0422006845, -0.0439315140,\n",
            "        -0.0359765142, -0.0446742475, -0.0385407209, -0.0422455817,\n",
            "        -0.0394763947, -0.0329519510, -0.0449010730, -0.0402336121,\n",
            "        -0.0318045989, -0.0405918062, -0.0410685241, -0.0397813320,\n",
            "        -0.0406818390, -0.0378309786, -0.0454936624, -0.0401642323])\n",
            "ctensor.grad: tensor([-2.2274386883, -2.2274386883, -2.2274386883, -1.9854285717,\n",
            "         0.0428851098, -1.3042968512,  0.3581734598, -0.3004850745,\n",
            "        -1.9854285717,  0.0428851098, -1.3042968512,  0.3581734598,\n",
            "        -0.3004850745, -1.9854285717,  0.0428851098, -1.3042968512,\n",
            "         0.3581734598, -0.3004850745])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.1188964844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8375107050), tensor(1.7883108854), tensor(1.1159163713), tensor(1.4624475241), tensor(0.8617964387), tensor(0.7159560323), tensor(1.6998137236), tensor(1.1715599298), tensor(1.3484745026), tensor(1.0908126831), tensor(1.0267277956), tensor(1.0916672945), tensor(1.8242003918), tensor(1.2462284565), tensor(1.6470398903), tensor(1.8022868633), tensor(1.6014719009), tensor(1.1342899799), tensor(0.8892095685), tensor(1.1047642231)]\n",
            "b:  [tensor(1.6353546381), tensor(0.7278127074), tensor(1.4620741606), tensor(1.3108102083), tensor(1.0107316971), tensor(1.9973111153), tensor(1.2803578377), tensor(1.3998069763), tensor(1.2870640755), tensor(1.0654810667), tensor(1.3657021523), tensor(1.2930798531), tensor(0.7653505802), tensor(1.2666286230), tensor(1.2441085577), tensor(0.8999338746), tensor(1.1025874615), tensor(1.4048857689), tensor(1.3878409863), tensor(1.5562967062)]\n",
            "c:  [tensor(0.0189873427), tensor(0.0189873427), tensor(0.0189873427), tensor(0.0170489121), tensor(-0.0003847221), tensor(0.0105861835), tensor(-0.0034304026), tensor(0.0018901584), tensor(0.0170489121), tensor(-0.0003847221), tensor(0.0105861835), tensor(-0.0034304026), tensor(0.0018901584), tensor(0.0170489121), tensor(-0.0003847221), tensor(0.0105861835), tensor(-0.0034304026), tensor(0.0018901584)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0615798235, -0.0198096037, -0.0437881947, -0.0292412043,\n",
            "        -0.0523831844, -0.0694485903, -0.0224419832, -0.0429373682,\n",
            "        -0.0381708331, -0.0441716909, -0.0555364788, -0.0504027903,\n",
            "        -0.0133960843, -0.0453157425, -0.0258546919, -0.0172478929,\n",
            "        -0.0291669369, -0.0525228977, -0.0608735681, -0.0515873730])\n",
            "btensor.grad: tensor([-0.0446953177, -0.0362541974, -0.0438319445, -0.0459382236,\n",
            "        -0.0375525355, -0.0457338989, -0.0397313237, -0.0438888073,\n",
            "        -0.0408295393, -0.0339236259, -0.0469577610, -0.0417071581,\n",
            "        -0.0334574804, -0.0421871543, -0.0427012444, -0.0423055887,\n",
            "        -0.0427134037, -0.0386156440, -0.0475205779, -0.0410950184])\n",
            "ctensor.grad: tensor([-2.3992266655, -2.3992266655, -2.3992266655, -2.1340188980,\n",
            "         0.0458813272, -1.4089180231,  0.3808502257, -0.3320635557,\n",
            "        -2.1340188980,  0.0458813272, -1.4089180231,  0.3808502257,\n",
            "        -0.3320635557, -2.1340188980,  0.0458813272, -1.4089180231,\n",
            "         0.3808502257, -0.3320635557])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.0993041992, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8378305435), tensor(1.7884080410), tensor(1.1161401272), tensor(1.4625930786), tensor(0.8620629907), tensor(0.7163176537), tensor(1.6999237537), tensor(1.1717797518), tensor(1.3486698866), tensor(1.0910371542), tensor(1.0270167589), tensor(1.0919280052), tensor(1.8242605925), tensor(1.2464630604), tensor(1.6471688747), tensor(1.8023699522), tensor(1.6016191244), tensor(1.1345634460), tensor(0.8895257115), tensor(1.1050318480)]\n",
            "b:  [tensor(1.6355839968), tensor(0.7280052900), tensor(1.4623008966), tensor(1.3110498190), tensor(1.0109272003), tensor(1.9975435734), tensor(1.2805618048), tensor(1.4000341892), tensor(1.2872743607), tensor(1.0656547546), tensor(1.3659471273), tensor(1.2932952642), tensor(0.7655262947), tensor(1.2668471336), tensor(1.2443299294), tensor(0.9001587629), tensor(1.1028113365), tensor(1.4050816298), tensor(1.3880885839), tensor(1.5565056801)]\n",
            "c:  [tensor(0.0202807710), tensor(0.0202807710), tensor(0.0202807710), tensor(0.0181967318), tensor(-0.0004092867), tensor(0.0113475844), tensor(-0.0036331140), tensor(0.0020735995), tensor(0.0181967318), tensor(-0.0004092867), tensor(0.0113475844), tensor(-0.0036331140), tensor(0.0020735995), tensor(0.0181967318), tensor(-0.0004092867), tensor(0.0113475844), tensor(-0.0036331140), tensor(0.0020735995)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0639652014, -0.0194279850, -0.0447443128, -0.0291215181,\n",
            "        -0.0533101559, -0.0723189116, -0.0219972134, -0.0439738035,\n",
            "        -0.0390767679, -0.0448881090, -0.0577935725, -0.0521527976,\n",
            "        -0.0120378137, -0.0469319820, -0.0257970840, -0.0166266449,\n",
            "        -0.0294511318, -0.0546817183, -0.0632343888, -0.0535342097])\n",
            "btensor.grad: tensor([-0.0458829179, -0.0385114849, -0.0453535020, -0.0479328930,\n",
            "        -0.0390966088, -0.0464976728, -0.0407860279, -0.0454464555,\n",
            "        -0.0420651436, -0.0347464085, -0.0489906371, -0.0430846810,\n",
            "        -0.0351404138, -0.0437076092, -0.0442690253, -0.0449779034,\n",
            "        -0.0447664261, -0.0391729772, -0.0495288968, -0.0418057442])\n",
            "ctensor.grad: tensor([-2.5868573189, -2.5868573189, -2.5868573189, -2.2956402302,\n",
            "         0.0491290800, -1.5228011608,  0.4054229259, -0.3668821454,\n",
            "        -2.2956402302,  0.0491290800, -1.5228011608,  0.4054229259,\n",
            "        -0.3668821454, -2.2956402302,  0.0491290800, -1.5228011608,\n",
            "         0.4054229259, -0.3668821454])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.0764160156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8381615877), tensor(1.7885015011), tensor(1.1163672209), tensor(1.4627361298), tensor(0.8623322248), tensor(0.7166930437), tensor(1.7000298500), tensor(1.1720036268), tensor(1.3488687277), tensor(1.0912634134), tensor(1.0273165703), tensor(1.0921968222), tensor(1.8243114948), tensor(1.2467051744), tensor(1.6472960711), tensor(1.8024480343), tensor(1.6017664671), tensor(1.1348472834), tensor(0.8898530006), tensor(1.1053086519)]\n",
            "b:  [tensor(1.6358181238), tensor(0.7282097340), tensor(1.4625344276), tensor(1.3112992048), tensor(1.0111300945), tensor(1.9977779388), tensor(1.2807699442), tensor(1.4002685547), tensor(1.2874900103), tensor(1.0658316612), tensor(1.3662019968), tensor(1.2935168743), tensor(0.7657104731), tensor(1.2670726776), tensor(1.2445585728), tensor(0.9003977776), tensor(1.1030454636), tensor(1.4052788019), tensor(1.3883459568), tensor(1.5567167997)]\n",
            "c:  [tensor(0.0216767639), tensor(0.0216767639), tensor(0.0216767639), tensor(0.0194324963), tensor(-0.0004356141), tensor(0.0121708969), tensor(-0.0038491993), tensor(0.0022761682), tensor(0.0194324963), tensor(-0.0004356141), tensor(0.0121708969), tensor(-0.0038491993), tensor(0.0022761682), tensor(0.0194324963), tensor(-0.0004356141), tensor(0.0121708969), tensor(-0.0038491993), tensor(0.0022761682)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0662099123, -0.0187030286, -0.0454194546, -0.0286188126,\n",
            "        -0.0538455248, -0.0750720501, -0.0212121010, -0.0447660685,\n",
            "        -0.0397605523, -0.0452563763, -0.0599505007, -0.0537532568,\n",
            "        -0.0101841688, -0.0484166145, -0.0254409760, -0.0156151131,\n",
            "        -0.0294706821, -0.0567659736, -0.0654575229, -0.0553603470])\n",
            "btensor.grad: tensor([-0.0468209088, -0.0408857167, -0.0467135310, -0.0498833954,\n",
            "        -0.0405762196, -0.0468796790, -0.0416362882, -0.0468699932,\n",
            "        -0.0431314707, -0.0353711843, -0.0509725809, -0.0443159342,\n",
            "        -0.0368387401, -0.0451065898, -0.0457288325, -0.0478029251,\n",
            "        -0.0468187332, -0.0394298434, -0.0514784455, -0.0422258377])\n",
            "ctensor.grad: tensor([-2.7919840813, -2.7919840813, -2.7919840813, -2.4715270996,\n",
            "         0.0526548438, -1.6466252804,  0.4321702719, -0.4051373303,\n",
            "        -2.4715270996,  0.0526548438, -1.6466252804,  0.4321702719,\n",
            "        -0.4051373303, -2.4715270996,  0.0526548438, -1.6466252804,\n",
            "         0.4321702719, -0.4051373303])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.0496826172, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8385027647), tensor(1.7885892391), tensor(1.1165958643), tensor(1.4628742933), tensor(0.8626015782), tensor(0.7170811892), tensor(1.7001298666), tensor(1.1722297668), tensor(1.3490694761), tensor(1.0914893150), tensor(1.0276262760), tensor(1.0924724340), tensor(1.8243501186), tensor(1.2469537258), tensor(1.6474195719), tensor(1.8025186062), tensor(1.6019122601), tensor(1.1351408958), tensor(0.8901903629), tensor(1.1055936813)]\n",
            "b:  [tensor(1.6360552311), tensor(0.7284266353), tensor(1.4627736807), tensor(1.3115578890), tensor(1.0113397837), tensor(1.9980118275), tensor(1.2809810638), tensor(1.4005089998), tensor(1.2877098322), tensor(1.0660102367), tensor(1.3664662838), tensor(1.2937434912), tensor(0.7659031153), tensor(1.2673043013), tensor(1.2447936535), tensor(0.9006516933), tensor(1.1032897234), tensor(1.4054752588), tensor(1.3886126280), tensor(1.5569281578)]\n",
            "c:  [tensor(0.0231849831), tensor(0.0231849831), tensor(0.0231849831), tensor(0.0207639933), tensor(-0.0004638583), tensor(0.0130614052), tensor(-0.0040799198), tensor(0.0024996488), tensor(0.0207639933), tensor(-0.0004638583), tensor(0.0130614052), tensor(-0.0040799198), tensor(0.0024996488), tensor(0.0207639933), tensor(-0.0004638583), tensor(0.0130614052), tensor(-0.0040799198), tensor(0.0024996488)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0682402253, -0.0175477415, -0.0457210541, -0.0276318789,\n",
            "        -0.0538662672, -0.0776335001, -0.0199984312, -0.0452254415,\n",
            "        -0.0401465297, -0.0451767147, -0.0619489402, -0.0551282838,\n",
            "        -0.0077215433, -0.0497035980, -0.0247037560, -0.0141188428,\n",
            "        -0.0291475058, -0.0587180257, -0.0674728751, -0.0569988787])\n",
            "btensor.grad: tensor([-0.0474207737, -0.0433804691, -0.0478485525, -0.0517414212,\n",
            "        -0.0419481769, -0.0467697680, -0.0422189236, -0.0480984002,\n",
            "        -0.0439612865, -0.0357257128, -0.0528537333, -0.0453335047,\n",
            "        -0.0385286920, -0.0463253260, -0.0470235646, -0.0507791638,\n",
            "        -0.0488426685, -0.0392978489, -0.0533237457, -0.0422613621])\n",
            "ctensor.grad: tensor([-3.0164370537, -3.0164370537, -3.0164370537, -2.6629934311,\n",
            "         0.0564884469, -1.7810167074,  0.4614413977, -0.4469613433,\n",
            "        -2.6629934311,  0.0564884469, -1.7810167074,  0.4614413977,\n",
            "        -0.4469613433, -2.6629934311,  0.0564884469, -1.7810167074,\n",
            "         0.4614413977, -0.4469613433])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(741.0179443359, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8388526440), tensor(1.7886685133), tensor(1.1168235540), tensor(1.4630044699), tensor(0.8628677130), tensor(0.7174807191), tensor(1.7002211809), tensor(1.1724560261), tensor(1.3492702246), tensor(1.0917118788), tensor(1.0279449224), tensor(1.0927534103), tensor(1.8243727684), tensor(1.2472072840), tensor(1.6475369930), tensor(1.8025788069), tensor(1.6020542383), tensor(1.1354432106), tensor(0.8905363083), tensor(1.1058855057)]\n",
            "b:  [tensor(1.6362931728), tensor(0.7286565900), tensor(1.4630171061), tensor(1.3118251562), tensor(1.0115556717), tensor(1.9982420206), tensor(1.2811932564), tensor(1.4007543325), tensor(1.2879321575), tensor(1.0661889315), tensor(1.3667391539), tensor(1.2939738035), tensor(0.7661039829), tensor(1.2675408125), tensor(1.2450340986), tensor(0.9009212255), tensor(1.1035437584), tensor(1.4056686163), tensor(1.3888876438), tensor(1.5571371317)]\n",
            "c:  [tensor(0.0248160586), tensor(0.0248160586), tensor(0.0248160586), tensor(0.0221997034), tensor(-0.0004941901), tensor(0.0140246535), tensor(-0.0043267547), tensor(0.0027458468), tensor(0.0221997034), tensor(-0.0004941901), tensor(0.0140246535), tensor(-0.0043267547), tensor(0.0027458468), tensor(0.0221997034), tensor(-0.0004941901), tensor(0.0140246535), tensor(-0.0043267547), tensor(0.0027458468)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0699709654, -0.0158600062, -0.0455407500, -0.0260417461,\n",
            "        -0.0532274246, -0.0799117088, -0.0182522535, -0.0452582538,\n",
            "        -0.0401456207, -0.0445198119, -0.0637187660, -0.0562023893,\n",
            "        -0.0045192242, -0.0507169962, -0.0234877169, -0.0120288543,\n",
            "        -0.0283872485, -0.0604702830, -0.0691921115, -0.0583694577])\n",
            "btensor.grad: tensor([-0.0475871526, -0.0459946990, -0.0486806929, -0.0534570217,\n",
            "        -0.0431662500, -0.0460342467, -0.0424428582, -0.0490600020,\n",
            "        -0.0444753170, -0.0357319117, -0.0545796454, -0.0460743308,\n",
            "        -0.0401793867, -0.0472988486, -0.0480899811, -0.0539073348,\n",
            "        -0.0507977009, -0.0386668146, -0.0550044775, -0.0418018103])\n",
            "ctensor.grad: tensor([-3.2621498108, -3.2621498108, -3.2621498108, -2.8714194298,\n",
            "         0.0606635250, -1.9264957905,  0.4936695397, -0.4923959076,\n",
            "        -2.8714194298,  0.0606635250, -1.9264957905,  0.4936695397,\n",
            "        -0.4923959076, -2.8714194298,  0.0606635250, -1.9264957905,\n",
            "         0.4936695397, -0.4923959076])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.9814453125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8392090201), tensor(1.7887361050), tensor(1.1170473099), tensor(1.4631229639), tensor(0.8631264567), tensor(0.7178896666), tensor(1.7003004551), tensor(1.1726796627), tensor(1.3494684696), tensor(1.0919275284), tensor(1.0282707214), tensor(1.0930377245), tensor(1.8243747950), tensor(1.2474641800), tensor(1.6476453543), tensor(1.8026248217), tensor(1.6021896601), tensor(1.1357529163), tensor(0.8908888102), tensor(1.1061824560)]\n",
            "b:  [tensor(1.6365290880), tensor(0.7289001942), tensor(1.4632626772), tensor(1.3120999336), tensor(1.0117764473), tensor(1.9984645844), tensor(1.2814042568), tensor(1.4010026455), tensor(1.2881550789), tensor(1.0663653612), tensor(1.3670195341), tensor(1.2942060232), tensor(0.7663127780), tensor(1.2677805424), tensor(1.2452782393), tensor(0.9012071490), tensor(1.1038069725), tensor(1.4058556557), tensor(1.3891699314), tensor(1.5573407412)]\n",
            "c:  [tensor(0.0265816581), tensor(0.0265816581), tensor(0.0265816581), tensor(0.0237488076), tensor(-0.0005267990), tensor(0.0150663508), tensor(-0.0045914566), tensor(0.0030165126), tensor(0.0237488076), tensor(-0.0005267990), tensor(0.0150663508), tensor(-0.0045914566), tensor(0.0030165126), tensor(0.0237488076), tensor(-0.0005267990), tensor(0.0150663508), tensor(-0.0045914566), tensor(0.0030165126)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0712770224, -0.0135128498, -0.0447437763, -0.0237005949,\n",
            "        -0.0517522097, -0.0817891359, -0.0158491135, -0.0447374582,\n",
            "        -0.0396444350, -0.0431307554, -0.0651608407, -0.0568654537,\n",
            "        -0.0004119873, -0.0513681173, -0.0216732472, -0.0092106014,\n",
            "        -0.0270785689, -0.0619406104, -0.0705049634, -0.0593823791])\n",
            "btensor.grad: tensor([-0.0471897274, -0.0487155318, -0.0491135418, -0.0549624264,\n",
            "        -0.0441589952, -0.0445105433, -0.0422075391, -0.0496638119,\n",
            "        -0.0445775986, -0.0352847576, -0.0560815930, -0.0464392304,\n",
            "        -0.0417601056, -0.0479356349, -0.0488389432, -0.0571827888,\n",
            "        -0.0526316166, -0.0373986065, -0.0564519763, -0.0407140255])\n",
            "ctensor.grad: tensor([-3.5311980247, -3.5311980247, -3.5311980247, -3.0982067585,\n",
            "         0.0652178079, -2.0833945274,  0.5294039845, -0.5413316488,\n",
            "        -3.0982067585,  0.0652178079, -2.0833945274,  0.5294039845,\n",
            "        -0.5413316488, -3.0982067585,  0.0652178079, -2.0833945274,\n",
            "         0.5294039845, -0.5413316488])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.9392089844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8395691514), tensor(1.7887879610), tensor(1.1172631979), tensor(1.4632251263), tensor(0.8633725643), tensor(0.7183052897), tensor(1.7003636360), tensor(1.1728972197), tensor(1.3496609926), tensor(1.0921316147), tensor(1.0286015272), tensor(1.0933226347), tensor(1.8243508339), tensor(1.2477219105), tensor(1.6477409601), tensor(1.8026523590), tensor(1.6023150682), tensor(1.1360679865), tensor(0.8912451863), tensor(1.1064820290)]\n",
            "b:  [tensor(1.6367595196), tensor(0.7291578650), tensor(1.4635077715), tensor(1.3123807907), tensor(1.0120006800), tensor(1.9986746311), tensor(1.2816112041), tensor(1.4012516737), tensor(1.2883757353), tensor(1.0665366650), tensor(1.3673058748), tensor(1.2944376469), tensor(0.7665289044), tensor(1.2680212259), tensor(1.2455240488), tensor(0.9015101194), tensor(1.1040784121), tensor(1.4060323238), tensor(1.3894578218), tensor(1.5575349331)]\n",
            "c:  [tensor(0.0284945164), tensor(0.0284945164), tensor(0.0284945164), tensor(0.0254211817), tensor(-0.0005618959), tensor(0.0161922239), tensor(-0.0048761242), tensor(0.0033132327), tensor(0.0254211817), tensor(-0.0005618959), tensor(0.0161922239), tensor(-0.0048761242), tensor(0.0033132327), tensor(0.0254211817), tensor(-0.0005618959), tensor(0.0161922239), tensor(-0.0048761242), tensor(0.0033132327)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0720303059, -0.0103593767, -0.0431672037, -0.0204309225,\n",
            "        -0.0492213964, -0.0831199884, -0.0126311779, -0.0435160100,\n",
            "        -0.0385094173, -0.0408235490, -0.0661639273, -0.0569892973,\n",
            "         0.0047867894, -0.0515378714, -0.0191167295, -0.0055061989,\n",
            "        -0.0250855088, -0.0630234480, -0.0712729692, -0.0599084496])\n",
            "btensor.grad: tensor([-0.0460857004, -0.0515325367, -0.0490283966, -0.0561715961,\n",
            "        -0.0448536351, -0.0420047045, -0.0413861275, -0.0497986972,\n",
            "        -0.0441427231, -0.0342674255, -0.0572685003, -0.0463140011,\n",
            "        -0.0432200879, -0.0481365025, -0.0491687059, -0.0605986714,\n",
            "        -0.0542802811, -0.0353361070, -0.0575737953, -0.0388363600])\n",
            "ctensor.grad: tensor([-3.8257150650, -3.8257150650, -3.8257150650, -3.3447463512,\n",
            "         0.0701937228, -2.2517461777,  0.5693355799, -0.5934399366,\n",
            "        -3.3447463512,  0.0701937228, -2.2517461777,  0.5693355799,\n",
            "        -0.5934399366, -3.3447463512,  0.0701937228, -2.2517461777,\n",
            "         0.5693355799, -0.5934399366])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.8903198242, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8399294019), tensor(1.7888190746), tensor(1.1174663305), tensor(1.4633052349), tensor(0.8635994792), tensor(0.7187239528), tensor(1.7004057169), tensor(1.1731042862), tensor(1.3498438597), tensor(1.0923185349), tensor(1.0289344788), tensor(1.0936048031), tensor(1.8242943287), tensor(1.2479773760), tensor(1.6478191614), tensor(1.8026559353), tensor(1.6024262905), tensor(1.1363859177), tensor(0.8916018605), tensor(1.1067811251)]\n",
            "b:  [tensor(1.6369799376), tensor(0.7294300199), tensor(1.4637491703), tensor(1.3126657009), tensor(1.0122264624), tensor(1.9988660812), tensor(1.2818102837), tensor(1.4014983177), tensor(1.2885909081), tensor(1.0666993856), tensor(1.3675960302), tensor(1.2946654558), tensor(0.7667514086), tensor(1.2682601213), tensor(1.2457687855), tensor(0.9018307924), tensor(1.1043567657), tensor(1.4061937332), tensor(1.3897491693), tensor(1.5577148199)]\n",
            "c:  [tensor(0.0305684302), tensor(0.0305684302), tensor(0.0305684302), tensor(0.0272273459), tensor(-0.0005997152), tensor(0.0174078010), tensor(-0.0051832930), tensor(0.0036372719), tensor(0.0272273459), tensor(-0.0005997152), tensor(0.0174078010), tensor(-0.0051832930), tensor(0.0036372719), tensor(0.0272273459), tensor(-0.0005997152), tensor(0.0174078010), tensor(-0.0051832930), tensor(0.0036372719)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0720557570, -0.0062182099, -0.0406151414, -0.0160259008,\n",
            "        -0.0453827381, -0.0837318897, -0.0084233284, -0.0414167643,\n",
            "        -0.0365785733, -0.0373775959, -0.0665888041, -0.0564239547,\n",
            "         0.0112987757, -0.0510895252, -0.0156473219, -0.0007257760,\n",
            "        -0.0222394466, -0.0635919571, -0.0713320971, -0.0598126054])\n",
            "btensor.grad: tensor([-0.0440878943, -0.0544312894, -0.0482821465, -0.0569835007,\n",
            "        -0.0451535657, -0.0382818580, -0.0398234725, -0.0493328273,\n",
            "        -0.0430326462, -0.0325359106, -0.0580376387, -0.0455650091,\n",
            "        -0.0445042551, -0.0477725267, -0.0489572883, -0.0641385913,\n",
            "        -0.0556697845, -0.0322879851, -0.0582631826, -0.0359730721])\n",
            "ctensor.grad: tensor([-4.1478285789, -4.1478285789, -4.1478285789, -3.6123287678,\n",
            "         0.0756385773, -2.4311528206,  0.6143379807, -0.6480787396,\n",
            "        -3.6123287678,  0.0756385773, -2.4311528206,  0.6143379807,\n",
            "        -0.6480787396, -3.6123287678,  0.0756385773, -2.4311528206,\n",
            "         0.6143379807, -0.6480787396])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.8307495117, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8402851820), tensor(1.7888234854), tensor(1.1176506281), tensor(1.4633564949), tensor(0.8637991548), tensor(0.7191410661), tensor(1.7004207373), tensor(1.1732953787), tensor(1.3500121832), tensor(1.0924812555), tensor(1.0292657614), tensor(1.0938796997), tensor(1.8241974115), tensor(1.2482266426), tensor(1.6478744745), tensor(1.8026292324), tensor(1.6025180817), tensor(1.1367033720), tensor(0.8919543028), tensor(1.1070756912)]\n",
            "b:  [tensor(1.6371848583), tensor(0.7297169566), tensor(1.4639827013), tensor(1.3129520416), tensor(1.0124511719), tensor(1.9990314245), tensor(1.2819969654), tensor(1.4017388821), tensor(1.2887963057), tensor(1.0668489933), tensor(1.3678873777), tensor(1.2948856354), tensor(0.7669791579), tensor(1.2684935331), tensor(1.2460091114), tensor(0.9021697044), tensor(1.1046402454), tensor(1.4063339233), tensor(1.3900411129), tensor(1.5578743219)]\n",
            "c:  [tensor(0.0328182168), tensor(0.0328182168), tensor(0.0328182168), tensor(0.0291783717), tensor(-0.0006405177), tensor(0.0187180918), tensor(-0.0055160490), tensor(0.0039893589), tensor(0.0291783717), tensor(-0.0006405177), tensor(0.0187180918), tensor(-0.0055160490), tensor(0.0039893589), tensor(0.0291783717), tensor(-0.0006405177), tensor(0.0187180918), tensor(-0.0055160490), tensor(0.0039893589)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0711539984, -0.0008847117, -0.0368594229, -0.0102431774,\n",
            "        -0.0399349928, -0.0834233761, -0.0030145645, -0.0382273197,\n",
            "        -0.0336569212, -0.0325342119, -0.0662653297, -0.0549732968,\n",
            "         0.0193805099, -0.0498567820, -0.0110637993,  0.0053475015,\n",
            "        -0.0183485746, -0.0634931326, -0.0704870224, -0.0589163601])\n",
            "btensor.grad: tensor([-0.0409856699, -0.0573875904, -0.0467043817, -0.0572759211,\n",
            "        -0.0449429601, -0.0330670476, -0.0373419523, -0.0481089205,\n",
            "        -0.0410730839, -0.0299147367, -0.0582616329, -0.0440353751,\n",
            "        -0.0455486029, -0.0466915667, -0.0480571985, -0.0677880645,\n",
            "        -0.0567047596, -0.0280280113, -0.0583863854, -0.0318957567])\n",
            "ctensor.grad: tensor([-4.4995698929, -4.4995698929, -4.4995698929, -3.9020509720,\n",
            "         0.0816049948, -2.6205823421,  0.6655117273, -0.7041739225,\n",
            "        -3.9020509720,  0.0816049948, -2.6205823421,  0.6655117273,\n",
            "        -0.7041739225, -3.9020509720,  0.0816049948, -2.6205823421,\n",
            "         0.6655117273, -0.7041739225])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.7631225586, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8406305909), tensor(1.7887940407), tensor(1.1178088188), tensor(1.4633704424), tensor(0.8639618158), tensor(0.7195507884), tensor(1.7004015446), tensor(1.1734639406), tensor(1.3501597643), tensor(1.0926111937), tensor(1.0295907259), tensor(1.0941418409), tensor(1.8240507841), tensor(1.2484648228), tensor(1.6479001045), tensor(1.8025643826), tensor(1.6025840044), tensor(1.1370160580), tensor(0.8922968507), tensor(1.1073608398)]\n",
            "b:  [tensor(1.6373674870), tensor(0.7300188541), tensor(1.4642032385), tensor(1.3132365942), tensor(1.0126715899), tensor(1.9991616011), tensor(1.2821656466), tensor(1.4019685984), tensor(1.2889866829), tensor(1.0669801235), tensor(1.3681763411), tensor(1.2950932980), tensor(0.7672105432), tensor(1.2687171698), tensor(1.2462406158), tensor(0.9025273323), tensor(1.1049265862), tensor(1.4064453840), tensor(1.3903300762), tensor(1.5580060482)]\n",
            "c:  [tensor(0.0352595672), tensor(0.0352595672), tensor(0.0352595672), tensor(0.0312857181), tensor(-0.0006845930), tensor(0.0201271642), tensor(-0.0058781677), tensor(0.0043693944), tensor(0.0312857181), tensor(-0.0006845930), tensor(0.0201271642), tensor(-0.0058781677), tensor(0.0043693944), tensor(0.0312857181), tensor(-0.0006845930), tensor(0.0201271642), tensor(-0.0058781677), tensor(0.0043693944)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0690861344,  0.0058792531, -0.0316370726, -0.0028002262,\n",
            "        -0.0325300694, -0.0819467306,  0.0038377047, -0.0337073505,\n",
            "        -0.0295203086, -0.0259902775, -0.0649985522, -0.0524225459,\n",
            "         0.0293252468, -0.0476479530, -0.0051370412,  0.0129684322,\n",
            "        -0.0131869316, -0.0625483990, -0.0685116053, -0.0570275486])\n",
            "btensor.grad: tensor([-0.0365312174, -0.0603772402, -0.0440962613, -0.0569094718,\n",
            "        -0.0440923870, -0.0260413587, -0.0337455869, -0.0459409058,\n",
            "        -0.0380717516, -0.0262143612, -0.0577897429, -0.0415333509,\n",
            "        -0.0462725200, -0.0447221696, -0.0463018119, -0.0715262294,\n",
            "        -0.0572752953, -0.0223004520, -0.0577933788, -0.0263417959])\n",
            "ctensor.grad: tensor([-4.8826999664, -4.8826999664, -4.8826999664, -4.2146935463,\n",
            "         0.0881507471, -2.8181457520,  0.7242375612, -0.7600707412,\n",
            "        -4.2146935463,  0.0881507471, -2.8181457520,  0.7242375612,\n",
            "        -0.7600707412, -4.2146935463,  0.0881507471, -2.8181457520,\n",
            "         0.7242375612, -0.7600707412])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.6835327148, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8409585357), tensor(1.7887222767), tensor(1.1179320812), tensor(1.4633373022), tensor(0.8640757799), tensor(0.7199459672), tensor(1.7003395557), tensor(1.1736018658), tensor(1.3502793312), tensor(1.0926983356), tensor(1.0299035311), tensor(1.0943844318), tensor(1.8238435984), tensor(1.2486860752), tensor(1.6478881836), tensor(1.8024523258), tensor(1.6026165485), tensor(1.1373188496), tensor(0.8926225901), tensor(1.1076303720)]\n",
            "b:  [tensor(1.6375197172), tensor(0.7303357124), tensor(1.4644044638), tensor(1.3135151863), tensor(1.0128839016), tensor(1.9992458820), tensor(1.2823096514), tensor(1.4021817446), tensor(1.2891557217), tensor(1.0670862198), tensor(1.3684586287), tensor(1.2952826023), tensor(0.7674434781), tensor(1.2689255476), tensor(1.2464581728), tensor(0.9029039741), tensor(1.1052129269), tensor(1.4065195322), tensor(1.3906116486), tensor(1.5581011772)]\n",
            "c:  [tensor(0.0379088260), tensor(0.0379088260), tensor(0.0379088260), tensor(0.0335609876), tensor(-0.0007322623), tensor(0.0216375962), tensor(-0.0062742778), tensor(0.0047760825), tensor(0.0335609876), tensor(-0.0007322623), tensor(0.0216375962), tensor(-0.0062742778), tensor(0.0047760825), tensor(0.0335609876), tensor(-0.0007322623), tensor(0.0216375962), tensor(-0.0062742778), tensor(0.0047760825)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0655906200,  0.0143413544, -0.0246537328,  0.0066163540,\n",
            "        -0.0227878094, -0.0790383816,  0.0124019384, -0.0275795758,\n",
            "        -0.0239206832, -0.0174205899, -0.0625639260, -0.0485234261,\n",
            "         0.0414405465, -0.0442411900,  0.0023889542,  0.0224082321,\n",
            "        -0.0065069795, -0.0605553389, -0.0651507974, -0.0539162159])\n",
            "btensor.grad: tensor([-0.0304473750, -0.0633718967, -0.0402370095, -0.0557197332,\n",
            "        -0.0424572527, -0.0168499053, -0.0288115144, -0.0426288545,\n",
            "        -0.0338150263, -0.0212136507, -0.0564569533, -0.0378605127,\n",
            "        -0.0465910733, -0.0416670144, -0.0435020626, -0.0753238797,\n",
            "        -0.0572600365, -0.0148215890, -0.0563127398, -0.0190217495])\n",
            "ctensor.grad: tensor([-5.2985143661, -5.2985143661, -5.2985143661, -4.5505380630,\n",
            "         0.0953386053, -3.0208656788,  0.7922201157, -0.8133764267,\n",
            "        -4.5505380630,  0.0953386053, -3.0208656788,  0.7922201157,\n",
            "        -0.8133764267, -4.5505380630,  0.0953386053, -3.0208656788,\n",
            "         0.7922201157, -0.8133764267])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.5911865234, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8412604332), tensor(1.7885982990), tensor(1.1180100441), tensor(1.4632456303), tensor(0.8641272783), tensor(0.7203179598), tensor(1.7002247572), tensor(1.1736996174), tensor(1.3503621817), tensor(1.0927306414), tensor(1.0301971436), tensor(1.0945993662), tensor(1.8235632181), tensor(1.2488830090), tensor(1.6478291750), tensor(1.8022824526), tensor(1.6026066542), tensor(1.1376053095), tensor(0.8929232359), tensor(1.1078771353)]\n",
            "b:  [tensor(1.6376318932), tensor(0.7306674123), tensor(1.4645788670), tensor(1.3137829304), tensor(1.0130832195), tensor(1.9992713928), tensor(1.2824211121), tensor(1.4023715258), tensor(1.2892960310), tensor(1.0671596527), tensor(1.3687289953), tensor(1.2954465151), tensor(0.7676755786), tensor(1.2691121101), tensor(1.2466554642), tensor(0.9032997489), tensor(1.1054955721), tensor(1.4065459967), tensor(1.3908804655), tensor(1.5581492186)]\n",
            "c:  [tensor(0.0407826267), tensor(0.0407826267), tensor(0.0407826267), tensor(0.0360155851), tensor(-0.0007838800), tensor(0.0232497845), tensor(-0.0067100492), tensor(0.0052064741), tensor(0.0360155851), tensor(-0.0007838800), tensor(0.0232497845), tensor(-0.0067100492), tensor(0.0052064741), tensor(0.0360155851), tensor(-0.0007838800), tensor(0.0232497845), tensor(-0.0067100492), tensor(0.0052064741)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0603762865,  0.0247943699, -0.0155835450,  0.0183423758,\n",
            "        -0.0102992058, -0.0744029284,  0.0229687691, -0.0195522904,\n",
            "        -0.0165744312, -0.0064605772, -0.0587199628, -0.0429956540,\n",
            "         0.0560652018, -0.0393879414,  0.0117981583,  0.0339638405,\n",
            "         0.0019747615, -0.0572891235, -0.0601261258, -0.0493410230])\n",
            "btensor.grad: tensor([-0.0224266443, -0.0663405955, -0.0348864496, -0.0535387695,\n",
            "        -0.0398754925, -0.0050973594, -0.0223013163, -0.0379509926,\n",
            "        -0.0280685425, -0.0146832466, -0.0540840030, -0.0327858329,\n",
            "        -0.0464182422, -0.0373139381, -0.0394587219, -0.0791562796,\n",
            "        -0.0565247536, -0.0052823722, -0.0537559986, -0.0096089840])\n",
            "ctensor.grad: tensor([-5.7476000786, -5.7476000786, -5.7476000786, -4.9091925621,\n",
            "         0.1032354012, -3.2243754864,  0.8715424538, -0.8607829809,\n",
            "        -4.9091925621,  0.1032354012, -3.2243754864,  0.8715424538,\n",
            "        -0.8607829809, -4.9091925621,  0.1032354012, -3.2243754864,\n",
            "         0.8715424538, -0.8607829809])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.4824829102, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8415261507), tensor(1.7884106636), tensor(1.1180305481), tensor(1.4630819559), tensor(0.8641005754), tensor(0.7206566930), tensor(1.7000455856), tensor(1.1737462282), tensor(1.3503981829), tensor(1.0926944017), tensor(1.0304632187), tensor(1.0947771072), tensor(1.8231955767), tensor(1.2490472794), tensor(1.6477123499), tensor(1.8020427227), tensor(1.6025439501), tensor(1.1378679276), tensor(0.8931890130), tensor(1.1080923080)]\n",
            "b:  [tensor(1.6376926899), tensor(0.7310137153), tensor(1.4647178650), tensor(1.3140338659), tensor(1.0132641792), tensor(1.9992233515), tensor(1.2824910879), tensor(1.4025299549), tensor(1.2893990278), tensor(1.0671916008), tensor(1.3689814806), tensor(1.2955769300), tensor(0.7679039240), tensor(1.2692693472), tensor(1.2468253374), tensor(0.9037148356), tensor(1.1057702303), tensor(1.4065128565), tensor(1.3911302090), tensor(1.5581381321)]\n",
            "c:  [tensor(0.0438973904), tensor(0.0438973904), tensor(0.0438973904), tensor(0.0386602730), tensor(-0.0008398352), tensor(0.0249611232), tensor(-0.0071923882), tensor(0.0056554442), tensor(0.0386602730), tensor(-0.0008398352), tensor(0.0249611232), tensor(-0.0071923882), tensor(0.0056554442), tensor(0.0386602730), tensor(-0.0008398352), tensor(0.0249611232), tensor(-0.0071923882), tensor(0.0056554442)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0531415939,  0.0375330001, -0.0041051805,  0.0327286720,\n",
            "         0.0053431988, -0.0677503347,  0.0358315706, -0.0093192756,\n",
            "        -0.0071953647,  0.0072489679, -0.0532101095, -0.0355441868,\n",
            "         0.0735253096, -0.0328443050,  0.0233695954,  0.0479411446,\n",
            "         0.0125383139, -0.0525207520, -0.0531592965, -0.0430443287])\n",
            "btensor.grad: tensor([-0.0121590607, -0.0692607462, -0.0277962089, -0.0501888394,\n",
            "        -0.0361964479,  0.0096201599, -0.0139856339, -0.0316827595,\n",
            "        -0.0206007957, -0.0063917637, -0.0504879057, -0.0260865688,\n",
            "        -0.0456652716, -0.0314553678, -0.0339798033, -0.0830131173,\n",
            "        -0.0549416542,  0.0066213012, -0.0499369502,  0.0022170544])\n",
            "ctensor.grad: tensor([-6.2295246124, -6.2295246124, -6.2295246124, -5.2893719673,\n",
            "         0.1119104102, -3.4226760864,  0.9646775723, -0.8979400396,\n",
            "        -5.2893719673,  0.1119104102, -3.4226760864,  0.9646775723,\n",
            "        -0.8979400396, -5.2893719673,  0.1119104102, -3.4226760864,\n",
            "         0.9646775723, -0.8979400396])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.3578491211, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8417441845), tensor(1.7881464958), tensor(1.1179801226), tensor(1.4628314972), tensor(0.8639779687), tensor(0.7209507823), tensor(1.6997892857), tensor(1.1737291813), tensor(1.3503756523), tensor(1.0925742388), tensor(1.0306922197), tensor(1.0949064493), tensor(1.8227249384), tensor(1.2491691113), tensor(1.6475254297), tensor(1.8017196655), tensor(1.6024166346), tensor(1.1380981207), tensor(0.8934090137), tensor(1.1082663536)]\n",
            "b:  [tensor(1.6376894712), tensor(0.7313743234), tensor(1.4648115635), tensor(1.3142614365), tensor(1.0134205818), tensor(1.9990848303), tensor(1.2825093269), tensor(1.4026480913), tensor(1.2894550562), tensor(1.0671722889), tensor(1.3692089319), tensor(1.2956646681), tensor(0.7681252956), tensor(1.2693887949), tensor(1.2469598055), tensor(0.9041492939), tensor(1.1060322523), tensor(1.4064069986), tensor(1.3913536072), tensor(1.5580543280)]\n",
            "c:  [tensor(0.0472686738), tensor(0.0472686738), tensor(0.0472686738), tensor(0.0415046550), tensor(-0.0009005516), tensor(0.0267651435), tensor(-0.0077296286), tensor(0.0061151227), tensor(0.0415046550), tensor(-0.0009005516), tensor(0.0267651435), tensor(-0.0077296286), tensor(0.0061151227), tensor(0.0415046550), tensor(-0.0009005516), tensor(0.0267651435), tensor(-0.0077296286), tensor(0.0061151227)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0436076522,  0.0528431088,  0.0100868046,  0.0500942469,\n",
            "         0.0245194435, -0.0588140488,  0.0512700081,  0.0034102499,\n",
            "         0.0044944528,  0.0240442455, -0.0457958579, -0.0258714110,\n",
            "         0.0941227078, -0.0243599415,  0.0373822451,  0.0646191388,\n",
            "         0.0254734159, -0.0460281968, -0.0439949036, -0.0348003805])\n",
            "btensor.grad: tensor([ 0.0006539337, -0.0721217692, -0.0187282264, -0.0455144346,\n",
            "        -0.0312783718,  0.0276963711, -0.0036574006, -0.0236221701,\n",
            "        -0.0112000704,  0.0038689375, -0.0454973578, -0.0175535083,\n",
            "        -0.0442721695, -0.0238952041, -0.0268900096, -0.0868889689,\n",
            "        -0.0524013042,  0.0211699903, -0.0446840525,  0.0167640448])\n",
            "ctensor.grad: tensor([-6.7425661087, -6.7425661087, -6.7425661087, -5.6887640953,\n",
            "         0.1214328483, -3.6080415249,  1.0744804144, -0.9193571806,\n",
            "        -5.6887640953,  0.1214328483, -3.6080415249,  1.0744804144,\n",
            "        -0.9193571806, -5.6887640953,  0.1214328483, -3.6080415249,\n",
            "         1.0744804144, -0.9193571806])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.2128906250, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8419018984), tensor(1.7877916098), tensor(1.1178439856), tensor(1.4624779224), tensor(0.8637404442), tensor(0.7211877704), tensor(1.6994416714), tensor(1.1736347675), tensor(1.3502819538), tensor(1.0923532248), tensor(1.0308736563), tensor(1.0949749947), tensor(1.8221344948), tensor(1.2492377758), tensor(1.6472551823), tensor(1.8012984991), tensor(1.6022115946), tensor(1.1382862329), tensor(0.8935711980), tensor(1.1083885431)]\n",
            "b:  [tensor(1.6376081705), tensor(0.7317490578), tensor(1.4648489952), tensor(1.3144583702), tensor(1.0135457516), tensor(1.9988374710), tensor(1.2824652195), tensor(1.4027161598), tensor(1.2894536257), tensor(1.0670909882), tensor(1.3694038391), tensor(1.2956998348), tensor(0.7683363557), tensor(1.2694612741), tensor(1.2470501661), tensor(0.9046033621), tensor(1.1062763929), tensor(1.4062141180), tensor(1.3915429115), tensor(1.5578829050)]\n",
            "c:  [tensor(0.0509103984), tensor(0.0509103984), tensor(0.0509103984), tensor(0.0445565991), tensor(-0.0009664856), tensor(0.0286506545), tensor(-0.0083316788), tensor(0.0065743709), tensor(0.0445565991), tensor(-0.0009664856), tensor(0.0286506545), tensor(-0.0083316788), tensor(0.0065743709), tensor(0.0445565991), tensor(-0.0009664856), tensor(0.0286506545), tensor(-0.0083316788), tensor(0.0065743709)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0315467119,  0.0709689409,  0.0272388458,  0.0707194805,\n",
            "         0.0475032330, -0.0473952293,  0.0695264339,  0.0188744962,\n",
            "         0.0187297575,  0.0441923440, -0.0362755060, -0.0137177706,\n",
            "         0.1180824041, -0.0137277842,  0.0540590584,  0.0842292607,\n",
            "         0.0410156250, -0.0376302600, -0.0324378610, -0.0244267881])\n",
            "btensor.grad: tensor([ 0.0162659995, -0.0749472976, -0.0074958205, -0.0393973887,\n",
            "        -0.0250354782,  0.0494683087,  0.0088145733, -0.0136134923,\n",
            "         0.0002893209,  0.0162552595, -0.0389924347, -0.0070301890,\n",
            "        -0.0422151536, -0.0144906342, -0.0180785358, -0.0908120275,\n",
            "        -0.0488226414,  0.0385857224, -0.0378721952,  0.0342863798])\n",
            "ctensor.grad: tensor([-7.2834448814, -7.2834448814, -7.2834448814, -6.1038846970,\n",
            "         0.1318679154, -3.7710218430,  1.2041009665, -0.9184959531,\n",
            "        -6.1038846970,  0.1318679154, -3.7710218430,  1.2041009665,\n",
            "        -0.9184959531, -6.1038846970,  0.1318679154, -3.7710218430,\n",
            "         1.2041009665, -0.9184959531])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.0470581055, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8419861197), tensor(1.7873313427), tensor(1.1176066399), tensor(1.4620040655), tensor(0.8633683920), tensor(0.7213549614), tensor(1.6989879608), tensor(1.1734486818), tensor(1.3501036167), tensor(1.0920140743), tensor(1.0309963226), tensor(1.0949695110), tensor(1.8214069605), tensor(1.2492418289), tensor(1.6468874216), tensor(1.8007639647), tensor(1.6019150019), tensor(1.1384223700), tensor(0.8936632872), tensor(1.1084476709)]\n",
            "b:  [tensor(1.6374340057), tensor(0.7321380973), tensor(1.4648189545), tensor(1.3146173954), tensor(1.0136330128), tensor(1.9984617233), tensor(1.2823479176), tensor(1.4027241468), tensor(1.2893840075), tensor(1.0669369698), tensor(1.3695584536), tensor(1.2956720591), tensor(0.7685340643), tensor(1.2694772482), tensor(1.2470878363), tensor(0.9050776362), tensor(1.1064974070), tensor(1.4059193134), tensor(1.3916902542), tensor(1.5576082468)]\n",
            "c:  [tensor(0.0548340231), tensor(0.0548340231), tensor(0.0548340231), tensor(0.0478216819), tensor(-0.0010381216), tensor(0.0306010898), tensor(-0.0090100802), tensor(0.0070184041), tensor(0.0478216819), tensor(-0.0010381216), tensor(0.0306010898), tensor(-0.0090100802), tensor(0.0070184041), tensor(0.0478216819), tensor(-0.0010381216), tensor(0.0306010898), tensor(-0.0090100802), tensor(0.0070184041)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0168427229,  0.0920553505,  0.0474797487,  0.0947642326,\n",
            "         0.0744148493, -0.0334337950,  0.0907450914,  0.0372147262,\n",
            "         0.0356694572,  0.0678248405, -0.0245407075,  0.0011048540,\n",
            "         0.1455065608, -0.0008172989,  0.0735417902,  0.1069034860,\n",
            "         0.0593242049, -0.0272283554, -0.0184139609, -0.0118312836])\n",
            "btensor.grad: tensor([ 0.0348255746, -0.0778041184,  0.0060001910, -0.0318019390,\n",
            "        -0.0174514279,  0.0751581490,  0.0234678388, -0.0016022325,\n",
            "         0.0139192343,  0.0308105946, -0.0309295952,  0.0055524111,\n",
            "        -0.0395422727, -0.0031868815, -0.0075408220, -0.0948604941,\n",
            "        -0.0442097187,  0.0589626729, -0.0294587612,  0.0549203157])\n",
            "ctensor.grad: tensor([-7.8472490311, -7.8472490311, -7.8472490311, -6.5301623344,\n",
            "         0.1432719976, -3.9008698463,  1.3568034172, -0.8880667090,\n",
            "        -6.5301623344,  0.1432719976, -3.9008698463,  1.3568034172,\n",
            "        -0.8880667090, -6.5301623344,  0.1432719976, -3.9008698463,\n",
            "         1.3568034172, -0.8880667090])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(739.8571166992, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8419838548), tensor(1.7867506742), tensor(1.1172528267), tensor(1.4613928795), tensor(0.8628427386), tensor(0.7214402556), tensor(1.6984132528), tensor(1.1731566191), tensor(1.3498269320), tensor(1.0915396214), tensor(1.0310493708), tensor(1.0948761702), tensor(1.8205252886), tensor(1.2491699457), tensor(1.6464082003), tensor(1.8001008034), tensor(1.6015127897), tensor(1.1384966373), tensor(0.8936733603), tensor(1.1084331274)]\n",
            "b:  [tensor(1.6371523142), tensor(0.7325422168), tensor(1.4647103548), tensor(1.3147314787), tensor(1.0136761665), tensor(1.9979376793), tensor(1.2821470499), tensor(1.4026623964), tensor(1.2892360687), tensor(1.0666997433), tensor(1.3696653843), tensor(1.2955714464), tensor(0.7687160373), tensor(1.2694276571), tensor(1.2470649481), tensor(0.9055735469), tensor(1.1066907644), tensor(1.4055081606), tensor(1.3917878866), tensor(1.5572150946)]\n",
            "c:  [tensor(0.0590477958), tensor(0.0590477958), tensor(0.0590477958), tensor(0.0513027906), tensor(-0.0011159648), tensor(0.0325942300), tensor(-0.0097779334), tensor(0.0074286843), tensor(0.0513027906), tensor(-0.0011159648), tensor(0.0325942300), tensor(-0.0097779334), tensor(0.0074286843), tensor(0.0513027906), tensor(-0.0011159648), tensor(0.0325942300), tensor(-0.0097779334), tensor(0.0074286843)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0004557371,  0.1161327213,  0.0707643628,  0.1222332716,\n",
            "         0.1051294804, -0.0170598030,  0.1149402857,  0.0584230423,\n",
            "         0.0553348921,  0.0948841870, -0.0106145442,  0.0186699033,\n",
            "         0.1763333082,  0.0143735409,  0.0958371907,  0.1326319128,\n",
            "         0.0804373622, -0.0148500204, -0.0020180345,  0.0029109418])\n",
            "btensor.grad: tensor([ 0.0563302897, -0.0808266103,  0.0217256546, -0.0228143036,\n",
            "        -0.0086392611,  0.1048170030,  0.0401678085,  0.0123390704,\n",
            "         0.0295972824,  0.0474423170, -0.0213905871,  0.0201251507,\n",
            "        -0.0363902599,  0.0099229515,  0.0045817494, -0.0991804004,\n",
            "        -0.0386755466,  0.0822230577, -0.0195257068,  0.0786408186])\n",
            "ctensor.grad: tensor([-8.4275417328, -8.4275417328, -8.4275417328, -6.9622154236,\n",
            "         0.1556864530, -3.9862768650,  1.5357064009, -0.8205600977,\n",
            "        -6.9622154236,  0.1556864530, -3.9862768650,  1.5357064009,\n",
            "        -0.8205600977, -6.9622154236,  0.1556864530, -3.9862768650,\n",
            "         1.5357064009, -0.8205600977])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(739.6418457031, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8418835402), tensor(1.7860354185), tensor(1.1167688370), tensor(1.4606282711), tensor(0.8621467948), tensor(0.7214335799), tensor(1.6977034807), tensor(1.1727451086), tensor(1.3494391441), tensor(1.0909143686), tensor(1.0310229063), tensor(1.0946818590), tensor(1.8194739819), tensor(1.2490115166), tensor(1.6458042860), tensor(1.7992947102), tensor(1.6009917259), tensor(1.1385000944), tensor(0.8935912848), tensor(1.1083353758)]\n",
            "b:  [tensor(1.6367495060), tensor(0.7329633832), tensor(1.4645130634), tensor(1.3147948980), tensor(1.0136704445), tensor(1.9972463846), tensor(1.2818541527), tensor(1.4025226831), tensor(1.2890008688), tensor(1.0663704872), tensor(1.3697185516), tensor(1.2953892946), tensor(0.7688811421), tensor(1.2693048716), tensor(1.2469753027), tensor(0.9060935378), tensor(1.1068531275), tensor(1.4049679041), tensor(1.3918296099), tensor(1.5566891432)]\n",
            "c:  [tensor(0.0635562018), tensor(0.0635562018), tensor(0.0635562018), tensor(0.0549999215), tensor(-0.0012005304), tensor(0.0346024781), tensor(-0.0106496327), tensor(0.0077832569), tensor(0.0549999215), tensor(-0.0012005304), tensor(0.0346024781), tensor(-0.0106496327), tensor(0.0077832569), tensor(0.0549999215), tensor(-0.0012005304), tensor(0.0346024781), tensor(-0.0106496327), tensor(0.0077832569)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0200651884,  0.1430423558,  0.0968054831,  0.1529207230,\n",
            "         0.1391897202,  0.0013322830,  0.1419560909,  0.0822909474,\n",
            "         0.0775624439,  0.1250446737,  0.0052979887,  0.0388678834,\n",
            "         0.2102729678,  0.0316818953,  0.1207711846,  0.1612235606,\n",
            "         0.1042184830, -0.0006955862,  0.0164091587,  0.0195440650])\n",
            "btensor.grad: tensor([ 0.0805718303, -0.0842372179,  0.0394618809, -0.0126741827,\n",
            "         0.0011334196,  0.1382502317,  0.0585781336,  0.0279316753,\n",
            "         0.0470348597,  0.0658491850, -0.0106260180,  0.0364325643,\n",
            "        -0.0330229811,  0.0245597661,  0.0179294348, -0.1040039062,\n",
            "        -0.0324702263,  0.1080588400, -0.0083413124,  0.1051996946])\n",
            "ctensor.grad: tensor([-9.0168123245, -9.0168123245, -9.0168123245, -7.3942642212,\n",
            "         0.1691313088, -4.0164933205,  1.7433978319, -0.7091449499,\n",
            "        -7.3942642212,  0.1691313088, -4.0164933205,  1.7433978319,\n",
            "        -0.7091449499, -7.3942642212,  0.1691313088, -4.0164933205,\n",
            "         1.7433978319, -0.7091449499])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(739.3997802734, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8416764736), tensor(1.7851732969), tensor(1.1161435843), tensor(1.4596964121), tensor(0.8612679243), tensor(0.7213283777), tensor(1.6968463659), tensor(1.1722033024), tensor(1.3489292860), tensor(1.0901259184), tensor(1.0309090614), tensor(1.0943750143), tensor(1.8182400465), tensor(1.2487578392), tensor(1.6450644732), tensor(1.7983334064), tensor(1.6003401279), tensor(1.1384260654), tensor(0.8934100270), tensor(1.1081476212)]\n",
            "b:  [tensor(1.6362140179), tensor(0.7334051728), tensor(1.4642192125), tensor(1.3148040771), tensor(1.0136134624), tensor(1.9963716269), tensor(1.2814635038), tensor(1.4022992849), tensor(1.2886723280), tensor(1.0659430027), tensor(1.3697140217), tensor(1.2951192856), tensor(0.7690304518), tensor(1.2691037655), tensor(1.2468158007), tensor(0.9066418409), tensor(1.1069833040), tensor(1.4042884111), tensor(1.3918114901), tensor(1.5560188293)]\n",
            "c:  [tensor(0.0683598146), tensor(0.0683598146), tensor(0.0683598146), tensor(0.0589103736), tensor(-0.0012923302), tensor(0.0365938954), tensor(-0.0116403932), tensor(0.0080576167), tensor(0.0589103736), tensor(-0.0012923302), tensor(0.0365938954), tensor(-0.0116403932), tensor(0.0080576167), tensor(0.0589103736), tensor(-0.0012923302), tensor(0.0365938954), tensor(-0.0116403932), tensor(0.0080576167)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.0414154530, 0.1724176109, 0.1250422597, 0.1863663197, 0.1757762432,\n",
            "        0.0210365057, 0.1714259386, 0.1083637774, 0.1019694209, 0.1576832235,\n",
            "        0.0227580369, 0.0613767803, 0.2467983663, 0.0507245064, 0.1479595155,\n",
            "        0.1922693104, 0.1303231716, 0.0148086548, 0.0362477303, 0.0375482142])\n",
            "btensor.grad: tensor([ 0.1070932895, -0.0883603990,  0.0587600768, -0.0018291175,\n",
            "         0.0113980249,  0.1749553382,  0.0781196356,  0.0446747094,\n",
            "         0.0657007098,  0.0855017900,  0.0009075105,  0.0540012717,\n",
            "        -0.0298563838,  0.0402151942,  0.0318971574, -0.1096658111,\n",
            "        -0.0260288715,  0.1358885169,  0.0036145449,  0.1340699196])\n",
            "ctensor.grad: tensor([-9.6072254181, -9.6072254181, -9.6072254181, -7.8209056854,\n",
            "         0.1835994720, -3.9828310013,  1.9815216064, -0.5487193465,\n",
            "        -7.8209056854,  0.1835994720, -3.9828310013,  1.9815216064,\n",
            "        -0.5487193465, -7.8209056854,  0.1835994720, -3.9828310013,\n",
            "         1.9815216064, -0.5487193465])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(739.1305541992, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8413582444), tensor(1.7841548920), tensor(1.1153705120), tensor(1.4585871696), tensor(0.8601995111), tensor(0.7211232781), tensor(1.6958324909), tensor(1.1715235710), tensor(1.3482896090), tensor(1.0891666412), tensor(1.0307036638), tensor(1.0939468145), tensor(1.8168143034), tensor(1.2484033108), tensor(1.6441805363), tensor(1.7972077131), tensor(1.5995491743), tensor(1.1382709742), tensor(0.8931270838), tensor(1.1078668833)]\n",
            "b:  [tensor(1.6355382204), tensor(0.7338732481), tensor(1.4638246298), tensor(1.3147586584), tensor(1.0135059357), tensor(1.9953010082), tensor(1.2809736729), tensor(1.4019901752), tensor(1.2882481813), tensor(1.0654149055), tensor(1.3696514368), tensor(1.2947586775), tensor(0.7691676617), tensor(1.2688230276), tensor(1.2465877533), tensor(0.9072248936), tensor(1.1070830822), tensor(1.4034641981), tensor(1.3917334080), tensor(1.5551966429)]\n",
            "c:  [tensor(0.0734556243), tensor(0.0734556243), tensor(0.0734556243), tensor(0.0630293339), tensor(-0.0013918567), tensor(0.0385339409), tensor(-0.0127655873), tensor(0.0082261302), tensor(0.0630293339), tensor(-0.0013918567), tensor(0.0385339409), tensor(-0.0127655873), tensor(0.0082261302), tensor(0.0630293339), tensor(-0.0013918567), tensor(0.0385339409), tensor(-0.0127655873), tensor(0.0082261302)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.0636475682, 0.2036748230, 0.1546078026, 0.2218571901, 0.2136811018,\n",
            "        0.0410213470, 0.2027704716, 0.1359362900, 0.1279310584, 0.1918572783,\n",
            "        0.0410912633, 0.0856435448, 0.2851503491, 0.0708941221, 0.1767973006,\n",
            "        0.2251463532, 0.1581849456, 0.0310115814, 0.0565839410, 0.0561473966])\n",
            "btensor.grad: tensor([ 0.1351581216, -0.0936173797,  0.0789265633,  0.0090748966,\n",
            "         0.0215025395,  0.2141250670,  0.0979753733,  0.0618284643,\n",
            "         0.0848332047,  0.1056228876,  0.0125274360,  0.0721251369,\n",
            "        -0.0274401493,  0.0561373681,  0.0456147790, -0.1166048646,\n",
            "        -0.0199663639,  0.1648451090,  0.0156275630,  0.1644381285])\n",
            "ctensor.grad: tensor([-10.1916179657, -10.1916179657, -10.1916179657,  -8.2379159927,\n",
            "          0.1990530640,  -3.8800876141,   2.2503886223,  -0.3370276690,\n",
            "         -8.2379159927,   0.1990530640,  -3.8800876141,   2.2503886223,\n",
            "         -0.3370276690,  -8.2379159927,   0.1990530640,  -3.8800876141,\n",
            "          2.2503886223,  -0.3370276690])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(738.8318481445, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8409301043), tensor(1.7829747200), tensor(1.1144485474), tensor(1.4572948217), tensor(0.8589426279), tensor(0.7208233476), tensor(1.6946562529), tensor(1.1707031727), tensor(1.3475165367), tensor(1.0880348682), tensor(1.0304065943), tensor(1.0933923721), tensor(1.8151924610), tensor(1.2479465008), tensor(1.6431480646), tensor(1.7959123850), tensor(1.5986139774), tensor(1.1380357742), tensor(0.8927459121), tensor(1.1074951887)]\n",
            "b:  [tensor(1.6347192526), tensor(0.7343758345), tensor(1.4633293152), tensor(1.3146626949), tensor(1.0133528709), tensor(1.9940277338), tensor(1.2803881168), tensor(1.4015980959), tensor(1.2877309322), tensor(1.0647886992), tensor(1.3695347309), tensor(1.2943091393), tensor(0.7692999244), tensor(1.2684662342), tensor(1.2462978363), tensor(0.9078515768), tensor(1.1071584225), tensor(1.4024951458), tensor(1.3915995359), tensor(1.5542204380)]\n",
            "c:  [tensor(0.0788378865), tensor(0.0788378865), tensor(0.0788378865), tensor(0.0673507899), tensor(-0.0014995682), tensor(0.0403877608), tensor(-0.0140399560), tensor(0.0082638934), tensor(0.0673507899), tensor(-0.0014995682), tensor(0.0403877608), tensor(-0.0140399560), tensor(0.0082638934), tensor(0.0673507899), tensor(-0.0014995682), tensor(0.0403877608), tensor(-0.0140399560), tensor(0.0082638934)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.0856253505, 0.2360404432, 0.1843830347, 0.2584731579, 0.2513796091,\n",
            "        0.0599814653, 0.2352412939, 0.1640848517, 0.1546235681, 0.2263593674,\n",
            "        0.0594064593, 0.1108866930, 0.3243775070, 0.0913656354, 0.2064883560,\n",
            "        0.2590551972, 0.1870398521, 0.0470362306, 0.0762389302, 0.0743349195])\n",
            "btensor.grad: tensor([ 0.1637946963, -0.1005174518,  0.0990526378,  0.0191989839,\n",
            "         0.0306042060,  0.2546525002,  0.1171169877,  0.0784227699,\n",
            "         0.1034445167,  0.1252368689,  0.0233520269,  0.0899018645,\n",
            "        -0.0264531318,  0.0713539124,  0.0579790473, -0.1253386736,\n",
            "        -0.0150656700,  0.1938188076,  0.0267749429,  0.1952425241])\n",
            "ctensor.grad: tensor([-10.7645196915, -10.7645196915, -10.7645196915,  -8.6429071426,\n",
            "          0.2154231071,  -3.7076420784,   2.5487375259,  -0.0755263418,\n",
            "         -8.6429071426,   0.2154231071,  -3.7076420784,   2.5487375259,\n",
            "         -0.0755263418,  -8.6429071426,   0.2154231071,  -3.7076420784,\n",
            "          2.5487375259,  -0.0755263418])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(738.5028686523, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8403999805), tensor(1.7816315889), tensor(1.1133831739), tensor(1.4558190107), tensor(0.8575068712), tensor(0.7204412222), tensor(1.6933163404), tensor(1.1697444916), tensor(1.3466112614), tensor(1.0867358446), tensor(1.0300233364), tensor(1.0927114487), tensor(1.8133752346), tensor(1.2473906279), tensor(1.6419675350), tensor(1.7944469452), tensor(1.5975339413), tensor(1.1377264261), tensor(0.8922765851), tensor(1.1070405245)]\n",
            "b:  [tensor(1.6337599754), tensor(0.7349238992), tensor(1.4627389908), tensor(1.3145247698), tensor(1.0131641626), tensor(1.9925516844), tensor(1.2797161341), tensor(1.4011313915), tensor(1.2871288061), tensor(1.0640726089), tensor(1.3693729639), tensor(1.2937777042), tensor(0.7694380879), tensor(1.2680425644), tensor(1.2459592819), tensor(0.9085337520), tensor(1.1072194576), tensor(1.4013874531), tensor(1.3914196491), tensor(1.5530942678)]\n",
            "c:  [tensor(0.0844993442), tensor(0.0844993442), tensor(0.0844993442), tensor(0.0718687177), tensor(-0.0016158752), tensor(0.0421227440), tensor(-0.0154768005), tensor(0.0081487801), tensor(0.0718687177), tensor(-0.0016158752), tensor(0.0421227440), tensor(-0.0154768005), tensor(0.0081487801), tensor(0.0718687177), tensor(-0.0016158752), tensor(0.0421227440), tensor(-0.0154768005), tensor(0.0081487801)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1060212851, 0.2686239183, 0.2130631208, 0.2951523066, 0.2871525288,\n",
            "        0.0764263868, 0.2679876089, 0.1917352974, 0.1810601503, 0.2598163784,\n",
            "        0.0766593516, 0.1361826956, 0.3634406924, 0.1111770272, 0.2361072749,\n",
            "        0.2930922806, 0.2159956694, 0.0618607998, 0.0938596129, 0.0909262896])\n",
            "btensor.grad: tensor([ 0.1918637455, -0.1096071005,  0.1180672944,  0.0275842845,\n",
            "         0.0377469622,  0.2952183783,  0.1343863606,  0.0933409333,\n",
            "         0.1204229593,  0.1432238817,  0.0323528945,  0.1062894464,\n",
            "        -0.0276287384,  0.0847450197,  0.0677114129, -0.1364390254,\n",
            "        -0.0122072697,  0.2215402871,  0.0359698534,  0.2252331972])\n",
            "ctensor.grad: tensor([-11.3229160309, -11.3229160309, -11.3229160309,  -9.0358533859,\n",
            "          0.2326137573,  -3.4699633121,   2.8736882210,   0.2302274406,\n",
            "         -9.0358533859,   0.2326137573,  -3.4699633121,   2.8736882210,\n",
            "          0.2302274406,  -9.0358533859,   0.2326137573,  -3.4699633121,\n",
            "          2.8736882210,   0.2302274406])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(738.1434326172, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8397827148), tensor(1.7801290751), tensor(1.1121866703), tensor(1.4541648626), tensor(0.8559105992), tensor(0.7199970484), tensor(1.6918156147), tensor(1.1686556339), tensor(1.3455801010), tensor(1.0852817297), tensor(1.0295644999), tensor(1.0919088125), tensor(1.8113685846), tensor(1.2467441559), tensor(1.6406439543), tensor(1.7928152084), tensor(1.5963133574), tensor(1.1373544931), tensor(0.8917363286), tensor(1.1065170765)]\n",
            "b:  [tensor(1.6326692104), tensor(0.7355310321), tensor(1.4620647430), tensor(1.3143585920), tensor(1.0129543543), tensor(1.9908796549), tensor(1.2789731026), tensor(1.4006043673), tensor(1.2864557505), tensor(1.0632803440), tensor(1.3691807985), tensor(1.2931765318), tensor(0.7695965171), tensor(1.2675669193), tensor(1.2455918789), tensor(0.9092860222), tensor(1.1072809696), tensor(1.4001539946), tensor(1.3912092447), tensor(1.5518287420)]\n",
            "c:  [tensor(0.0904326588), tensor(0.0904326588), tensor(0.0904326588), tensor(0.0765782520), tensor(-0.0017411298), tensor(0.0437107943), tensor(-0.0170872882), tensor(0.0078633633), tensor(0.0765782520), tensor(-0.0017411298), tensor(0.0437107943), tensor(-0.0170872882), tensor(0.0078633633), tensor(0.0765782520), tensor(-0.0017411298), tensor(0.0437107943), tensor(-0.0170872882), tensor(0.0078633633)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1234586239, 0.3005083799, 0.2392944694, 0.3308272362, 0.3192555904,\n",
            "        0.0888375044, 0.3001369238, 0.2177802324, 0.2062259018, 0.2908236086,\n",
            "        0.0917698145, 0.1605369449, 0.4013182819, 0.1292994618, 0.2647123337,\n",
            "        0.3263573647, 0.2441236973, 0.0743963718, 0.1080530882, 0.1046873927])\n",
            "btensor.grad: tensor([ 0.2181629688, -0.1214208603,  0.1348447800,  0.0332253575,\n",
            "         0.0419531316,  0.3344150782,  0.1486034989,  0.1054081544,\n",
            "         0.1346142888,  0.1584509611,  0.0384439230,  0.1202246547,\n",
            "        -0.0316855982,  0.0951316655,  0.0734845698, -0.1504578590,\n",
            "        -0.0123035908,  0.2467018068,  0.0420698524,  0.2531093359])\n",
            "ctensor.grad: tensor([-11.8666305542, -11.8666305542, -11.8666305542,  -9.4190626144,\n",
            "          0.2505092025,  -3.1761028767,   3.2209770679,   0.5708328485,\n",
            "         -9.4190626144,   0.2505092025,  -3.1761028767,   3.2209770679,\n",
            "          0.5708328485,  -9.4190626144,   0.2505092025,  -3.1761028767,\n",
            "          3.2209770679,   0.5708328485])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(737.7521972656, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8390994668), tensor(1.7784746885), tensor(1.1108776331), tensor(1.4523421526), tensor(0.8541799784), tensor(0.7195178270), tensor(1.6901609898), tensor(1.1674495935), tensor(1.3444342613), tensor(1.0836912394), tensor(1.0290458202), tensor(1.0909937620), tensor(1.8091830015), tensor(1.2460203171), tensor(1.6391867399), tensor(1.7910249233), tensor(1.5949604511), tensor(1.1369364262), tensor(0.8911486268), tensor(1.1059447527)]\n",
            "b:  [tensor(1.6314613819), tensor(0.7362130284), tensor(1.4613231421), tensor(1.3141826391), tensor(1.0127427578), tensor(1.9890252352), tensor(1.2781795263), tensor(1.4000368118), tensor(1.2857309580), tensor(1.0624309778), tensor(1.3689777851), tensor(1.2925229073), tensor(0.7697927356), tensor(1.2670598030), tensor(1.2452217340), tensor(0.9101253748), tensor(1.1073619127), tensor(1.3988134861), tensor(1.3909893036), tensor(1.5504405499)]\n",
            "c:  [tensor(0.0966316164), tensor(0.0966316164), tensor(0.0966316164), tensor(0.0814766511), tensor(-0.0018756222), tensor(0.0451300666), tensor(-0.0188799743), tensor(0.0073963772), tensor(0.0814766511), tensor(-0.0018756222), tensor(0.0451300666), tensor(-0.0188799743), tensor(0.0073963772), tensor(0.0814766511), tensor(-0.0018756222), tensor(0.0451300666), tensor(-0.0188799743), tensor(0.0073963772)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1366463304, 0.3308682442, 0.2618191838, 0.3645534515, 0.3461214304,\n",
            "        0.0958477259, 0.3309189081, 0.2412064075, 0.2291691452, 0.3181046844,\n",
            "        0.1037338376, 0.1830177009, 0.4371247292, 0.1447742581, 0.2914404869,\n",
            "        0.3580545485, 0.2705703378, 0.0836212039, 0.1175408363, 0.1144640744])\n",
            "btensor.grad: tensor([ 0.2415624261, -0.1364051700,  0.1483136117,  0.0351904929,\n",
            "         0.0423258096,  0.3708910346,  0.1587166786,  0.1135187447,\n",
            "         0.1449652910,  0.1698746681,  0.0406058431,  0.1307246089,\n",
            "        -0.0392429680,  0.1014144719,  0.0740386844, -0.1678711176,\n",
            "        -0.0161869526,  0.2681004703,  0.0439843535,  0.2776445150])\n",
            "ctensor.grad: tensor([-12.3979148865, -12.3979148865, -12.3979148865,  -9.7967967987,\n",
            "          0.2689847648,  -2.8385446072,   3.5853712559,   0.9339718819,\n",
            "         -9.7967967987,   0.2689847648,  -2.8385446072,   3.5853712559,\n",
            "          0.9339718819,  -9.7967967987,   0.2689847648,  -2.8385446072,\n",
            "          3.5853712559,   0.9339718819])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(737.3267211914, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8383768201), tensor(1.7766793966), tensor(1.1094796658), tensor(1.4503641129), tensor(0.8523473144), tensor(0.7190358639), tensor(1.6883622408), tensor(1.1661436558), tensor(1.3431885242), tensor(1.0819879770), tensor(1.0284870863), tensor(1.0899794102), tensor(1.8068319559), tensor(1.2452362776), tensor(1.6376086473), tensor(1.7890870571), tensor(1.5934871435), tensor(1.1364929676), tensor(0.8905421495), tensor(1.1053482294)]\n",
            "b:  [tensor(1.6301556826), tensor(0.7369874120), tensor(1.4605352879), tensor(1.3140190840), tensor(1.0125520229), tensor(1.9870078564), tensor(1.2773600817), tensor(1.3994530439), tensor(1.2849777937), tensor(1.0615476370), tensor(1.3687878847), tensor(1.2918379307), tensor(0.7700464725), tensor(1.2665463686), tensor(1.2448800802), tensor(0.9110704660), tensor(1.1074844599), tensor(1.3973897696), tensor(1.3907853365), tensor(1.5489515066)]\n",
            "c:  [tensor(0.1030920520), tensor(0.1030920520), tensor(0.1030920520), tensor(0.0865639076), tensor(-0.0020195807), tensor(0.0463658199), tensor(-0.0208605733), tensor(0.0067435429), tensor(0.0865639076), tensor(-0.0020195807), tensor(0.0463658199), tensor(-0.0208605733), tensor(0.0067435429), tensor(0.0865639076), tensor(-0.0020195807), tensor(0.0463658199), tensor(-0.0208605733), tensor(0.0067435429)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1445300579, 0.3590541780, 0.2795988321, 0.3956093788, 0.3665318489,\n",
            "        0.0963881016, 0.3597531319, 0.2611964047, 0.2491370291, 0.3406561017,\n",
            "        0.1117438078, 0.2028624415, 0.4702081382, 0.1568183303, 0.3156104684,\n",
            "        0.3875775039, 0.2946553826, 0.0886896849, 0.1212934256, 0.1192933321])\n",
            "btensor.grad: tensor([ 0.2611288428, -0.1548765749,  0.1575758755,  0.0327213407,\n",
            "         0.0381458849,  0.4034836292,  0.1638853550,  0.1167521775,\n",
            "         0.1506267190,  0.1766799688,  0.0379727185,  0.1370038986,\n",
            "        -0.0507424846,  0.1026783139,  0.0683225095, -0.1890144944,\n",
            "        -0.0245110989,  0.2847523093,  0.0407958627,  0.2978160381])\n",
            "ctensor.grad: tensor([-12.9208650589, -12.9208650589, -12.9208650589, -10.1745147705,\n",
            "          0.2879168987,  -2.4715046883,   3.9611966610,   1.3056685925,\n",
            "        -10.1745147705,   0.2879168987,  -2.4715046883,   3.9611966610,\n",
            "          1.3056685925, -10.1745147705,   0.2879168987,  -2.4715046883,\n",
            "          3.9611966610,   1.3056685925])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(736.8652954102, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8376448154), tensor(1.7747560740), tensor(1.1080201864), tensor(1.4482462406), tensor(0.8504487276), tensor(0.7185868621), tensor(1.6864308119), tensor(1.1647574902), tensor(1.3418602943), tensor(1.0801988840), tensor(1.0279107094), tensor(1.0888816118), tensor(1.8043309450), tensor(1.2444117069), tensor(1.6359246969), tensor(1.7870141268), tensor(1.5919073820), tensor(1.1360479593), tensor(0.8899489641), tensor(1.1047557592)]\n",
            "b:  [tensor(1.6287746429), tensor(0.7378722429), tensor(1.4597253799), tensor(1.3138926029), tensor(1.0124073029), tensor(1.9848512411), tensor(1.2765421867), tensor(1.3988808393), tensor(1.2842226028), tensor(1.0606559515), tensor(1.3686382771), tensor(1.2911450863), tensor(0.7703784704), tensor(1.2660549879), tensor(1.2446022034), tensor(0.9121407270), tensor(1.1076729298), tensor(1.3959097862), tensor(1.3906261921), tensor(1.5473870039)]\n",
            "c:  [tensor(0.1098122373), tensor(0.1098122373), tensor(0.1098122373), tensor(0.0918429196), tensor(-0.0021731767), tensor(0.0474103875), tensor(-0.0230319910), tensor(0.0059076911), tensor(0.0918429196), tensor(-0.0021731767), tensor(0.0474103875), tensor(-0.0230319910), tensor(0.0059076911), tensor(0.0918429196), tensor(-0.0021731767), tensor(0.0474103875), tensor(-0.0230319910), tensor(0.0059076911)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1464009881, 0.3846658766, 0.2919064164, 0.4235831499, 0.3797185421,\n",
            "        0.0898016691, 0.3862974644, 0.2772273123, 0.2656343579, 0.3578304052,\n",
            "        0.1152825952, 0.2195698023, 0.5002029538, 0.1649118662, 0.3367874622,\n",
            "        0.4145828485, 0.3159506321, 0.0890110135, 0.1186345816, 0.1185052991])\n",
            "btensor.grad: tensor([ 0.2762084305, -0.1769613475,  0.1619912684,  0.0252983272,\n",
            "         0.0289398246,  0.4313318729,  0.1635723710,  0.1144460291,\n",
            "         0.1510364413,  0.1783272028,  0.0299215615,  0.1385621428,\n",
            "        -0.0663969517,  0.0982878655,  0.0555646420, -0.2140540481,\n",
            "        -0.0376892090,  0.2960001528,  0.0318378806,  0.3128930926])\n",
            "ctensor.grad: tensor([-13.4403743744, -13.4403743744, -13.4403743744, -10.5580272675,\n",
            "          0.3071919680,  -2.0891337395,   4.3428339958,   1.6717038155,\n",
            "        -10.5580272675,   0.3071919680,  -2.0891337395,   4.3428339958,\n",
            "          1.6717038155, -10.5580272675,   0.3071919680,  -2.0891337395,\n",
            "          4.3428339958,   1.6717038155])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(736.3655395508, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8369351029), tensor(1.7727181911), tensor(1.1065282822), tensor(1.4460042715), tensor(0.8485216498), tensor(0.7182073593), tensor(1.6843783855), tensor(1.1633120775), tensor(1.3404679298), tensor(1.0783519745), tensor(1.0273399353), tensor(1.0877169371), tensor(1.8016957045), tensor(1.2435675859), tensor(1.6341506243), tensor(1.7848192453), tensor(1.5902358294), tensor(1.1356264353), tensor(0.8894025087), tensor(1.1041969061)]\n",
            "b:  [tensor(1.6273422241), tensor(0.7388852239), tensor(1.4589192867), tensor(1.3138291836), tensor(1.0123347044), tensor(1.9825816154), tensor(1.2757542133), tensor(1.3983495235), tensor(1.2834928036), tensor(1.0597829819), tensor(1.3685576916), tensor(1.2904691696), tensor(0.7708093524), tensor(1.2656153440), tensor(1.2444254160), tensor(0.9133555293), tensor(1.1079522371), tensor(1.3944021463), tensor(1.3905425072), tensor(1.5457745790)]\n",
            "c:  [tensor(0.1167928129), tensor(0.1167928129), tensor(0.1167928129), tensor(0.0973192304), tensor(-0.0023365333), tensor(0.0482624173), tensor(-0.0253945515), tensor(0.0048982464), tensor(0.0973192304), tensor(-0.0023365333), tensor(0.0482624173), tensor(-0.0253945515), tensor(0.0048982464), tensor(0.0973192304), tensor(-0.0023365333), tensor(0.0482624173), tensor(-0.0253945515), tensor(0.0048982464)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1419379115, 0.4075730145, 0.2983790040, 0.4483879805, 0.3854212761,\n",
            "        0.0759047270, 0.4104793072, 0.2890798748, 0.2784786820, 0.3693746924,\n",
            "        0.1141529083, 0.2329464257, 0.5270392895, 0.1688317060, 0.3548102677,\n",
            "        0.4389800727, 0.3343079686, 0.0843105912, 0.1092864275, 0.1117728353])\n",
            "btensor.grad: tensor([ 0.2864791751, -0.2025960386,  0.1612173319,  0.0126949549,\n",
            "         0.0145137943,  0.4539201558,  0.1575859189,  0.1062532961,\n",
            "         0.1459687948,  0.1745997667,  0.0161258876,  0.1351830363,\n",
            "        -0.0861790776,  0.0879275650,  0.0353530347, -0.2429593801,\n",
            "        -0.0558531284,  0.3015296757,  0.0167476535,  0.3224873543])\n",
            "ctensor.grad: tensor([-13.9611501694, -13.9611501694, -13.9611501694, -10.9526166916,\n",
            "          0.3267128766,  -1.7040609121,   4.7251200676,   2.0188896656,\n",
            "        -10.9526166916,   0.3267128766,  -1.7040609121,   4.7251200676,\n",
            "          2.0188896656, -10.9526166916,   0.3267128766,  -1.7040609121,\n",
            "          4.7251200676,   2.0188896656])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(735.8257446289, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8362790346), tensor(1.7705787420), tensor(1.1050332785), tensor(1.4436531067), tensor(0.8466023803), tensor(0.7179325819), tensor(1.6822160482), tensor(1.1618278027), tensor(1.3390289545), tensor(1.0764749050), tensor(1.0267975330), tensor(1.0865014791), tensor(1.7989411354), tensor(1.2427242994), tensor(1.6323016882), tensor(1.7825145721), tensor(1.5884866714), tensor(1.1352533102), tensor(0.8889356852), tensor(1.1037014723)]\n",
            "b:  [tensor(1.6258825064), tensor(0.7400429845), tensor(1.4581432343), tensor(1.3138543367), tensor(1.0123599768), tensor(1.9802262783), tensor(1.2750239372), tensor(1.3978888988), tensor(1.2828153372), tensor(1.0589550734), tensor(1.3685749769), tensor(1.2898342609), tensor(0.7713586092), tensor(1.2652573586), tensor(1.2443872690), tensor(0.9147331119), tensor(1.1083465815), tensor(1.3928953409), tensor(1.3905651569), tensor(1.5441417694)]\n",
            "c:  [tensor(0.1240363196), tensor(0.1240363196), tensor(0.1240363196), tensor(0.1030004472), tensor(-0.0025097344), tensor(0.0489255935), tensor(-0.0279463362), tensor(0.0037302810), tensor(0.1030004472), tensor(-0.0025097344), tensor(0.0489255935), tensor(-0.0279463362), tensor(0.0037302810), tensor(0.1030004472), tensor(-0.0025097344), tensor(0.0489255935), tensor(-0.0279463362), tensor(0.0037302810)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1312140226, 0.4278948605, 0.2990097404, 0.4702441692, 0.3838510513,\n",
            "        0.0549520254, 0.4324589968, 0.2968602180, 0.2877943516, 0.3754076362,\n",
            "        0.1084719002, 0.2430880368, 0.5509107709, 0.1686676741, 0.3697753549,\n",
            "        0.4609306753, 0.3498394489, 0.0746154785, 0.0933646560, 0.0990948081])\n",
            "btensor.grad: tensor([ 0.2919365168, -0.2315531969,  0.1552057266, -0.0050349832,\n",
            "        -0.0050569624,  0.4710772932,  0.1460595131,  0.0921274424,\n",
            "         0.1355046034,  0.1655929089, -0.0034575164,  0.1269729137,\n",
            "        -0.1098483652,  0.0715868324,  0.0076181591, -0.2755208611,\n",
            "        -0.0788631439,  0.3013643622, -0.0045217872,  0.3265528083])\n",
            "ctensor.grad: tensor([-14.4870090485, -14.4870090485, -14.4870090485, -11.3624315262,\n",
            "          0.3464021385,  -1.3263520002,   5.1035685539,   2.3359305859,\n",
            "        -11.3624315262,   0.3464021385,  -1.3263520002,   5.1035685539,\n",
            "          2.3359305859, -11.3624315262,   0.3464021385,  -1.3263520002,\n",
            "          5.1035685539,   2.3359305859])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(735.2417602539, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8357058167), tensor(1.7683489323), tensor(1.1035628319), tensor(1.4412049055), tensor(0.8447242975), tensor(0.7177946568), tensor(1.6799529791), tensor(1.1603232622), tensor(1.3375591040), tensor(1.0745930672), tensor(1.0263043642), tensor(1.0852496624), tensor(1.7960799932), tensor(1.2419004440), tensor(1.6303917170), tensor(1.7801107168), tensor(1.5866721869), tensor(1.1349521875), tensor(0.8885791302), tensor(1.1032975912)]\n",
            "b:  [tensor(1.6244182587), tensor(0.7413602471), tensor(1.4574223757), tensor(1.3139922619), tensor(1.0125073195), tensor(1.9778115749), tensor(1.2743768692), tensor(1.3975273371), tensor(1.2822152376), tensor(1.0581969023), tensor(1.3687180281), tensor(1.2892628908), tensor(0.7720435262), tensor(1.2650096416), tensor(1.2445242405), tensor(0.9162899852), tensor(1.1088782549), tensor(1.3914161921), tensor(1.3907239437), tensor(1.5425151587)]\n",
            "c:  [tensor(0.1315465271), tensor(0.1315465271), tensor(0.1315465271), tensor(0.1088954881), tensor(-0.0026928354), tensor(0.0494070947), tensor(-0.0306835640), tensor(0.0024232587), tensor(0.1088954881), tensor(-0.0026928354), tensor(0.0494070947), tensor(-0.0306835640), tensor(0.0024232587), tensor(0.1088954881), tensor(-0.0026928354), tensor(0.0494070947), tensor(-0.0306835640), tensor(0.0024232587)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1146438718, 0.4459581971, 0.2940919399, 0.4896359444, 0.3756207228,\n",
            "        0.0275814533, 0.4526029825, 0.3009190261, 0.2939590812, 0.3763661683,\n",
            "        0.0986364484, 0.2503564060, 0.5722296834, 0.1647815108, 0.3819967508,\n",
            "        0.4807816148, 0.3628993034, 0.0602311492, 0.0713118315, 0.0807785094])\n",
            "btensor.grad: tensor([ 0.2928595543, -0.2634508908,  0.1441814601, -0.0275870264,\n",
            "        -0.0294714123,  0.4829463065,  0.1294112206,  0.0723077208,\n",
            "         0.1200158000,  0.1516453028, -0.0286150873,  0.1142856479,\n",
            "        -0.1369804293,  0.0495454073, -0.0273935199, -0.3113797903,\n",
            "        -0.1063439846,  0.2958253622, -0.0317469835,  0.3253237605])\n",
            "ctensor.grad: tensor([-15.0204057693, -15.0204057693, -15.0204057693, -11.7900781631,\n",
            "          0.3662020564,  -0.9630039930,   5.4744563103,   2.6140444279,\n",
            "        -11.7900781631,   0.3662020564,  -0.9630039930,   5.4744563103,\n",
            "          2.6140444279, -11.7900781631,   0.3662020564,  -0.9630039930,\n",
            "          5.4744563103,   2.6140444279])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(734.6106567383, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8352413177), tensor(1.7660377026), tensor(1.1021420956), tensor(1.4386688471), tensor(0.8429160714), tensor(0.7178210616), tensor(1.6775959730), tensor(1.1588141918), tensor(1.3360712528), tensor(1.0727285147), tensor(1.0258780718), tensor(1.0839731693), tensor(1.7931222916), tensor(1.2411117554), tensor(1.6284320354), tensor(1.7776156664), tensor(1.5848022699), tensor(1.1347438097), tensor(0.8883599639), tensor(1.1030107737)]\n",
            "b:  [tensor(1.6229695082), tensor(0.7428493500), tensor(1.4567794800), tensor(1.3142645359), tensor(1.0127986670), tensor(1.9753620625), tensor(1.2738355398), tensor(1.3972911835), tensor(1.2817149162), tensor(1.0575305223), tensor(1.3690127134), tensor(1.2887746096), tensor(0.7728787661), tensor(1.2648981810), tensor(1.2448701859), tensor(0.9180403352), tensor(1.1095669270), tensor(1.3899890184), tensor(1.3910462856), tensor(1.5409188271)]\n",
            "c:  [tensor(0.1393276751), tensor(0.1393276751), tensor(0.1393276751), tensor(0.1150136888), tensor(-0.0028858725), tensor(0.0497160554), tensor(-0.0336009488), tensor(0.0009997752), tensor(0.1150136888), tensor(-0.0028858725), tensor(0.0497160554), tensor(-0.0336009488), tensor(0.0009997752), tensor(0.1150136888), tensor(-0.0028858725), tensor(0.0497160554), tensor(-0.0336009488), tensor(0.0009997752)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0928944945,  0.4622350335,  0.2841540575,  0.5072150230,\n",
            "         0.3616437912, -0.0052767992,  0.4714125395,  0.3018078208,\n",
            "         0.2975668013,  0.3729088008,  0.0852510631,  0.2553051114,\n",
            "         0.5915417671,  0.1577384472,  0.3919454813,  0.4990088046,\n",
            "         0.3739932775,  0.0416870713,  0.0438327789,  0.0573580861])\n",
            "btensor.grad: tensor([ 0.2897458971, -0.2978147566,  0.1285782456, -0.0544571877,\n",
            "        -0.0582686774,  0.4899041951,  0.1082597375,  0.0472389609,\n",
            "         0.1000719666,  0.1332709789, -0.0589280725,  0.0976651311,\n",
            "        -0.1670422852,  0.0222987235, -0.0691839457, -0.3500715494,\n",
            "        -0.1377255917,  0.2854382396, -0.0644623041,  0.3192642331])\n",
            "ctensor.grad: tensor([-15.5622911453, -15.5622911453, -15.5622911453, -12.2364034653,\n",
            "          0.3860740066,  -0.6179190874,   5.8347687721,   2.8469667435,\n",
            "        -12.2364034653,   0.3860740066,  -0.6179190874,   5.8347687721,\n",
            "          2.8469667435, -12.2364034653,   0.3860740066,  -0.6179190874,\n",
            "          5.8347687721,   2.8469667435])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(733.9311523438, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8349072337), tensor(1.7636513710), tensor(1.1007927656), tensor(1.4360501766), tensor(0.8412010670), tensor(0.7180336714), tensor(1.6751487255), tensor(1.1573132277), tensor(1.3345745802), tensor(1.0708993673), tensor(1.0255328417), tensor(1.0826801062), tensor(1.7900749445), tensor(1.2403705120), tensor(1.6264312267), tensor(1.7750349045), tensor(1.5828835964), tensor(1.1346455812), tensor(0.8883010149), tensor(1.1028631926)]\n",
            "b:  [tensor(1.6215534210), tensor(0.7445200086), tensor(1.4562346935), tensor(1.3146896362), tensor(1.0132530928), tensor(1.9728996754), tensor(1.2734186649), tensor(1.3972035646), tensor(1.2813329697), tensor(1.0569751263), tensor(1.3694819212), tensor(1.2883857489), tensor(0.7738759518), tensor(1.2649458647), tensor(1.2454555035), tensor(0.9199956656), tensor(1.1104285717), tensor(1.3886346817), tensor(1.3915566206), tensor(1.5393738747)]\n",
            "c:  [tensor(0.1473838240), tensor(0.1473838240), tensor(0.1473838240), tensor(0.1213640198), tensor(-0.0030888706), tensor(0.0498621762), tensor(-0.0366919897), tensor(-0.0005157376), tensor(0.1213640198), tensor(-0.0030888706), tensor(0.0498621762), tensor(-0.0366919897), tensor(-0.0005157376), tensor(0.1213640198), tensor(-0.0030888706), tensor(0.0498621762), tensor(-0.0366919897), tensor(-0.0005157376)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0668128729,  0.4772669673,  0.2698732018,  0.5237362385,\n",
            "         0.3430022001, -0.0425215960,  0.4894556999,  0.3001879454,\n",
            "         0.2993417084,  0.3658255935,  0.0690490305,  0.2586084008,\n",
            "         0.6094630361,  0.1482539177,  0.4001713395,  0.5161412358,\n",
            "         0.3837357759,  0.0196495056,  0.0117855668,  0.0295266807])\n",
            "btensor.grad: tensor([ 0.2832212746, -0.3341343999,  0.1089611053, -0.0850113034,\n",
            "        -0.0908958316,  0.4924834371,  0.0833691955,  0.0175148249,\n",
            "         0.0763897300,  0.1110812426, -0.0938445926,  0.0777710080,\n",
            "        -0.1994427741, -0.0095271766, -0.1170729995, -0.3910688162,\n",
            "        -0.1723265648,  0.2708662450, -0.1020650268,  0.3089826107])\n",
            "ctensor.grad: tensor([-16.1122894287, -16.1122894287, -16.1122894287, -12.7006578445,\n",
            "          0.4059958458,  -0.2922404706,   6.1820783615,   3.0310254097,\n",
            "        -12.7006578445,   0.4059958458,  -0.2922404706,   6.1820783615,\n",
            "          3.0310254097, -12.7006578445,   0.4059958458,  -0.2922404706,\n",
            "          6.1820783615,   3.0310254097])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(733.1990356445, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8347206712), tensor(1.7611932755), tensor(1.0995328426), tensor(1.4333503246), tensor(0.8395967484), tensor(0.7184485793), tensor(1.6726121902), tensor(1.1558294296), tensor(1.3330742121), tensor(1.0691196918), tensor(1.0252786875), tensor(1.0813752413), tensor(1.7869418859), tensor(1.2396849394), tensor(1.6243948936), tensor(1.7723714113), tensor(1.5809197426), tensor(1.1346713305), tensor(0.8884205222), tensor(1.1028729677)]\n",
            "b:  [tensor(1.6201835871), tensor(0.7463794947), tensor(1.4558048248), tensor(1.3152823448), tensor(1.0138869286), tensor(1.9704432487), tensor(1.2731409073), tensor(1.3972846270), tensor(1.2810842991), tensor(1.0565465689), tensor(1.3701456785), tensor(1.2881091833), tensor(0.7750439644), tensor(1.2651720047), tensor(1.2463068962), tensor(0.9221648574), tensor(1.1114755869), tensor(1.3873705864), tensor(1.3922759295), tensor(1.5378980637)]\n",
            "c:  [tensor(0.1557182521), tensor(0.1557182521), tensor(0.1557182521), tensor(0.1279543191), tensor(-0.0033018505), tensor(0.0498546287), tensor(-0.0399491675), tensor(-0.0020980886), tensor(0.1279543191), tensor(-0.0033018505), tensor(0.0498546287), tensor(-0.0399491675), tensor(-0.0020980886), tensor(0.1279543191), tensor(-0.0033018505), tensor(0.0498546287), tensor(-0.0399491675), tensor(-0.0020980886)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0373115540,  0.4916118085,  0.2519958913,  0.5399745703,\n",
            "         0.3208585978, -0.0829787254,  0.5073056221,  0.2967646420,\n",
            "         0.3000748158,  0.3559432924,  0.0508199334,  0.2609791160,\n",
            "         0.6266064048,  0.1371122003,  0.4072572589,  0.5327070355,\n",
            "         0.3927612305, -0.0051475167, -0.0238973498, -0.0019467473])\n",
            "btensor.grad: tensor([ 0.2739781737, -0.3718928397,  0.0859687626, -0.1185519099,\n",
            "        -0.1267742813,  0.4912919998,  0.0555584431, -0.0162017196,\n",
            "         0.0497326851,  0.0857179165, -0.1327450275,  0.0553047657,\n",
            "        -0.2336008102, -0.0452347398, -0.1702736616, -0.4338367581,\n",
            "        -0.2094137669,  0.2528160214, -0.1438550353,  0.2951514721])\n",
            "ctensor.grad: tensor([-1.6668849945e+01, -1.6668849945e+01, -1.6668849945e+01,\n",
            "        -1.3180609703e+01,  4.2595976591e-01,  1.5095070004e-02,\n",
            "         6.5143547058e+00,  3.1647021770e+00, -1.3180609703e+01,\n",
            "         4.2595976591e-01,  1.5095070004e-02,  6.5143547058e+00,\n",
            "         3.1647021770e+00, -1.3180609703e+01,  4.2595976591e-01,\n",
            "         1.5095070004e-02,  6.5143547058e+00,  3.1647021770e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(732.4116821289, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8346941471), tensor(1.7586643696), tensor(1.0983763933), tensor(1.4305669069), tensor(0.8381149769), tensor(0.7190760374), tensor(1.6699846983), tensor(1.1543682814), tensor(1.3315714598), tensor(1.0673993826), tensor(1.0251220465), tensor(1.0800596476), tensor(1.7837241888), tensor(1.2390593290), tensor(1.6223261356), tensor(1.7696254253), tensor(1.5789113045), tensor(1.1348310709), tensor(0.8887319565), tensor(1.1030542850)]\n",
            "b:  [tensor(1.6188700199), tensor(0.7484325171), tensor(1.4555035830), tensor(1.3160542250), tensor(1.0147136450), tensor(1.9680085182), tensor(1.2730126381), tensor(1.3975507021), tensor(1.2809798717), tensor(1.0562577248), tensor(1.3710206747), tensor(1.2879543304), tensor(0.7763887644), tensor(1.2655926943), tensor(1.2474466562), tensor(0.9245541096), tensor(1.1127167940), tensor(1.3862106800), tensor(1.3932214975), tensor(1.5365058184)]\n",
            "c:  [tensor(0.1643330157), tensor(0.1643330157), tensor(0.1643330157), tensor(0.1347907335), tensor(-0.0035248352), tensor(0.0497012325), tensor(-0.0433640517), tensor(-0.0037222682), tensor(0.1347907335), tensor(-0.0035248352), tensor(0.0497012325), tensor(-0.0433640517), tensor(-0.0037222682), tensor(0.1347907335), tensor(-0.0035248352), tensor(0.0497012325), tensor(-0.0433640517), tensor(-0.0037222682)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0052989125,  0.5057870746,  0.2312783301,  0.5566760302,\n",
            "         0.2963562012, -0.1254914999,  0.5254943371,  0.2922239304,\n",
            "         0.3005588949,  0.3440644145,  0.0313373208,  0.2631125450,\n",
            "         0.6435498595,  0.1251116395,  0.4137557745,  0.5491859913,\n",
            "         0.4016897678, -0.0319495201, -0.0622907281, -0.0362728238])\n",
            "btensor.grad: tensor([ 0.2627029121, -0.4106066823,  0.0602508783, -0.1543754637,\n",
            "        -0.1653464437,  0.4869385064,  0.0256575346, -0.0532203168,\n",
            "         0.0208832026,  0.0577696562, -0.1749963164,  0.0309591889,\n",
            "        -0.2689648569, -0.0841266811, -0.2279537022, -0.4778530002,\n",
            "        -0.2482483387,  0.2319884449, -0.1891065240,  0.2784491777])\n",
            "ctensor.grad: tensor([-17.2295398712, -17.2295398712, -17.2295398712, -13.6728363037,\n",
            "          0.4459691644,   0.3067910671,   6.8297700882,   3.2483587265,\n",
            "        -13.6728363037,   0.4459691644,   0.3067910671,   6.8297700882,\n",
            "          3.2483587265, -13.6728363037,   0.4459691644,   0.3067910671,\n",
            "          6.8297700882,   3.2483587265])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(731.5678710938, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8348360062), tensor(1.7560632229), tensor(1.0973342657), tensor(1.4276944399), tensor(0.8367622495), tensor(0.7199209929), tensor(1.6672623158), tensor(1.1529324055), tensor(1.3300637007), tensor(1.0657449961), tensor(1.0250654221), tensor(1.0787315369), tensor(1.7804203033), tensor(1.2384942770), tensor(1.6202254295), tensor(1.7667956352), tensor(1.5768560171), tensor(1.1351313591), tensor(0.8892446160), tensor(1.1034178734)]\n",
            "b:  [tensor(1.6176199913), tensor(0.7506818175), tensor(1.4553414583), tensor(1.3170132637), tensor(1.0157443285), tensor(1.9656085968), tensor(1.2730405331), tensor(1.3980151415), tensor(1.2810270786), tensor(1.0561189651), tensor(1.3721207380), tensor(1.2879275084), tensor(0.7779141665), tensor(1.2662204504), tensor(1.2488931417), tensor(0.9271673560), tensor(1.1141574383), tensor(1.3851656914), tensor(1.3944071531), tensor(1.5352083445)]\n",
            "c:  [tensor(0.1732287109), tensor(0.1732287109), tensor(0.1732287109), tensor(0.1418773234), tensor(-0.0037578540), tensor(0.0494080745), tensor(-0.0469272882), tensor(-0.0053640977), tensor(0.1418773234), tensor(-0.0037578540), tensor(0.0494080745), tensor(-0.0469272882), tensor(-0.0053640977), tensor(0.1418773234), tensor(-0.0037578540), tensor(0.0494080745), tensor(-0.0469272882), tensor(-0.0053640977)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0283744335,  0.5202264786,  0.2084159255,  0.5744872689,\n",
            "         0.2705484629, -0.1689925194,  0.5444824696,  0.2871764302,\n",
            "         0.3015512228,  0.3308878243,  0.0113199949,  0.2656131685,\n",
            "         0.6607742310,  0.1130061150,  0.4201527834,  0.5659570694,\n",
            "         0.4110652804, -0.0600481629, -0.1025361419, -0.0727123618])\n",
            "btensor.grad: tensor([ 0.2500171363, -0.4498603344,  0.0324165821, -0.1918089390,\n",
            "        -0.2061279565,  0.4799820781, -0.0055726171, -0.0928959846,\n",
            "        -0.0094481111,  0.0277464390, -0.2200022638,  0.0053629875,\n",
            "        -0.3050758243, -0.1255486012, -0.2892906070, -0.5226475000,\n",
            "        -0.2881329060,  0.2090087384, -0.2371227741,  0.2594982982])\n",
            "ctensor.grad: tensor([-17.7913799286, -17.7913799286, -17.7913799286, -14.1731710434,\n",
            "          0.4660374522,   0.5863194466,   7.1264715195,   3.2836585045,\n",
            "        -14.1731710434,   0.4660374522,   0.5863194466,   7.1264715195,\n",
            "          3.2836585045, -14.1731710434,   0.4660374522,   0.5863194466,\n",
            "          7.1264715195,   3.2836585045])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(730.6639404297, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8351508379), tensor(1.7533868551), tensor(1.0964140892), tensor(1.4247246981), tensor(0.8355404735), tensor(0.7209836841), tensor(1.6644390821), tensor(1.1515216827), tensor(1.3285450935), tensor(1.0641599894), tensor(1.0251084566), tensor(1.0773864985), tensor(1.7770270109), tensor(1.2379868031), tensor(1.6180912256), tensor(1.7638791800), tensor(1.5747493505), tensor(1.1355752945), tensor(0.8899639249), tensor(1.1039707661)]\n",
            "b:  [tensor(1.6164376736), tensor(0.7531282902), tensor(1.4553264380), tensor(1.3181644678), tensor(1.0169879198), tensor(1.9632540941), tensor(1.2732276917), tensor(1.3986883163), tensor(1.2812302113), tensor(1.0561387539), tensor(1.3734568357), tensor(1.2880321741), tensor(0.7796218395), tensor(1.2670650482), tensor(1.2506606579), tensor(0.9300063848), tensor(1.1157996655), tensor(1.3842436075), tensor(1.3958433867), tensor(1.5340141058)]\n",
            "c:  [tensor(0.1824041605), tensor(0.1824041605), tensor(0.1824041605), tensor(0.1492157578), tensor(-0.0040009469), tensor(0.0489793457), tensor(-0.0506285317), tensor(-0.0070007546), tensor(0.1492157578), tensor(-0.0040009469), tensor(0.0489793457), tensor(-0.0506285317), tensor(-0.0070007546), tensor(0.1492157578), tensor(-0.0040009469), tensor(0.0489793457), tensor(-0.0506285317), tensor(-0.0070007546)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0629636645,  0.5352668762,  0.1840328276,  0.5939543843,\n",
            "         0.2443572283, -0.2125387192,  0.5646356344,  0.2821422517,\n",
            "         0.3037320971,  0.3170118630, -0.0086114705,  0.2690087855,\n",
            "         0.6786624193,  0.1014891267,  0.4268518090,  0.5833007097,\n",
            "         0.4213421941, -0.0887966156, -0.1438654661, -0.1105840802])\n",
            "btensor.grad: tensor([ 0.2364690304, -0.4892927706,  0.0030109584, -0.2302398980,\n",
            "        -0.2487164736,  0.4708973169, -0.0374338627, -0.1346330792,\n",
            "        -0.0406179428, -0.0039503574, -0.2672186494, -0.0209383368,\n",
            "        -0.3415379524, -0.1689130217, -0.3534982800, -0.5678019524,\n",
            "        -0.3284368515,  0.1844092458, -0.2872550488,  0.2388448715])\n",
            "ctensor.grad: tensor([-18.3509140015, -18.3509140015, -18.3509140015, -14.6768712997,\n",
            "          0.4861855209,   0.8574559689,   7.4024877548,   3.2733132839,\n",
            "        -14.6768712997,   0.4861855209,   0.8574559689,   7.4024877548,\n",
            "          3.2733132839, -14.6768712997,   0.4861855209,   0.8574559689,\n",
            "          7.4024877548,   3.2733132839])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(729.6989135742, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8356400728), tensor(1.7506312132), tensor(1.0956208706), tensor(1.4216471910), tensor(0.8344476819), tensor(0.7222603559), tensor(1.6615080833), tensor(1.1501340866), tensor(1.3270065784), tensor(1.0626455545), tensor(1.0252481699), tensor(1.0760180950), tensor(1.7735395432), tensor(1.2375310659), tensor(1.6159205437), tensor(1.7608722448), tensor(1.5725851059), tensor(1.1361634731), tensor(0.8908920884), tensor(1.1047173738)]\n",
            "b:  [tensor(1.6153252125), tensor(0.7557713389), tensor(1.4554640055), tensor(1.3195102215), tensor(1.0184519291), tensor(1.9609538317), tensor(1.2735743523), tensor(1.3995778561), tensor(1.2815905809), tensor(1.0563238859), tensor(1.3750377893), tensor(1.2882696390), tensor(0.7815121412), tensor(1.2681337595), tensor(1.2527599335), tensor(0.9330711961), tensor(1.1176426411), tensor(1.3834505081), tensor(1.3975380659), tensor(1.5329294205)]\n",
            "c:  [tensor(0.1918563992), tensor(0.1918563992), tensor(0.1918563992), tensor(0.1568052322), tensor(-0.0042541670), tensor(0.0484175086), tensor(-0.0544562899), tensor(-0.0086110849), tensor(0.1568052322), tensor(-0.0042541670), tensor(0.0484175086), tensor(-0.0544562899), tensor(-0.0086110849), tensor(0.1568052322), tensor(-0.0042541670), tensor(0.0484175086), tensor(-0.0544562899), tensor(-0.0086110849)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.0978455544,  0.5511260033,  0.1586486399,  0.6154898405,\n",
            "         0.2185586691, -0.2553372383,  0.5862028599,  0.2775174081,\n",
            "         0.3076924682,  0.3028798103, -0.0279452205,  0.2736800611,\n",
            "         0.6974855661,  0.0911538601,  0.4341459870,  0.6013833284,\n",
            "         0.4328601956, -0.1176420450, -0.1856353283, -0.1493124068])\n",
            "btensor.grad: tensor([ 0.2224920243, -0.5286148787, -0.0275033712, -0.2691404521,\n",
            "        -0.2928009331,  0.4600449204, -0.0693284273, -0.1779080629,\n",
            "        -0.0720794201, -0.0370292664, -0.3161875308, -0.0475001931,\n",
            "        -0.3780544996, -0.2137310803, -0.4198545516, -0.6129604578,\n",
            "        -0.3686056137,  0.1586111188, -0.3389398456,  0.2169257998])\n",
            "ctensor.grad: tensor([-18.9044857025, -18.9044857025, -18.9044857025, -15.1789588928,\n",
            "          0.5064405203,   1.1236712933,   7.6555175781,   3.2206609249,\n",
            "        -15.1789588928,   0.5064405203,   1.1236712933,   7.6555175781,\n",
            "          3.2206609249, -15.1789588928,   0.5064405203,   1.1236712933,\n",
            "          7.6555175781,   3.2206609249])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(728.6725463867, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8363026977), tensor(1.7477915287), tensor(1.0949574709), tensor(1.4184502363), tensor(0.8334788084), tensor(0.7237440944), tensor(1.6584613323), tensor(1.1487661600), tensor(1.3254369497), tensor(1.0612014532), tensor(1.0254795551), tensor(1.0746185780), tensor(1.7699525356), tensor(1.2371184826), tensor(1.6137093306), tensor(1.7577710152), tensor(1.5703557730), tensor(1.1368939877), tensor(0.8920286298), tensor(1.1056593657)]\n",
            "b:  [tensor(1.6142830849), tensor(0.7586092949), tensor(1.4557577372), tensor(1.3210505247), tensor(1.0201426744), tensor(1.9587153196), tensor(1.2740781307), tensor(1.4006892443), tensor(1.2821074724), tensor(1.0566804409), tensor(1.3768702745), tensor(1.2886394262), tensor(0.7835841179), tensor(1.2694317102), tensor(1.2551983595), tensor(0.9363602996), tensor(1.1196835041), tensor(1.3827908039), tensor(1.3994964361), tensor(1.5319590569)]\n",
            "c:  [tensor(0.2015805691), tensor(0.2015805691), tensor(0.2015805691), tensor(0.1646424383), tensor(-0.0045175846), tensor(0.0477235988), tensor(-0.0583977625), tensor(-0.0101758810), tensor(0.1646424383), tensor(-0.0045175846), tensor(0.0477235988), tensor(-0.0583977625), tensor(-0.0101758810), tensor(0.1646424383), tensor(-0.0045175846), tensor(0.0477235988), tensor(-0.0583977625), tensor(-0.0101758810)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.1325200200,  0.5679266453,  0.1326872706,  0.6393866539,\n",
            "         0.1937783957, -0.2967444658,  0.6093515158,  0.2735910416,\n",
            "         0.3139219582,  0.2888123393, -0.0462717414,  0.2799022198,\n",
            "         0.7174034119,  0.0825074911,  0.4422416687,  0.6202566624,\n",
            "         0.4458634853, -0.1461110711, -0.2273082137, -0.1884036660])\n",
            "btensor.grad: tensor([ 0.2084227502, -0.5675944686, -0.0587448478, -0.3080540597,\n",
            "        -0.3381465375,  0.4476973712, -0.1007456183, -0.2222837210,\n",
            "        -0.1033827066, -0.0713100433, -0.3665075898, -0.0739633441,\n",
            "        -0.4143903553, -0.2595912814, -0.4876880348, -0.6578165293,\n",
            "        -0.4081634283,  0.1319367737, -0.3916670084,  0.1940845251])\n",
            "ctensor.grad: tensor([-19.4483451843, -19.4483451843, -19.4483451843, -15.6744041443,\n",
            "          0.5268354416,   1.3878176212,   7.8829436302,   3.1295912266,\n",
            "        -15.6744041443,   0.5268354416,   1.3878176212,   7.8829436302,\n",
            "          3.1295912266, -15.6744041443,   0.5268354416,   1.3878176212,\n",
            "          7.8829436302,   3.1295912266])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(727.5833129883, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8371356130), tensor(1.7448630333), tensor(1.0944250822), tensor(1.4151210785), tensor(0.8326262832), tensor(0.7254253626), tensor(1.6552906036), tensor(1.1474133730), tensor(1.3238228559), tensor(1.0598263741), tensor(1.0257959366), tensor(1.0731793642), tensor(1.7662601471), tensor(1.2367386818), tensor(1.6114530563), tensor(1.7545715570), tensor(1.5680533648), tensor(1.1377630234), tensor(0.8933708668), tensor(1.1067966223)]\n",
            "b:  [tensor(1.6133105755), tensor(0.7616394758), tensor(1.4562097788), tensor(1.3227834702), tensor(1.0220656395), tensor(1.9565452337), tensor(1.2747343779), tensor(1.4020260572), tensor(1.2827782631), tensor(1.0572139025), tensor(1.3789594173), tensor(1.2891397476), tensor(0.7858359218), tensor(1.2709624767), tensor(1.2579803467), tensor(0.9398708344), tensor(1.1219170094), tensor(1.3822677135), tensor(1.4017214775), tensor(1.5311061144)]\n",
            "c:  [tensor(0.2115698308), tensor(0.2115698308), tensor(0.2115698308), tensor(0.1727215946), tensor(-0.0047912891), tensor(0.0468975976), tensor(-0.0624386668), tensor(-0.0116780410), tensor(0.1727215946), tensor(-0.0047912891), tensor(0.0468975976), tensor(-0.0624386668), tensor(-0.0116780410), tensor(0.1727215946), tensor(-0.0047912891), tensor(0.0468975976), tensor(-0.0624386668), tensor(-0.0116780410)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.1665864587,  0.5856880546,  0.1064743102,  0.6658200026,\n",
            "         0.1705102921, -0.3362516165,  0.6341490746,  0.2705493569,\n",
            "         0.3228252232,  0.2750149071, -0.0632811189,  0.2878409624,\n",
            "         0.7384851575,  0.0759595633,  0.4512602091,  0.6398888230,\n",
            "         0.4604878426, -0.1738091707, -0.2684425116, -0.2274598479])\n",
            "btensor.grad: tensor([ 0.1945109367, -0.6060400009, -0.0904088318, -0.3465943038,\n",
            "        -0.3845883608,  0.4340289235, -0.1312509179, -0.2673707604,\n",
            "        -0.1341595054, -0.1066883802, -0.4178380966, -0.1000574231,\n",
            "        -0.4503627419, -0.3061548471, -0.5563863516, -0.7021078467,\n",
            "        -0.4467012882,  0.1046184301, -0.4450097680,  0.1705805659])\n",
            "ctensor.grad: tensor([-19.9785289764, -19.9785289764, -19.9785289764, -16.1583156586,\n",
            "          0.5474090576,   1.6520009041,   8.0818080902,   3.0043206215,\n",
            "        -16.1583156586,   0.5474090576,   1.6520009041,   8.0818080902,\n",
            "          3.0043206215, -16.1583156586,   0.5474090576,   1.6520009041,\n",
            "          8.0818080902,   3.0043206215])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(726.4321899414, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8381343484), tensor(1.7418413162), tensor(1.0940237045), tensor(1.4116467237), tensor(0.8318806291), tensor(0.7272927165), tensor(1.6519876719), tensor(1.1460709572), tensor(1.3221492767), tensor(1.0585184097), tensor(1.0261896849), tensor(1.0716915131), tensor(1.7624565363), tensor(1.2363795042), tensor(1.6091468334), tensor(1.7512707710), tensor(1.5656694174), tensor(1.1387650967), tensor(0.8949142694), tensor(1.1081273556)]\n",
            "b:  [tensor(1.6124058962), tensor(0.7648584247), tensor(1.4568209648), tensor(1.3247056007), tensor(1.0242255926), tensor(1.9544495344), tensor(1.2755367756), tensor(1.4035902023), tensor(1.2835987806), tensor(1.0579295158), tensor(1.3813087940), tensor(1.2897675037), tensor(0.7882651091), tensor(1.2727282047), tensor(1.2611072063), tensor(0.9435988665), tensor(1.1243363619), tensor(1.3818836212), tensor(1.4042143822), tensor(1.5303732157)]\n",
            "c:  [tensor(0.2218154073), tensor(0.2218154073), tensor(0.2218154073), tensor(0.1810345501), tensor(-0.0050753918), tensor(0.0459388755), tensor(-0.0665630922), tensor(-0.0131027773), tensor(0.1810345501), tensor(-0.0050753918), tensor(0.0459388755), tensor(-0.0665630922), tensor(-0.0131027773), tensor(0.1810345501), tensor(-0.0050753918), tensor(0.0459388755), tensor(-0.0665630922), tensor(-0.0131027773)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.1997476816,  0.6043552160,  0.0802699029,  0.6948687434,\n",
            "         0.1491343975, -0.3734685183,  0.6605945826,  0.2684816122,\n",
            "         0.3347160518,  0.2615994215, -0.0787551254,  0.2975639701,\n",
            "         0.7607201934,  0.0718446374,  0.4612486660,  0.6601608992,\n",
            "         0.4767941833, -0.2004104257, -0.3086804152, -0.2661485672])\n",
            "btensor.grad: tensor([ 0.1809307784, -0.6437914371, -0.1222352087, -0.3844372034,\n",
            "        -0.4319862723,  0.4191488028, -0.1604885459, -0.3128404617,\n",
            "        -0.1641133428, -0.1431148052, -0.4698688388, -0.1255630851,\n",
            "        -0.4858359098, -0.3531363010, -0.6253673434, -0.7456051707,\n",
            "        -0.4838653803,  0.0768183768, -0.4985855818,  0.1465873718])\n",
            "ctensor.grad: tensor([-20.4911365509, -20.4911365509, -20.4911365509, -16.6259059906,\n",
            "          0.5682053566,   1.9174448252,   8.2488565445,   2.8494725227,\n",
            "        -16.6259059906,   0.5682053566,   1.9174448252,   8.2488565445,\n",
            "          2.8494725227, -16.6259059906,   0.5682053566,   1.9174448252,\n",
            "          8.2488565445,   2.8494725227])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(725.2182617188, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8392931819), tensor(1.7387222052), tensor(1.0937523842), tensor(1.4080140591), tensor(0.8312309980), tensor(0.7293333411), tensor(1.6485445499), tensor(1.1447339058), tensor(1.3204001188), tensor(1.0572754145), tensor(1.0266524553), tensor(1.0701462030), tensor(1.7585364580), tensor(1.2360274792), tensor(1.6067858934), tensor(1.7478662729), tensor(1.5631955862), tensor(1.1398932934), tensor(0.8966528773), tensor(1.1096483469)]\n",
            "b:  [tensor(1.6115669012), tensor(0.7682620287), tensor(1.4575910568), tensor(1.3268121481), tensor(1.0266267061), tensor(1.9524339437), tensor(1.2764775753), tensor(1.4053821564), tensor(1.2845637798), tensor(1.0588324070), tensor(1.3839204311), tensor(1.2905191183), tensor(0.7908685207), tensor(1.2747296095), tensor(1.2645776272), tensor(0.9475393295), tensor(1.1269330978), tensor(1.3816404343), tensor(1.4069746733), tensor(1.5297620296)]\n",
            "c:  [tensor(0.2323065102), tensor(0.2323065102), tensor(0.2323065102), tensor(0.1895708740), tensor(-0.0053700283), tensor(0.0448466316), tensor(-0.0707534105), tensor(-0.0144377518), tensor(0.1895708740), tensor(-0.0053700283), tensor(0.0448466316), tensor(-0.0707534105), tensor(-0.0144377518), tensor(0.1895708740), tensor(-0.0053700283), tensor(0.0448466316), tensor(-0.0707534105), tensor(-0.0144377518)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.2317703962,  0.6238126755,  0.0542649031,  0.7265210748,\n",
            "         0.1299265623, -0.4081237316,  0.6886271238,  0.2674060762,\n",
            "         0.3498229086,  0.2486038506, -0.0925457925,  0.3090689778,\n",
            "         0.7840239406,  0.0704124570,  0.4721965790,  0.6809061170,\n",
            "         0.4947674870, -0.2256394625, -0.3477252722, -0.3042098284])\n",
            "btensor.grad: tensor([ 0.1678030193, -0.6807162166, -0.1540137827, -0.4213034809,\n",
            "        -0.4802248180,  0.4031194448, -0.1881588697, -0.3583994210,\n",
            "        -0.1930076480, -0.1805753708, -0.5223186016, -0.1503219604,\n",
            "        -0.5206789374, -0.4002807140, -0.6940944195, -0.7880974412,\n",
            "        -0.5193500519,  0.0486466587, -0.5520568490,  0.1222342253])\n",
            "ctensor.grad: tensor([-20.9822139740, -20.9822139740, -20.9822139740, -17.0726509094,\n",
            "          0.5892726183,   2.1844863892,   8.3806362152,   2.6699495316,\n",
            "        -17.0726509094,   0.5892726183,   2.1844863892,   8.3806362152,\n",
            "          2.6699495316, -17.0726509094,   0.5892726183,   2.1844863892,\n",
            "          8.3806362152,   2.6699495316])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(723.9447631836, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8406056166), tensor(1.7355027199), tensor(1.0936093330), tensor(1.4042105675), tensor(0.8306655884), tensor(0.7315335274), tensor(1.6449539661), tensor(1.1433975697), tensor(1.3185585737), tensor(1.0560953617), tensor(1.0271753073), tensor(1.0685347319), tensor(1.7544951439), tensor(1.2356683016), tensor(1.6043655872), tensor(1.7443567514), tensor(1.5606238842), tensor(1.1411396265), tensor(0.8985795379), tensor(1.1113555431)]\n",
            "b:  [tensor(1.6107908487), tensor(0.7718454599), tensor(1.4585188627), tensor(1.3290969133), tensor(1.0292726755), tensor(1.9505040646), tensor(1.2775475979), tensor(1.4074010849), tensor(1.2856670618), tensor(1.0599277020), tensor(1.3867950439), tensor(1.2913900614), tensor(0.7936424613), tensor(1.2769664526), tensor(1.2683879137), tensor(0.9516862631), tensor(1.1296975613), tensor(1.3815394640), tensor(1.4100003242), tensor(1.5292739868)]\n",
            "c:  [tensor(0.2430303842), tensor(0.2430303842), tensor(0.2430303842), tensor(0.1983180344), tensor(-0.0056753601), tensor(0.0436202846), tensor(-0.0749902129), tensor(-0.0156732295), tensor(0.1983180344), tensor(-0.0056753601), tensor(0.0436202846), tensor(-0.0749902129), tensor(-0.0156732295), tensor(0.1983180344), tensor(-0.0056753601), tensor(0.0436202846), tensor(-0.0749902129), tensor(-0.0156732295)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.2624874115,  0.6438905597,  0.0286065042,  0.7606906295,\n",
            "         0.1130846739, -0.4400372505,  0.7181285620,  0.2672761381,\n",
            "         0.3682990372,  0.2360133827, -0.1045598239,  0.3222912848,\n",
            "         0.8082647920,  0.0718455911,  0.4840565622,  0.7019153237,\n",
            "         0.5143352747, -0.2492717505, -0.3853355646, -0.3414299488])\n",
            "btensor.grad: tensor([ 0.1552117765, -0.7166872025, -0.1855595708, -0.4569534361,\n",
            "        -0.5291892290,  0.3859762549, -0.2140005827, -0.4037775397,\n",
            "        -0.2206467390, -0.2190605402, -0.5749121904, -0.1741956472,\n",
            "        -0.5547835827, -0.4473693371, -0.7620474696, -0.8293892741,\n",
            "        -0.5528922081,  0.0201921761, -0.6051202416,  0.0976005197])\n",
            "ctensor.grad: tensor([-21.4477577209, -21.4477577209, -21.4477577209, -17.4943218231,\n",
            "          0.6106637716,   2.4526944160,   8.4736051559,   2.4709563255,\n",
            "        -17.4943218231,   0.6106637716,   2.4526944160,   8.4736051559,\n",
            "          2.4709563255, -17.4943218231,   0.6106637716,   2.4526944160,\n",
            "          8.4736051559,   2.4709563255])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(722.6125488281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8420644999), tensor(1.7321807146), tensor(1.0935922861), tensor(1.4002244473), tensor(0.8301718831), tensor(0.7338791490), tensor(1.6412091255), tensor(1.1420575380), tensor(1.3166074753), tensor(1.0549764633), tensor(1.0277491808), tensor(1.0668491125), tensor(1.7503287792), tensor(1.2352869511), tensor(1.6018818617), tensor(1.7407419682), tensor(1.5579469204), tensor(1.1424951553), tensor(0.9006860256), tensor(1.1132436991)]\n",
            "b:  [tensor(1.6100746393), tensor(0.7756034732), tensor(1.4596023560), tensor(1.3315528631), tensor(1.0321663618), tensor(1.9486652613), tensor(1.2787365913), tensor(1.4096447229), tensor(1.2869013548), tensor(1.0612204075), tensor(1.3899317980), tensor(1.2923754454), tensor(0.7965826392), tensor(1.2794373035), tensor(1.2725315094), tensor(0.9560327530), tensor(1.1326189041), tensor(1.3815819025), tensor(1.4132877588), tensor(1.5289102793)]\n",
            "c:  [tensor(0.2539722919), tensor(0.2539722919), tensor(0.2539722919), tensor(0.2072615027), tensor(-0.0059915772), tensor(0.0422597416), tensor(-0.0792523623), tensor(-0.0168022756), tensor(0.2072615027), tensor(-0.0059915772), tensor(0.0422597416), tensor(-0.0792523623), tensor(-0.0168022756), tensor(0.2072615027), tensor(-0.0059915772), tensor(0.0422597416), tensor(-0.0792523623), tensor(-0.0168022756)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.2917736769,  0.6644055843,  0.0034023523,  0.7972298265,\n",
            "         0.0987374783, -0.4691255093,  0.7489570379,  0.2679985762,\n",
            "         0.3902232349,  0.2237828821, -0.1147629172,  0.3371199965,\n",
            "         0.8332762718,  0.0762584805,  0.4967476130,  0.7229588032,\n",
            "         0.5353863239, -0.2711156607, -0.4212920666, -0.3776400685])\n",
            "btensor.grad: tensor([ 0.1432306767, -0.7515978813, -0.2167015374, -0.4911818206,\n",
            "        -0.5787385106,  0.3677679598, -0.2377981544, -0.4487188458,\n",
            "        -0.2468653917, -0.2585475445, -0.6273580790, -0.1970655918,\n",
            "        -0.5880366564, -0.4941776991, -0.8287223577, -0.8693017960,\n",
            "        -0.5842577219, -0.0084760785, -0.6574885845,  0.0727425814])\n",
            "ctensor.grad: tensor([-21.8838176727, -21.8838176727, -21.8838176727, -17.8869209290,\n",
            "          0.6324341297,   2.7210848331,   8.5242938995,   2.2580921650,\n",
            "        -17.8869209290,   0.6324341297,   2.7210848331,   8.5242938995,\n",
            "          2.2580921650, -17.8869209290,   0.6324341297,   2.7210848331,\n",
            "          8.5242938995,   2.2580921650])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(721.2254028320, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8436623216), tensor(1.7287549973), tensor(1.0936986208), tensor(1.3960447311), tensor(0.8297372460), tensor(0.7363561392), tensor(1.6373045444), tensor(1.1407104731), tensor(1.3145295382), tensor(1.0539172888), tensor(1.0283650160), tensor(1.0650820732), tensor(1.7460345030), tensor(1.2348685265), tensor(1.5993310213), tensor(1.7370229959), tensor(1.5551581383), tensor(1.1439502239), tensor(0.9029631615), tensor(1.1153073311)]\n",
            "b:  [tensor(1.6094151735), tensor(0.7795302272), tensor(1.4608387947), tensor(1.3341718912), tensor(1.0353099108), tensor(1.9469225407), tensor(1.2800335884), tensor(1.4121096134), tensor(1.2882590294), tensor(1.0627152920), tensor(1.3933286667), tensor(1.2934696674), tensor(0.7996842861), tensor(1.2821397781), tensor(1.2769998312), tensor(0.9605711102), tensor(1.1356852055), tensor(1.3817683458), tensor(1.4168323278), tensor(1.5286717415)]\n",
            "c:  [tensor(0.2651155591), tensor(0.2651155591), tensor(0.2651155591), tensor(0.2163849324), tensor(-0.0063188965), tensor(0.0407658368), tensor(-0.0835170299), tensor(-0.0178206917), tensor(0.2163849324), tensor(-0.0063188965), tensor(0.0407658368), tensor(-0.0835170299), tensor(-0.0178206917), tensor(0.2163849324), tensor(-0.0063188965), tensor(0.0407658368), tensor(-0.0835170299), tensor(-0.0178206917)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.3195621967,  0.6851377487, -0.0212765038,  0.8359391093,\n",
            "         0.0869280100, -0.4954032898,  0.7809281349,  0.2694240212,\n",
            "         0.4155936837,  0.2118423432, -0.1231679469,  0.3534071445,\n",
            "         0.8588500023,  0.0836838484,  0.5101623535,  0.7437920570,\n",
            "         0.5577662587, -0.2910203338, -0.4554278851, -0.4127198458])\n",
            "btensor.grad: tensor([ 0.1319042295, -0.7853535414, -0.2472804189, -0.5238157511,\n",
            "        -0.6287138462,  0.3485469520, -0.2593888640, -0.4929893315,\n",
            "        -0.2715433240, -0.2989823818, -0.6793633699, -0.2188377976,\n",
            "        -0.6203338504, -0.5404917598, -0.8936575651, -0.9076683521,\n",
            "        -0.6132563353, -0.0372787118, -0.7089060545,  0.0477051139])\n",
            "ctensor.grad: tensor([-22.2865161896, -22.2865161896, -22.2865161896, -18.2468566895,\n",
            "          0.6546385884,   2.9878072739,   8.5293321609,   2.0368309021,\n",
            "        -18.2468566895,   0.6546385884,   2.9878072739,   8.5293321609,\n",
            "          2.0368309021, -18.2468566895,   0.6546385884,   2.9878072739,\n",
            "          8.5293321609,   2.0368309021])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(719.7865600586, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8453913331), tensor(1.7252255678), tensor(1.0939254761), tensor(1.3916618824), tensor(0.8293489814), tensor(0.7389509678), tensor(1.6332353354), tensor(1.1393535137), tensor(1.3123078346), tensor(1.0529166460), tensor(1.0290142298), tensor(1.0632270575), tensor(1.7416106462), tensor(1.2343980074), tensor(1.5967100859), tensor(1.7332019806), tensor(1.5522515774), tensor(1.1454944611), tensor(0.9054011106), tensor(1.1175401211)]\n",
            "b:  [tensor(1.6088086367), tensor(0.7836195827), tensor(1.4622243643), tensor(1.3369454145), tensor(1.0387045145), tensor(1.9452805519), tensor(1.2814266682), tensor(1.4147913456), tensor(1.2897318602), tensor(1.0644166470), tensor(1.3969815969), tensor(1.2946667671), tensor(0.8029420376), tensor(1.2850701809), tensor(1.2817817926), tensor(0.9652927518), tensor(1.1388838291), tensor(1.3820990324), tensor(1.4206279516), tensor(1.5285590887)]\n",
            "c:  [tensor(0.2764415741), tensor(0.2764415741), tensor(0.2764415741), tensor(0.2256703377), tensor(-0.0066575627), tensor(0.0391403623), tensor(-0.0877599418), tensor(-0.0187271815), tensor(0.2256703377), tensor(-0.0066575627), tensor(0.0391403623), tensor(-0.0877599418), tensor(-0.0187271815), tensor(0.2256703377), tensor(-0.0066575627), tensor(0.0391403623), tensor(-0.0877599418), tensor(-0.0187271815)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.3458063006,  0.7058827281, -0.0453749895,  0.8765635490,\n",
            "         0.0776553154, -0.5189710855,  0.8138496876,  0.2713930309,\n",
            "         0.4443421662,  0.2001297176, -0.1298314482,  0.3709927201,\n",
            "         0.8847632408,  0.0941069722,  0.5241909623,  0.7641950846,\n",
            "         0.5813124776, -0.3088563085, -0.4875876904, -0.4465657175])\n",
            "btensor.grad: tensor([ 0.1213093251, -0.8178738356, -0.2771245837, -0.5547029972,\n",
            "        -0.6789129376,  0.3284089565, -0.2786257863, -0.5363476872,\n",
            "        -0.2945606112, -0.3402627707, -0.7305871248, -0.2394129634,\n",
            "        -0.6515546441, -0.5860809088, -0.9563860893, -0.9443321824,\n",
            "        -0.6397228241, -0.0661258996, -0.7591198683,  0.0225251913])\n",
            "ctensor.grad: tensor([-22.6520328522, -22.6520328522, -22.6520328522, -18.5708065033,\n",
            "          0.6773319840,   3.2509517670,   8.4858179092,   1.8129777908,\n",
            "        -18.5708065033,   0.6773319840,   3.2509517670,   8.4858179092,\n",
            "          1.8129777908, -18.5708065033,   0.6773319840,   3.2509517670,\n",
            "          8.4858179092,   1.8129777908])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(718.3006591797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8472439051), tensor(1.7215933800), tensor(1.0942697525), tensor(1.3870677948), tensor(0.8289948106), tensor(0.7416511178), tensor(1.6289978027), tensor(1.1379849911), tensor(1.3099262714), tensor(1.0519738197), tensor(1.0296884775), tensor(1.0612787008), tensor(1.7370567322), tensor(1.2338608503), tensor(1.5940165520), tensor(1.7292822599), tensor(1.5492224693), tensor(1.1471171379), tensor(0.9079893827), tensor(1.1199357510)]\n",
            "b:  [tensor(1.6082510948), tensor(0.7878650427), tensor(1.4637547731), tensor(1.3398640156), tensor(1.0423500538), tensor(1.9437431097), tensor(1.2829037905), tensor(1.4176841974), tensor(1.2913110256), tensor(1.0663278103), tensor(1.4008849859), tensor(1.2959603071), tensor(0.8063499928), tensor(1.2882237434), tensor(1.2868642807), tensor(0.9701884985), tensor(1.1422015429), tensor(1.3825736046), tensor(1.4246674776), tensor(1.5285729170)]\n",
            "c:  [tensor(0.2879299223), tensor(0.2879299223), tensor(0.2879299223), tensor(0.2350982875), tensor(-0.0070078443), tensor(0.0373863429), tensor(-0.0919555500), tensor(-0.0195231512), tensor(0.2350982875), tensor(-0.0070078443), tensor(0.0373863429), tensor(-0.0919555500), tensor(-0.0195231512), tensor(0.2350982875), tensor(-0.0070078443), tensor(0.0373863429), tensor(-0.0919555500), tensor(-0.0195231512)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.3705105186,  0.7264278531, -0.0688627958,  0.9188149571,\n",
            "         0.0708303452, -0.5400283337,  0.8474955559,  0.2736977935,\n",
            "         0.4763148427,  0.1885709465, -0.1348469555,  0.3896807134,\n",
            "         0.9107760191,  0.1074253917,  0.5387108326,  0.7839537859,\n",
            "         0.6058297753, -0.3245361447, -0.5176581144, -0.4791240394])\n",
            "btensor.grad: tensor([ 0.1115114912, -0.8490957022, -0.3060704470, -0.5837227106,\n",
            "        -0.7290961742,  0.3074893057, -0.2954227328, -0.5785666108,\n",
            "        -0.3158332705, -0.3822302818, -0.7806825638, -0.2587053180,\n",
            "        -0.6815858483, -0.6307144165, -1.0164904594, -0.9791539311,\n",
            "        -0.6635398865, -0.0949150920, -0.8079093099, -0.0027576089])\n",
            "ctensor.grad: tensor([-22.9766693115, -22.9766693115, -22.9766693115, -18.8559112549,\n",
            "          0.7005630732,   3.5080373287,   8.3912210464,   1.5919394493,\n",
            "        -18.8559112549,   0.7005630732,   3.5080373287,   8.3912210464,\n",
            "          1.5919394493, -18.8559112549,   0.7005630732,   3.5080373287,\n",
            "          8.3912210464,   1.5919394493])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(716.7734375000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8492123485), tensor(1.7178604603), tensor(1.0947283506), tensor(1.3822559118), tensor(0.8286632895), tensor(0.7444453835), tensor(1.6245895624), tensor(1.1366044283), tensor(1.3073698282), tensor(1.0510882139), tensor(1.0303802490), tensor(1.0592322350), tensor(1.7323734760), tensor(1.2332434654), tensor(1.5912485123), tensor(1.7252677679), tensor(1.5460668802), tensor(1.1488071680), tensor(0.9107170701), tensor(1.1224874258)]\n",
            "b:  [tensor(1.6077381372), tensor(0.7922598720), tensor(1.4654244184), tensor(1.3429179192), tensor(1.0462449789), tensor(1.9423131943), tensor(1.2844523191), tensor(1.4207812548), tensor(1.2929874659), tensor(1.0684511662), tensor(1.4050313234), tensor(1.2973434925), tensor(0.8099015355), tensor(1.2915945053), tensor(1.2922321558), tensor(0.9752485752), tensor(1.1456246376), tensor(1.3831911087), tensor(1.4289426804), tensor(1.5287133455)]\n",
            "c:  [tensor(0.2995584011), tensor(0.2995584011), tensor(0.2995584011), tensor(0.2446480840), tensor(-0.0073700319), tensor(0.0355080105), tensor(-0.0960774124), tensor(-0.0202126671), tensor(0.2446480840), tensor(-0.0073700319), tensor(0.0355080105), tensor(-0.0960774124), tensor(-0.0202126671), tensor(0.2446480840), tensor(-0.0073700319), tensor(0.0355080105), tensor(-0.0960774124), tensor(-0.0202126671)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.3936903477,  0.7465780973, -0.0917257965,  0.9623652101,\n",
            "         0.0663051605, -0.5588499308,  0.8816372156,  0.2761241794,\n",
            "         0.5112923384,  0.1771182120, -0.1383439302,  0.4092918336,\n",
            "         0.9366432428,  0.1234759092,  0.5535998344,  0.8028962016,\n",
            "         0.6311200857, -0.3380019069, -0.5455349684, -0.5103360415])\n",
            "btensor.grad: tensor([ 0.1025999486, -0.8789702654, -0.3339304626, -0.6107745171,\n",
            "        -0.7789856195,  0.2859849632, -0.3097141385, -0.6194230914,\n",
            "        -0.3352781534, -0.4246729612, -0.8292635679, -0.2766291499,\n",
            "        -0.7103074789, -0.6741456985, -1.0735760927, -1.0120112896,\n",
            "        -0.6846145391, -0.1235051751, -0.8550512195, -0.0280863643])\n",
            "ctensor.grad: tensor([-23.2569713593, -23.2569713593, -23.2569713593, -19.0996036530,\n",
            "          0.7243750095,   3.7566621304,   8.2437238693,   1.3790321350,\n",
            "        -19.0996036530,   0.7243750095,   3.7566621304,   8.2437238693,\n",
            "          1.3790321350, -19.0996036530,   0.7243750095,   3.7566621304,\n",
            "          8.2437238693,   1.3790321350])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(715.2094116211, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8512893915), tensor(1.7140296698), tensor(1.0952981710), tensor(1.3772215843), tensor(0.8283439875), tensor(0.7473244071), tensor(1.6200094223), tensor(1.1352121830), tensor(1.3046250343), tensor(1.0502594709), tensor(1.0310827494), tensor(1.0570842028), tensor(1.7275629044), tensor(1.2325333357), tensor(1.5884047747), tensor(1.7211633921), tensor(1.5427819490), tensor(1.1505533457), tensor(0.9135727882), tensor(1.1251883507)]\n",
            "b:  [tensor(1.6072647572), tensor(0.7967972755), tensor(1.4672271013), tensor(1.3460968733), tensor(1.0503863096), tensor(1.9409924746), tensor(1.2860597372), tensor(1.4240747690), tensor(1.2947516441), tensor(1.0707877874), tensor(1.4094109535), tensor(1.2988090515), tensor(0.8135895729), tensor(1.2951750755), tensor(1.2978686094), tensor(0.9804626107), tensor(1.1491391659), tensor(1.3839497566), tensor(1.4334445000), tensor(1.5289803743)]\n",
            "c:  [tensor(0.3113032877), tensor(0.3113032877), tensor(0.3113032877), tensor(0.2542980313), tensor(-0.0077444315), tensor(0.0335108675), tensor(-0.1000985205), tensor(-0.0208020881), tensor(0.2542980313), tensor(-0.0077444315), tensor(0.0335108675), tensor(-0.1000985205), tensor(-0.0208020881), tensor(0.2542980313), tensor(-0.0077444315), tensor(0.0335108675), tensor(-0.1000985205), tensor(-0.0208020881)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.4154074192,  0.7661544085, -0.1139669716,  1.0068535805,\n",
            "         0.0638620853, -0.5758069754,  0.9160242081,  0.2784419954,\n",
            "         0.5489651561,  0.1657392383, -0.1404951662,  0.4296157658,\n",
            "         0.9621081352,  0.1420187354,  0.5687372684,  0.8208726645,\n",
            "         0.6569813490, -0.3492313027, -0.5711473227, -0.5401953459])\n",
            "btensor.grad: tensor([ 0.0946801603, -0.9074787498, -0.3605306447, -0.6357920766,\n",
            "        -0.8282675743,  0.2641438842, -0.3214924335, -0.6587011814,\n",
            "        -0.3528389335, -0.4673161507, -0.8759302497, -0.2931057811,\n",
            "        -0.7376105785, -0.7161172628, -1.1272922754, -1.0428016186,\n",
            "        -0.7029067278, -0.1517373621, -0.9003626108, -0.0534087420])\n",
            "ctensor.grad: tensor([-23.4897880554, -23.4897880554, -23.4897880554, -19.2998847961,\n",
            "          0.7487992644,   3.9942841530,   8.0422096252,   1.1788406372,\n",
            "        -19.2998847961,   0.7487992644,   3.9942841530,   8.0422096252,\n",
            "          1.1788406372, -19.2998847961,   0.7487992644,   3.9942841530,\n",
            "          8.0422096252,   1.1788406372])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(713.6168823242, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8534680009), tensor(1.7101045847), tensor(1.0959761143), tensor(1.3719620705), tensor(0.8280278444), tensor(0.7502809167), tensor(1.6152573824), tensor(1.1338101625), tensor(1.3016800880), tensor(1.0494873524), tensor(1.0317902565), tensor(1.0548318624), tensor(1.7226283550), tensor(1.2317194939), tensor(1.5854847431), tensor(1.7169743776), tensor(1.5393658876), tensor(1.1523444653), tensor(0.9165449739), tensor(1.1280317307)]\n",
            "b:  [tensor(1.6068253517), tensor(0.8014703393), tensor(1.4691554308), tensor(1.3493905067), tensor(1.0547692776), tensor(1.9397810698), tensor(1.2877136469), tensor(1.4275556803), tensor(1.2965940237), tensor(1.0733368397), tensor(1.4140121937), tensor(1.3003493547), tensor(0.8174064755), tensor(1.2989568710), tensor(1.3037551641), tensor(0.9858198166), tensor(1.1527311802), tensor(1.3848468065), tensor(1.4381628036), tensor(1.5293735266)]\n",
            "c:  [tensor(0.3231393993), tensor(0.3231393993), tensor(0.3231393993), tensor(0.2640255392), tensor(-0.0081313606), tensor(0.0314014815), tensor(-0.1039917916), tensor(-0.0212998539), tensor(0.2640255392), tensor(-0.0081313606), tensor(0.0314014815), tensor(-0.1039917916), tensor(-0.0212998539), tensor(0.2640255392), tensor(-0.0081313606), tensor(0.0314014815), tensor(-0.1039917916), tensor(-0.0212998539)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.4357234836,  0.7850108743, -0.1355923116,  1.0518960953,\n",
            "         0.0632268190, -0.5913066864,  0.9504078627,  0.2804138064,\n",
            "         0.5889822841,  0.1544240117, -0.1414923072,  0.4504733086,\n",
            "         0.9869194627,  0.1627584696,  0.5840150118,  0.8377960920,\n",
            "         0.6832175255, -0.3582341671, -0.5944398642, -0.5686824322])\n",
            "btensor.grad: tensor([ 0.0878778249, -0.9346110821, -0.3856666386, -0.6587277055,\n",
            "        -0.8765949011,  0.2422744036, -0.3307760954, -0.6961907744,\n",
            "        -0.3684660792, -0.5098181963, -0.9202549458, -0.3080656528,\n",
            "        -0.7633799911, -0.7563637495, -1.1773151159, -1.0714385509,\n",
            "        -0.7184016705, -0.1794198751, -0.9436572194, -0.0786377788])\n",
            "ctensor.grad: tensor([-23.6722488403, -23.6722488403, -23.6722488403, -19.4549980164,\n",
            "          0.7738574147,   4.2187733650,   7.7865395546,   0.9955321550,\n",
            "        -19.4549980164,   0.7738574147,   4.2187733650,   7.7865395546,\n",
            "          0.9955321550, -19.4549980164,   0.7738574147,   4.2187733650,\n",
            "          7.7865395546,   0.9955321550])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(712.0012207031, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8557416797), tensor(1.7060894966), tensor(1.0967592001), tensor(1.3664766550), tensor(0.8277076483), tensor(0.7533100843), tensor(1.6103347540), tensor(1.1324011087), tensor(1.2985255718), tensor(1.0487713814), tensor(1.0324981213), tensor(1.0524735451), tensor(1.7175742388), tensor(1.2307928801), tensor(1.5824881792), tensor(1.7127063274), tensor(1.5358177423), tensor(1.1541697979), tensor(0.9196218252), tensor(1.1310107708)]\n",
            "b:  [tensor(1.6064137220), tensor(0.8062722683), tensor(1.4712011814), tensor(1.3527883291), tensor(1.0593873262), tensor(1.9386774302), tensor(1.2894017696), tensor(1.4312142134), tensor(1.2985045910), tensor(1.0760958195), tensor(1.4188212156), tensor(1.3019566536), tensor(0.8213440776), tensor(1.3029299974), tensor(1.3098721504), tensor(0.9913091063), tensor(1.1563868523), tensor(1.3858785629), tensor(1.4430867434), tensor(1.5298919678)]\n",
            "c:  [tensor(0.3350403905), tensor(0.3350403905), tensor(0.3350403905), tensor(0.2738074362), tensor(-0.0085311383), tensor(0.0291874930), tensor(-0.1077304855), tensor(-0.0217158999), tensor(0.2738074362), tensor(-0.0085311383), tensor(0.0291874930), tensor(-0.1077304855), tensor(-0.0217158999), tensor(0.2738074362), tensor(-0.0085311383), tensor(0.0291874930), tensor(-0.1077304855), tensor(-0.0217158999)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.4547414780,  0.8030084372, -0.1566206217,  1.0970828533,\n",
            "         0.0640400648, -0.6058330536,  0.9845160246,  0.2818000913,\n",
            "         0.6309010983,  0.1431848109, -0.1415700018,  0.4716631472,\n",
            "         1.0108246803,  0.1853274107,  0.5993218422,  0.8536057472,\n",
            "         0.7096304297, -0.3650643229, -0.6153737307, -0.5958002210])\n",
            "btensor.grad: tensor([ 0.0823185220, -0.9603911638, -0.4091558754, -0.6795659065,\n",
            "        -0.9235981107,  0.2207291126, -0.3376340866, -0.7317082286,\n",
            "        -0.3821233511, -0.5518033504, -0.9618119001, -0.3214577436,\n",
            "        -0.7875163555, -0.7946211696, -1.2233890295, -1.0978605747,\n",
            "        -0.7311371565, -0.2063475549, -0.9847840667, -0.1036967039])\n",
            "ctensor.grad: tensor([-23.8020000458, -23.8020000458, -23.8020000458, -19.5637969971,\n",
            "          0.7995545864,   4.4279747009,   7.4773802757,   0.8320922852,\n",
            "        -19.5637969971,   0.7995545864,   4.4279747009,   7.4773802757,\n",
            "          0.8320922852, -19.5637969971,   0.7995545864,   4.4279747009,\n",
            "          7.4773802757,   0.8320922852])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(710.3703613281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8581044078), tensor(1.7019891739), tensor(1.0976444483), tensor(1.3607667685), tensor(0.8273780346), tensor(0.7564092875), tensor(1.6052443981), tensor(1.1309891939), tensor(1.2951543331), tensor(1.0481109619), tensor(1.0332028866), tensor(1.0500084162), tensor(1.7124062777), tensor(1.2297462225), tensor(1.5794153214), tensor(1.7083648443), tensor(1.5321375132), tensor(1.1560187340), tensor(0.9227913022), tensor(1.1341184378)]\n",
            "b:  [tensor(1.6060229540), tensor(0.8111965060), tensor(1.4733551741), tensor(1.3562798500), tensor(1.0642317533), tensor(1.9376778603), tensor(1.2911126614), tensor(1.4350395203), tensor(1.3004734516), tensor(1.0790599585), tensor(1.4238220453), tensor(1.3036228418), tensor(0.8253936768), tensor(1.3070831299), tensor(1.3161985874), tensor(0.9969191551), tensor(1.1600927114), tensor(1.3870400190), tensor(1.4482047558), tensor(1.5305342674)]\n",
            "c:  [tensor(0.3469789922), tensor(0.3469789922), tensor(0.3469789922), tensor(0.2836201489), tensor(-0.0089440793), tensor(0.0268771891), tensor(-0.1112887859), tensor(-0.0220614094), tensor(0.2836201489), tensor(-0.0089440793), tensor(0.0268771891), tensor(-0.1112887859), tensor(-0.0220614094), tensor(0.2836201489), tensor(-0.0089440793), tensor(0.0268771891), tensor(-0.1112887859), tensor(-0.0220614094)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.4725465775,  0.8200528622, -0.1770547032,  1.1419889927,\n",
            "         0.0659272671, -0.6198433638,  1.0180777311,  0.2823760509,\n",
            "         0.6742556691,  0.1320729256, -0.1409591138,  0.4930199683,\n",
            "         1.0335830450,  0.2093230486,  0.6145648360,  0.8683001995,\n",
            "         0.7360380888, -0.3697853088, -0.6338994503, -0.6215361357])\n",
            "btensor.grad: tensor([ 0.0781568661, -0.9848502278, -0.4308001697, -0.6982986927,\n",
            "        -0.9688898325,  0.1999040246, -0.3421676159, -0.7650666833,\n",
            "        -0.3937839270, -0.5928337574, -1.0001664162, -0.3332316875,\n",
            "        -0.8099144697, -0.8306156397, -1.2652754784, -1.1220136881,\n",
            "        -0.7411721945, -0.2322817445, -1.0236057043, -0.1284717321])\n",
            "ctensor.grad: tensor([-23.8772201538, -23.8772201538, -23.8772201538, -19.6254215240,\n",
            "          0.8258823752,   4.6206088066,   7.1165957451,   0.6910207868,\n",
            "        -19.6254215240,   0.8258823752,   4.6206088066,   7.1165957451,\n",
            "          0.6910207868, -19.6254215240,   0.8258823752,   4.6206088066,\n",
            "          7.1165957451,   0.6910207868])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(708.7323608398, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8605506420), tensor(1.6978088617), tensor(1.0986289978), tensor(1.3548359871), tensor(0.8270359039), tensor(0.7595784664), tensor(1.5999903679), tensor(1.1295796633), tensor(1.2915617228), tensor(1.0475052595), tensor(1.0339026451), tensor(1.0474365950), tensor(1.7071315050), tensor(1.2285748720), tensor(1.5762671232), tensor(1.7039552927), tensor(1.5283262730), tensor(1.1578813791), tensor(0.9260413647), tensor(1.1373479366)]\n",
            "b:  [tensor(1.6056454182), tensor(0.8162367344), tensor(1.4756072760), tensor(1.3598546982), tensor(1.0692923069), tensor(1.9367768764), tensor(1.2928352356), tensor(1.4390201569), tensor(1.3024907112), tensor(1.0822223425), tensor(1.4289965630), tensor(1.3053396940), tensor(0.8295462132), tensor(1.3114036322), tensor(1.3227126598), tensor(1.0026385784), tensor(1.1638357639), tensor(1.3883249760), tensor(1.4535049200), tensor(1.5312986374)]\n",
            "c:  [tensor(0.3589273393), tensor(0.3589273393), tensor(0.3589273393), tensor(0.2934399545), tensor(-0.0093704863), tensor(0.0244795959), tensor(-0.1146421656), tensor(-0.0223480146), tensor(0.2934399545), tensor(-0.0093704863), tensor(0.0244795959), tensor(-0.1146421656), tensor(-0.0223480146), tensor(0.2934399545), tensor(-0.0093704863), tensor(0.0244795959), tensor(-0.1146421656), tensor(-0.0223480146)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.4892522097,  0.8360518217, -0.1969197989,  1.1861658096,\n",
            "         0.0684320927, -0.6338365078,  1.0508122444,  0.2819157839,\n",
            "         0.7185115814,  0.1211374104, -0.1399410367,  0.5143616199,\n",
            "         1.0549484491,  0.2342766523,  0.6296398044,  0.8819033504,\n",
            "         0.7622467875, -0.3725192547, -0.6500111818, -0.6458995938])\n",
            "btensor.grad: tensor([ 0.0755187273, -1.0080412626, -0.4504223466, -0.7149657607,\n",
            "        -1.0121114254,  0.1802021861, -0.3445258737, -0.7961238623,\n",
            "        -0.4034528136, -0.6324784756, -1.0349133015, -0.3433753252,\n",
            "        -0.8305020928, -0.8640985489, -1.3028140068, -1.1438755989,\n",
            "        -0.7486194372, -0.2570024133, -1.0600259304, -0.1528698206])\n",
            "ctensor.grad: tensor([-23.8966655731, -23.8966655731, -23.8966655731, -19.6396255493,\n",
            "          0.8528142571,   4.7951855659,   6.7067561150,   0.5732088089,\n",
            "        -19.6396255493,   0.8528142571,   4.7951855659,   6.7067561150,\n",
            "          0.5732088089, -19.6396255493,   0.8528142571,   4.7951855659,\n",
            "          6.7067561150,   0.5732088089])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(707.0933837891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8630751967), tensor(1.6935540438), tensor(1.0997098684), tensor(1.3486901522), tensor(0.8266802430), tensor(0.7628194690), tensor(1.5945781469), tensor(1.1281784773), tensor(1.2877459526), tensor(1.0469528437), tensor(1.0345963240), tensor(1.0447587967), tensor(1.7017579079), tensor(1.2272762060), tensor(1.5730447769), tensor(1.6994829178), tensor(1.5243856907), tensor(1.1597483158), tensor(0.9293594956), tensor(1.1406921148)]\n",
            "b:  [tensor(1.6052726507), tensor(0.8213867545), tensor(1.4779465199), tensor(1.3635026217), tensor(1.0745567083), tensor(1.9359666109), tensor(1.2945594788), tensor(1.4431437254), tensor(1.3045463562), tensor(1.0855736732), tensor(1.4343247414), tensor(1.3070991039), tensor(0.8337921500), tensor(1.3158776760), tensor(1.3293919563), tensor(1.0084556341), tensor(1.1676037312), tensor(1.3897262812), tensor(1.4589745998), tensor(1.5321824551)]\n",
            "c:  [tensor(0.3708572388), tensor(0.3708572388), tensor(0.3708572388), tensor(0.3032431006), tensor(-0.0098106423), tensor(0.0220037308), tensor(-0.1177680492), tensor(-0.0225877017), tensor(0.3032431006), tensor(-0.0098106423), tensor(0.0220037308), tensor(-0.1177680492), tensor(-0.0225877017), tensor(0.3032431006), tensor(-0.0098106423), tensor(0.0220037308), tensor(-0.1177680492), tensor(-0.0225877017)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5049128532,  0.8509542942, -0.2161858380,  1.2291737795,\n",
            "         0.0711280107, -0.6482063532,  1.0824421644,  0.2802454829,\n",
            "         0.7631446123,  0.1104820892, -0.1387449801,  0.5355485082,\n",
            "         1.0747106075,  0.2597349286,  0.6444780231,  0.8944861889,\n",
            "         0.7881105542, -0.3733766675, -0.6636315584, -0.6688445807])\n",
            "btensor.grad: tensor([ 0.0745545030, -1.0300037861, -0.4678443074, -0.7295856476,\n",
            "        -1.0528745651,  0.1620616019, -0.3448508978, -0.8247246742,\n",
            "        -0.4111176133, -0.6702558994, -1.0656445026, -0.3518716097,\n",
            "        -0.8491911292, -0.8948101997, -1.3358550072, -1.1634130478,\n",
            "        -0.7535881996, -0.2802577317, -1.0939344168, -0.1767547131])\n",
            "ctensor.grad: tensor([-23.8598136902, -23.8598136902, -23.8598136902, -19.6062812805,\n",
            "          0.8803112507,   4.9517288208,   6.2517595291,   0.4793751240,\n",
            "        -19.6062812805,   0.8803112507,   4.9517288208,   6.2517595291,\n",
            "          0.4793751240, -19.6062812805,   0.8803112507,   4.9517288208,\n",
            "          6.2517595291,   0.4793751240])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(705.4619140625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8656734228), tensor(1.6892305613), tensor(1.1008841991), tensor(1.3423374891), tensor(0.8263127804), tensor(0.7661362886), tensor(1.5890148878), tensor(1.1267926693), tensor(1.2837080956), tensor(1.0464519262), tensor(1.0352847576), tensor(1.0419768095), tensor(1.6962947845), tensor(1.2258503437), tensor(1.5697499514), tensor(1.6949523687), tensor(1.5203185081), tensor(1.1616110802), tensor(0.9327332973), tensor(1.1441439390)]\n",
            "b:  [tensor(1.6048959494), tensor(0.8266408443), tensor(1.4803612232), tensor(1.3672138453), tensor(1.0800110102), tensor(1.9352372885), tensor(1.2962763309), tensor(1.4473977089), tensor(1.3066304922), tensor(1.0891025066), tensor(1.4397848845), tensor(1.3088929653), tensor(0.8381218910), tensor(1.3204904795), tensor(1.3362137079), tensor(1.0143588781), tensor(1.1713849306), tensor(1.3912355900), tensor(1.4646011591), tensor(1.5331827402)]\n",
            "c:  [tensor(0.3827406466), tensor(0.3827406466), tensor(0.3827406466), tensor(0.3130062521), tensor(-0.0102647990), tensor(0.0194591805), tensor(-0.1206459254), tensor(-0.0227916948), tensor(0.3130062521), tensor(-0.0102647990), tensor(0.0194591805), tensor(-0.1206459254), tensor(-0.0227916948), tensor(0.3130062521), tensor(-0.0102647990), tensor(0.0194591805), tensor(-0.1206459254), tensor(-0.0227916948)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5196441412,  0.8646928668, -0.2348730564,  1.2705361843,\n",
            "         0.0734949112, -0.6633687019,  1.1126635075,  0.2771619558,\n",
            "         0.8075637817,  0.1001757160, -0.1376760602,  0.5563992858,\n",
            "         1.0926294327,  0.2851719260,  0.6589742899,  0.9061157107,\n",
            "         0.8134349585, -0.3725456595, -0.6747605801, -0.6903650165])\n",
            "btensor.grad: tensor([ 0.0753453076, -1.0508230925, -0.4829456806, -0.7422467470,\n",
            "        -1.0908677578,  0.1458536983, -0.3433651924, -0.8507942557,\n",
            "        -0.4168314338, -0.7057561874, -1.0920400620, -0.3587709069,\n",
            "        -0.8659498692, -0.9225611687, -1.3643414974, -1.1806387901,\n",
            "        -0.7562500238, -0.3018516898, -1.1253035069, -0.2000512481])\n",
            "ctensor.grad: tensor([-23.7668190002, -23.7668190002, -23.7668190002, -19.5262794495,\n",
            "          0.9083136916,   5.0891013145,   5.7557559013,   0.4079878628,\n",
            "        -19.5262794495,   0.9083136916,   5.0891013145,   5.7557559013,\n",
            "          0.4079878628, -19.5262794495,   0.9083136916,   5.0891013145,\n",
            "          5.7557559013,   0.4079878628])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(703.8448486328, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8683403730), tensor(1.6848442554), tensor(1.1021485329), tensor(1.3357882500), tensor(0.8259367943), tensor(0.7695336938), tensor(1.5833086967), tensor(1.1254297495), tensor(1.2794517279), tensor(1.0460000038), tensor(1.0359692574), tensor(1.0390926600), tensor(1.6907520294), tensor(1.2242995501), tensor(1.5663844347), tensor(1.6903676987), tensor(1.5161279440), tensor(1.1634616852), tensor(0.9361494780), tensor(1.1476955414)]\n",
            "b:  [tensor(1.6045057774), tensor(0.8319934011), tensor(1.4828389883), tensor(1.3709787130), tensor(1.0856397152), tensor(1.9345772266), tensor(1.2979773283), tensor(1.4517686367), tensor(1.3087334633), tensor(1.0927950144), tensor(1.4453536272), tensor(1.3107132912), tensor(0.8425252438), tensor(1.3252259493), tensor(1.3431545496), tensor(1.0203363895), tensor(1.1751685143), tensor(1.3928432465), tensor(1.4703712463), tensor(1.5342956781)]\n",
            "c:  [tensor(0.3945499361), tensor(0.3945499361), tensor(0.3945499361), tensor(0.3227061629), tensor(-0.0107331807), tensor(0.0168544855), tensor(-0.1232582703), tensor(-0.0229711849), tensor(0.3227061629), tensor(-0.0107331807), tensor(0.0168544855), tensor(-0.1232582703), tensor(-0.0229711849), tensor(0.3227061629), tensor(-0.0107331807), tensor(0.0168544855), tensor(-0.1232582703), tensor(-0.0229711849)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5333863497,  0.8772667050, -0.2528707087,  1.3098497391,\n",
            "         0.0751928091, -0.6794854403,  1.1412371397,  0.2725836635,\n",
            "         0.8512802720,  0.0903942138, -0.1368992031,  0.5768362284,\n",
            "         1.1085512638,  0.3101657629,  0.6730992794,  0.9169271588,\n",
            "         0.8381187320, -0.3701256216, -0.6832360029, -0.7103293538])\n",
            "btensor.grad: tensor([ 0.0780295059, -1.0705074072, -0.4955559075, -0.7529633045,\n",
            "        -1.1257402897,  0.1320030689, -0.3402084708, -0.8741939068,\n",
            "        -0.4205843210, -0.7385075092, -1.1137478352, -0.3640626669,\n",
            "        -0.8806754947, -0.9470975995, -1.3881604671, -1.1955031157,\n",
            "        -0.7567117214, -0.3215207756, -1.1540220976, -0.2225944996])\n",
            "ctensor.grad: tensor([-23.6185913086, -23.6185913086, -23.6185913086, -19.3998374939,\n",
            "          0.9367626905,   5.2093911171,   5.2246880531,   0.3589808941,\n",
            "        -19.3998374939,   0.9367626905,   5.2093911171,   5.2246880531,\n",
            "          0.3589808941, -19.3998374939,   0.9367626905,   5.2093911171,\n",
            "          5.2246880531,   0.3589808941])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(702.2489013672, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8710718751), tensor(1.6804014444), tensor(1.1034997702), tensor(1.3290553093), tensor(0.8255587816), tensor(0.7730182409), tensor(1.5774697065), tensor(1.1240981817), tensor(1.2749835253), tensor(1.0455944538), tensor(1.0366531610), tensor(1.0361096859), tensor(1.6851409674), tensor(1.2226288319), tensor(1.5629509687), tensor(1.6857329607), tensor(1.5118182898), tensor(1.1652936935), tensor(0.9395951033), tensor(1.1513394117)]\n",
            "b:  [tensor(1.6040929556), tensor(0.8374393582), tensor(1.4853674173), tensor(1.3747882843), tensor(1.0914263725), tensor(1.9339736700), tensor(1.2996557951), tensor(1.4562435150), tensor(1.3108462095), tensor(1.0966362953), tensor(1.4510067701), tensor(1.3125528097), tensor(0.8469924331), tensor(1.3300677538), tensor(1.3501915932), tensor(1.0263768435), tensor(1.1789447069), tensor(1.3945392370), tensor(1.4762721062), tensor(1.5355176926)]\n",
            "c:  [tensor(0.4062582850), tensor(0.4062582850), tensor(0.4062582850), tensor(0.3323208690), tensor(-0.0112159615), tensor(0.0141995745), tensor(-0.1255897731), tensor(-0.0231347624), tensor(0.3323208690), tensor(-0.0112159615), tensor(0.0141995745), tensor(-0.1255897731), tensor(-0.0231347624), tensor(0.3323208690), tensor(-0.0112159615), tensor(0.0141995745), tensor(-0.1255897731), tensor(-0.0231347624)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5463042259,  0.8885675073, -0.2702451646,  1.3465833664,\n",
            "         0.0756068230, -0.6969041824,  1.1678068638,  0.2663072944,\n",
            "         0.8936517835,  0.0811155140, -0.1367714107,  0.5965999365,\n",
            "         1.1222047806,  0.3341418505,  0.6866871119,  0.9269441366,\n",
            "         0.8619208336, -0.3664122820, -0.6891210079, -0.7287787795])\n",
            "btensor.grad: tensor([ 0.0825557113, -1.0891873837, -0.5056914687, -0.7619120479,\n",
            "        -1.1573348045,  0.1207199097, -0.3356904387, -0.8949669600,\n",
            "        -0.4225394726, -0.7682564259, -1.1306278706, -0.3679132462,\n",
            "        -0.8934349418, -0.9683601856, -1.4074141979, -1.2080800533,\n",
            "        -0.7552400827, -0.3392063081, -1.1801776886, -0.2444031537])\n",
            "ctensor.grad: tensor([-23.4167060852, -23.4167060852, -23.4167060852, -19.2294063568,\n",
            "          0.9655610323,   5.3098220825,   4.6630182266,   0.3271558881,\n",
            "        -19.2294063568,   0.9655610323,   5.3098220825,   4.6630182266,\n",
            "          0.3271558881, -19.2294063568,   0.9655610323,   5.3098220825,\n",
            "          4.6630182266,   0.3271558881])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(700.6811523438, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8738619089), tensor(1.6759078503), tensor(1.1049329042), tensor(1.3221527338), tensor(0.8251848221), tensor(0.7765944004), tensor(1.5715084076), tensor(1.1228055954), tensor(1.2703114748), tensor(1.0452307463), tensor(1.0373392105), tensor(1.0330306292), tensor(1.6794730425), tensor(1.2208442688), tensor(1.5594516993), tensor(1.6810508966), tensor(1.5073938370), tensor(1.1671003103), tensor(0.9430549145), tensor(1.1550661325)]\n",
            "b:  [tensor(1.6036472321), tensor(0.8429729939), tensor(1.4879328012), tensor(1.3786331415), tensor(1.0973526239), tensor(1.9334112406), tensor(1.3013048172), tensor(1.4608079195), tensor(1.3129589558), tensor(1.1006085873), tensor(1.4567182064), tensor(1.3144037724), tensor(0.8515124917), tensor(1.3349977732), tensor(1.3573009968), tensor(1.0324678421), tensor(1.1827037334), tensor(1.3963118792), tensor(1.4822897911), tensor(1.5368435383)]\n",
            "c:  [tensor(0.4178400636), tensor(0.4178400636), tensor(0.4178400636), tensor(0.3418278694), tensor(-0.0117132924), tensor(0.0115003148), tensor(-0.1276297122), tensor(-0.0232927222), tensor(0.3418278694), tensor(-0.0117132924), tensor(0.0115003148), tensor(-0.1276297122), tensor(-0.0232927222), tensor(0.3418278694), tensor(-0.0117132924), tensor(0.0115003148), tensor(-0.1276297122), tensor(-0.0232927222)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5580042601,  0.8987296820, -0.2866365314,  1.3805098534,\n",
            "         0.0747942924, -0.7152373791,  1.1922705173,  0.2585108876,\n",
            "         0.9343996048,  0.0727493167, -0.1372088492,  0.6158012152,\n",
            "         1.1335961819,  0.3569061160,  0.6998532414,  0.9364086986,\n",
            "         0.8848859668, -0.3613133132, -0.6919679642, -0.7453331947])\n",
            "btensor.grad: tensor([ 0.0891415253, -1.1067229509, -0.5130846500, -0.7689656019,\n",
            "        -1.1852405071,  0.1124841273, -0.3298041821, -0.9128781557,\n",
            "        -0.4225580692, -0.7944533825, -1.1422941685, -0.3702024221,\n",
            "        -0.9040120244, -0.9860039353, -1.4218845367, -1.2182116508,\n",
            "        -0.7517951131, -0.3545388877, -1.2035406828, -0.2651663721])\n",
            "ctensor.grad: tensor([-23.1635742188, -23.1635742188, -23.1635742188, -19.0140304565,\n",
            "          0.9946626425,   5.3985190392,   4.0798892975,   0.3159207106,\n",
            "        -19.0140304565,   0.9946626425,   5.3985190392,   4.0798892975,\n",
            "          0.3159207106, -19.0140304565,   0.9946626425,   5.3985190392,\n",
            "          4.0798892975,   0.3159207106])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(699.1463012695, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8767076731), tensor(1.6713711023), tensor(1.1064455509), tensor(1.3150986433), tensor(0.8248270154), tensor(0.7802708149), tensor(1.5654381514), tensor(1.1215623617), tensor(1.2654485703), tensor(1.0449069738), tensor(1.0380339622), tensor(1.0298619270), tensor(1.6737619638), tensor(1.2189564705), tensor(1.5558911562), tensor(1.6763256788), tensor(1.5028614998), tensor(1.1688781977), tensor(0.9465166926), tensor(1.1588683128)]\n",
            "b:  [tensor(1.6031607389), tensor(0.8485904932), tensor(1.4905238152), tensor(1.3825064898), tensor(1.1034014225), tensor(1.9328763485), tensor(1.3029208183), tensor(1.4654498100), tensor(1.3150650263), tensor(1.1046954393), tensor(1.4624637365), tensor(1.3162611723), tensor(0.8560765982), tensor(1.3399999142), tensor(1.3644616604), tensor(1.0385990143), tensor(1.1864385605), tensor(1.3981515169), tensor(1.4884127378), tensor(1.5382701159)]\n",
            "c:  [tensor(0.4292709529), tensor(0.4292709529), tensor(0.4292709529), tensor(0.3512085676), tensor(-0.0122252377), tensor(0.0087701641), tensor(-0.1293673515), tensor(-0.0234468933), tensor(0.3512085676), tensor(-0.0122252377), tensor(0.0087701641), tensor(-0.1293673515), tensor(-0.0234468933), tensor(0.3512085676), tensor(-0.0122252377), tensor(0.0087701641), tensor(-0.1293673515), tensor(-0.0234468933)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5691471100,  0.9073556066, -0.3025404215,  1.4108076096,\n",
            "         0.0715560913, -0.7352800369,  1.2140476704,  0.2486585975,\n",
            "         0.9725775719,  0.0647533163, -0.1389416456,  0.6337457299,\n",
            "         1.1422114372,  0.3775624037,  0.7121107578,  0.9450522065,\n",
            "         0.9064714313, -0.3555660844, -0.6923587322, -0.7604303360])\n",
            "btensor.grad: tensor([ 0.0972943306, -1.1235011816, -0.5182041526, -0.7746663690,\n",
            "        -1.2097601891,  0.1069735289, -0.3232075572, -0.9283773899,\n",
            "        -0.4212186933, -0.8173720837, -1.1491124630, -0.3714765310,\n",
            "        -0.9128166437, -1.0004197359, -1.4321237803, -1.2262227535,\n",
            "        -0.7469727397, -0.3679197729, -1.2245876789, -0.2853133678])\n",
            "ctensor.grad: tensor([-22.8617649078, -22.8617649078, -22.8617649078, -18.7613868713,\n",
            "          1.0238900185,   5.4603004456,   3.4752917290,   0.3083414435,\n",
            "        -18.7613868713,   1.0238900185,   5.4603004456,   3.4752917290,\n",
            "          0.3083414435, -18.7613868713,   1.0238900185,   5.4603004456,\n",
            "          3.4752917290,   0.3083414435])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(697.6496582031, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8795965910), tensor(1.6667946577), tensor(1.1080274582), tensor(1.3079079390), tensor(0.8244876862), tensor(0.7840441465), tensor(1.5592695475), tensor(1.1203721762), tensor(1.2604038715), tensor(1.0446131229), tensor(1.0387374163), tensor(1.0266038179), tensor(1.6680179834), tensor(1.2169712782), tensor(1.5522692204), tensor(1.6715569496), tensor(1.4982239008), tensor(1.1706180573), tensor(0.9499589801), tensor(1.1627311707)]\n",
            "b:  [tensor(1.6026206017), tensor(0.8542837501), tensor(1.4931235313), tensor(1.3863965273), tensor(1.1095501184), tensor(1.9323490858), tensor(1.3044958115), tensor(1.4701519012), tensor(1.3171525002), tensor(1.1088736057), tensor(1.4682134390), tensor(1.3181152344), tensor(0.8606709838), tensor(1.3450521231), tensor(1.3716466427), tensor(1.0447555780), tensor(1.1901379824), tensor(1.4000420570), tensor(1.4946237803), tensor(1.5397882462)]\n",
            "c:  [tensor(0.4405287504), tensor(0.4405287504), tensor(0.4405287504), tensor(0.3604384065), tensor(-0.0127519201), tensor(0.0060023610), tensor(-0.1308031082), tensor(-0.0236142389), tensor(0.3604384065), tensor(-0.0127519201), tensor(0.0060023610), tensor(-0.1308031082), tensor(-0.0236142389), tensor(0.3604384065), tensor(-0.0127519201), tensor(0.0060023610), tensor(-0.1308031082), tensor(-0.0236142389)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5777869225,  0.9152854681, -0.3163803816,  1.4381296635,\n",
            "         0.0678656101, -0.7546697855,  1.2337138653,  0.2380418777,\n",
            "         1.0089417696,  0.0587711036, -0.1406858265,  0.6516099572,\n",
            "         1.1487939358,  0.3970410228,  0.7243769169,  0.9537346959,\n",
            "         0.9275225997, -0.3479636908, -0.6884588003, -0.7725639343])\n",
            "btensor.grad: tensor([ 0.1080220118, -1.1386518478, -0.5199360847, -0.7780068517,\n",
            "        -1.2297379971,  0.1054505408, -0.3150039911, -0.9404183626,\n",
            "        -0.4174933434, -0.8356236219, -1.1499516964, -0.3708015680,\n",
            "        -0.9188797474, -1.0104382038, -1.4370021820, -1.2313238382,\n",
            "        -0.7398843169, -0.3781121671, -1.2422177792, -0.3036358953])\n",
            "ctensor.grad: tensor([-22.5156230927, -22.5156230927, -22.5156230927, -18.4596748352,\n",
            "          1.0533642769,   5.5356063843,   2.8715074062,   0.3346906900,\n",
            "        -18.4596748352,   1.0533642769,   5.5356063843,   2.8715074062,\n",
            "          0.3346906900, -18.4596748352,   1.0533642769,   5.5356063843,\n",
            "          2.8715074062,   0.3346906900])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(696.1953735352, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8825381994), tensor(1.6621931791), tensor(1.1096856594), tensor(1.3006099463), tensor(0.8241971135), tensor(0.7879365087), tensor(1.5530242920), tensor(1.1192543507), tensor(1.2552027702), tensor(1.0443581343), tensor(1.0394673347), tensor(1.0232743025), tensor(1.6622626781), tensor(1.2149115801), tensor(1.5485987663), tensor(1.6667548418), tensor(1.4934966564), tensor(1.1723284721), tensor(0.9533808827), tensor(1.1666561365)]\n",
            "b:  [tensor(1.6020282507), tensor(0.8600553870), tensor(1.4957295656), tensor(1.3903055191), tensor(1.1157890558), tensor(1.9318242073), tensor(1.3060361147), tensor(1.4749104977), tensor(1.3192242384), tensor(1.1131345034), tensor(1.4739515781), tensor(1.3199697733), tensor(0.8652942777), tensor(1.3501464128), tensor(1.3788446188), tensor(1.0509327650), tensor(1.1938042641), tensor(1.4019824266), tensor(1.5009200573), tensor(1.5414034128)]\n",
            "c:  [tensor(0.4515919983), tensor(0.4515919983), tensor(0.4515919983), tensor(0.3695118725), tensor(-0.0132931462), tensor(0.0032400035), tensor(-0.1319204420), tensor(-0.0237673651), tensor(0.3695118725), tensor(-0.0132931462), tensor(0.0032400035), tensor(-0.1319204420), tensor(-0.0237673651), tensor(0.3695118725), tensor(-0.0132931462), tensor(0.0032400035), tensor(-0.1319204420), tensor(-0.0237673651)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5883275270,  0.9202927351, -0.3316378593,  1.4595885277,\n",
            "         0.0581184626, -0.7784765959,  1.2490487099,  0.2235630602,\n",
            "         1.0402246714,  0.0510079041, -0.1459923983,  0.6659072042,\n",
            "         1.1510603428,  0.4119444489,  0.7340815067,  0.9604154229,\n",
            "         0.9454518557, -0.3420841098, -0.6843808889, -0.7849978209])\n",
            "btensor.grad: tensor([ 0.1184765324, -1.1543291807, -0.5212044716, -0.7818086147,\n",
            "        -1.2477979660,  0.1049818695, -0.3080533147, -0.9517176151,\n",
            "        -0.4143421054, -0.8521877527, -1.1476230621, -0.3709111214,\n",
            "        -0.9246634841, -1.0188490152, -1.4396038055, -1.2354353666,\n",
            "        -0.7332660556, -0.3880665302, -1.2592636347, -0.3230235577])\n",
            "ctensor.grad: tensor([-22.1264896393, -22.1264896393, -22.1264896393, -18.1469497681,\n",
            "          1.0824513435,   5.5247144699,   2.2346799374,   0.3062539101,\n",
            "        -18.1469497681,   1.0824513435,   5.5247144699,   2.2346799374,\n",
            "          0.3062539101, -18.1469497681,   1.0824513435,   5.5247144699,\n",
            "          2.2346799374,   0.3062539101])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(694.7877197266, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8854796290), tensor(1.6575522423), tensor(1.1113778353), tensor(1.2932012081), tensor(0.8239117265), tensor(0.7918955088), tensor(1.5466976166), tensor(1.1181850433), tensor(1.2498329878), tensor(1.0440984964), tensor(1.0401948690), tensor(1.0198473930), tensor(1.6564887762), tensor(1.2127590179), tensor(1.5448596478), tensor(1.6619019508), tensor(1.4886629581), tensor(1.1739727259), tensor(0.9567238688), tensor(1.1705982685)]\n",
            "b:  [tensor(1.6013495922), tensor(0.8658789992), tensor(1.4983019829), tensor(1.3941998482), tensor(1.1220736504), tensor(1.9312577248), tensor(1.3075129986), tensor(1.4796867371), tensor(1.3212461472), tensor(1.1174290180), tensor(1.4796261787), tensor(1.3217948675), tensor(0.8699128628), tensor(1.3552378416), tensor(1.3860052824), tensor(1.0570999384), tensor(1.1974058151), tensor(1.4039323330), tensor(1.5072615147), tensor(1.5430819988)]\n",
            "c:  [tensor(0.4624449015), tensor(0.4624449015), tensor(0.4624449015), tensor(0.3783766329), tensor(-0.0138494186), tensor(0.0003889478), tensor(-0.1327608079), tensor(-0.0239934530), tensor(0.3783766329), tensor(-0.0138494186), tensor(0.0003889478), tensor(-0.1327608079), tensor(-0.0239934530), tensor(0.3783766329), tensor(-0.0138494186), tensor(0.0003889478), tensor(-0.1327608079), tensor(-0.0239934530)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5882815123,  0.9281890392, -0.3384364545,  1.4817588329,\n",
            "         0.0570788383, -0.7917981148,  1.2653381824,  0.2138525248,\n",
            "         1.0739626884,  0.0519329309, -0.1455103159,  0.6853891611,\n",
            "         1.1547706127,  0.4305083156,  0.7478292584,  0.9705727696,\n",
            "         0.9667285681, -0.3288618922, -0.6686028242, -0.7884253263])\n",
            "btensor.grad: tensor([ 0.1357405931, -1.1647186279, -0.5144947767, -0.7788767815,\n",
            "        -1.2569143772,  0.1132951975, -0.2953793406, -0.9552474618,\n",
            "        -0.4043810964, -0.8588925600, -1.1349099874, -0.3650240898,\n",
            "        -0.9237210155, -1.0182822943, -1.4321256876, -1.2334319353,\n",
            "        -0.7203112245, -0.3899700642, -1.2682986259, -0.3357213736])\n",
            "ctensor.grad: tensor([-21.7058238983, -21.7058238983, -21.7058238983, -17.7295246124,\n",
            "          1.1125454903,   5.7021112442,   1.6807303429,   0.4521746337,\n",
            "        -17.7295246124,   1.1125454903,   5.7021112442,   1.6807303429,\n",
            "          0.4521746337, -17.7295246124,   1.1125454903,   5.7021112442,\n",
            "          1.6807303429,   0.4521746337])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(693.4294433594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8885408044), tensor(1.6529399157), tensor(1.1131982803), tensor(1.2857756615), tensor(0.8237968087), tensor(0.7960715890), tensor(1.5403624773), tensor(1.1172603369), tensor(1.2443938255), tensor(1.0439373255), tensor(1.0410231352), tensor(1.0164240599), tensor(1.6507716179), tensor(1.2106149197), tensor(1.5411262512), tensor(1.6570584774), tensor(1.4837996960), tensor(1.1756446362), tensor(0.9600901604), tensor(1.1746406555)]\n",
            "b:  [tensor(1.6006531715), tensor(0.8718088269), tensor(1.5009086132), tensor(1.3981482983), tensor(1.1284577847), tensor(1.9307160378), tensor(1.3089983463), tensor(1.4845414162), tensor(1.3232901096), tensor(1.1218222380), tensor(1.4852889776), tensor(1.3236578703), tensor(0.8745840192), tensor(1.3603855371), tensor(1.3931893110), tensor(1.0632981062), tensor(1.2010085583), tensor(1.4059617519), tensor(1.5137128830), tensor(1.5448992252)]\n",
            "c:  [tensor(0.4730628133), tensor(0.4730628133), tensor(0.4730628133), tensor(0.3871190548), tensor(-0.0144190844), tensor(-0.0022538097), tensor(-0.1332112551), tensor(-0.0240461510), tensor(0.3871190548), tensor(-0.0144190844), tensor(-0.0022538097), tensor(-0.1332112551), tensor(-0.0240461510), tensor(0.3871190548), tensor(-0.0144190844), tensor(-0.0022538097), tensor(-0.1332112551), tensor(-0.0240461510)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.6122351885,  0.9224758148, -0.3640817404,  1.4851169586,\n",
            "         0.0229831934, -0.8352153301,  1.2670297623,  0.1849333197,\n",
            "         1.0878338814,  0.0322384387, -0.1656594276,  0.6846717596,\n",
            "         1.1434332132,  0.4288160205,  0.7466723919,  0.9686965942,\n",
            "         0.9726552963, -0.3343856931, -0.6732597351, -0.8084667921])\n",
            "btensor.grad: tensor([ 0.1392892748, -1.1859641075, -0.5213157535, -0.7896906734,\n",
            "        -1.2768260241,  0.1083398461, -0.2970635891, -0.9709446430,\n",
            "        -0.4087850451, -0.8786538839, -1.1325485706, -0.3726010323,\n",
            "        -0.9342300892, -1.0295326710, -1.4368064404, -1.2396287918,\n",
            "        -0.7205486894, -0.4058825374, -1.2902823687, -0.3634339273])\n",
            "ctensor.grad: tensor([-21.2358303070, -21.2358303070, -21.2358303070, -17.4848232269,\n",
            "          1.1393320560,   5.2855148315,   0.9009026289,   0.1053944230,\n",
            "        -17.4848232269,   1.1393320560,   5.2855148315,   0.9009026289,\n",
            "          0.1053944230, -17.4848232269,   1.1393320560,   5.2855148315,\n",
            "          0.9009026289,   0.1053944230])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(692.1231079102, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8913206458), tensor(1.6481797695), tensor(1.1148312092), tensor(1.2781412601), tensor(0.8233911395), tensor(0.7999995947), tensor(1.5338619947), tensor(1.1162148714), tensor(1.2386564016), tensor(1.0435403585), tensor(1.0416604280), tensor(1.0127301216), tensor(1.6449419260), tensor(1.2082260847), tensor(1.5371922255), tensor(1.6520553827), tensor(1.4787075520), tensor(1.1770514250), tensor(0.9631013274), tensor(1.1784830093)]\n",
            "b:  [tensor(1.5997081995), tensor(0.8776596189), tensor(1.5033013821), tensor(1.4019180536), tensor(1.1347059011), tensor(1.9299427271), tensor(1.3102699518), tensor(1.4892427921), tensor(1.3251181841), tensor(1.1260423660), tensor(1.4906891584), tensor(1.3253415823), tensor(0.8790975809), tensor(1.3653432131), tensor(1.4001324177), tensor(1.0693563223), tensor(1.2043917179), tensor(1.4078164101), tensor(1.5200300217), tensor(1.5466094017)]\n",
            "c:  [tensor(0.4834584296), tensor(0.4834584296), tensor(0.4834584296), tensor(0.3954145014), tensor(-0.0150068402), tensor(-0.0055717044), tensor(-0.1336368918), tensor(-0.0246799141), tensor(0.3954145014), tensor(-0.0150068402), tensor(-0.0055717044), tensor(-0.1336368918), tensor(-0.0246799141), tensor(0.3954145014), tensor(-0.0150068402), tensor(-0.0055717044), tensor(-0.1336368918), tensor(-0.0246799141)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5559642315,  0.9520395994, -0.3265863955,  1.5268886089,\n",
            "         0.0811295509, -0.7855961323,  1.3000959158,  0.2091005445,\n",
            "         1.1474754810,  0.0793968737, -0.1274584234,  0.7387817502,\n",
            "         1.1659272909,  0.4777703881,  0.7867951393,  1.0006303787,\n",
            "         1.0184319019, -0.2813551426, -0.6022294760, -0.7684689164])\n",
            "btensor.grad: tensor([ 0.1889933944, -1.1701600552, -0.4785482287, -0.7539581060,\n",
            "        -1.2496223450,  0.1546546817, -0.2543282509, -0.9402813911,\n",
            "        -0.3656237125, -0.8440343142, -1.0800325871, -0.3367460370,\n",
            "        -0.9027063847, -0.9915415645, -1.3886256218, -1.2116549015,\n",
            "        -0.6766256690, -0.3709239364, -1.2634308338, -0.3420471251])\n",
            "ctensor.grad: tensor([-20.7912216187, -20.7912216187, -20.7912216187, -16.5909156799,\n",
            "          1.1755119562,   6.6357889175,   0.8512614965,   1.2675244808,\n",
            "        -16.5909156799,   1.1755119562,   6.6357889175,   0.8512614965,\n",
            "          1.2675244808, -16.5909156799,   1.1755119562,   6.6357889175,\n",
            "          0.8512614965,   1.2675244808])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(690.8732910156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8950520158), tensor(1.6438597441), tensor(1.1172422171), tensor(1.2710045576), tensor(0.8242015839), tensor(0.8051408529), tensor(1.5277628899), tensor(1.1159156561), tensor(1.2334233522), tensor(1.0439428091), tensor(1.0430514812), tensor(1.0096727610), tensor(1.6395953894), tensor(1.2064515352), tensor(1.5337333679), tensor(1.6474407911), tensor(1.4740530252), tensor(1.1791127920), tensor(0.9668852687), tensor(1.1830186844)]\n",
            "b:  [tensor(1.5992183685), tensor(0.8839877248), tensor(1.5062165260), tensor(1.4062145948), tensor(1.1414809227), tensor(1.9296891689), tensor(1.3120193481), tensor(1.4944688082), tensor(1.3274554014), tensor(1.1308661699), tensor(1.4965204000), tensor(1.3275128603), tensor(0.8840738535), tensor(1.3708137274), tensor(1.4075801373), tensor(1.0757619143), tensor(1.2082266808), tensor(1.4102518559), tensor(1.5269286633), tensor(1.5489631891)]\n",
            "c:  [tensor(0.4935273230), tensor(0.4935273230), tensor(0.4935273230), tensor(0.4042427838), tensor(-0.0155971041), tensor(-0.0067084669), tensor(-0.1328966320), tensor(-0.0234728009), tensor(0.4042427838), tensor(-0.0155971041), tensor(-0.0067084669), tensor(-0.1328966320), tensor(-0.0234728009), tensor(0.4042427838), tensor(-0.0155971041), tensor(-0.0067084669), tensor(-0.1328966320), tensor(-0.0234728009)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.7462779284,  0.8640071154, -0.4821918309,  1.4273355007,\n",
            "        -0.1620845795, -1.0282562971,  1.2198328972,  0.0598541424,\n",
            "         1.0466159582, -0.0805001259, -0.2782119513,  0.6114788055,\n",
            "         1.0693141222,  0.3548998833,  0.6917815208,  0.9229233265,\n",
            "         0.9309109449, -0.4122658372, -0.7567881346, -0.9071392417])\n",
            "btensor.grad: tensor([ 0.0979632363, -1.2656245232, -0.5830224156, -0.8593113422,\n",
            "        -1.3550020456,  0.0507225990, -0.3498750925, -1.0452061892,\n",
            "        -0.4674375057, -0.9647687674, -1.1662600040, -0.4342575073,\n",
            "        -0.9952540398, -1.0940928459, -1.4895418882, -1.2811169624,\n",
            "        -0.7669854164, -0.4870944619, -1.3797223568, -0.4707581997])\n",
            "ctensor.grad: tensor([-20.1377601624, -20.1377601624, -20.1377601624, -17.6565914154,\n",
            "          1.1805281639,   2.2735247612,  -1.4805127382,  -2.4142255783,\n",
            "        -17.6565914154,   1.1805281639,   2.2735247612,  -1.4805127382,\n",
            "         -2.4142255783, -17.6565914154,   1.1805281639,   2.2735247612,\n",
            "         -1.4805127382,  -2.4142255783])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(689.7147216797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8956060410), tensor(1.6380712986), tensor(1.1171984673), tensor(1.2621394396), tensor(0.8212857842), tensor(0.8066353202), tensor(1.5202571154), tensor(1.1135084629), tensor(1.2261261940), tensor(1.0417048931), tensor(1.0420898199), tensor(1.0042949915), tensor(1.6328285933), tensor(1.2025046349), tensor(1.5285387039), tensor(1.6414239407), tensor(1.4676738977), tensor(1.1787807941), tensor(0.9676384926), tensor(1.1852444410)]\n",
            "b:  [tensor(1.5968502760), tensor(0.8889291286), tensor(1.5071849823), tensor(1.4086837769), tensor(1.1465231180), tensor(1.9274302721), tensor(1.3119679689), tensor(1.4979370832), tensor(1.3278971910), tensor(1.1336570978), tensor(1.5004030466), tensor(1.3279657364), tensor(0.8874279857), tensor(1.3744109869), tensor(1.4129935503), tensor(1.0808534622), tensor(1.2102755308), tensor(1.4107356071), tensor(1.5319942236), tensor(1.5494683981)]\n",
            "c:  [tensor(0.5035555363), tensor(0.5035555363), tensor(0.5035555363), tensor(0.4102984369), tensor(-0.0162412785), tensor(-0.0151694547), tensor(-0.1348552108), tensor(-0.0285158660), tensor(0.4102984369), tensor(-0.0162412785), tensor(-0.0151694547), tensor(-0.1348552108), tensor(-0.0285158660), tensor(0.4102984369), tensor(-0.0162412785), tensor(-0.0151694547), tensor(-0.1348552108), tensor(-0.0285158660)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.1108065844,  1.1576783657,  0.0087480843,  1.7730340958,\n",
            "         0.5831630230, -0.2988946438,  1.5011575222,  0.4814445078,\n",
            "         1.4594361782,  0.4475761652,  0.1923426688,  1.0755513906,\n",
            "         1.3533710241,  0.7893734574,  1.0389409065,  1.2033599615,\n",
            "         1.2758321762,  0.0663991570, -0.1506450176, -0.4451552033])\n",
            "btensor.grad: tensor([ 0.4736123681, -0.9882833958, -0.1936918795, -0.4938414991,\n",
            "        -1.0084471703,  0.4517766535,  0.0102842450, -0.6936485767,\n",
            "        -0.0883573890, -0.5581812859, -0.7765299678, -0.0905844569,\n",
            "        -0.6708247662, -0.7194541693, -1.0826736689, -1.0183112621,\n",
            "        -0.4097740054, -0.0967538655, -1.0131167173, -0.1010411084])\n",
            "ctensor.grad: tensor([-20.0563907623, -20.0563907623, -20.0563907623, -12.1112842560,\n",
            "          1.2883487940,  16.9219741821,   3.9171516895,  10.0861291885,\n",
            "        -12.1112842560,   1.2883487940,  16.9219741821,   3.9171516895,\n",
            "         10.0861291885, -12.1112842560,   1.2883487940,  16.9219741821,\n",
            "          3.9171516895,  10.0861291885])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(689.0982055664, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.9075985551), tensor(1.6375268698), tensor(1.1260033846), tensor(1.2595295906), tensor(0.8324864507), tensor(0.8218213320), tensor(1.5177812576), tensor(1.1191122532), tensor(1.2259262800), tensor(1.0489825010), tensor(1.0497456789), tensor(1.0071181059), tensor(1.6313980818), tensor(1.2062467337), tensor(1.5294080973), tensor(1.6402866840), tensor(1.4672127962), tensor(1.1867372990), tensor(0.9787010550), tensor(1.1955735683)]\n",
            "b:  [tensor(1.6006141901), tensor(0.8987775445), tensor(1.5146332979), tensor(1.4173408747), tensor(1.1574106216), tensor(1.9318014383), tensor(1.3179126978), tensor(1.5073946714), tensor(1.3346374035), tensor(1.1433222294), tensor(1.5103673935), tensor(1.3342171907), tensor(0.8962200880), tensor(1.3841925859), tensor(1.4249049425), tensor(1.0902374983), tensor(1.2181423903), tensor(1.4179074764), tensor(1.5433954000), tensor(1.5566630363)]\n",
            "c:  [tensor(0.5120915771), tensor(0.5120915771), tensor(0.5120915771), tensor(0.4253149629), tensor(-0.0167561211), tensor(0.0028687846), tensor(-0.1259040534), tensor(-0.0111818332), tensor(0.4253149629), tensor(-0.0167561211), tensor(0.0028687846), tensor(-0.1259040534), tensor(-0.0111818332), tensor(0.4253149629), tensor(-0.0167561211), tensor(0.0028687846), tensor(-0.1259040534), tensor(-0.0111818332)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-2.3985018730,  0.1088844538, -1.7609800100,  0.5219590664,\n",
            "        -2.2401282787, -3.0372071266,  0.4951725006, -1.1207463741,\n",
            "         0.0399727151, -1.4555215836, -1.5311635733, -0.5646228194,\n",
            "         0.2860913277, -0.7484260798, -0.1738844961,  0.2274419218,\n",
            "         0.0922258496, -1.5913116932, -2.2125167847, -2.0658316612])\n",
            "btensor.grad: tensor([-0.7527789474, -1.9696828127, -1.4896656275, -1.7314157486,\n",
            "        -2.1775107384, -0.8742314577, -1.1889380217, -1.8915203810,\n",
            "        -1.3480418921, -1.9330337048, -1.9928662777, -1.2502901554,\n",
            "        -1.7584264278, -1.9563158751, -2.3822727203, -1.8768069744,\n",
            "        -1.5733646154, -1.4343643188, -2.2802393436, -1.4389257431])\n",
            "ctensor.grad: tensor([-17.0720634460, -17.0720634460, -17.0720634460, -30.0330505371,\n",
            "          1.0296865702, -36.0764770508, -17.9023227692, -34.6680641174,\n",
            "        -30.0330505371,   1.0296865702, -36.0764770508, -17.9023227692,\n",
            "        -34.6680641174, -30.0330505371,   1.0296865702, -36.0764770508,\n",
            "        -17.9023227692, -34.6680641174])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(694.7931518555, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8823296428), tensor(1.6192536354), tensor(1.1050102711), tensor(1.2366800308), tensor(0.8006351590), tensor(0.7938956618), tensor(1.4983364344), tensor(1.0985952616), tensor(1.2022182941), tensor(1.0253659487), tensor(1.0289518833), tensor(0.9830322862), tensor(1.6125134230), tensor(1.1841242313), tensor(1.5098818541), tensor(1.6225765944), tensor(1.4468042850), tensor(1.1664117575), tensor(0.9549975991), tensor(1.1781862974)]\n",
            "b:  [tensor(1.5820312500), tensor(0.8900141716), tensor(1.4982690811), tensor(1.4029172659), tensor(1.1458266973), tensor(1.9121118784), tensor(1.3018647432), tensor(1.4947185516), tensor(1.3180449009), tensor(1.1272604465), tensor(1.4969509840), tensor(1.3189983368), tensor(0.8844496012), tensor(1.3705096245), tensor(1.4116021395), tensor(1.0830513239), tensor(1.2038282156), tensor(1.4007378817), tensor(1.5309650898), tensor(1.5401624441)]\n",
            "c:  [tensor(0.5201002955), tensor(0.5201002955), tensor(0.5201002955), tensor(0.4108213484), tensor(-0.0177621283), tensor(-0.0659989566), tensor(-0.1530703157), tensor(-0.0696049109), tensor(0.4108213484), tensor(-0.0177621283), tensor(-0.0659989566), tensor(-0.1530703157), tensor(-0.0696049109), tensor(0.4108213484), tensor(-0.0177621283), tensor(-0.0659989566), tensor(-0.1530703157), tensor(-0.0696049109)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([5.0537848473, 3.6546533108, 4.1986165047, 4.5699191093, 6.3702573776,\n",
            "        5.5851297379, 3.8889584541, 4.1033887863, 4.7415871620, 4.7233061790,\n",
            "        4.1587657928, 4.8171639442, 3.7769365311, 4.4245080948, 3.9052529335,\n",
            "        3.5420169830, 4.0817003250, 4.0651183128, 4.7406945229, 3.4774651527])\n",
            "btensor.grad: tensor([3.7165882587, 1.7526792288, 3.2728407383, 2.8847131729, 2.3167943954,\n",
            "        3.9379208088, 3.2095806599, 2.5352129936, 3.3184900284, 3.2123532295,\n",
            "        2.6832890511, 3.0437672138, 2.3540928364, 2.7365944386, 2.6605603695,\n",
            "        1.4372437000, 2.8628277779, 3.4339280128, 2.4860618114, 3.3001108170])\n",
            "ctensor.grad: tensor([-16.0174617767, -16.0174617767, -16.0174617767,  28.9872512817,\n",
            "          2.0120155811, 137.7354736328,  54.3325119019, 116.8461456299,\n",
            "         28.9872512817,   2.0120155811, 137.7354736328,  54.3325119019,\n",
            "        116.8461456299,  28.9872512817,   2.0120155811, 137.7354736328,\n",
            "         54.3325119019, 116.8461456299])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(773.3954467773, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.0577598810), tensor(1.6763113737), tensor(1.2128659487), tensor(1.3177608252), tensor(1.0986175537), tensor(1.0499931574), tensor(1.5505516529), tensor(1.2029104233), tensor(1.2927623987), tensor(1.1645548344), tensor(1.1435841322), tensor(1.0968360901), tensor(1.6732430458), tensor(1.2756400108), tensor(1.5769867897), tensor(1.6754028797), tensor(1.5150758028), tensor(1.2660790682), tensor(1.0970175266), tensor(1.2739052773)]\n",
            "b:  [tensor(1.6390478611), tensor(0.9336823225), tensor(1.5558265448), tensor(1.4572457075), tensor(1.1971242428), tensor(1.9706283808), tensor(1.3566206694), tensor(1.5493291616), tensor(1.3736714125), tensor(1.1855871677), tensor(1.5520766973), tensor(1.3710550070), tensor(0.9328653812), tensor(1.4259290695), tensor(1.4689003229), tensor(1.1242605448), tensor(1.2542586327), tensor(1.4600005150), tensor(1.5876135826), tensor(1.6005995274)]\n",
            "c:  [tensor(0.4581671953), tensor(0.4581671953), tensor(0.4581671953), tensor(0.5196059942), tensor(-0.0169735514), tensor(0.1979188770), tensor(-0.0565926060), tensor(0.1420579553), tensor(0.5196059942), tensor(-0.0169735514), tensor(0.1979188770), tensor(-0.0565926060), tensor(0.1420579553), tensor(0.5196059942), tensor(-0.0169735514), tensor(0.1979188770), tensor(-0.0565926060), tensor(0.1420579553)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-35.0860481262, -11.4115495682, -21.5711345673, -16.2161521912,\n",
            "        -59.5964736938, -51.2195014954, -10.4430322647, -20.8630352020,\n",
            "        -18.1088237762, -27.8377857208, -22.9264450073, -22.7607669830,\n",
            "        -12.1459159851, -18.3031673431, -13.4209861755, -10.5652456284,\n",
            "        -13.6543149948, -19.9334526062, -28.4039916992, -19.1437950134])\n",
            "btensor.grad: tensor([-11.4033203125,  -8.7336359024, -11.5115041733, -10.8656826019,\n",
            "        -10.2595214844, -11.7033119202, -10.9511842728, -10.9221191406,\n",
            "        -11.1252937317, -11.6653528214, -11.0251483917, -10.4113321304,\n",
            "         -9.6831607819, -11.0838985443, -11.4596376419,  -8.2418384552,\n",
            "        -10.0860776901, -11.8525190353, -11.3296899796, -12.0874176025])\n",
            "ctensor.grad: tensor([ 123.8661880493,  123.8661880493,  123.8661880493, -217.5692901611,\n",
            "          -1.5771540403, -527.8356323242, -192.9554138184, -423.3256835938,\n",
            "        -217.5692901611,   -1.5771540403, -527.8356323242, -192.9554138184,\n",
            "        -423.3256835938, -217.5692901611,   -1.5771540403, -527.8356323242,\n",
            "        -192.9554138184, -423.3256835938])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(890.4710083008, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.9623334408), tensor(1.6212129593), tensor(1.1354044676), tensor(1.2577574253), tensor(0.9994007349), tensor(0.9419576526), tensor(1.4961823225), tensor(1.1292963028), tensor(1.2259292603), tensor(1.0819498301), tensor(1.0603243113), tensor(1.0201566219), tensor(1.6211394072), tensor(1.2016881704), tensor(1.5177228451), tensor(1.6243653297), tensor(1.4561481476), tensor(1.1833660603), tensor(1.0024281740), tensor(1.1926757097)]\n",
            "b:  [tensor(1.5520423651), tensor(0.8162239790), tensor(1.4628005028), tensor(1.3517191410), tensor(1.0856000185), tensor(1.8838260174), tensor(1.2584297657), tensor(1.4509887695), tensor(1.2739994526), tensor(1.0794216394), tensor(1.4447954893), tensor(1.2730021477), tensor(0.8193454742), tensor(1.3191399574), tensor(1.3543767929), tensor(1.0141632557), tensor(1.1452383995), tensor(1.3635885715), tensor(1.4783835411), tensor(1.5106440783)]\n",
            "c:  [tensor(0.3879991174), tensor(0.3879991174), tensor(0.3879991174), tensor(0.5162848234), tensor(-0.0172815304), tensor(0.1841066182), tensor(-0.0634241328), tensor(0.1251317710), tensor(0.5162848234), tensor(-0.0172815304), tensor(0.1841066182), tensor(-0.0634241328), tensor(0.1251317710), tensor(0.5162848234), tensor(-0.0172815304), tensor(0.1841066182), tensor(-0.0634241328), tensor(0.1251317710)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([19.0852870941, 11.0196800232, 15.4923028946, 12.0006895065,\n",
            "        19.8433666229, 21.6070995331, 10.8738765717, 14.7228193283,\n",
            "        13.3666315079, 16.5209999084, 16.6519680023, 15.3358831406,\n",
            "        10.4207248688, 14.7903709412, 11.8527984619, 10.2075128555,\n",
            "        11.7855205536, 16.5425987244, 18.9178600311, 16.2459030151])\n",
            "btensor.grad: tensor([17.4011001587, 23.4916648865, 18.6052131653, 21.1053161621,\n",
            "        22.3048496246, 17.3604850769, 19.6381874084, 19.6680908203,\n",
            "        19.9343872070, 21.2331123352, 21.4562320709, 19.6105785370,\n",
            "        22.7039794922, 21.3578262329, 22.9047050476, 22.0194702148,\n",
            "        21.8040580750, 19.2823944092, 21.8460063934, 17.9910888672])\n",
            "ctensor.grad: tensor([140.3361663818, 140.3361663818, 140.3361663818,   6.6423468590,\n",
            "          0.6159579158,  27.6245269775,  13.6630516052,  33.8523597717,\n",
            "          6.6423468590,   0.6159579158,  27.6245269775,  13.6630516052,\n",
            "         33.8523597717,   6.6423468590,   0.6159579158,  27.6245269775,\n",
            "         13.6630516052,  33.8523597717])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(807.1011352539, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8896971941), tensor(1.5879197121), tensor(1.0799561739), tensor(1.2214835882), tensor(0.9231362343), tensor(0.8563488722), tensor(1.4640164375), tensor(1.0782712698), tensor(1.1823172569), tensor(1.0232408047), tensor(1.0005313158), tensor(0.9675559402), tensor(1.5907776356), tensor(1.1509822607), tensor(1.4804555178), tensor(1.5941572189), tensor(1.4195594788), tensor(1.1238169670), tensor(0.9311802387), tensor(1.1333669424)]\n",
            "b:  [tensor(1.4966738224), tensor(0.7392734289), tensor(1.4007841349), tensor(1.2799541950), tensor(1.0108655691), tensor(1.8270040751), tensor(1.1948552132), tensor(1.3839867115), tensor(1.2088260651), tensor(1.0100598335), tensor(1.3707635403), tensor(1.2093143463), tensor(0.7467876077), tensor(1.2465550900), tensor(1.2738020420), tensor(0.9393950105), tensor(1.0721431971), tensor(1.3006316423), tensor(1.4022855759), tensor(1.4513419867)]\n",
            "c:  [tensor(0.3408508003), tensor(0.3408508003), tensor(0.3408508003), tensor(0.5133914948), tensor(-0.0175571293), tensor(0.1713900268), tensor(-0.0697556883), tensor(0.1096130610), tensor(0.5133914948), tensor(-0.0175571293), tensor(0.1713900268), tensor(-0.0697556883), tensor(0.1096130610), tensor(0.5133914948), tensor(-0.0175571293), tensor(0.1713900268), tensor(-0.0697556883), tensor(0.1096130610)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([14.5272445679,  6.6586618423, 11.0896530151,  7.2547769547,\n",
            "        15.2528953552, 17.1217613220,  6.4331798553, 10.2050065994,\n",
            "         8.7223930359, 11.7418003082, 11.9586048126, 10.5201416016,\n",
            "         6.0723605156, 10.1411809921,  7.4534678459,  6.0416274071,\n",
            "         7.3177242279, 11.9098300934, 14.2495861053, 11.8617515564])\n",
            "btensor.grad: tensor([11.0737094879, 15.3901128769, 12.4032726288, 14.3529872894,\n",
            "        14.9468965530, 11.3643846512, 12.7149124146, 13.4004096985,\n",
            "        13.0346698761, 13.8723697662, 14.8063888550, 12.7375516891,\n",
            "        14.5115737915, 14.5169639587, 16.1149539948, 14.9536514282,\n",
            "        14.6190328598, 12.5913858414, 15.2195816040, 11.8604259491])\n",
            "ctensor.grad: tensor([94.2966156006, 94.2966156006, 94.2966156006,  5.7866520882,\n",
            "         0.5511962175, 25.4331665039, 12.6631145477, 31.0374126434,\n",
            "         5.7866520882,  0.5511962175, 25.4331665039, 12.6631145477,\n",
            "        31.0374126434,  5.7866520882,  0.5511962175, 25.4331665039,\n",
            "        12.6631145477, 31.0374126434])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(767.8674926758, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.8346220255), tensor(1.5697951317), tensor(1.0407983065), tensor(1.2020376921), tensor(0.8646006584), tensor(0.7884808779), tensor(1.4473456144), tensor(1.0439212322), tensor(1.1554218531), tensor(0.9824669957), tensor(0.9584024549), tensor(0.9328272939), tensor(1.5754578114), tensor(1.1173180342), tensor(1.4586635828), tensor(1.5783581734), tensor(1.3986548185), tensor(1.0815705061), tensor(0.8779388666), tensor(1.0904473066)]\n",
            "b:  [tensor(1.4630205631), tensor(0.6929618716), tensor(1.3605400324), tensor(1.2325577736), tensor(0.9629815221), tensor(1.7908327579), tensor(1.1557534933), tensor(1.3393493891), tensor(1.1681091785), tensor(0.9671540856), tensor(1.3206548691), tensor(1.1699204445), tensor(0.7046222687), tensor(1.1985965967), tensor(1.2180573940), tensor(0.8909623027), tensor(1.0252501965), tensor(1.2612103224), tensor(1.3501267433), tensor(1.4134974480)]\n",
            "c:  [tensor(0.3107034862), tensor(0.3107034862), tensor(0.3107034862), tensor(0.5108256936), tensor(-0.0178135708), tensor(0.1593132913), tensor(-0.0758030191), tensor(0.0949721783), tensor(0.5108256936), tensor(-0.0178135708), tensor(0.1593132913), tensor(-0.0758030191), tensor(0.0949721783), tensor(0.5108256936), tensor(-0.0178135708), tensor(0.1593132913), tensor(-0.0758030191), tensor(0.0949721783)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([11.0150318146,  3.6249046326,  7.8315839767,  3.8891737461,\n",
            "        11.7071189880, 13.5736036301,  3.3341534138,  6.8700017929,\n",
            "         5.3790869713,  8.1547670364,  8.4257774353,  6.9457278252,\n",
            "         3.0639588833,  6.7328457832,  4.3583903313,  3.1598155499,\n",
            "         4.1809425354,  8.4492845535, 10.6482791901,  8.5839223862])\n",
            "btensor.grad: tensor([ 6.7306418419,  9.2623081207,  8.0488252640,  9.4792737961,\n",
            "         9.5768117905,  7.2342605591,  7.8203358650,  8.9274673462,\n",
            "         8.1433811188,  8.5811510086, 10.0217418671,  7.8787755966,\n",
            "         8.4330625534,  9.5916891098, 11.1489267349,  9.6865386963,\n",
            "         9.3786125183,  7.8842539787, 10.4317550659,  7.5689167976])\n",
            "ctensor.grad: tensor([60.2946281433, 60.2946281433, 60.2946281433,  5.1316380501,\n",
            "         0.5128811598, 24.1534690857, 12.0946578979, 29.2817707062,\n",
            "         5.1316380501,  0.5128811598, 24.1534690857, 12.0946578979,\n",
            "        29.2817707062,  5.1316380501,  0.5128811598, 24.1534690857,\n",
            "        12.0946578979, 29.2817707062])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(749.7548217773, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7927643061), tensor(1.5616120100), tensor(1.0132154226), tensor(1.1937433481), tensor(0.8194183111), tensor(0.7345560193), tensor(1.4408286810), tensor(1.0213313103), tensor(1.1398855448), tensor(0.9545891285), tensor(0.9290814996), tensor(0.9106852412), tensor(1.5699062347), tensor(1.0955777168), tensor(1.4471638203), tensor(1.5719277859), tensor(1.3881143332), tensor(1.0517944098), tensor(0.8382403255), tensor(1.0594127178)]\n",
            "b:  [tensor(1.4436237812), tensor(0.6682903171), tensor(1.3350390196), tensor(1.2022147179), tensor(0.9337888360), tensor(1.7683699131), tensor(1.1331990957), tensor(1.3102300167), tensor(1.1439905167), tensor(0.9422398210), tensor(1.2872756720), tensor(1.1469634771), tensor(0.6832620502), tensor(1.1677609682), tensor(1.1799927950), tensor(0.8614907861), tensor(0.9967141151), tensor(1.2376606464), tensor(1.3148239851), tensor(1.3901405334)]\n",
            "c:  [tensor(0.2927121222), tensor(0.2927121222), tensor(0.2927121222), tensor(0.5084679127), tensor(-0.0180634428), tensor(0.1473113000), tensor(-0.0818349272), tensor(0.0805720165), tensor(0.5084679127), tensor(-0.0180634428), tensor(0.1473113000), tensor(-0.0818349272), tensor(0.0805720165), tensor(0.5084679127), tensor(-0.0180634428), tensor(0.1473113000), tensor(-0.0818349272), tensor(0.0805720165)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 8.3715391159,  1.6366151571,  5.5165662766,  1.6588729620,\n",
            "         9.0364723206, 10.7849731445,  1.3033763170,  4.5179767609,\n",
            "         3.1072645187,  5.5755786896,  5.8641862869,  4.4284052849,\n",
            "         1.1103200912,  4.3480601311,  2.2999498844,  1.2860783339,\n",
            "         2.1081037521,  5.9552087784,  7.9397039413,  6.2069282532])\n",
            "btensor.grad: tensor([3.8793554306, 4.9343104362, 5.1002116203, 6.0686101913, 5.8385429382,\n",
            "        4.4925618172, 4.5108771324, 5.8238759041, 4.8237442970, 4.9828491211,\n",
            "        6.6758317947, 4.5913982391, 4.2720456123, 6.1671323776, 7.6129307747,\n",
            "        5.8943080902, 5.7072191238, 4.7099237442, 7.0605559349, 4.6713786125])\n",
            "ctensor.grad: tensor([35.9827041626, 35.9827041626, 35.9827041626,  4.7156171799,\n",
            "         0.4997424483, 24.0039844513, 12.0638122559, 28.8003177643,\n",
            "         4.7156171799,  0.4997424483, 24.0039844513, 12.0638122559,\n",
            "        28.8003177643,  4.7156171799,  0.4997424483, 24.0039844513,\n",
            "        12.0638122559, 28.8003177643])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(740.9357910156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7607492208), tensor(1.5595576763), tensor(0.9936062098), tensor(1.1923282146), tensor(0.7841124535), tensor(0.6916615367), tensor(1.4405477047), tensor(1.0067331791), tensor(1.1316490173), tensor(0.9356577396), tensor(0.9088115096), tensor(0.8969894648), tensor(1.5702821016), tensor(1.0818687677), tensor(1.4421461821), tensor(1.5712009668), tensor(1.3840147257), tensor(1.0308049917), tensor(0.8085982800), tensor(1.0368716717)]\n",
            "b:  [tensor(1.4332519770), tensor(0.6579601169), tensor(1.3192820549), tensor(1.1836042404), tensor(0.9171473384), tensor(1.7547484636), tensor(1.1214917898), tensor(1.2917430401), tensor(1.1307899952), tensor(0.9289986491), tensor(1.2654436827), tensor(1.1347787380), tensor(0.6751230359), tensor(1.1485813856), tensor(1.1543588638), tensor(0.8453927636), tensor(0.9806897640), tensor(1.2244812250), tensor(1.2912732363), tensor(1.3763250113)]\n",
            "c:  [tensor(0.2833063602), tensor(0.2833063602), tensor(0.2833063602), tensor(0.5061973333), tensor(-0.0183182471), tensor(0.1347509921), tensor(-0.0881511271), tensor(0.0657382682), tensor(0.5061973333), tensor(-0.0183182471), tensor(0.1347509921), tensor(-0.0881511271), tensor(0.0657382682), tensor(0.5061973333), tensor(-0.0183182471), tensor(0.1347509921), tensor(-0.0881511271), tensor(0.0657382682)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 6.4030213356,  0.4108564556,  3.9218482971,  0.2830246687,\n",
            "         7.0611658096,  8.5788974762,  0.0561850071,  2.9196307659,\n",
            "         1.6473078728,  3.7862825394,  4.0540022850,  2.7391612530,\n",
            "        -0.0751641393,  2.7417826653,  1.0035374165,  0.1453629285,\n",
            "         0.8199127913,  4.1978740692,  5.9284038544,  4.5081996918])\n",
            "btensor.grad: tensor([2.0743503571, 2.0660443306, 3.1513817310, 3.7220861912, 3.3282988071,\n",
            "        2.7242865562, 2.3414688110, 3.6973915100, 2.6400945187, 2.6482386589,\n",
            "        4.3663940430, 2.4369597435, 1.6278030872, 3.8359203339, 5.1267781258,\n",
            "        3.2196044922, 3.2048683167, 2.6358816624, 4.7101387978, 2.7630968094])\n",
            "ctensor.grad: tensor([18.8115348816, 18.8115348816, 18.8115348816,  4.5411434174,\n",
            "         0.5096099377, 25.1206054688, 12.6324043274, 29.6674900055,\n",
            "         4.5411434174,  0.5096099377, 25.1206054688, 12.6324043274,\n",
            "        29.6674900055,  4.5411434174,  0.5096099377, 25.1206054688,\n",
            "        12.6324043274, 29.6674900055])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(735.7558593750, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7362026572), tensor(1.5610461235), tensor(0.9794527292), tensor(1.1948361397), tensor(0.7561061978), tensor(0.6577764750), tensor(1.4438451529), tensor(0.9974850416), tensor(1.1278840303), tensor(0.9228349328), tensor(0.8949538469), tensor(0.8887789249), tensor(1.5739834309), tensor(1.0734864473), tensor(1.4410146475), tensor(1.5736936331), tensor(1.3836777210), tensor(1.0160549879), tensor(0.7865478396), tensor(1.0205181837)]\n",
            "b:  [tensor(1.4284783602), tensor(0.6568552852), tensor(1.3099555969), tensor(1.1731389761), tensor(0.9088695049), tensor(1.7467529774), tensor(1.1168743372), tensor(1.2806509733), tensor(1.1247287989), tensor(0.9231414795), tensor(1.2517075539), tensor(1.1296005249), tensor(0.6750659943), tensor(1.1373866796), tensor(1.1375993490), tensor(0.8388221860), tensor(0.9731962681), tensor(1.2180100679), tensor(1.2760765553), tensor(1.3687686920)]\n",
            "c:  [tensor(0.2801288366), tensor(0.2801288366), tensor(0.2801288366), tensor(0.5039070845), tensor(-0.0185881872), tensor(0.1209543645), tensor(-0.0950658321), tensor(0.0498205647), tensor(0.5039070845), tensor(-0.0185881872), tensor(0.1209543645), tensor(-0.0950658321), tensor(0.0498205647), tensor(0.5039070845), tensor(-0.0185881872), tensor(0.1209543645), tensor(-0.0950658321), tensor(0.0498205647)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.9093136787, -0.2976941764,  2.8306958675, -0.5015753508,\n",
            "         5.6012477875,  6.7770161629, -0.6594862938,  1.8496315479,\n",
            "         0.7529960275,  2.5645635128,  2.7715308666,  1.6421067715,\n",
            "        -0.7402590513,  1.6764742136,  0.2263021469, -0.4985266924,\n",
            "         0.0674127936,  2.9500124454,  4.4100914001,  3.2706925869])\n",
            "btensor.grad: tensor([0.9547346830, 0.2209686041, 1.8652969599, 2.0930421352, 1.6555712223,\n",
            "        1.5990977287, 0.9234925508, 2.2184171677, 1.2122400999, 1.1714292765,\n",
            "        2.7472281456, 1.0356441736, 0.0114077190, 2.2389380932, 3.3518955708,\n",
            "        1.3141171932, 1.4986963272, 1.2942315340, 3.0393419266, 1.5112537146])\n",
            "ctensor.grad: tensor([ 6.3550376892,  6.3550376892,  6.3550376892,  4.5804395676,\n",
            "         0.5398815870, 27.5932617188, 13.8294029236, 31.8354072571,\n",
            "         4.5804395676,  0.5398815870, 27.5932617188, 13.8294029236,\n",
            "        31.8354072571,  4.5804395676,  0.5398815870, 27.5932617188,\n",
            "        13.8294029236, 31.8354072571])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(731.5786132812, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7177518606), tensor(1.5644553900), tensor(0.9692146778), tensor(1.1993832588), tensor(0.7336789966), tensor(0.6318092346), tensor(1.4490619898), tensor(0.9919407964), tensor(1.1267974377), tensor(0.9142989516), tensor(0.8859072328), tensor(0.8841497898), tensor(1.5793730021), tensor(1.0687704086), tensor(1.4421447515), tensor(1.5778348446), tensor(1.3854197264), tensor(1.0060440302), tensor(0.7706450224), tensor(1.0090467930)]\n",
            "b:  [tensor(1.4272509813), tensor(0.6619521976), tensor(1.3050729036), tensor(1.1686608791), tensor(0.9064663053), tensor(1.7424252033), tensor(1.1171492338), tensor(1.2750397921), tensor(1.1235588789), tensor(0.9220797420), tensor(1.2440445423), tensor(1.1291836500), tensor(0.6801695228), tensor(1.1320033073), tensor(1.1276303530), tensor(0.8394630551), tensor(0.9718466401), tensor(1.2160490751), tensor(1.2672586441), tensor(1.3654836416)]\n",
            "c:  [tensor(0.2818718851), tensor(0.2818718851), tensor(0.2818718851), tensor(0.5015338063), tensor(-0.0188813750), tensor(0.1052618623), tensor(-0.1028670669), tensor(0.0323235095), tensor(0.5015338063), tensor(-0.0188813750), tensor(0.1052618623), tensor(-0.1028670669), tensor(0.0323235095), tensor(0.5015338063), tensor(-0.0188813750), tensor(0.1052618623), tensor(-0.1028670669), tensor(0.0323235095)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.6901590824, -0.6818457842,  2.0476086140, -0.9094246626,\n",
            "         4.4854393005,  5.1934518814, -1.0433665514,  1.1088514328,\n",
            "         0.2173298150,  1.7071993351,  1.8093276024,  0.9258245230,\n",
            "        -1.0779147148,  0.9432080984, -0.2260234356, -0.8282515407,\n",
            "        -0.3483981490,  2.0021927357,  3.1805648804,  2.2942662239])\n",
            "btensor.grad: tensor([ 0.2454815507, -1.0193778276,  0.9765328169,  0.8956248760,\n",
            "         0.4806450307,  0.8655520678, -0.0549831390,  1.1222308874,\n",
            "         0.2339727879,  0.2123485208,  1.5326008797,  0.0833666325,\n",
            "        -1.0207024813,  1.0766744614,  1.9937883615, -0.1281728745,\n",
            "         0.2699306011,  0.3921870589,  1.7635729313,  0.6570087075])\n",
            "ctensor.grad: tensor([-3.4860868454, -3.4860868454, -3.4860868454,  4.7466068268,\n",
            "         0.5863770247, 31.3850116730, 15.6024694443, 34.9941139221,\n",
            "         4.7466068268,  0.5863770247, 31.3850116730, 15.6024694443,\n",
            "        34.9941139221,  4.7466068268,  0.5863770247, 31.3850116730,\n",
            "        15.6024694443, 34.9941139221])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(727.1049194336, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7050964832), tensor(1.5689116716), tensor(0.9622759223), tensor(1.2049139738), tensor(0.7159672976), tensor(0.6137461066), tensor(1.4553066492), tensor(0.9893266559), tensor(1.1274499893), tensor(0.9091475010), tensor(0.8810363412), tensor(0.8821083307), tensor(1.5855442286), tensor(1.0669882298), tensor(1.4446778297), tensor(1.5827407837), tensor(1.3883363008), tensor(1.0002743006), tensor(0.7605276108), tensor(1.0021162033)]\n",
            "b:  [tensor(1.4285868406), tensor(0.6719143391), tensor(1.3037236929), tensor(1.1692130566), tensor(0.9088955522), tensor(1.7408003807), tensor(1.1213492155), tensor(1.2740844488), tensor(1.1262559891), tensor(0.9246175289), tensor(1.2416431904), tensor(1.1325019598), tensor(0.6892403364), tensor(1.1315404177), tensor(1.1237421036), tensor(0.8462436795), tensor(0.9755930305), tensor(1.2175816298), tensor(1.2640841007), tensor(1.3655011654)]\n",
            "c:  [tensor(0.2880670130), tensor(0.2880670130), tensor(0.2880670130), tensor(0.4991535544), tensor(-0.0192009080), tensor(0.0873357356), tensor(-0.1116579250), tensor(0.0133253094), tensor(0.4991535544), tensor(-0.0192009080), tensor(0.0873357356), tensor(-0.1116579250), tensor(0.0133253094), tensor(0.4991535544), tensor(-0.0192009080), tensor(0.0873357356), tensor(-0.1116579250), tensor(0.0133253094)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.5310807228, -0.8912621737,  1.3877518177, -1.1061327457,\n",
            "         3.5423388481,  3.6126220226, -1.2489242554,  0.5228267908,\n",
            "        -0.1305001676,  1.0302901268,  0.9741824865,  0.4082932770,\n",
            "        -1.2342494726,  0.3564339876, -0.5066171288, -0.9811998010,\n",
            "        -0.5833259225,  1.1539525986,  2.0234820843,  1.3861221075])\n",
            "btensor.grad: tensor([-0.2671685517, -1.9924302101,  0.2698485851, -0.1104406714,\n",
            "        -0.4858444035,  0.3249704838, -0.8399872184,  0.1910719126,\n",
            "        -0.5394293070, -0.5075554848,  0.4802812338, -0.6636555791,\n",
            "        -1.8141642809,  0.0925884545,  0.7776441574, -1.3561235666,\n",
            "        -0.7492771149, -0.3065123260,  0.6349018812, -0.0035116076])\n",
            "ctensor.grad: tensor([-12.3902559280, -12.3902559280, -12.3902559280,   4.7604780197,\n",
            "          0.6390643716,  35.8522453308,  17.5817127228,  37.9963989258,\n",
            "          4.7604780197,   0.6390643716,  35.8522453308,  17.5817127228,\n",
            "         37.9963989258,   4.7604780197,   0.6390643716,  35.8522453308,\n",
            "         17.5817127228,  37.9963989258])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(721.7768554688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.6991785765), tensor(1.5741714239), tensor(0.9590221047), tensor(1.2110375166), tensor(0.7030946612), tensor(0.6049026847), tensor(1.4623061419), tensor(0.9896766543), tensor(1.1296775341), tensor(0.9073711634), tensor(0.8806502819), tensor(0.8824872375), tensor(1.5921777487), tensor(1.0683095455), tensor(1.4484084845), tensor(1.5880858898), tensor(1.3921861649), tensor(0.9992867112), tensor(0.7570742369), tensor(1.0003706217)]\n",
            "b:  [tensor(1.4324017763), tensor(0.6865461469), tensor(1.3059314489), tensor(1.1748569012), tensor(0.9163265824), tensor(1.7417837381), tensor(1.1294449568), tensor(1.2778619528), tensor(1.1327711344), tensor(0.9307446480), tensor(1.2447081804), tensor(1.1395381689), tensor(0.7022626996), tensor(1.1362318993), tensor(1.1265650988), tensor(0.8589061499), tensor(0.9844680429), tensor(1.2225821018), tensor(1.2669104338), tensor(1.3686841726)]\n",
            "c:  [tensor(0.2986972332), tensor(0.2986972332), tensor(0.2986972332), tensor(0.4972520471), tensor(-0.0195385236), tensor(0.0681505576), tensor(-0.1208936274), tensor(-0.0054654330), tensor(0.4972520471), tensor(-0.0195385236), tensor(0.0681505576), tensor(-0.1208936274), tensor(-0.0054654330), tensor(0.4972520471), tensor(-0.0195385236), tensor(0.0681505576), tensor(-0.1208936274), tensor(-0.0054654330)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1835768223, -1.0519613028,  0.6507617235, -1.2246986628,\n",
            "         2.5745229721,  1.7686877251, -1.3999001980, -0.0699969232,\n",
            "        -0.4455125928,  0.3552652597,  0.0772148371, -0.0757841170,\n",
            "        -1.3266929388, -0.2642538548, -0.7461200953, -1.0690095425,\n",
            "        -0.7699623108,  0.1975214481,  0.6906800270,  0.3491273820])\n",
            "btensor.grad: tensor([-0.7629803419, -2.9263584614, -0.4415453970, -1.1287795305,\n",
            "        -1.4862012863, -0.1966722012, -1.6191587448, -0.7554916143,\n",
            "        -1.3030245304, -1.2254240513, -0.6129904985, -1.4072347879,\n",
            "        -2.6044752598, -0.9383054972, -0.5645990968, -2.5324990749,\n",
            "        -1.7750074863, -1.0000953674, -0.5652737617, -0.6365963221])\n",
            "ctensor.grad: tensor([-21.2604255676, -21.2604255676, -21.2604255676,   3.8030390739,\n",
            "          0.6752300262,  38.3703498840,  18.4713973999,  37.5814819336,\n",
            "          3.8030390739,   0.6752300262,  38.3703498840,  18.4713973999,\n",
            "         37.5814819336,   3.8030390739,   0.6752300262,  38.3703498840,\n",
            "         18.4713973999,  37.5814819336])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(716.0728149414, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7017974257), tensor(1.5804113150), tensor(0.9606713653), tensor(1.2177935839), tensor(0.6961267591), tensor(0.6074606180), tensor(1.4701641798), tensor(0.9935282469), tensor(1.1339007616), tensor(0.9095993042), tensor(0.8856141567), tensor(0.8856928945), tensor(1.5993345976), tensor(1.0735257864), tensor(1.4535826445), tensor(1.5939416885), tensor(1.3972010612), tensor(1.0042803288), tensor(0.7620097399), tensor(1.0050152540)]\n",
            "b:  [tensor(1.4391212463), tensor(0.7057489753), tensor(1.3122187853), tensor(1.1860272884), tensor(0.9293465018), tensor(1.7458318472), tensor(1.1416789293), tensor(1.2867604494), tensor(1.1434328556), tensor(0.9410774708), tensor(1.2537468672), tensor(1.1507312059), tensor(0.7193664312), tensor(1.1467471123), tensor(1.1372457743), tensor(0.8770207167), tensor(0.9987886548), tensor(1.2314860821), tensor(1.2764505148), tensor(1.3753058910)]\n",
            "c:  [tensor(0.3132291734), tensor(0.3132291734), tensor(0.3132291734), tensor(0.4970782697), tensor(-0.0198702589), tensor(0.0515136756), tensor(-0.1287598163), tensor(-0.0196423456), tensor(0.4970782697), tensor(-0.0198702589), tensor(0.0515136756), tensor(-0.1287598163), tensor(-0.0196423456), tensor(0.4970782697), tensor(-0.0198702589), tensor(0.0515136756), tensor(-0.1287598163), tensor(-0.0196423456)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.5237756968, -1.2479755878, -0.3298501372, -1.3512175083,\n",
            "         1.3935794830, -0.5115873814, -1.5716050863, -0.7703206539,\n",
            "        -0.8446561694, -0.4456244707, -0.9927762151, -0.6411316395,\n",
            "        -1.4313775301, -1.0432553291, -1.0348427296, -1.1711596251,\n",
            "        -1.0029809475, -0.9987328053, -0.9871054292, -0.9289267659])\n",
            "btensor.grad: tensor([-1.3439030647, -3.8405694962, -1.2574605942, -2.2340819836,\n",
            "        -2.6039781570, -0.8096252680, -2.4468038082, -1.7796993256,\n",
            "        -2.1323556900, -2.0665662289, -1.8077384233, -2.2385971546,\n",
            "        -3.4207510948, -2.1030344963, -2.1361291409, -3.6229162216,\n",
            "        -2.8641276360, -1.7807978392, -1.9080181122, -1.3243477345])\n",
            "ctensor.grad: tensor([-29.0638771057, -29.0638771057, -29.0638771057,   0.3475750387,\n",
            "          0.6634715796,  33.2737617493,  15.7323884964,  28.3538246155,\n",
            "          0.3475750387,   0.6634715796,  33.2737617493,  15.7323884964,\n",
            "         28.3538246155,   0.3475750387,   0.6634715796,  33.2737617493,\n",
            "         15.7323884964,  28.3538246155])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(711.3737182617, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7124731541), tensor(1.5872646570), tensor(0.9672443867), tensor(1.2247068882), tensor(0.6951351762), tensor(0.6205705404), tensor(1.4784101248), tensor(1.0003800392), tensor(1.1398334503), tensor(0.9154283404), tensor(0.8952598572), tensor(0.8912743926), tensor(1.6066220999), tensor(1.0822263956), tensor(1.4598753452), tensor(1.6000015736), tensor(1.4030821323), tensor(1.0147432089), tensor(0.7747477293), tensor(1.0154958963)]\n",
            "b:  [tensor(1.4481240511), tensor(0.7275022268), tensor(1.3218746185), tensor(1.2014403343), tensor(0.9465749860), tensor(1.7524685860), tensor(1.1567833424), tensor(1.2996557951), tensor(1.1571564674), tensor(0.9547714591), tensor(1.2674474716), tensor(1.1650553942), tensor(0.7387649417), tensor(1.1618638039), tensor(1.1543554068), tensor(0.8982743025), tensor(1.0169559717), tensor(1.2433896065), tensor(1.2913681269), tensor(1.3846210241)]\n",
            "c:  [tensor(0.3295576870), tensor(0.3295576870), tensor(0.3295576870), tensor(0.4996741414), tensor(-0.0201786123), tensor(0.0413392186), tensor(-0.1335320622), tensor(-0.0257971678), tensor(0.4996741414), tensor(-0.0201786123), tensor(0.0413392186), tensor(-0.1335320622), tensor(-0.0257971678), tensor(0.4996741414), tensor(-0.0201786123), tensor(0.0413392186), tensor(-0.1335320622), tensor(-0.0257971678)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-2.1351432800, -1.3706688881, -1.3146046400, -1.3826525211,\n",
            "         0.1983189583, -2.6219873428, -1.6491883993, -1.3703620434,\n",
            "        -1.1865435839, -1.1658099890, -1.9291419983, -1.1163008213,\n",
            "        -1.4575107098, -1.7401115894, -1.2585515976, -1.2119693756,\n",
            "        -1.1762249470, -2.0925867558, -2.5475988388, -2.0961208344])\n",
            "btensor.grad: tensor([-1.8005616665, -4.3506531715, -1.9311634302, -3.0826094151,\n",
            "        -3.4456930161, -1.3273417950, -3.0208706856, -2.5790605545,\n",
            "        -2.7447311878, -2.7388029099, -2.7401225567, -2.8648393154,\n",
            "        -3.8797049522, -3.0233454704, -3.4219377041, -4.2507143021,\n",
            "        -3.6334605217, -2.3807084560, -2.9835124016, -1.8630383015])\n",
            "ctensor.grad: tensor([-32.6570472717, -32.6570472717, -32.6570472717,  -5.1917443275,\n",
            "          0.6167085171,  20.3489093781,   9.5444850922,  12.3096427917,\n",
            "         -5.1917443275,   0.6167085171,  20.3489093781,   9.5444850922,\n",
            "         12.3096427917,  -5.1917443275,   0.6167085171,  20.3489093781,\n",
            "          9.5444850922,  12.3096427917])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(707.7501220703, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(0.7255071402), tensor(1.5930391550), tensor(0.9752736092), tensor(1.2299921513), tensor(0.6963818669), tensor(0.6367154121), tensor(1.4852590561), tensor(1.0073714256), tensor(1.1451154947), tensor(0.9217528105), tensor(0.9056669474), tensor(0.8965191841), tensor(1.6125006676), tensor(1.0911390781), tensor(1.4653993845), tensor(1.6048483849), tensor(1.4080140591), tensor(1.0264714956), tensor(0.7895712256), tensor(1.0276601315)]\n",
            "b:  [tensor(1.4567136765), tensor(0.7477717996), tensor(1.3318192959), tensor(1.2171444893), tensor(0.9637956023), tensor(1.7591236830), tensor(1.1712954044), tensor(1.3130728006), tensor(1.1705375910), tensor(0.9682614207), tensor(1.2818608284), tensor(1.1790595055), tensor(0.7565975189), tensor(1.1775023937), tensor(1.1728080511), tensor(0.9187337756), tensor(1.0347919464), tensor(1.2550827265), tensor(1.3073357344), tensor(1.3938986063)]\n",
            "c:  [tensor(0.3454815149), tensor(0.3454815149), tensor(0.3454815149), tensor(0.5036242604), tensor(-0.0204900634), tensor(0.0342217237), tensor(-0.1367650628), tensor(-0.0282308869), tensor(0.5036242604), tensor(-0.0204900634), tensor(0.0342217237), tensor(-0.1367650628), tensor(-0.0282308869), tensor(0.5036242604), tensor(-0.0204900634), tensor(0.0342217237), tensor(-0.1367650628), tensor(-0.0282308869)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-2.6067962646, -1.1549031734, -1.6058499813, -1.0570580959,\n",
            "        -0.2493419647, -3.2289733887, -1.3697823286, -1.3982706070,\n",
            "        -1.0564079285, -1.2648884058, -2.0814132690, -1.0489531755,\n",
            "        -1.1757249832, -1.7825407982, -1.1048194170, -0.9693654180,\n",
            "        -0.9863898158, -2.3456597328, -2.9646971226, -2.4328572750])\n",
            "btensor.grad: tensor([-1.7179160118, -4.0539169312, -1.9889358282, -3.1408197880,\n",
            "        -3.4441184998, -1.3310261965, -2.9024074078, -2.6834051609,\n",
            "        -2.6762325764, -2.6979970932, -2.8826630116, -2.8008291721,\n",
            "        -3.5665168762, -3.1277089119, -3.6905322075, -4.0918998718,\n",
            "        -3.5671854019, -2.3386292458, -3.1935162544, -1.8555278778])\n",
            "ctensor.grad: tensor([-31.8476276398, -31.8476276398, -31.8476276398,  -7.9002027512,\n",
            "          0.6229004860,  14.2349853516,   6.4660072327,   4.8674364090,\n",
            "         -7.9002027512,   0.6229004860,  14.2349853516,   6.4660072327,\n",
            "          4.8674364090,  -7.9002027512,   0.6229004860,  14.2349853516,\n",
            "          6.4660072327,   4.8674364090])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Post-training score table\n",
            "                 Burnley          Arsenal          Bournemouth      Brighton         Everton          Sheffield United Newcastle        Brentford        Chelsea          Man United       Nott'm Forest    Fulham           Liverpool        Wolves           Tottenham        Man City         Aston Villa      West Ham         Crystal Palace   Luton            \n",
            "Burnley          1.175            0.833            1.160            1.216            0.921            1.403            1.147            1.189            1.098            0.927            1.113            1.072            0.901            1.083            1.160            1.021            1.037            1.093            1.151            1.144            \n",
            "Arsenal          2.611            1.645            2.508            2.414            1.937            3.096            2.320            2.501            2.291            1.948            2.401            2.273            1.691            2.284            2.332            1.948            2.099            2.359            2.466            2.535            \n",
            "Bournemouth      1.615            1.116            1.589            1.608            1.254            1.915            1.531            1.609            1.486            1.265            1.519            1.460            1.176            1.471            1.546            1.335            1.391            1.493            1.569            1.574            \n",
            "Brighton         2.125            1.395            2.059            1.984            1.612            2.487            1.911            2.049            1.888            1.628            1.962            1.875            1.427            1.879            1.922            1.627            1.744            1.932            2.020            2.073            \n",
            "Everton          1.236            0.922            1.242            1.274            1.001            1.441            1.212            1.256            1.173            1.015            1.175            1.149            0.972            1.155            1.228            1.087            1.115            1.159            1.220            1.215            \n",
            "Sheffield United 1.054            0.770            1.047            1.098            0.830            1.242            1.034            1.067            0.988            0.846            0.993            0.969            0.827            0.970            1.050            0.938            0.943            0.978            1.031            1.031            \n",
            "Newcastle        2.461            1.564            2.367            2.278            1.828            2.901            2.188            2.355            2.159            1.846            2.256            2.145            1.607            2.150            2.202            1.848            1.986            2.219            2.321            2.392            \n",
            "Brentford        1.703            1.163            1.670            1.660            1.313            2.003            1.585            1.675            1.548            1.327            1.587            1.526            1.214            1.533            1.601            1.377            1.446            1.562            1.639            1.663            \n",
            "Chelsea          1.917            1.282            1.870            1.842            1.461            2.257            1.762            1.873            1.726            1.478            1.778            1.706            1.330            1.712            1.778            1.517            1.605            1.751            1.836            1.869            \n",
            "Man United       1.567            1.095            1.548            1.559            1.230            1.851            1.487            1.562            1.448            1.238            1.478            1.421            1.150            1.434            1.501            1.298            1.356            1.453            1.526            1.531            \n",
            "Nott'm Forest    1.503            1.036            1.475            1.487            1.156            1.770            1.412            1.485            1.370            1.173            1.399            1.350            1.092            1.353            1.430            1.243            1.288            1.377            1.447            1.467            \n",
            "Fulham           1.523            1.065            1.504            1.516            1.192            1.796            1.444            1.516            1.406            1.202            1.432            1.379            1.120            1.391            1.459            1.265            1.318            1.409            1.480            1.488            \n",
            "Liverpool        2.701            1.692            2.586            2.459            1.999            3.181            2.372            2.569            2.351            2.014            2.474            2.343            1.724            2.348            2.382            1.985            2.152            2.433            2.539            2.625            \n",
            "Wolves           1.818            1.224            1.776            1.759            1.386            2.139            1.679            1.780            1.641            1.405            1.685            1.621            1.274            1.625            1.696            1.454            1.530            1.660            1.742            1.773            \n",
            "Tottenham        2.443            1.560            2.354            2.264            1.824            2.881            2.177            2.344            2.150            1.839            2.247            2.136            1.599            2.142            2.189            1.837            1.976            2.210            2.311            2.377            \n",
            "Man City         2.692            1.685            2.576            2.450            1.994            3.176            2.363            2.559            2.344            2.006            2.468            2.334            1.718            2.342            2.373            1.977            2.143            2.426            2.531            2.615            \n",
            "Aston Villa      2.337            1.504            2.257            2.185            1.752            2.761            2.098            2.252            2.068            1.765            2.155            2.050            1.548            2.059            2.111            1.776            1.904            2.119            2.218            2.273            \n",
            "West Ham         1.678            1.127            1.636            1.638            1.275            1.985            1.557            1.646            1.515            1.290            1.556            1.494            1.186            1.499            1.575            1.355            1.414            1.531            1.608            1.634            \n",
            "Crystal Palace   1.353            0.974            1.346            1.373            1.070            1.588            1.304            1.359            1.263            1.085            1.276            1.239            1.028            1.245            1.321            1.158            1.194            1.257            1.322            1.325            \n",
            "Luton            1.630            1.083            1.583            1.609            1.234            1.951            1.525            1.608            1.476            1.241            1.522            1.450            1.155            1.462            1.540            1.322            1.374            1.493            1.567            1.580            \n",
            "\n",
            "\n",
            "\n",
            "=============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Historic Bookies Estimate:\n",
        "==========================\n",
        "\n",
        "The historic bookies estimate for goals landed by team i against team j is\n",
        "\n",
        "a_i b_j\n",
        "\n",
        "where\n",
        "\n",
        "a_i = A_i C^{-1/2}\n",
        "b_j = B_j C^{-1/2}\n",
        "\n",
        "A_i = goals landed per game by team i\n",
        "B_j = goals conceded per game by team j\n",
        "C   = average goals per game of all teams\n",
        "\n",
        "\n",
        "The historic 1980's max likelihood models\n",
        "=========================================\n",
        "\n",
        "Starting with Maher, the 1980's max likelihood model starts with this guess and\n",
        "perfects it by max likelihood, when team i lands k goals agaist team j\n",
        "loss was minus the log of the predicted probability by Poisson\n",
        "\n",
        "- log ( e^{-a_ib_j}(a_ib_j)^k /k!)\n",
        "\n",
        "\n",
        "A gitgub user said chi squared shows HST,AST,HR,AR have a significant effect\n",
        "\n",
        "HST = home shots on target\n",
        "AST = away shots on target\n",
        "HR  = home red cards\n",
        "AR  = away red cards\n",
        "HS  = home shots\n",
        "AS  = away shots\n",
        "HC  = home quarter kicks\n",
        "AC  = away corner kicks\n",
        "HF  = home fouls\n",
        "AF  = away fouls\n",
        "\n",
        "\n",
        "New Cross-entropy AI model with a neural layer\n",
        "==========================================\n",
        "\n",
        "Since gradient descent generalizes max likelihood we can replace a_ib_j by\n",
        "\n",
        "a_i b_j  +  (c_0 sigma ( c_3 HST + c_4 HR +c_5 HS + c_6 HC _c_7 HF)\n",
        "             + ...\n",
        "             +c_2 sigma(c_13 HST + c_14 HR + c_15 HS + c_16 HCC + c_17 HF) )b_j\n",
        "\n",
        "when i is the home team and\n",
        "\n",
        "a_i b_j  +  c_0 sigma ( c_1 AST + c_2 AR  ..... +AF  ) b_j\n",
        "\n",
        "when i is the away team, with sigma being the sigmoid function\n",
        "\n",
        "\n",
        "   sigma(x)=softmax(0,x) = e^x/(e^0+e^x) = 1/(1+e^{-x})\n",
        "\n",
        "\n",
        "\n",
        "This puts a *neural layer* behind the standard max likelihood model from the 1980s\n",
        "\n",
        "The weights are now the a_i,  b_i,   and c_i\n",
        "\n",
        "Since the c_i are shared by all teams the training rate for the c_i should be lower\n",
        "\n",
        "As in the model which this generalizes, the training is cross entropy versus the Poisson distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A comment about the way the databases are stored, variables like HST and AST refer to 'home'\n",
        "and 'away' team but we do not make a distinction between home versus away.\n",
        "\n",
        "This means that each row of a data table is interpreted as if it were two rows,\n",
        "one giving information about team i against team j, the other giving information\n",
        "about team j against team i.\n",
        "\n",
        "For instance to calculate the average goals scored by any team over all games\n",
        "each row gives goals scored by a home team and goals scored by an away team\n",
        "and we have to add 2 to total games.\n",
        "\n",
        "That is when we say total games it really means the sum over all teams\n",
        "of the number of games that team played in, which is twice the number\n",
        "of games.\n",
        "\n",
        "That explains the line  totalgames=totalgames+2 each time a row is read in.\n",
        "\n",
        "There is no need to change this architecture to include things causing a home\n",
        "team advantage. This starts with a constant taking the value 1 in the first line\n",
        "\n",
        "loss -=  ...\n",
        "\n",
        "and taking the value 0 in the second line\n",
        "\n",
        "loss -= ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import torch as t\n",
        "import torch.nn as n\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def sigma(x):\n",
        "  return t.exp(x)/(t.exp(t.tensor(1.0))+t.exp(x))-t.tensor(0.5)\n",
        "\n",
        "\n",
        "\n",
        "t.set_printoptions(precision=10)\n",
        "#print(\"beep boop\")\n",
        "#print(\"Aston Villa loses\")\n",
        "print(\"=============================================================\")\n",
        "\n",
        "#GITHUB LOCATION:\n",
        "#https://github.com/Pavlos01232/Match_Outcome_Prediction\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0304.csv?raw=true')\n",
        "\n",
        "\n",
        "#\"DEEP learning\" just means \"hidden\" layers\n",
        "\n",
        "\n",
        "#df.to_csv(r'C:\\Users\\Pavlos\\Desktop\\export_dataframe.csv', sep='\\t', encoding='utf-8')\n",
        "#print (df[2])\n",
        "#file_list = os.listdir('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/')\n",
        "#df = pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0405.csv?raw=true',sep='\\t', lineterminator='\\r')\n",
        "#print(df)\n",
        "\n",
        "#read function\n",
        "\n",
        "first = \"https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL\"\n",
        "last = \".csv?raw=true\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Starting with an array of data frames,\n",
        "and an array of column names, make a\n",
        "single array with the chosen columns\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine(dataFrames,columnNames):\n",
        " t=[]\n",
        " for i in range(0,len(dataFrames)):\n",
        "    theseColumns=dataFrames[i].columns.values[0].split(\",\")\n",
        "    for j in range(0 ,len(dataFrames[i])):\n",
        "      row=dataFrames[i].values[j][0].split(\",\")\n",
        "      newEntry=[]\n",
        "      for k in range(0, len(columnNames)):\n",
        "         for m in range(0, len(theseColumns)):\n",
        "             if(columnNames[k]==theseColumns[m] and m<=len(row)):\n",
        "                newEntry.append(row[m])\n",
        "      t.append(newEntry)\n",
        " return t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "can read years 3 to 23, 13*********************, there's something wrong with 14\n",
        "since the first two files in the training data are formatted incorrectly\n",
        "converts csv to dataframe\n",
        "'''\n",
        "\n",
        "df=[]\n",
        "\n",
        "for i in range(23, 24):\n",
        "  result = first + str('{:02.0f}'.format(i)) + str('{:02.0f}'.format(i+1)) + last\n",
        "  x = pd.read_csv(result, sep='\\t', encoding = 'unicode_escape', lineterminator='\\r')\n",
        "  df.append(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get an array with each (home) team listed once\n",
        "from an array of data frames with column \"HomeTeam\"\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getTeams(df):\n",
        " teams=[]\n",
        " homeTeams=combine(df,[\"HomeTeam\"])\n",
        " for i in range(len(homeTeams)):\n",
        "  if(len(homeTeams[i])>0):\n",
        "   found=False\n",
        "   for j in range(len(teams)):\n",
        "    if homeTeams[i][0] == teams[j]:\n",
        "      found=True\n",
        "      break\n",
        "   if found:\n",
        "    continue\n",
        "   teams.append(homeTeams[i][0])\n",
        " return teams\n",
        "\n",
        "\n",
        "'''\n",
        "Get the list of teams from the array of data frames called df\n",
        "and print it to the console\n",
        "'''\n",
        "\n",
        "\n",
        "teams=getTeams(df)\n",
        "print(\"\\nteams:\")\n",
        "print(teams)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Create an array called Data with just the team names and scores\n",
        "from the data framees in the array of frames df, and print it\n",
        "\n",
        "The list [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\"] can be\n",
        "made longer if other columnts may be useful to use\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Data=combine(df, [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\", \"HS\",\"AS\",\"HC\",\"AC\",\"HF\",\"AF\"])\n",
        "print(\"\\n\\ndata: (team names respective goals scored, respective shots on target, respective red cards, respective shots, respective corner-kicks, respective fouls)\")\n",
        "print(Data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get the numerical index of a team name x in the array teams\n",
        "otherwise just return x\n",
        "'''\n",
        "\n",
        "def getIndex(x,teams):\n",
        "  for i in range(len(teams)):\n",
        "   if(teams[i]==x):\n",
        "    return i\n",
        "  return x\n",
        "\n",
        "\n",
        "print(\"\\n\\nIndex assigned to Everton:\")\n",
        "print(getIndex(\"Everton\",teams))\n",
        "\n",
        "\n",
        "'''\n",
        "Replace any occurrence of names from the array teams\n",
        "which occur anywhere in A by their actual  numbers\n",
        "'''\n",
        "\n",
        "def teamsToNumbers(A,teams):\n",
        "  B=[]\n",
        "  for i in range(len(A)):\n",
        "    B.append([])\n",
        "    for j in range(len(A[i])):\n",
        "      B[i].append(getIndex(A[i][j],teams))\n",
        "  return B\n",
        "\n",
        "\n",
        "'''\n",
        "Create Data2 which is a copy of Data but with team names\n",
        "replaced by their index\n",
        "'''\n",
        "\n",
        "print(\"\\n\\ndata2, using the team's index number instead of name\")\n",
        "Data2=teamsToNumbers(Data,teams)\n",
        "#print(Data2)\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "Assume Data2 has team indices in column 0 and 1 and scores\n",
        "in cols 2 and 3\n",
        "\n",
        "A[i] is array of average goals landed per game by tean i\n",
        "B[i] is array of average goals conceded per game by team i\n",
        "C is total games played by all teams (twide the number of games)\n",
        "games[i]=total games played by team i\n",
        "a[i]*b[j]=first approx of expected goals landed by i when playing\n",
        "    against j\n",
        "'''\n",
        "\n",
        "A=[0]*len(teams)\n",
        "B=[0]*len(teams)\n",
        "games=[0]*len(teams)\n",
        "a=[0]*len(teams)\n",
        "b=[0]*len(teams)\n",
        "C=0\n",
        "totalGames=0\n",
        "\n",
        "for i in range(len(Data2)):\n",
        "  if(len(Data2[i])<2):\n",
        "    continue\n",
        "  games[Data2[i][0]]+=1\n",
        "  games[Data2[i][1]]+=1\n",
        "  A[Data2[i][0]]+=int(Data2[i][2])\n",
        "  B[Data2[i][0]]+=int(Data2[i][3])\n",
        "  A[Data2[i][1]]+=int(Data2[i][3])\n",
        "  B[Data2[i][1]]+=int(Data2[i][2])\n",
        "  C+=int(Data2[i][2])+int(Data2[i][3])\n",
        "  totalGames+=2\n",
        "\n",
        "\n",
        "'''\n",
        "Initial estimates of a,b,c\n",
        "'''\n",
        "\n",
        "for i in range(len(A)):\n",
        "  a[i]=A[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "for i in range(len(B)):\n",
        "  b[i]=B[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "#c is another set of hidden weights for our weightrix. they are initally nonzero to avoid a stationary point.\n",
        "\n",
        "c=[0.001,0.002,0.004,-0.001,-0.002,-0.004,0.0005,0.0001,0.01,0.002,0.005,0.007,0.009,0.003,0.007,0.009,0.002,-0.001]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "AI training function\n",
        "\n",
        "The training is by gradient descent, the loss\n",
        "function will be cross entropy loss function against Poisson\n",
        "using loss.backward()\n",
        "\n",
        "The hidden weights at the moment are the entries of a,b,c\n",
        "the array c  is shared for all teams.\n",
        "These enter into the calculation of mu (which we\n",
        "call muHome or muAway during training) and are\n",
        "hidden as they have no direct meaning.\n",
        "\n",
        "Thus mu as a function of the entries of a,b,c is\n",
        "learned by training, the weights are the entries\n",
        "of the three arrays.\n",
        "\n",
        "\n",
        "\n",
        "tau is the training rate for c which should be small\n",
        "compared to eta\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "flag=0   exclude nothing\n",
        "flag=1   exclude entries of exclusions\n",
        "flag=2   exclude entries not in exclusions\n",
        "'''\n",
        "\n",
        "def elementOf(i,A):\n",
        "  for j in range(len(A)):\n",
        "    if(A[j]==i):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def train(eta,tau,flag,exclusions):\n",
        "  loss=t.tensor(0.0)\n",
        "\n",
        "\n",
        "\n",
        "  atensor=t.tensor(a,requires_grad=True)\n",
        "  btensor=t.tensor(b,requires_grad=True)\n",
        "  ctensor=t.tensor(c,requires_grad=True)\n",
        "\n",
        "\n",
        "  for i in range(len(Data2)):\n",
        "    if(len(Data2[i])==0):\n",
        "      continue\n",
        "    if (flag==1 and elementOf(i,exclusions)):\n",
        "      continue\n",
        "    if (flag==2 and not elementOf(i,exclusions)):\n",
        "      continue\n",
        "    homeTeam=int(Data2[i][0])\n",
        "    awayTeam=int(Data2[i][1])\n",
        "    homeGoals=int(Data2[i][2])\n",
        "    awayGoals=int(Data2[i][3])\n",
        "    HST=t.tensor(float(Data2[i][4]))\n",
        "    AST=t.tensor(float(Data2[i][5]))\n",
        "    HR=t.tensor(float(Data2[i][6]))\n",
        "    AR=t.tensor(float(Data2[i][7]))\n",
        "    HS=t.tensor(float(Data2[i][8]))\n",
        "    AS=t.tensor(float(Data2[i][9]))\n",
        "    HC=t.tensor(float(Data2[i][10]))\n",
        "    AC=t.tensor(float(Data2[i][11]))\n",
        "    HF=t.tensor(float(Data2[i][12]))\n",
        "    AF=t.tensor(float(Data2[i][13]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    The reason there are two lines of code for the loss is that\n",
        "    each game can be thought of as  two 'rows' of data where we label\n",
        "    the home team as team i  or team j.\n",
        "\n",
        "    Thus teams are interpreted symmetrically and there is not yet any\n",
        "    home team advantage but this can be put in\n",
        "    without modifying the architecture as a constant which is 1 in the\n",
        "    first line and 0 in the second\n",
        "    '''\n",
        "\n",
        "\n",
        "    muHome=atensor[homeTeam]*btensor[awayTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*HST+ctensor[4]*HR+ctensor[5]*HS+ctensor[6]*HC+ctensor[7]*HF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*HST+ctensor[9]*HR+ctensor[10]*HS+ctensor[11]*HC+ctensor[12]*HF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*HST+ctensor[14]*HR+ctensor[15]*HS+ctensor[16]*HC+ctensor[17]*HF)\n",
        "    muHome=muHome+neural*btensor[awayTeam]\n",
        "\n",
        "    muAway=atensor[awayTeam]*btensor[homeTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*AST+ctensor[4]*AR+ctensor[5]*AS+ctensor[6]*AC+ctensor[7]*AF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*AST+ctensor[9]*AR+ctensor[10]*AS+ctensor[11]*AC+ctensor[12]*AF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*AST+ctensor[14]*AR+ctensor[15]*AS+ctensor[16]*AC+ctensor[17]*AF)\n",
        "    muAway=muAway+neural*btensor[homeTeam]\n",
        "\n",
        "\n",
        "    loss-=t.log(t.exp(-muHome)*t.pow(muHome,homeGoals)/math.factorial(homeGoals))\n",
        "    loss-=t.log(t.exp(-muAway)*t.pow(muAway,awayGoals)/math.factorial(awayGoals))\n",
        "\n",
        "  loss.backward()\n",
        "  for i in range(len(c)):\n",
        "    c[i]=c[i]-tau*ctensor.grad[i]\n",
        "  for i in range(len(a)):\n",
        "    a[i]=a[i]-eta*atensor.grad[i]\n",
        "  for i in range(len(b)):\n",
        "    b[i]=b[i]-eta*btensor.grad[i]\n",
        "  print(\"\\n\\nCross-entropy loss vs Poisson: \"+str(loss))\n",
        "  print(\"\\n\\nWeights:\\n\\na:  \"+str(a))\n",
        "  print(\"b:  \"+str(b))\n",
        "  print(\"c:  \"+str(c))\n",
        "  print(\"\\n\\n\\n\")\n",
        "  print(\"Partial derivatives of loss w/r to hidden weights:\")\n",
        "  print(\"\\natensor.grad: \"+str(atensor.grad))\n",
        "  print(\"btensor.grad: \"+str(btensor.grad))\n",
        "  print(\"ctensor.grad: \"+str(ctensor.grad))\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Use weights to construct predicted expected goals scored by i\n",
        "against j and then find probability of k goals scored using\n",
        "Poisson when given values of R S ST  C F are provided\n",
        "'''\n",
        "\n",
        "# non-tensor version of sigma for using in the field\n",
        "\n",
        "def mathsigma(x):\n",
        "  return math.exp(x)/(math.exp(0)+math.exp(x))-0.5\n",
        "\n",
        "def goalProb(i,j,k,ST,R,S,C,F):\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return math.exp(-mu)*mu**k/math.factorial(k)\n",
        "\n",
        "\n",
        "def goalProb2(i,j,k):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  print(\"\\nNaive prediction of ST, R, S, C, F: \"+str(ST)+\", \"+str(R)+\", \"+str(S)+\", \"+str(C)+\", \"+str(F))\n",
        "  return goalProb(i,j,k,ST,R,S,C,F)\n",
        "\n",
        "\n",
        "'''\n",
        "Pre-training estimates of ST, R, S, C, F\n",
        "'''\n",
        "\n",
        "def preTrain(i,j,u,v):\n",
        "  X=0\n",
        "  Y=0\n",
        "  Z=0\n",
        "  for s in range(len(Data2)):\n",
        "    if(len(Data2[s])<2):\n",
        "      continue\n",
        "    if(Data2[s][0]==i):\n",
        "      X+=int(Data2[s][u])\n",
        "    if(Data2[s][0]==j):\n",
        "      Y+=int(Data2[s][u])\n",
        "    if(Data2[s][1]==i):\n",
        "      X+=int(Data2[s][v])\n",
        "    if(Data2[s][1]==j):\n",
        "      Y+=int(Data2[s][v])\n",
        "    Z+=int(Data2[s][u])+int(Data2[s][v])\n",
        "  return (X/games[i])*(Y/games[j])/(Z/totalGames)\n",
        "\n",
        "def preTrainST(i,j):\n",
        "  return preTrain(i,j,4,5)\n",
        "\n",
        "def preTrainR(i,j):\n",
        "  return preTrain(i,j,6,7)\n",
        "\n",
        "def preTrainS(i,j):\n",
        "  return preTrain(i,j,8,9)\n",
        "\n",
        "def preTrainC(i,j):\n",
        "  return preTrain(i,j,10,11)\n",
        "\n",
        "def preTrainF(i,j):\n",
        "  return preTrain(i,j,12,13)\n",
        "\n",
        "\n",
        "def expectedGoals(i,j):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return mu\n",
        "\n",
        "'''\n",
        "Test training a bit\n",
        "'''\n",
        "print(\"\\n\\n Pre-training score table\")\n",
        "st=\"{:<17}\".format(\"\")\n",
        "for i in range(len(teams)):\n",
        "  st+=\"{:<17}\".format(teams[i])\n",
        "print (st)\n",
        "for i in range(len(teams)):\n",
        "  st=\"{:<17}\".format(teams[i])\n",
        "  for j in range(len(teams)):\n",
        "    st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "  print(st)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "c=[0.]*18\n",
        "train(0,0,2,[0])\n",
        "print(\"In game 0 Everton scored:\"\n",
        "+str(Data2[0][2])\n",
        "+\"against Arsenal\")\n",
        "print(\"\\nThe before-training estimate for the probability that this happened is:\")\n",
        "print(goalProb(0,12,2,0,0,0,0,0))\n",
        "print(\"nln game 0 Arsenal scored:\"\n",
        "+str(Data2[0][3])\n",
        "+\"against Everton\")\n",
        "print(\"\\n The loss so far is the sum of minus the log of both probabilities.\")\n",
        "print(\"The product of the two probabilities is the score actually was 2 against 1\")\n",
        "print(\"Please verify that the cross entropy loss should be minus the log of the probability that the score really was 2 against 1\")\n",
        "print(\"\\nthis should agree with the calculation in the next-to-the-last formula of Maher's paper on page 110\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TRAINING\n",
        "#NUMBER OF ITRATIONS,\n",
        "#TRAINING RATE FOR ETA, TAU.\n",
        "\n",
        "'''\n",
        "flag=0 means use all data\n",
        "flag=1 means exclude games indexed by indicated array\n",
        "flag=1 means use ONLY games indexxed by indicated array\n",
        "\n",
        "'''\n",
        "\n",
        "flag=0\n",
        "for i in range(150):\n",
        "    train(0.005,0.0005,flag, [4,5,6])\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n Post-training score table\")\n",
        "st=\"{:<17}\".format(\"\")\n",
        "for i in range(len(teams)):\n",
        "  st+=\"{:<17}\".format(teams[i])\n",
        "print (st)\n",
        "for i in range(len(teams)):\n",
        "  st=\"{:<17}\".format(teams[i])\n",
        "  for j in range(len(teams)):\n",
        "    st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "  print(st)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "    VALIDATION TESTS\n",
        "\n",
        "With the hidden weights a,b,c still saved, we run it on unseen  data to see\n",
        "what is the loss,  eta=0, tau=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for i in range(20):\n",
        "  print(\"*************************************************\")\n",
        "print(\"\\n Validation:\\n\")\n",
        "print(\"test unseen validation set using only earlier traning\")\n",
        "for i in range(1):\n",
        "    train(0.,0.,2, [4,5,6])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The validation set will be able to reduce its loss\n",
        "if we allow it to train on itself (no longer unseen)\n",
        "but if the reduction is small that is good evidence\n",
        "that the  loss when unseen is still acceptable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#allow validation set to train further on itself\n",
        "\n",
        "for i in range(10):\n",
        "  train(.001,0,2, [4,5,6])\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "print(\"=============================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yb6-hsGw363e"
      }
    }
  ]
}