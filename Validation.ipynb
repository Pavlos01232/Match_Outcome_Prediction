{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavlos01232/Match_Outcome_Prediction/blob/main/Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsKoaXp4LRvc",
        "outputId": "ca2642ac-c5b6-4d4d-9d73-fcd9800cb507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================================\n",
            "\n",
            "teams:\n",
            "['Arsenal', 'Birmingham', 'Blackburn', 'Fulham', 'Leicester', 'Man United', 'Portsmouth', 'Charlton', 'Leeds', 'Liverpool', 'Bolton', 'Chelsea', 'Everton', 'Man City', 'Newcastle', 'Southampton', 'Tottenham', 'Wolves', 'Aston Villa', 'Middlesbrough']\n",
            "381\n",
            "excluded: [137, 141, 144, 150, 153, 163, 191, 191, 201]\n",
            "\n",
            "CAUTION: check that the teams list was not disrupted by exclusions:\n",
            "['Arsenal', 'Birmingham', 'Blackburn', 'Fulham', 'Leicester', 'Man United', 'Portsmouth', 'Charlton', 'Leeds', 'Liverpool', 'Bolton', 'Chelsea', 'Everton', 'Man City', 'Newcastle', 'Southampton', 'Tottenham', 'Wolves', 'Aston Villa', 'Middlesbrough']\n",
            "\n",
            "\n",
            " Pre-training score table\n",
            "                 Arsenal          Birmingham       Blackburn        Fulham           Leicester        Man United       Portsmouth       Charlton         Leeds            Liverpool        Bolton           Chelsea          Everton          Man City         Newcastle        Southampton      Tottenham        Wolves           Aston Villa      Middlesbrough    \n",
            "Arsenal          0.984            1.839            2.265            1.692            2.479            1.338            2.086            1.954            3.033            1.418            2.125            1.150            2.184            2.069            1.456            1.692            2.204            2.833            1.653            1.993            \n",
            "Birmingham       0.580            1.085            1.336            0.998            1.462            0.789            1.230            1.152            1.789            0.836            1.253            0.678            1.288            1.220            0.859            0.998            1.300            1.671            0.975            1.175            \n",
            "Blackburn        0.627            1.172            1.443            1.078            1.579            0.852            1.329            1.245            1.932            0.903            1.354            0.732            1.391            1.318            0.928            1.078            1.404            1.805            1.053            1.269            \n",
            "Fulham           0.707            1.321            1.627            1.216            1.781            0.961            1.498            1.404            2.179            1.018            1.527            0.826            1.569            1.486            1.046            1.216            1.583            2.035            1.187            1.431            \n",
            "Leicester        0.637            1.192            1.468            1.096            1.606            0.867            1.351            1.266            1.965            0.919            1.377            0.745            1.415            1.341            0.943            1.096            1.428            1.836            1.071            1.291            \n",
            "Man United       0.859            1.606            1.978            1.478            2.165            1.168            1.821            1.707            2.649            1.238            1.856            1.004            1.907            1.807            1.272            1.478            1.925            2.474            1.443            1.740            \n",
            "Portsmouth       0.624            1.166            1.436            1.073            1.571            0.848            1.322            1.239            1.923            0.899            1.347            0.729            1.384            1.312            0.923            1.073            1.397            1.796            1.048            1.263            \n",
            "Charlton         0.688            1.286            1.584            1.184            1.734            0.936            1.459            1.367            2.122            0.992            1.486            0.804            1.528            1.447            1.018            1.184            1.541            1.982            1.156            1.394            \n",
            "Leeds            0.527            0.985            1.213            0.906            1.328            0.717            1.117            1.047            1.625            0.759            1.138            0.616            1.170            1.108            0.780            0.906            1.180            1.518            0.885            1.067            \n",
            "Liverpool        0.742            1.387            1.709            1.276            1.870            1.009            1.573            1.474            2.288            1.070            1.603            0.867            1.648            1.561            1.098            1.276            1.662            2.137            1.247            1.503            \n",
            "Bolton           0.638            1.192            1.468            1.096            1.606            0.867            1.351            1.266            1.966            0.919            1.377            0.745            1.415            1.341            0.943            1.096            1.428            1.836            1.071            1.291            \n",
            "Chelsea          0.904            1.690            2.081            1.555            2.278            1.229            1.916            1.796            2.787            1.303            1.953            1.056            2.007            1.901            1.338            1.555            2.025            2.603            1.519            1.831            \n",
            "Everton          0.584            1.092            1.344            1.004            1.472            0.794            1.238            1.160            1.801            0.842            1.261            0.682            1.296            1.228            0.864            1.004            1.308            1.682            0.981            1.183            \n",
            "Man City         0.742            1.387            1.709            1.276            1.870            1.009            1.573            1.474            2.288            1.069            1.603            0.867            1.648            1.561            1.098            1.276            1.662            2.137            1.247            1.503            \n",
            "Newcastle        0.698            1.305            1.607            1.200            1.759            0.949            1.480            1.386            2.152            1.006            1.508            0.815            1.549            1.468            1.033            1.200            1.563            2.010            1.172            1.413            \n",
            "Southampton      0.582            1.088            1.340            1.001            1.467            0.792            1.234            1.156            1.795            0.839            1.257            0.680            1.292            1.224            0.861            1.001            1.304            1.676            0.978            1.179            \n",
            "Tottenham        0.624            1.166            1.436            1.073            1.571            0.848            1.322            1.239            1.923            0.899            1.347            0.729            1.384            1.312            0.923            1.073            1.397            1.796            1.048            1.263            \n",
            "Wolves           0.513            0.959            1.180            0.882            1.292            0.697            1.087            1.018            1.581            0.739            1.108            0.599            1.138            1.078            0.759            0.882            1.149            1.477            0.861            1.038            \n",
            "Aston Villa      0.651            1.218            1.500            1.120            1.641            0.886            1.381            1.294            2.008            0.939            1.407            0.761            1.446            1.370            0.964            1.120            1.459            1.876            1.094            1.319            \n",
            "Middlesbrough    0.594            1.110            1.367            1.021            1.496            0.807            1.259            1.179            1.831            0.856            1.282            0.694            1.318            1.249            0.879            1.021            1.330            1.710            0.997            1.202            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.1118164062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6709330082), tensor(0.9855833650), tensor(1.0651217699), tensor(1.2005378008), tensor(1.0831576586), tensor(1.4590579271), tensor(1.0593695641), tensor(1.1690218449), tensor(0.8958318233), tensor(1.2603852749), tensor(1.0831162930), tensor(1.5352828503), tensor(0.9918068051), tensor(1.2607750893), tensor(1.1854354143), tensor(0.9887589216), tensor(1.0598530769), tensor(0.8716757894), tensor(1.1063210964), tensor(1.0085955858)]\n",
            "b:  [tensor(0.5890264511), tensor(1.1001325846), tensor(1.3547258377), tensor(1.0122984648), tensor(1.4830055237), tensor(0.8007828593), tensor(1.2476278543), tensor(1.1690807343), tensor(1.8144108057), tensor(0.8482682705), tensor(1.2711720467), tensor(0.6880910993), tensor(1.3067215681), tensor(1.2379393578), tensor(0.8711426854), tensor(1.0121316910), tensor(1.3181455135), tensor(1.6947032213), tensor(0.9886969328), tensor(1.1918424368)]\n",
            "c:  [tensor(0.0006175477), tensor(0.0027526792), tensor(0.0045260028), tensor(-0.0009359902), tensor(-0.0020010551), tensor(-0.0039534150), tensor(0.0004909860), tensor(9.2386362667e-05), tensor(0.0101514030), tensor(0.0019977042), tensor(0.0051151915), tensor(0.0069832145), tensor(0.0089936294), tensor(0.0032852963), tensor(0.0069956831), tensor(0.0092156576), tensor(0.0019680320), tensor(-0.0010110710)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0013159513,  0.1090390980, -1.0619471073,  0.1389383078,\n",
            "        -0.5141189694,  1.0078197718, -0.0203450322, -0.0190247297,\n",
            "        -1.2154382467,  0.6287783980, -0.4312559962,  0.9570941925,\n",
            "         0.3964468837, -0.1509771347,  0.2631532550, -0.0469359159,\n",
            "        -0.9873890877, -1.2938888073,  0.2418425083, -0.0718551278])\n",
            "btensor.grad: tensor([-0.9869332314,  0.2287415862,  0.4156872630, -0.0434427857,\n",
            "         0.1952003390, -0.7560182810,  0.1243171990, -0.1368646622,\n",
            "         0.4651151896, -0.3225972652,  0.1185947657, -0.8735825419,\n",
            "        -0.3567301631, -0.3229476213, -0.2276645601,  0.2901619673,\n",
            "         0.3365422487,  0.5435038805,  0.0770481825,  0.1835277081])\n",
            "ctensor.grad: tensor([ 0.7649046183, -1.5053580999, -1.0520049334, -0.1280196607,\n",
            "         0.0021100282, -0.0931706205,  0.0180280395,  0.0152272647,\n",
            "        -0.3028057218,  0.0045918403, -0.2303827852,  0.0335717201,\n",
            "         0.0127398307, -0.5705927014,  0.0086340727, -0.4313150644,\n",
            "         0.0639361590,  0.0221418776])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.1027832031, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6704405546), tensor(0.9855325818), tensor(1.0656441450), tensor(1.2004716396), tensor(1.0834118128), tensor(1.4585633278), tensor(1.0593817234), tensor(1.1690334082), tensor(0.8964279294), tensor(1.2600781918), tensor(1.0833295584), tensor(1.5348130465), tensor(0.9916154742), tensor(1.2608511448), tensor(1.1853080988), tensor(0.9887840748), tensor(1.0603388548), tensor(0.8723095655), tensor(1.1062045097), tensor(1.0086330175)]\n",
            "b:  [tensor(0.5895038247), tensor(1.1000225544), tensor(1.3545241356), tensor(1.0123215914), tensor(1.4829115868), tensor(0.8011524677), tensor(1.2475688457), tensor(1.1691498756), tensor(1.8141834736), tensor(0.8484274149), tensor(1.2711163759), tensor(0.6885160208), tensor(1.3068985939), tensor(1.2381004095), tensor(0.8712558746), tensor(1.0119917393), tensor(1.3179826736), tensor(1.6944377422), tensor(0.9886612296), tensor(1.1917544603)]\n",
            "c:  [tensor(0.0002371759), tensor(0.0035210110), tensor(0.0050811702), tensor(-0.0008964083), tensor(-0.0020017065), tensor(-0.0039245724), tensor(0.0004854462), tensor(8.7726461061e-05), tensor(0.0103601562), tensor(0.0019945439), tensor(0.0052742213), tensor(0.0069602686), tensor(0.0089851487), tensor(0.0036091628), tensor(0.0069907978), tensor(0.0094609717), tensor(0.0019322231), tensor(-0.0010227578)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9850000143,  0.1015684903, -1.0448001623,  0.1322108507,\n",
            "        -0.5083445907,  0.9892809391, -0.0243722200, -0.0232278109,\n",
            "        -1.1922364235,  0.6141474843, -0.4266042113,  0.9396738410,\n",
            "         0.3826368451, -0.1521935463,  0.2546274066, -0.0502510071,\n",
            "        -0.9714962244, -1.2675226927,  0.2332085371, -0.0749291778])\n",
            "btensor.grad: tensor([-0.9547532797,  0.2200677395,  0.4033325315, -0.0462604165,\n",
            "         0.1877907813, -0.7392326593,  0.1181134880, -0.1383063793,\n",
            "         0.4546557069, -0.3183014393,  0.1113611460, -0.8498744369,\n",
            "        -0.3540348411, -0.3221771717, -0.2263329178,  0.2799330950,\n",
            "         0.3256517053,  0.5308840275,  0.0714056492,  0.1760041714])\n",
            "ctensor.grad: tensor([ 7.6074355841e-01, -1.5366635323e+00, -1.1103345156e+00,\n",
            "        -7.9163759947e-02,  1.3029429829e-03, -5.7685099542e-02,\n",
            "         1.1079593562e-02,  9.3197971582e-03, -4.1750568151e-01,\n",
            "         6.3202078454e-03, -3.1805947423e-01,  4.5892223716e-02,\n",
            "         1.6961079091e-02, -6.4773315191e-01,  9.7705814987e-03,\n",
            "        -4.9062830210e-01,  7.1617849171e-02,  2.3373723030e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0938720703, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6699562073), tensor(0.9854856133), tensor(1.0661582947), tensor(1.2004089355), tensor(1.0836632252), tensor(1.4580779076), tensor(1.0593960285), tensor(1.1690472364), tensor(0.8970128894), tensor(1.2597783804), tensor(1.0835406780), tensor(1.5343518257), tensor(0.9914310575), tensor(1.2609279156), tensor(1.1851850748), tensor(0.9888109565), tensor(1.0608168840), tensor(0.8729306459), tensor(1.1060922146), tensor(1.0086721182)]\n",
            "b:  [tensor(0.5899658799), tensor(1.0999169350), tensor(1.3543286324), tensor(1.0123462677), tensor(1.4828214645), tensor(0.8015140295), tensor(1.2475129366), tensor(1.1692198515), tensor(1.8139613867), tensor(0.8485845923), tensor(1.2710644007), tensor(0.6889296174), tensor(1.3070744276), tensor(1.2382612228), tensor(0.8713685274), tensor(1.0118569136), tensor(1.3178253174), tensor(1.6941785812), tensor(0.9886283875), tensor(1.1916702986)]\n",
            "c:  [tensor(-0.0001440614), tensor(0.0043116296), tensor(0.0056696227), tensor(-0.0008811896), tensor(-0.0020019568), tensor(-0.0039134705), tensor(0.0004833282), tensor(8.5950625362e-05), tensor(0.0106277727), tensor(0.0019905015), tensor(0.0054784035), tensor(0.0069311513), tensor(0.0089747794), tensor(0.0039740768), tensor(0.0069853128), tensor(0.0097379955), tensor(0.0018924683), tensor(-0.0010348058)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9687896967,  0.0939787030, -1.0282356739,  0.1253823042,\n",
            "        -0.5029408932,  0.9708632231, -0.0285561979, -0.0275894403,\n",
            "        -1.1698869467,  0.5996037126, -0.4222519398,  0.9223560095,\n",
            "         0.3688921928, -0.1535487175,  0.2460777760, -0.0537302494,\n",
            "        -0.9561660290, -1.2421441078,  0.2245180011, -0.0781738758])\n",
            "btensor.grad: tensor([-0.9240638018,  0.2113382816,  0.3910022378, -0.0492334366,\n",
            "         0.1802945435, -0.7231596708,  0.1118082404, -0.1399017572,\n",
            "         0.4441915751, -0.3143783212,  0.1040444374, -0.8272282481,\n",
            "        -0.3515959680, -0.3216681480, -0.2252553552,  0.2697426081,\n",
            "         0.3147901893,  0.5182709694,  0.0656664371,  0.1684188843])\n",
            "ctensor.grad: tensor([ 7.6247471571e-01, -1.5812368393e+00, -1.1769050360e+00,\n",
            "        -3.0437400565e-02,  5.0036126049e-04, -2.2204123437e-02,\n",
            "         4.2359735817e-03,  3.5516689532e-03, -5.3523349762e-01,\n",
            "         8.0848596990e-03, -4.0836483240e-01,  5.8234415948e-02,\n",
            "         2.0738407969e-02, -7.2982829809e-01,  1.0970435105e-02,\n",
            "        -5.5404686928e-01,  7.9509504139e-02,  2.4095894769e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0847167969, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6694798470), tensor(0.9854424596), tensor(1.0666644573), tensor(1.2003496885), tensor(1.0839121342), tensor(1.4576016665), tensor(1.0594124794), tensor(1.1690633297), tensor(0.8975870609), tensor(1.2594858408), tensor(1.0837497711), tensor(1.5338993073), tensor(0.9912534356), tensor(1.2610054016), tensor(1.1850663424), tensor(0.9888396263), tensor(1.0612876415), tensor(0.8735395074), tensor(1.1059843302), tensor(1.0087128878)]\n",
            "b:  [tensor(0.5904132724), tensor(1.0998156071), tensor(1.3541393280), tensor(1.0123724937), tensor(1.4827351570), tensor(0.8018679023), tensor(1.2474602461), tensor(1.1692906618), tensor(1.8137445450), tensor(0.8487400413), tensor(1.2710161209), tensor(0.6893324256), tensor(1.3072491884), tensor(1.2384219170), tensor(0.8714807630), tensor(1.0117270947), tensor(1.3176733255), tensor(1.6939257383), tensor(0.9885984659), tensor(1.1915899515)]\n",
            "c:  [tensor(-0.0005291076), tensor(0.0051314156), tensor(0.0062958375), tensor(-0.0008904416), tensor(-0.0020018048), tensor(-0.0039202268), tensor(0.0004846092), tensor(8.7022213847e-05), tensor(0.0109563628), tensor(0.0019855509), tensor(0.0057295426), tensor(0.0068958225), tensor(0.0089628054), tensor(0.0043828879), tensor(0.0069791917), tensor(0.0100490954), tensor(0.0018486525), tensor(-0.0010469028)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9526791573,  0.0862664580, -1.0122476816,  0.1184511185,\n",
            "        -0.4979213476,  0.9525669813, -0.0328958333, -0.0321121216,\n",
            "        -1.1483775377,  0.5851577520, -0.4181947708,  0.9051393270,\n",
            "         0.3552100658, -0.1550379992,  0.2375060916, -0.0573682785,\n",
            "        -0.9414000511, -1.2177346945,  0.2157661915, -0.0815815330])\n",
            "btensor.grad: tensor([-0.8948228359,  0.2025454044,  0.3786880970, -0.0523714423,\n",
            "         0.1727072150, -0.7077962160,  0.1053844094, -0.1416453123,\n",
            "         0.4337243438, -0.3108416796,  0.0966304541, -0.8056324720,\n",
            "        -0.3494164944, -0.3214337826, -0.2244429290,  0.2595941424,\n",
            "         0.3039576411,  0.5056568384,  0.0598230362,  0.1607674360])\n",
            "ctensor.grad: tensor([ 7.7009230852e-01, -1.6395717859e+00, -1.2524299622e+00,\n",
            "         1.8504055217e-02, -3.0388569576e-04,  1.3512500562e-02,\n",
            "        -2.5620525703e-03, -2.1431786008e-03, -6.5717995167e-01,\n",
            "         9.9012125283e-03, -5.0227820873e-01,  7.0657871664e-02,\n",
            "         2.3947553709e-02, -8.1762248278e-01,  1.2242657132e-02,\n",
            "        -6.2220072746e-01,  8.7631598115e-02,  2.4194050580e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0751953125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6690114737), tensor(0.9854032397), tensor(1.0671628714), tensor(1.2002940178), tensor(1.0841587782), tensor(1.4571344852), tensor(1.0594311953), tensor(1.1690816879), tensor(0.8981509209), tensor(1.2592004538), tensor(1.0839569569), tensor(1.5334552526), tensor(0.9910826683), tensor(1.2610837221), tensor(1.1849519014), tensor(0.9888702035), tensor(1.0617512465), tensor(0.8741366267), tensor(1.1058808565), tensor(1.0087554455)]\n",
            "b:  [tensor(0.5908467770), tensor(1.0997188091), tensor(1.3539561033), tensor(1.0124003887), tensor(1.4826526642), tensor(0.8022144437), tensor(1.2474107742), tensor(1.1693624258), tensor(1.8135329485), tensor(0.8488938808), tensor(1.2709715366), tensor(0.6897249818), tensor(1.3074229956), tensor(1.2385826111), tensor(0.8715927005), tensor(1.0116024017), tensor(1.3175266981), tensor(1.6936792135), tensor(0.9885715246), tensor(1.1915134192)]\n",
            "c:  [tensor(-0.0009209278), tensor(0.0059875902), tensor(0.0069646970), tensor(-0.0009244445), tensor(-0.0020012469), tensor(-0.0039450801), tensor(0.0004892957), tensor(9.0937071946e-05), tensor(0.0113486610), tensor(0.0019796581), tensor(0.0060299607), tensor(0.0068542124), tensor(0.0089495815), tensor(0.0048388601), tensor(0.0069723930), tensor(0.0103969919), tensor(0.0018006512), tensor(-0.0010586679)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9366667271,  0.0784189701, -0.9968258739,  0.1114042997,\n",
            "        -0.4932880998,  0.9343901873, -0.0373974144, -0.0368120670,\n",
            "        -1.1277053356,  0.5708098412, -0.4144193530,  0.8880162835,\n",
            "         0.3415790796, -0.1566534042,  0.2289121151, -0.0611624718,\n",
            "        -0.9271847010, -1.1942887306,  0.2069588900, -0.0851545334])\n",
            "btensor.grad: tensor([-0.8670086861,  0.1936719418,  0.3663779497, -0.0556823015,\n",
            "         0.1650219113, -0.6931302547,  0.0988432765, -0.1435385942,\n",
            "         0.4232622981, -0.3076964617,  0.0891127586, -0.7850795984,\n",
            "        -0.3474967778, -0.3214932680, -0.2239028066,  0.2494802475,\n",
            "         0.2931452990,  0.4930394888,  0.0538632870,  0.1530437469])\n",
            "ctensor.grad: tensor([ 7.8364026546e-01, -1.7123492956e+00, -1.3377192020e+00,\n",
            "         6.8005621433e-02, -1.1159558780e-03,  4.9706250429e-02,\n",
            "        -9.3729421496e-03, -7.8297173604e-03, -7.8459709883e-01,\n",
            "         1.1785316281e-02, -6.0083639622e-01,  8.3219870925e-02,\n",
            "         2.6447199285e-02, -9.1194486618e-01,  1.3597104698e-02,\n",
            "        -6.9579207897e-01,  9.6002496779e-02,  2.3530084640e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0665283203, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6685510874), tensor(0.9853680134), tensor(1.0676538944), tensor(1.2002419233), tensor(1.0844032764), tensor(1.4566763639), tensor(1.0594521761), tensor(1.1691025496), tensor(0.8987048268), tensor(1.2589222193), tensor(1.0841623545), tensor(1.5330197811), tensor(0.9909186959), tensor(1.2611628771), tensor(1.1848417521), tensor(0.9889027476), tensor(1.0622080564), tensor(0.8747225404), tensor(1.1057817936), tensor(1.0087999105)]\n",
            "b:  [tensor(0.5912670493), tensor(1.0996264219), tensor(1.3537790775), tensor(1.0124299526), tensor(1.4825741053), tensor(0.8025540113), tensor(1.2473646402), tensor(1.1694352627), tensor(1.8133265972), tensor(0.8490463495), tensor(1.2709307671), tensor(0.6901077628), tensor(1.3075959682), tensor(1.2387435436), tensor(0.8717045188), tensor(1.0114827156), tensor(1.3173855543), tensor(1.6934390068), tensor(0.9885476232), tensor(1.1914408207)]\n",
            "c:  [tensor(-0.0013225076), tensor(0.0068878108), tensor(0.0076815672), tensor(-0.0009836517), tensor(-0.0020002758), tensor(-0.0039883903), tensor(0.0004974233), tensor(9.7723721410e-05), tensor(0.0118080704), tensor(0.0019727810), tensor(0.0063825357), tensor(0.0068062241), tensor(0.0089355446), tensor(0.0053457147), tensor(0.0069648707), tensor(0.0107847936), tensor(0.0017483312), tensor(-0.0010696402)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9207571745,  0.0704298019, -0.9819628596,  0.1042392254,\n",
            "        -0.4890445173,  0.9163289070, -0.0420648158, -0.0416966677,\n",
            "        -1.1078510284,  0.5565775633, -0.4109136462,  0.8709882498,\n",
            "         0.3280028105, -0.1583758593,  0.2203028202, -0.0650997162,\n",
            "        -0.9135079384, -1.1717866659,  0.1980953217, -0.0888761282])\n",
            "btensor.grad: tensor([-0.8405805826,  0.1847239137,  0.3540645838, -0.0591655970,\n",
            "         0.1572343707, -0.6791599989,  0.0921788514, -0.1455644369,\n",
            "         0.4128160477, -0.3049615026,  0.0814720392, -0.7655557394,\n",
            "        -0.3458366990, -0.3218601942, -0.2236418873,  0.2394186854,\n",
            "         0.2823654115,  0.4804188013,  0.0477855206,  0.1452496052])\n",
            "ctensor.grad: tensor([ 0.8031597137, -1.8004407883, -1.4337401390,  0.1184145585,\n",
            "        -0.0019420476,  0.0866202563, -0.0162553005, -0.0135732926,\n",
            "        -0.9188188314,  0.0137540977, -0.7051501274,  0.0959766582,\n",
            "         0.0280743353, -1.0137095451,  0.0150445914, -0.7756042480,\n",
            "         0.1046399400,  0.0219445527])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0559082031, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6680985689), tensor(0.9853368402), tensor(1.0681377649), tensor(1.2001934052), tensor(1.0846458673), tensor(1.4562271833), tensor(1.0594756603), tensor(1.1691259146), tensor(0.8992491961), tensor(1.2586510181), tensor(1.0843662024), tensor(1.5325927734), tensor(0.9907614589), tensor(1.2612429857), tensor(1.1847358942), tensor(0.9889373183), tensor(1.0626581907), tensor(0.8752976656), tensor(1.1056872606), tensor(1.0088462830)]\n",
            "b:  [tensor(0.5916748047), tensor(1.0995385647), tensor(1.3536082506), tensor(1.0124614239), tensor(1.4824994802), tensor(0.8028869629), tensor(1.2473219633), tensor(1.1695091724), tensor(1.8131253719), tensor(0.8491976857), tensor(1.2708939314), tensor(0.6904813051), tensor(1.3077682257), tensor(1.2389048338), tensor(0.8718163371), tensor(1.0113680363), tensor(1.3172497749), tensor(1.6932051182), tensor(0.9885268211), tensor(1.1913721561)]\n",
            "c:  [tensor(-0.0017368498), tensor(0.0078402888), tensor(0.0084523838), tensor(-0.0010686910), tensor(-0.0019988816), tensor(-0.0040506390), tensor(0.0005090575), tensor(0.0001074444), tensor(0.0123387109), tensor(0.0019648683), tensor(0.0067907441), tensor(0.0067517320), tensor(0.0089212228), tensor(0.0059076883), tensor(0.0069565722), tensor(0.0112160509), tensor(0.0016915512), tensor(-0.0010792639)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9049566984,  0.0622987449, -0.9676384330,  0.0969536304,\n",
            "        -0.4851942956,  0.8983972073, -0.0468848348, -0.0467656851,\n",
            "        -1.0887968540,  0.5424773097, -0.4076535702,  0.8540594578,\n",
            "         0.3144846559, -0.1601927280,  0.2116950750, -0.0691576004,\n",
            "        -0.9003527164, -1.1502035856,  0.1891815066, -0.0927379727])\n",
            "btensor.grad: tensor([-0.8155171871,  0.1756833196,  0.3417499661, -0.0628255606,\n",
            "         0.1493519545, -0.6658732891,  0.0853895247, -0.1477108002,\n",
            "         0.4024060965, -0.3026401997,  0.0737046003, -0.7470598817,\n",
            "        -0.3444325030, -0.3225445747, -0.2236640453,  0.2294125557,\n",
            "         0.2716265917,  0.4678008556,  0.0415887833,  0.1373969316])\n",
            "ctensor.grad: tensor([ 0.8286841512, -1.9049557447, -1.5416338444,  0.1700786799,\n",
            "        -0.0027883998,  0.1244970262, -0.0232683420, -0.0194413438,\n",
            "        -1.0612818003,  0.0158256423, -0.8164166808,  0.1089840457,\n",
            "         0.0286437999, -1.1239467859,  0.0165970791, -0.8625147939,\n",
            "         0.1135601476,  0.0192475058])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0460205078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6676539183), tensor(0.9853098392), tensor(1.0686147213), tensor(1.2001485825), tensor(1.0848867893), tensor(1.4557868242), tensor(1.0595015287), tensor(1.1691519022), tensor(0.8997844458), tensor(1.2583867311), tensor(1.0845685005), tensor(1.5321741104), tensor(0.9906109571), tensor(1.2613240480), tensor(1.1846343279), tensor(0.9889739752), tensor(1.0631020069), tensor(0.8758624196), tensor(1.1055971384), tensor(1.0088946819)]\n",
            "b:  [tensor(0.5920706987), tensor(1.0994552374), tensor(1.3534435034), tensor(1.0124948025), tensor(1.4824287891), tensor(0.8032135963), tensor(1.2472827435), tensor(1.1695841551), tensor(1.8129293919), tensor(0.8493480682), tensor(1.2708610296), tensor(0.6908460855), tensor(1.3079398870), tensor(1.2390666008), tensor(0.8719283342), tensor(1.0112582445), tensor(1.3171193600), tensor(1.6929775476), tensor(0.9885091782), tensor(1.1913074255)]\n",
            "c:  [tensor(-0.0021669995), tensor(0.0088539040), tensor(0.0092837298), tensor(-0.0011803623), tensor(-0.0019970508), tensor(-0.0041324268), tensor(0.0005242938), tensor(0.0001201968), tensor(0.0129454890), tensor(0.0019558584), tensor(0.0072587188), tensor(0.0066905823), tensor(0.0089072529), tensor(0.0065295948), tensor(0.0069474382), tensor(0.0116948113), tensor(0.0016301618), tensor(-0.0010868735)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8892735243,  0.0540290773, -0.9538320899,  0.0895469189,\n",
            "        -0.4817347527,  0.8806005716, -0.0518409610, -0.0520249605,\n",
            "        -1.0705212355,  0.5285366774, -0.4046121836,  0.8372336626,\n",
            "         0.3010320067, -0.1620669365,  0.2031043768, -0.0733160973,\n",
            "        -0.8877024651, -1.1295135021,  0.1802386642, -0.0967145562])\n",
            "btensor.grad: tensor([-0.7917820215,  0.1665564179,  0.3294332027, -0.0666520000,\n",
            "         0.1413761526, -0.6532685757,  0.0784859359, -0.1499518156,\n",
            "         0.3920561075, -0.3007403612,  0.0658136606, -0.7295820713,\n",
            "        -0.3432670534, -0.3235601187, -0.2239715606,  0.2194762230,\n",
            "         0.2609472871,  0.4551944733,  0.0352737904,  0.1294883490])\n",
            "ctensor.grad: tensor([ 0.8602992892, -2.0272302628, -1.6626918316,  0.2233425826,\n",
            "        -0.0036612824,  0.1635759622, -0.0304725748, -0.0255048349,\n",
            "        -1.2135561705,  0.0180194918, -0.9359492064,  0.1222990528,\n",
            "         0.0279400460, -1.2438133955,  0.0182679184, -0.9575201273,\n",
            "         0.1227787882,  0.0152191315])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0346679688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6672170162), tensor(0.9852870107), tensor(1.0690850019), tensor(1.2001075745), tensor(1.0851261616), tensor(1.4553552866), tensor(1.0595300198), tensor(1.1691806316), tensor(0.9003109336), tensor(1.2581293583), tensor(1.0847693682), tensor(1.5317637920), tensor(0.9904671311), tensor(1.2614060640), tensor(1.1845370531), tensor(0.9890127182), tensor(1.0635397434), tensor(0.8764172792), tensor(1.1055115461), tensor(1.0089451075)]\n",
            "b:  [tensor(0.5924553871), tensor(1.0993765593), tensor(1.3532849550), tensor(1.0125300884), tensor(1.4823621511), tensor(0.8035342693), tensor(1.2472469807), tensor(1.1696603298), tensor(1.8127385378), tensor(0.8494976759), tensor(1.2708321810), tensor(0.6912026405), tensor(1.3081110716), tensor(1.2392290831), tensor(0.8720406294), tensor(1.0111534595), tensor(1.3169941902), tensor(1.6927562952), tensor(0.9884947538), tensor(1.1912466288)]\n",
            "c:  [tensor(-0.0026160153), tensor(0.0099383378), tensor(0.0101829534), tensor(-0.0013196378), tensor(-0.0019947672), tensor(-0.0042344732), tensor(0.0005432593), tensor(0.0001361171), tensor(0.0136341667), tensor(0.0019456799), tensor(0.0077913115), tensor(0.0066225915), tensor(0.0088943932), tensor(0.0072169024), tensor(0.0069374023), tensor(0.0122256819), tensor(0.0015640059), tensor(-0.0010916746)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8737291098,  0.0456339419, -0.9405160546,  0.0820255280,\n",
            "        -0.4786553383,  0.8629570007, -0.0569283068, -0.0574654341,\n",
            "        -1.0529966354,  0.5147905350, -0.4017498493,  0.8205206394,\n",
            "         0.2876642346, -0.1639684439,  0.1945595145, -0.0775370598,\n",
            "        -0.8755189180, -1.1096919775,  0.1712793112, -0.1007681489])\n",
            "btensor.grad: tensor([-0.7693493366,  0.1573473215,  0.3171213865, -0.0706344247,\n",
            "         0.1333207786, -0.6413282156,  0.0714736283, -0.1522604227,\n",
            "         0.3818071485, -0.2992656827,  0.0577913523, -0.7131088972,\n",
            "        -0.3423287868, -0.3249177933, -0.2245630622,  0.2096438408,\n",
            "         0.2503513992,  0.4426190853,  0.0288480520,  0.1215436459])\n",
            "ctensor.grad: tensor([ 0.8980317116, -2.1688680649, -1.7984467745,  0.2785507739,\n",
            "        -0.0045670564,  0.2040928602, -0.0379309915, -0.0318405293,\n",
            "        -1.3773552179,  0.0203569941, -1.0651848316,  0.1359814852,\n",
            "         0.0257193763, -1.3746154308,  0.0200720448, -1.0617412329,\n",
            "         0.1323119253,  0.0096022664])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0211181641, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6667878628), tensor(0.9852684736), tensor(1.0695488453), tensor(1.2000703812), tensor(1.0853641033), tensor(1.4549325705), tensor(1.0595610142), tensor(1.1692122221), tensor(0.9008290172), tensor(1.2578786612), tensor(1.0849689245), tensor(1.5313618183), tensor(0.9903299212), tensor(1.2614890337), tensor(1.1844439507), tensor(0.9890536070), tensor(1.0639716387), tensor(0.8769626021), tensor(1.1054303646), tensor(1.0089975595)]\n",
            "b:  [tensor(0.5928294659), tensor(1.0993025303), tensor(1.3531324863), tensor(1.0125675201), tensor(1.4822995663), tensor(0.8038492799), tensor(1.2472147942), tensor(1.1697375774), tensor(1.8125526905), tensor(0.8496468067), tensor(1.2708073854), tensor(0.6915514469), tensor(1.3082818985), tensor(1.2393923998), tensor(0.8721533418), tensor(1.0110534430), tensor(1.3168742657), tensor(1.6925412416), tensor(0.9884836078), tensor(1.1911898851)]\n",
            "c:  [tensor(-0.0030869674), tensor(0.0111042392), tensor(0.0111582959), tensor(-0.0014876577), tensor(-0.0019920112), tensor(-0.0043576094), tensor(0.0005661143), tensor(0.0001553840), tensor(0.0144114587), tensor(0.0019342491), tensor(0.0083941696), tensor(0.0065475414), tensor(0.0088835387), tensor(0.0079758139), tensor(0.0069263889), tensor(0.0128139015), tensor(0.0014929161), tensor(-0.0010927267)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8583440781,  0.0371308029, -0.9276505113,  0.0744117498,\n",
            "        -0.4759443402,  0.8454935551, -0.0620958209, -0.0630781651,\n",
            "        -1.0361806154,  0.5012897849, -0.3990135789,  0.8039429188,\n",
            "         0.2744175792, -0.1658439636,  0.1861056089, -0.0817594528,\n",
            "        -0.8637573719, -1.0906825066,  0.1623440981, -0.1048508883])\n",
            "btensor.grad: tensor([-0.7481904030,  0.1480703354,  0.3048346639, -0.0747615099,\n",
            "         0.1252151877, -0.6300339699,  0.0643723309, -0.1545859575,\n",
            "         0.3717113733, -0.2982251048,  0.0496520996, -0.6976392269,\n",
            "        -0.3415847123, -0.3266158104, -0.2254287302,  0.1999560595,\n",
            "         0.2398805022,  0.4301050901,  0.0223306417,  0.1135959625])\n",
            "ctensor.grad: tensor([ 9.4190424681e-01, -2.3318026066e+00, -1.9506850243e+00,\n",
            "         3.3603981137e-01, -5.5121136829e-03,  2.4627262354e-01,\n",
            "        -4.5709844679e-02, -3.8533911109e-02, -1.5545830727e+00,\n",
            "         2.2861730307e-02, -1.2057170868e+00,  1.5010009706e-01,\n",
            "         2.1708928049e-02, -1.5178234577e+00,  2.2026330233e-02,\n",
            "        -1.1764401197e+00,  1.4217950404e-01,  2.1043438464e-03])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0076904297, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6663663387), tensor(0.9852541685), tensor(1.0700064898), tensor(1.2000370026), tensor(1.0856008530), tensor(1.4545184374), tensor(1.0595946312), tensor(1.1692466736), tensor(0.9013390541), tensor(1.2576346397), tensor(1.0851670504), tensor(1.5309680700), tensor(0.9901992679), tensor(1.2615728378), tensor(1.1843550205), tensor(0.9890965819), tensor(1.0643978119), tensor(0.8774988055), tensor(1.1053535938), tensor(1.0090520382)]\n",
            "b:  [tensor(0.5931935906), tensor(1.0992331505), tensor(1.3529862165), tensor(1.0126069784), tensor(1.4822410345), tensor(0.8041589856), tensor(1.2471861839), tensor(1.1698160172), tensor(1.8123717308), tensor(0.8497955799), tensor(1.2707866430), tensor(0.6918930411), tensor(1.3084523678), tensor(1.2395566702), tensor(0.8722665906), tensor(1.0109581947), tensor(1.3167594671), tensor(1.6923323870), tensor(0.9884757400), tensor(1.1911370754)]\n",
            "c:  [tensor(-0.0035829130), tensor(0.0123633835), tensor(0.0122190351), tensor(-0.0016857252), tensor(-0.0019887597), tensor(-0.0045027728), tensor(0.0005930543), tensor(0.0001782248), tensor(0.0152851297), tensor(0.0019214691), tensor(0.0090738172), tensor(0.0064651733), tensor(0.0088757342), tensor(0.0088133719), tensor(0.0069143139), tensor(0.0134654278), tensor(0.0014167125), tensor(-0.0010889237)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8431565762,  0.0285560191, -0.9151825309,  0.0667246580,\n",
            "        -0.4735822380,  0.8282560110, -0.0673139095, -0.0688372850,\n",
            "        -1.0200219154,  0.4881055653, -0.3963305950,  0.7875343561,\n",
            "         0.2613368034, -0.1676219702,  0.1777937412, -0.0859229565,\n",
            "        -0.8523621559, -1.0724346638,  0.1534779072, -0.1088987589])\n",
            "btensor.grad: tensor([-0.7282733917,  0.1387487650,  0.2925991416, -0.0789954066,\n",
            "         0.1170932651, -0.6193579435,  0.0572169423, -0.1568707228,\n",
            "         0.3618383408, -0.2976001501,  0.0414093733, -0.6831527948,\n",
            "        -0.3410001695, -0.3286561966, -0.2265501469,  0.1904661059,\n",
            "         0.2295855582,  0.4176954031,  0.0157473087,  0.1056848764])\n",
            "ctensor.grad: tensor([ 0.9918910861, -2.5182881355, -2.1214780807,  0.3961349130,\n",
            "        -0.0065028877,  0.2903265655, -0.0538801216, -0.0456815362,\n",
            "        -1.7473424673,  0.0255600102, -1.3592941761,  0.1647358984,\n",
            "         0.0156095605, -1.6751167774,  0.0241499953, -1.3030518293,\n",
            "         0.1524071246, -0.0076062176])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9914550781, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6659522057), tensor(0.9852442145), tensor(1.0704580545), tensor(1.2000075579), tensor(1.0858366489), tensor(1.4541127682), tensor(1.0596308708), tensor(1.1692839861), tensor(0.9018412828), tensor(1.2573969364), tensor(1.0853638649), tensor(1.5305824280), tensor(0.9900749922), tensor(1.2616574764), tensor(1.1842701435), tensor(0.9891415238), tensor(1.0648183823), tensor(0.8780262470), tensor(1.1052812338), tensor(1.0091084242)]\n",
            "b:  [tensor(0.5935483575), tensor(1.0991684198), tensor(1.3528460264), tensor(1.0126485825), tensor(1.4821865559), tensor(0.8044636250), tensor(1.2471611500), tensor(1.1698955297), tensor(1.8121955395), tensor(0.8499442935), tensor(1.2707700729), tensor(0.6922278404), tensor(1.3086225986), tensor(1.2397221327), tensor(0.8723805547), tensor(1.0108675957), tensor(1.3166496754), tensor(1.6921296120), tensor(0.9884711504), tensor(1.1910880804)]\n",
            "c:  [tensor(-0.0041068681), tensor(0.0137288691), tensor(0.0133756502), tensor(-0.0019152967), tensor(-0.0019849869), tensor(-0.0046709934), tensor(0.0006243137), tensor(0.0002049227), tensor(0.0162641201), tensor(0.0019072284), tensor(0.0098377457), tensor(0.0063751768), tensor(0.0088721812), tensor(0.0097375698), tensor(0.0069010812), tensor(0.0141870258), tensor(0.0013351961), tensor(-0.0010789747)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8282181025,  0.0199559331, -0.9030392170,  0.0590051413,\n",
            "        -0.4715316892,  0.8112931252, -0.0725077689, -0.0747144222,\n",
            "        -1.0044450760,  0.4753330350, -0.3936082721,  0.7713389993,\n",
            "         0.2484921217, -0.1692132950,  0.1697044373, -0.0899171829,\n",
            "        -0.8412587643, -1.0548703671,  0.1447502971, -0.1128133535])\n",
            "btensor.grad: tensor([-0.7095674276,  0.1294162869,  0.2804633379, -0.0832966566,\n",
            "         0.1090045050, -0.6092603207,  0.0500485897, -0.1590205431,\n",
            "         0.3522893786, -0.2973866463,  0.0330936909, -0.6696306467,\n",
            "        -0.3405181170, -0.3310317993, -0.2279056609,  0.1812574267,\n",
            "         0.2195483297,  0.4054569006,  0.0091378689,  0.0978724957])\n",
            "ctensor.grad: tensor([ 1.0479103327, -2.7309713364, -2.3132302761,  0.4591430128,\n",
            "        -0.0075457999,  0.3364416063, -0.0625187010, -0.0533958673,\n",
            "        -1.9579819441,  0.0284814835, -1.5278568268,  0.1799927056,\n",
            "         0.0071064830, -1.8483965397,  0.0264649764, -1.4431954622,\n",
            "         0.1630328000, -0.0198978242])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9736328125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6655454636), tensor(0.9852384925), tensor(1.0709036589), tensor(1.1999819279), tensor(1.0860714912), tensor(1.4537154436), tensor(1.0596696138), tensor(1.1693242788), tensor(0.9023359418), tensor(1.2571654320), tensor(1.0855592489), tensor(1.5302047729), tensor(0.9899569750), tensor(1.2617427111), tensor(1.1841892004), tensor(0.9891883135), tensor(1.0652335882), tensor(0.8785451651), tensor(1.1052130461), tensor(1.0091667175)]\n",
            "b:  [tensor(0.5938943624), tensor(1.0991083384), tensor(1.3527117968), tensor(1.0126923323), tensor(1.4821360111), tensor(0.8047634959), tensor(1.2471396923), tensor(1.1699759960), tensor(1.8120239973), tensor(0.8500930667), tensor(1.2707576752), tensor(0.6925563812), tensor(1.3087925911), tensor(1.2398890257), tensor(0.8724952936), tensor(1.0107814074), tensor(1.3165447712), tensor(1.6919329166), tensor(0.9884698987), tensor(1.1910429001)]\n",
            "c:  [tensor(-0.0046617412), tensor(0.0152153438), tensor(0.0146400286), tensor(-0.0021779684), tensor(-0.0019806633), tensor(-0.0048633800), tensor(0.0006601695), tensor(0.0002358283), tensor(0.0173586700), tensor(0.0018913985), tensor(0.0106945075), tensor(0.0062771705), tensor(0.0088742375), tensor(0.0107574780), tensor(0.0068865828), tensor(0.0149863651), tensor(0.0012481379), tensor(-0.0010613939)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8135956526,  0.0114127696, -0.8911285996,  0.0513138771,\n",
            "        -0.4697306156,  0.7946826220, -0.0775876343, -0.0806580782,\n",
            "        -0.9893584251,  0.4630972147, -0.3907217979,  0.7554180622,\n",
            "         0.2359781265, -0.1704947948,  0.1619462967, -0.0936230421,\n",
            "        -0.8303439617, -1.0378869772,  0.1362563968, -0.1164841652])\n",
            "btensor.grad: tensor([-0.6920260191,  0.1201315522,  0.2684894800, -0.0876043439,\n",
            "         0.1010283306, -0.5996947289,  0.0429379344, -0.1609261036,\n",
            "         0.3431912065, -0.2975580692,  0.0247498751, -0.6570550799,\n",
            "        -0.3400574923, -0.3337116241, -0.2294522077,  0.1724314094,\n",
            "         0.2098709345,  0.3934715986,  0.0025614500,  0.0902489424])\n",
            "ctensor.grad: tensor([ 1.1097464561, -2.9729487896, -2.5287561417,  0.5253433585,\n",
            "        -0.0086472314,  0.3847729564, -0.0717115551, -0.0618111491,\n",
            "        -2.1890988350,  0.0316597894, -1.7135242224,  0.1960126609,\n",
            "        -0.0041121673, -2.0398154259,  0.0289965682, -1.5986788273,\n",
            "         0.1741164178, -0.0351616591])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9533691406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6651457548), tensor(0.9852370024), tensor(1.0713433027), tensor(1.1999601126), tensor(1.0863054991), tensor(1.4533262253), tensor(1.0597108603), tensor(1.1693675518), tensor(0.9028232694), tensor(1.2569396496), tensor(1.0857529640), tensor(1.5298348665), tensor(0.9898450375), tensor(1.2618284225), tensor(1.1841118336), tensor(0.9892367721), tensor(1.0656433105), tensor(0.8790558577), tensor(1.1051490307), tensor(1.0092265606)]\n",
            "b:  [tensor(0.5942321420), tensor(1.0990529060), tensor(1.3525834084), tensor(1.0127382278), tensor(1.4820894003), tensor(0.8050587773), tensor(1.2471216917), tensor(1.1700571775), tensor(1.8118566275), tensor(0.8502420783), tensor(1.2707494497), tensor(0.6928790808), tensor(1.3089623451), tensor(1.2400573492), tensor(0.8726108670), tensor(1.0106993914), tensor(1.3164443970), tensor(1.6917419434), tensor(0.9884718657), tensor(1.1910014153)]\n",
            "c:  [tensor(-0.0052502714), tensor(0.0168392323), tensor(0.0160256792), tensor(-0.0024754552), tensor(-0.0019757566), tensor(-0.0050810920), tensor(0.0007009470), tensor(0.0002713725), tensor(0.0185804591), tensor(0.0018738316), tensor(0.0116538061), tensor(0.0061706724), tensor(0.0088833962), tensor(0.0118833892), tensor(0.0068706959), tensor(0.0158721283), tensor(0.0011552616), tensor(-0.0010344898)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7993862629,  0.0030265749, -0.8793194294,  0.0437265635,\n",
            "        -0.4681062102,  0.7785334587, -0.0824332833, -0.0865936279,\n",
            "        -0.9746363163,  0.4515658617, -0.3874996901,  0.7398583889,\n",
            "         0.2239276171, -0.1713103056,  0.1546525955, -0.0968599319,\n",
            "        -0.8194739819, -1.0213525295,  0.1281216741, -0.1197275519])\n",
            "btensor.grad: tensor([-0.6756013632,  0.1109722257,  0.2567657828, -0.0918251872,\n",
            "         0.0932654291, -0.5905891657,  0.0359794497, -0.1624354124,\n",
            "         0.3347167969, -0.2980756760,  0.0164470673, -0.6453887224,\n",
            "        -0.3395121694, -0.3366564512, -0.2311240137,  0.1641341448,\n",
            "         0.2006941587,  0.3818650246, -0.0038913488,  0.0829198360])\n",
            "ctensor.grad: tensor([ 1.1770601273, -3.2477769852, -2.7713022232,  0.5949733853,\n",
            "        -0.0098134028,  0.4354239702, -0.0815550387, -0.0710884109,\n",
            "        -2.4435784817,  0.0351335779, -1.9185965061,  0.2129961252,\n",
            "        -0.0183178242, -2.2518224716,  0.0317740999, -1.7715255022,\n",
            "         0.1857526302, -0.0538079739])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9290771484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6647528410), tensor(0.9852395058), tensor(1.0717769861), tensor(1.1999419928), tensor(1.0865387917), tensor(1.4529447556), tensor(1.0597542524), tensor(1.1694138050), tensor(0.9033033252), tensor(1.2567192316), tensor(1.0859447718), tensor(1.5294724703), tensor(0.9897387624), tensor(1.2619141340), tensor(1.1840378046), tensor(0.9892864823), tensor(1.0660475492), tensor(0.8795583844), tensor(1.1050887108), tensor(1.0092877150)]\n",
            "b:  [tensor(0.5945622325), tensor(1.0990018845), tensor(1.3524607420), tensor(1.0127861500), tensor(1.4820464849), tensor(0.8053497076), tensor(1.2471070290), tensor(1.1701388359), tensor(1.8116930723), tensor(0.8503915071), tensor(1.2707452774), tensor(0.6931963563), tensor(1.3091317415), tensor(1.2402272224), tensor(0.8727272749), tensor(1.0106210709), tensor(1.3163483143), tensor(1.6915565729), tensor(0.9884769320), tensor(1.1909633875)]\n",
            "c:  [tensor(-0.0058749081), tensor(0.0186190028), tensor(0.0175479893), tensor(-0.0028095611), tensor(-0.0019702315), tensor(-0.0053253095), tensor(0.0007470272), tensor(0.0003120855), tensor(0.0199427567), tensor(0.0018543579), tensor(0.0127265686), tensor(0.0060550557), tensor(0.0089012403), tensor(0.0131269693), tensor(0.0068532801), tensor(0.0168541130), tensor(0.0010562149), tensor(-0.0009963819)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7857089043, -0.0050525069, -0.8674525619,  0.0363558531,\n",
            "        -0.4665565193,  0.7629793882, -0.0868695974, -0.0924165249,\n",
            "        -0.9601070881,  0.4409521818, -0.3837326765,  0.7247780561,\n",
            "         0.2125226259, -0.1714441776,  0.1480095387, -0.0993993282,\n",
            "        -0.8084759712, -1.0050801039,  0.1205234528, -0.1223420501])\n",
            "btensor.grad: tensor([-0.6602306366,  0.1020503044,  0.2454204559, -0.0958397388,\n",
            "         0.0858601183, -0.5818516016,  0.0292954743, -0.1633470058,\n",
            "         0.3270872831, -0.2988843322,  0.0082827806, -0.6345854402,\n",
            "        -0.3387421370, -0.3398090601, -0.2328391969,  0.1565503478,\n",
            "         0.1922023743,  0.3707950115, -0.0101114511,  0.0760504007])\n",
            "ctensor.grad: tensor([ 1.2492730618, -3.5595407486, -3.0446202755,  0.6682119370,\n",
            "        -0.0110502727,  0.4884346724, -0.0921604633, -0.0814258605,\n",
            "        -2.7245936394,  0.0389474519, -2.1455245018,  0.2312338501,\n",
            "        -0.0356874317, -2.4871602058,  0.0348318405, -1.9639689922,\n",
            "         0.1980934888, -0.0762159526])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9018554688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6643664837), tensor(0.9852458239), tensor(1.0722045898), tensor(1.1999273300), tensor(1.0867712498), tensor(1.4525706768), tensor(1.0597995520), tensor(1.1694628000), tensor(0.9037761092), tensor(1.2565034628), tensor(1.0861343145), tensor(1.5291173458), tensor(0.9896377921), tensor(1.2619994879), tensor(1.1839666367), tensor(0.9893369675), tensor(1.0664460659), tensor(0.8800528049), tensor(1.1050318480), tensor(1.0093497038)]\n",
            "b:  [tensor(0.5948851705), tensor(1.0989551544), tensor(1.3523434401), tensor(1.0128358603), tensor(1.4820070267), tensor(0.8056364059), tensor(1.2470954657), tensor(1.1702204943), tensor(1.8115327358), tensor(0.8505414724), tensor(1.2707450390), tensor(0.6935086250), tensor(1.3093005419), tensor(1.2403987646), tensor(0.8728445172), tensor(1.0105460882), tensor(1.3162560463), tensor(1.6913763285), tensor(0.9884849191), tensor(1.1909284592)]\n",
            "c:  [tensor(-0.0065376847), tensor(0.0205754340), tensor(0.0192245021), tensor(-0.0031821388), tensor(-0.0019640499), tensor(-0.0055971844), tensor(0.0007988562), tensor(0.0003586206), tensor(0.0214605443), tensor(0.0018327812), tensor(0.0139249843), tensor(0.0059294770), tensor(0.0089293458), tensor(0.0145014208), tensor(0.0068341750), tensor(0.0179433152), tensor(0.0009505246), tensor(-0.0009450316)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7727310658, -0.0126287937, -0.8553034663,  0.0293554068,\n",
            "        -0.4649251103,  0.7482059002, -0.0906878412, -0.0979764462,\n",
            "        -0.9455481768,  0.4315395057, -0.3791338205,  0.7103333473,\n",
            "         0.2019945979, -0.1706280708,  0.1422609091, -0.1009417772,\n",
            "        -0.7971124649, -0.9888463020,  0.1136738658, -0.1240375042])\n",
            "btensor.grad: tensor([-6.4582979679e-01,  9.3513965607e-02,  2.3462063074e-01,\n",
            "        -9.9484622478e-02,  7.8995630145e-02, -5.7335364819e-01,\n",
            "         2.3060798645e-02, -1.6340172291e-01,  3.2059454918e-01,\n",
            "        -2.9989415407e-01,  3.8862228394e-04, -6.2457817793e-01,\n",
            "        -3.3755350113e-01, -3.4308099747e-01, -2.3447889090e-01,\n",
            "         1.4992624521e-01,  1.8464726210e-01,  3.6048102379e-01,\n",
            "        -1.5937805176e-02,  6.9850921631e-02])\n",
            "ctensor.grad: tensor([ 1.3255534172, -3.9128637314, -3.3530271053,  0.7451555133,\n",
            "        -0.0123633500,  0.5437498093, -0.1036579832, -0.0930701792,\n",
            "        -3.0355749130,  0.0431533605, -2.3968319893,  0.2511572540,\n",
            "        -0.0562118180, -2.7489020824,  0.0382100157, -2.1784048080,\n",
            "         0.2113806158, -0.1027004346])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8679199219, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6639862061), tensor(0.9852555394), tensor(1.0726258755), tensor(1.1999158859), tensor(1.0870027542), tensor(1.4522033930), tensor(1.0598464012), tensor(1.1695142984), tensor(0.9042414427), tensor(1.2562916279), tensor(1.0863209963), tensor(1.5287690163), tensor(0.9895414710), tensor(1.2620837688), tensor(1.1838977337), tensor(0.9893875122), tensor(1.0668386221), tensor(0.8805390000), tensor(1.1049779654), tensor(1.0094119310)]\n",
            "b:  [tensor(0.5952013135), tensor(1.0989123583), tensor(1.3522311449), tensor(1.0128871202), tensor(1.4819705486), tensor(0.8059188724), tensor(1.2470867634), tensor(1.1703016758), tensor(1.8113749027), tensor(0.8506919742), tensor(1.2707486153), tensor(0.6938162446), tensor(1.3094683886), tensor(1.2405719757), tensor(0.8729624748), tensor(1.0104738474), tensor(1.3161668777), tensor(1.6912007332), tensor(0.9884954691), tensor(1.1908961535)]\n",
            "c:  [tensor(-0.0072400123), tensor(0.0227318816), tensor(0.0210752226), tensor(-0.0035950332), tensor(-0.0019571711), tensor(-0.0058977776), tensor(0.0008569578), tensor(0.0004117863), tensor(0.0231506266), tensor(0.0018088751), tensor(0.0152624790), tensor(0.0057927724), tensor(0.0089691188), tensor(0.0160216354), tensor(0.0068131969), tensor(0.0191519931), tensor(0.0008375266), tensor(-0.0008783288)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7606735229, -0.0194396675, -0.8425880671,  0.0229375362,\n",
            "        -0.4630203843,  0.7344560623, -0.0935811400, -0.1030888557,\n",
            "        -0.9306689501,  0.4236949980, -0.3733354807,  0.6967355013,\n",
            "         0.1926683784, -0.1685059071,  0.1377225518, -0.1010913849,\n",
            "        -0.7850700617, -0.9723341465,  0.1078835130, -0.1244406700])\n",
            "btensor.grad: tensor([-0.6322897673,  0.0855777264,  0.2245953679, -0.1025469303,\n",
            "         0.0729311854, -0.5649111271,  0.0175074339, -0.1622521877,\n",
            "         0.3156201243, -0.3009818792, -0.0070475340, -0.6152738929,\n",
            "        -0.3356948495, -0.3463351727, -0.2358756065,  0.1445911527,\n",
            "         0.1783498824,  0.3512035608, -0.0211575031,  0.0645936728])\n",
            "ctensor.grad: tensor([ 1.4046552181, -4.3128957748, -3.7014400959,  0.8257889748,\n",
            "        -0.0137575017,  0.6011863351, -0.1162032560, -0.1063314900,\n",
            "        -3.3801660538,  0.0478121899, -2.6749899387,  0.2734097242,\n",
            "        -0.0795453265, -3.0404303074,  0.0419561975, -2.4173560143,\n",
            "         0.2259959579, -0.1334056556])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8270263672, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6636112928), tensor(0.9852681160), tensor(1.0730403662), tensor(1.1999071836), tensor(1.0872330666), tensor(1.4518424273), tensor(1.0598939657), tensor(1.1695680618), tensor(0.9046989679), tensor(1.2560826540), tensor(1.0865039825), tensor(1.5284268856), tensor(0.9894490242), tensor(1.2621660233), tensor(1.1838303804), tensor(0.9894371629), tensor(1.0672246218), tensor(0.8810165524), tensor(1.1049262285), tensor(1.0094734430)]\n",
            "b:  [tensor(0.5955110788), tensor(1.0988731384), tensor(1.3521233797), tensor(1.0129394531), tensor(1.4819365740), tensor(0.8061970472), tensor(1.2470803261), tensor(1.1703814268), tensor(1.8112186193), tensor(0.8508429527), tensor(1.2707555294), tensor(0.6941195130), tensor(1.3096348047), tensor(1.2407466173), tensor(0.8730808496), tensor(1.0104033947), tensor(1.3160799742), tensor(1.6910290718), tensor(0.9885082245), tensor(1.1908658743)]\n",
            "c:  [tensor(-0.0079824263), tensor(0.0251145065), tensor(0.0231229067), tensor(-0.0040500057), tensor(-0.0019495528), tensor(-0.0062279738), tensor(0.0009219496), tensor(0.0004725865), tensor(0.0250316765), tensor(0.0017823773), tensor(0.0167535804), tensor(0.0056432984), tensor(0.0090215122), tensor(0.0177043341), tensor(0.0067901337), tensor(0.0204936638), tensor(0.0007162592), tensor(-0.0007942455)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7498182058, -0.0251232386, -0.8289309740,  0.0173705816,\n",
            "        -0.4605755806,  0.7220438719, -0.0951601863, -0.1074911356,\n",
            "        -0.9150829315,  0.4178898335, -0.3658645749,  0.6842624545,\n",
            "         0.1849510670, -0.1646226645,  0.1348062754, -0.0993506908,\n",
            "        -0.7719465494, -0.9551422596,  0.1035416126, -0.1230804324])\n",
            "btensor.grad: tensor([-0.6194722652,  0.0785072446,  0.2156468630, -0.1047338247,\n",
            "         0.0679950193, -0.5563119650,  0.0129423738, -0.1594517231,\n",
            "         0.3126546144, -0.3019819260, -0.0137768984, -0.6065390706,\n",
            "        -0.3328319490, -0.3493958712, -0.2368081808,  0.1409671903,\n",
            "         0.1737270355,  0.3433403969, -0.0254960060,  0.0606448650])\n",
            "ctensor.grad: tensor([ 1.4848279953, -4.7652516365, -4.0953683853,  0.9099448919,\n",
            "        -0.0152365901,  0.6603920460, -0.1299836636, -0.1216002554,\n",
            "        -3.7621004581,  0.0529958159, -2.9822018147,  0.2989479005,\n",
            "        -0.1047876850, -3.3653964996,  0.0461269878, -2.6833412647,\n",
            "         0.2425346524, -0.1681665480])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7790527344, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6632410288), tensor(0.9852827191), tensor(1.0734473467), tensor(1.1999006271), tensor(1.0874617100), tensor(1.4514867067), tensor(1.0599414110), tensor(1.1696234941), tensor(0.9051480889), tensor(1.2558753490), tensor(1.0866820812), tensor(1.5280902386), tensor(0.9893593192), tensor(1.2622451782), tensor(1.1837633848), tensor(0.9894846678), tensor(1.0676032305), tensor(0.8814849257), tensor(1.1048756838), tensor(1.0095331669)]\n",
            "b:  [tensor(0.5958146453), tensor(1.0988367796), tensor(1.3520193100), tensor(1.0129922628), tensor(1.4819042683), tensor(0.8064706922), tensor(1.2470754385), tensor(1.1704586744), tensor(1.8110624552), tensor(0.8509942889), tensor(1.2707653046), tensor(0.6944186091), tensor(1.3097990751), tensor(1.2409225702), tensor(0.8731993437), tensor(1.0103336573), tensor(1.3159942627), tensor(1.6908603907), tensor(0.9885225296), tensor(1.1908366680)]\n",
            "c:  [tensor(-0.0087642614), tensor(0.0277524311), tensor(0.0253933519), tensor(-0.0045486335), tensor(-0.0019411512), tensor(-0.0065883715), tensor(0.0009945638), tensor(0.0005422721), tensor(0.0271241702), tensor(0.0017529826), tensor(0.0184136108), tensor(0.0054787039), tensor(0.0090865940), tensor(0.0195681453), tensor(0.0067647388), tensor(0.0219830014), tensor(0.0005853036), tensor(-0.0006910935)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7405291796, -0.0292233825, -0.8138512969,  0.0130168200,\n",
            "        -0.4572423100,  0.7113902569, -0.0949302316, -0.1108359098,\n",
            "        -0.8982990980,  0.4147256613, -0.3561165333,  0.6732702255,\n",
            "         0.1793808341, -0.1583976746,  0.1340411901, -0.0950616598,\n",
            "        -0.7572202682, -0.9367476702,  0.1011464000, -0.1193421483])\n",
            "btensor.grad: tensor([-0.6071869135,  0.0726705790,  0.2081710100, -0.1056706309,\n",
            "         0.0646241680, -0.5472404957,  0.0097641051, -0.1544332504,\n",
            "         0.3123283386, -0.3026698232, -0.0194748640, -0.5981998444,\n",
            "        -0.3285404742, -0.3520205021, -0.2369897813,  0.1395929456,\n",
            "         0.1713173836,  0.3373732567, -0.0285787582,  0.0584728718])\n",
            "ctensor.grad: tensor([ 1.5636703968, -5.2758502960, -4.5408897400,  0.9972555637,\n",
            "        -0.0168031044,  0.7207954526, -0.1452284157, -0.1393711865,\n",
            "        -4.1849875450,  0.0587892421, -3.3200607300,  0.3291887939,\n",
            "        -0.1301636845, -3.7276217937,  0.0507901125, -2.9786744118,\n",
            "         0.2619112730, -0.2063039541])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7218017578, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6628743410), tensor(0.9852982759), tensor(1.0738457441), tensor(1.1998955011), tensor(1.0876879692), tensor(1.4511351585), tensor(1.0599875450), tensor(1.1696798801), tensor(0.9055879116), tensor(1.2556678057), tensor(1.0868537426), tensor(1.5277581215), tensor(0.9892709851), tensor(1.2623196840), tensor(1.1836953163), tensor(0.9895283580), tensor(1.0679733753), tensor(0.8819431663), tensor(1.1048250198), tensor(1.0095894337)]\n",
            "b:  [tensor(0.5961122513), tensor(1.0988025665), tensor(1.3519179821), tensor(1.0130447149), tensor(1.4818725586), tensor(0.8067393303), tensor(1.2470711470), tensor(1.1705318689), tensor(1.8109047413), tensor(0.8511456847), tensor(1.2707771063), tensor(0.6947136521), tensor(1.3099602461), tensor(1.2410994768), tensor(0.8733173609), tensor(1.0102630854), tensor(1.3159083128), tensor(1.6906933784), tensor(0.9885374904), tensor(1.1908073425)]\n",
            "c:  [tensor(-0.0095832329), tensor(0.0306777358), tensor(0.0279155876), tensor(-0.0050921803), tensor(-0.0019319224), tensor(-0.0069791400), tensor(0.0010756726), tensor(0.0006224063), tensor(0.0294501539), tensor(0.0017203359), tensor(0.0202581156), tensor(0.0052955956), tensor(0.0091628684), tensor(0.0216335934), tensor(0.0067367256), tensor(0.0236355513), tensor(0.0004425470), tensor(-0.0005679525)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7332766056, -0.0311451852, -0.7967367768,  0.0103445053,\n",
            "        -0.4525735974,  0.7030158043, -0.0922448039, -0.1126780510,\n",
            "        -0.8796818256,  0.4149709940, -0.3433240056,  0.6642183661,\n",
            "         0.1766415238, -0.1490856409,  0.1360908747, -0.0873982906,\n",
            "        -0.7402323484, -0.9164775610,  0.1013460159, -0.1124551296])\n",
            "btensor.grad: tensor([-0.5952001810,  0.0685316920,  0.2026779056, -0.1048679948,\n",
            "         0.0633776262, -0.5373134613,  0.0084874630, -0.1464650631,\n",
            "         0.3154312372, -0.3027406931, -0.0237098932, -0.5900294781,\n",
            "        -0.3222601116, -0.3538807631, -0.2360291779,  0.1411650777,\n",
            "         0.1718164533,  0.3339245319, -0.0299392939,  0.0586768389])\n",
            "ctensor.grad: tensor([ 1.6379427910, -5.8506088257, -5.0444707870,  1.0870940685,\n",
            "        -0.0184576027,  0.7815368176, -0.1622174829, -0.1602684259,\n",
            "        -4.6519670486,  0.0652933940, -3.6890091896,  0.3662166595,\n",
            "        -0.1525492966, -4.1308956146,  0.0560267493, -3.3050980568,\n",
            "         0.2855130732, -0.2462820560])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.6507568359, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6625100374), tensor(0.9853133559), tensor(1.0742341280), tensor(1.1998904943), tensor(1.0879110098), tensor(1.4507863522), tensor(1.0600306988), tensor(1.1697361469), tensor(0.9060171247), tensor(1.2554579973), tensor(1.0870170593), tensor(1.5274292231), tensor(0.9891821742), tensor(1.2623875141), tensor(1.1836243868), tensor(0.9895660281), tensor(1.0683333874), tensor(0.8823899031), tensor(1.1047725677), tensor(1.0096402168)]\n",
            "b:  [tensor(0.5964038372), tensor(1.0987691879), tensor(1.3518180847), tensor(1.0130956173), tensor(1.4818401337), tensor(0.8070023656), tensor(1.2470662594), tensor(1.1705992222), tensor(1.8107432127), tensor(0.8512966037), tensor(1.2707901001), tensor(0.6950045228), tensor(1.3101168871), tensor(1.2412767410), tensor(0.8734340668), tensor(1.0101897717), tensor(1.3158202171), tensor(1.6905264854), tensor(0.9885519743), tensor(1.1907763481)]\n",
            "c:  [tensor(-0.0104349200), tensor(0.0339252055), tensor(0.0307219252), tensor(-0.0056814319), tensor(-0.0019218234), tensor(-0.0073998384), tensor(0.0011663183), tensor(0.0007149408), tensor(0.0320327543), tensor(0.0016840219), tensor(0.0223019198), tensor(0.0050890744), tensor(0.0092462907), tensor(0.0239229202), tensor(0.0067057582), tensor(0.0254672039), tensor(0.0002848382), tensor(-0.0004253297)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7286492586, -0.0301285386, -0.7768205404,  0.0099562407,\n",
            "        -0.4459985495,  0.6975839138, -0.0862734020, -0.1124475002,\n",
            "        -0.8584375381,  0.4195660651, -0.3265424967,  0.6576985121,\n",
            "         0.1775982976, -0.1357626915,  0.1417915821, -0.0753338337,\n",
            "        -0.7201429605, -0.8934836388,  0.1049475074, -0.1014761329])\n",
            "btensor.grad: tensor([-0.5831956863,  0.0666878223,  0.1998173594, -0.1017125845,\n",
            "         0.0649621338, -0.5260332823,  0.0097653568, -0.1346427202,\n",
            "         0.3229604363, -0.3018009067, -0.0259416103, -0.5817168355,\n",
            "        -0.3132932186, -0.3545515537, -0.2334392667,  0.1465414166,\n",
            "         0.1760879904,  0.3337756395, -0.0289785862,  0.0620108843])\n",
            "ctensor.grad: tensor([ 1.7033742666, -6.4949393272, -5.6126751900,  1.1785032749,\n",
            "        -0.0201980658,  0.8413969278, -0.1812915653, -0.1850692034,\n",
            "        -5.1651978493,  0.0726279244, -4.0876083374,  0.4130427837,\n",
            "        -0.1668441892, -4.5786528587,  0.0619345792, -3.6633055210,\n",
            "         0.3154176176, -0.2852455676])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.5649414062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6621463299), tensor(0.9853259921), tensor(1.0746107101), tensor(1.1998841763), tensor(1.0881294012), tensor(1.4504383802), tensor(1.0600687265), tensor(1.1697908640), tensor(0.9064339399), tensor(1.2552431822), tensor(1.0871694088), tensor(1.5271019936), tensor(0.9890905023), tensor(1.2624461651), tensor(1.1835483313), tensor(0.9895948172), tensor(1.0686813593), tensor(0.8828232288), tensor(1.1047160625), tensor(1.0096828938)]\n",
            "b:  [tensor(0.5966892242), tensor(1.0987352133), tensor(1.3517178297), tensor(1.0131433010), tensor(1.4818049669), tensor(0.8072587252), tensor(1.2470591068), tensor(1.1706581116), tensor(1.8105751276), tensor(0.8514462709), tensor(1.2708028555), tensor(0.6952909827), tensor(1.3102673292), tensor(1.2414535284), tensor(0.8735483885), tensor(1.0101113319), tensor(1.3157275915), tensor(1.6903575659), tensor(0.9885644317), tensor(1.1907416582)]\n",
            "c:  [tensor(-0.0113121867), tensor(0.0375316888), tensor(0.0338477492), tensor(-0.0063164937), tensor(-0.0019108139), tensor(-0.0078492034), tensor(0.0012677481), tensor(0.0008223041), tensor(0.0348953120), tensor(0.0016435550), tensor(0.0245576873), tensor(0.0048521021), tensor(0.0093288654), tensor(0.0264596604), tensor(0.0066714427), tensor(0.0274932832), tensor(0.0001075075), tensor(-0.0002661444)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7273782492, -0.0252379775, -0.7531635165,  0.0125815868,\n",
            "        -0.4368059635,  0.6959226131, -0.0760113597, -0.1094310284,\n",
            "        -0.8335852623,  0.4296730161, -0.3046026230,  0.6544388533,\n",
            "         0.1833121777, -0.1172953844,  0.1521611810, -0.0575987101,\n",
            "        -0.6959302425, -0.8667099476,  0.1129490733, -0.0852558613])\n",
            "btensor.grad: tensor([-0.5707981586,  0.0678733587,  0.2003938556, -0.0954418182,\n",
            "         0.0702582151, -0.5127725601,  0.0144177377, -0.1178321838,\n",
            "         0.3361294866, -0.2993617654, -0.0254751444, -0.5728834867,\n",
            "        -0.3007833362, -0.3534898758, -0.2286110669,  0.1567841172,\n",
            "         0.1851920635,  0.3379001617, -0.0249685049,  0.0694056749])\n",
            "ctensor.grad: tensor([ 1.7545329332, -7.2129645348, -6.2516479492,  1.2701234818,\n",
            "        -0.0220189691,  0.8987299204, -0.2028594613, -0.2147265077,\n",
            "        -5.7251133919,  0.0809337497, -4.5115361214,  0.4739447534,\n",
            "        -0.1651490033, -5.0734806061,  0.0686307698, -4.0521588326,\n",
            "         0.3546614647, -0.3183705807])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.4602050781, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6617811918), tensor(0.9853336811), tensor(1.0749729872), tensor(1.1998746395), tensor(1.0883414745), tensor(1.4500888586), tensor(1.0600988865), tensor(1.1698422432), tensor(0.9068359137), tensor(1.2550199032), tensor(1.0873074532), tensor(1.5267742872), tensor(0.9889929891), tensor(1.2624922991), tensor(1.1834641695), tensor(0.9896111488), tensor(1.0690145493), tensor(0.8832406998), tensor(1.1046527624), tensor(1.0097141266)]\n",
            "b:  [tensor(0.5969679952), tensor(1.0986987352), tensor(1.3516151905), tensor(1.0131858587), tensor(1.4817647934), tensor(0.8075070977), tensor(1.2470474243), tensor(1.1707054377), tensor(1.8103969097), tensor(0.8515936732), tensor(1.2708135843), tensor(0.6955724955), tensor(1.3104091883), tensor(1.2416285276), tensor(0.8736587763), tensor(1.0100247860), tensor(1.3156273365), tensor(1.6901838779), tensor(0.9885729551), tensor(1.1907006502)]\n",
            "c:  [tensor(-0.0122045223), tensor(0.0415349156), tensor(0.0373308733), tensor(-0.0069965562), tensor(-0.0018988587), tensor(-0.0083248997), tensor(0.0013814457), tensor(0.0009474893), tensor(0.0380600765), tensor(0.0015983677), tensor(0.0270338953), tensor(0.0045746872), tensor(0.0093967887), tensor(0.0292678494), tensor(0.0066333152), tensor(0.0297271386), tensor(-9.6290110378e-05), tensor(-9.7117765108e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7303273678, -0.0153574049, -0.7246406674,  0.0191117525,\n",
            "        -0.4241598248,  0.6990137100, -0.0602317750, -0.1027768850,\n",
            "        -0.8039669991,  0.4466527700, -0.2761405706,  0.6553139687,\n",
            "         0.1950469613, -0.0923287868,  0.1684136391, -0.0327028036,\n",
            "        -0.6663609743, -0.8348951340,  0.1265234947, -0.0624587536])\n",
            "btensor.grad: tensor([-0.5575546026,  0.0729678273,  0.2053737640, -0.0851558447,\n",
            "         0.0803139806, -0.4967887402,  0.0234193504, -0.0947021246,\n",
            "         0.3563867807, -0.2948436141, -0.0214952230, -0.5630664825,\n",
            "        -0.2836909592, -0.3500360250, -0.2208066285,  0.1731610298,\n",
            "         0.2004030198,  0.3474525213, -0.0170313120,  0.0819605589])\n",
            "ctensor.grad: tensor([ 1.7846705914, -8.0064506531, -6.9662466049,  1.3601244688,\n",
            "        -0.0239102915,  0.9513928890, -0.2273952365, -0.2503704429,\n",
            "        -6.3295321465,  0.0903745890, -4.9524154663,  0.5548301935,\n",
            "        -0.1358460188, -5.6163764000,  0.0762549788, -4.4677104950,\n",
            "         0.4075950980, -0.3380531967])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.3342285156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6614120007), tensor(0.9853332639), tensor(1.0753179789), tensor(1.1998593807), tensor(1.0885450840), tensor(1.4497348070), tensor(1.0601176023), tensor(1.1698880196), tensor(0.9072200656), tensor(1.2547838688), tensor(1.0874272585), tensor(1.5264436007), tensor(0.9888858795), tensor(1.2625219822), tensor(1.1833682060), tensor(0.9896106124), tensor(1.0693296194), tensor(0.8836389780), tensor(1.1045792103), tensor(1.0097299814)]\n",
            "b:  [tensor(0.5972394943), tensor(1.0986572504), tensor(1.3515073061), tensor(1.0132207870), tensor(1.4817166328), tensor(0.8077456951), tensor(1.2470284700), tensor(1.1707372665), tensor(1.8102042675), tensor(0.8517374396), tensor(1.2708201408), tensor(0.6958483458), tensor(1.3105396032), tensor(1.2418001890), tensor(0.8737633824), tensor(1.0099261999), tensor(1.3155157566), tensor(1.6900019646), tensor(0.9885750413), tensor(1.1906502247)]\n",
            "c:  [tensor(-0.0130974492), tensor(0.0459716357), tensor(0.0412103124), tensor(-0.0077196346), tensor(-0.0018859307), tensor(-0.0088232709), tensor(0.0015091549), tensor(0.0010941237), tensor(0.0415463634), tensor(0.0015477997), tensor(0.0297321323), tensor(0.0042429036), tensor(0.0094281239), tensor(0.0323707722), tensor(0.0065908297), tensor(0.0321781226), tensor(-0.0003364155), tensor(6.9348869147e-05)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 7.3849976063e-01,  7.8645348549e-04, -6.8998503685e-01,\n",
            "         3.0568957329e-02, -4.0711027384e-01,  7.0800137520e-01,\n",
            "        -3.7533819675e-02, -9.1500401497e-02, -7.6828122139e-01,\n",
            "         4.7205066681e-01, -2.3960876465e-01,  6.6134023666e-01,\n",
            "         2.1423763037e-01, -5.9329390526e-02,  1.9189661741e-01,\n",
            "         1.0453462601e-03, -6.3003170490e-01, -7.9658126831e-01,\n",
            "         1.4702266455e-01, -3.1614899635e-02])\n",
            "btensor.grad: tensor([-0.5429646969,  0.0829659700,  0.2158813477, -0.0698255301,\n",
            "         0.0963509083, -0.4772228003,  0.0378946960, -0.0637111664,\n",
            "         0.3853887320, -0.2875688672, -0.0130608082, -0.5517416000,\n",
            "        -0.2608391941, -0.3434319496, -0.2091885507,  0.1971064210,\n",
            "         0.2231564671,  0.3637703657, -0.0041837692,  0.1009337902])\n",
            "ctensor.grad: tensor([ 1.7858542204, -8.8734426498, -7.7588768005,  1.4461565018,\n",
            "        -0.0258561093,  0.9967429638, -0.2554184496, -0.2932687700,\n",
            "        -6.9725761414,  0.1011360884, -5.3964753151,  0.6635671258,\n",
            "        -0.0626702681, -6.2058434486,  0.0849711522, -4.9019651413,\n",
            "         0.4802507162, -0.3329332471])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.1805419922, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6610355377), tensor(0.9853209853), tensor(1.0756418705), tensor(1.1998354197), tensor(1.0887373686), tensor(1.4493727684), tensor(1.0601208210), tensor(1.1699253321), tensor(0.9075826406), tensor(1.2545300722), tensor(1.0875239372), tensor(1.5261068344), tensor(0.9887647033), tensor(1.2625303268), tensor(1.1832561493), tensor(0.9895879030), tensor(1.0696223974), tensor(0.8840140700), tensor(1.1044912338), tensor(1.0097255707)]\n",
            "b:  [tensor(0.5975027680), tensor(1.0986077785), tensor(1.3513907194), tensor(1.0132449865), tensor(1.4816567898), tensor(0.8079722524), tensor(1.2469989061), tensor(1.1707488298), tensor(1.8099918365), tensor(0.8518758416), tensor(1.2708196640), tensor(0.6961175203), tensor(1.3106551170), tensor(1.2419666052), tensor(0.8738598228), tensor(1.0098111629), tensor(1.3153882027), tensor(1.6898077726), tensor(0.9885677099), tensor(1.1905864477)]\n",
            "c:  [tensor(-0.0139720840), tensor(0.0508750416), tensor(0.0455243140), tensor(-0.0084823119), tensor(-0.0018720140), tensor(-0.0093391053), tensor(0.0016528774), tensor(0.0012664889), tensor(0.0453682616), tensor(0.0014910896), tensor(0.0326439813), tensor(0.0038378406), tensor(0.0093902266), tensor(0.0357891768), tensor(0.0065433462), tensor(0.0348489434), tensor(-0.0006267448), tensor(0.0002128651)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7529687881,  0.0245351493, -0.6478562951,  0.0480312109,\n",
            "        -0.3846662641,  0.7241179943, -0.0064058602, -0.0745397806,\n",
            "        -0.7251642942,  0.5074824095, -0.1933838129,  0.6736178398,\n",
            "         0.2423974276, -0.0166248083,  0.2240446806,  0.0454502106,\n",
            "        -0.5854487419, -0.7501819134,  0.1758548617,  0.0087965727])\n",
            "btensor.grad: tensor([-0.5265389681,  0.0989025235,  0.2331171632, -0.0483646393,\n",
            "         0.1196667552, -0.4531723261,  0.0590487421, -0.0232074261,\n",
            "         0.4249044657, -0.2768554091,  0.0008394718, -0.5383511782,\n",
            "        -0.2309686393, -0.3328474760, -0.1928611100,  0.2301473618,\n",
            "         0.2550064623,  0.3882865906,  0.0146052837,  0.1276375055])\n",
            "ctensor.grad: tensor([ 1.7492691278, -9.8068084717, -8.6280069351,  1.5253549814,\n",
            "        -0.0278332066,  1.0316696167, -0.2874450982, -0.3447304070,\n",
            "        -7.6437988281,  0.1134202406, -5.8236989975,  0.8101258278,\n",
            "         0.0757952780, -6.8368124962,  0.0949667320, -5.3416423798,\n",
            "         0.5806585550, -0.2870324254])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.9968261719, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6606481075), tensor(0.9852924347), tensor(1.0759403706), tensor(1.1997991800), tensor(1.0889153481), tensor(1.4489984512), tensor(1.0601035357), tensor(1.1699507236), tensor(0.9079193473), tensor(1.2542527914), tensor(1.0875918865), tensor(1.5257601738), tensor(0.9886242151), tensor(1.2625116110), tensor(1.1831231117), tensor(0.9895368218), tensor(1.0698879957), tensor(0.8843611479), tensor(1.1043840647), tensor(1.0096955299)]\n",
            "b:  [tensor(0.5977567434), tensor(1.0985469818), tensor(1.3512616158), tensor(1.0132548809), tensor(1.4815809727), tensor(0.8081841469), tensor(1.2469549179), tensor(1.1707346439), tensor(1.8097535372), tensor(0.8520069122), tensor(1.2708090544), tensor(0.6963787079), tensor(1.3107515574), tensor(1.2421253920), tensor(0.8739452958), tensor(1.0096743107), tensor(1.3152394295), tensor(1.6895965338), tensor(0.9885475636), tensor(1.1905047894)]\n",
            "c:  [tensor(-0.0148051009), tensor(0.0562716387), tensor(0.0503076129), tensor(-0.0092795361), tensor(-0.0018571093), tensor(-0.0098655112), tensor(0.0018148228), tensor(0.0014694371), tensor(0.0495321862), tensor(0.0014273736), tensor(0.0357478708), tensor(0.0033346827), tensor(0.0092373807), tensor(0.0395390466), tensor(0.0064901230), tensor(0.0377325714), tensor(-0.0009862120), tensor(0.0003025118)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7747802734,  0.0571449399, -0.5970070362,  0.0725220442,\n",
            "        -0.3559463620,  0.7485904694,  0.0346483588, -0.0508979559,\n",
            "        -0.6733820438,  0.5544905663, -0.1359377503,  0.6932340860,\n",
            "         0.2809634805,  0.0374058485,  0.2661911249,  0.1022217274,\n",
            "        -0.5311625004, -0.6941763163,  0.2143588662,  0.0601141453])\n",
            "btensor.grad: tensor([-0.5078974962,  0.1217055917,  0.2582514882, -0.0197820663,\n",
            "         0.1515240520, -0.4237838984,  0.0880516469,  0.0284292698,\n",
            "         0.4766686559, -0.2621134520,  0.0211373568, -0.5224260688,\n",
            "        -0.1928689927, -0.3174965382, -0.1710035205,  0.2737571001,\n",
            "         0.2974452972,  0.4224047661,  0.0402822495,  0.1633043289])\n",
            "ctensor.grad: tensor([  1.6660333872, -10.7931919098,  -9.5665979385,   1.5944482088,\n",
            "         -0.0298094787,   1.0528116226,  -0.3238906264,  -0.4058963656,\n",
            "         -8.3278512955,   0.1274320036,  -6.2077817917,   1.0063159466,\n",
            "          0.3056914806,  -7.4997420311,   0.1064466462,  -5.7672562599,\n",
            "          0.7189344764,  -0.1792933792])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.7780761719, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6602456570), tensor(0.9852426648), tensor(1.0762085915), tensor(1.1997467279), tensor(1.0890755653), tensor(1.4486072063), tensor(1.0600601435), tensor(1.1699606180), tensor(0.9082254171), tensor(1.2539457083), tensor(1.0876249075), tensor(1.5253996849), tensor(0.9884586930), tensor(1.2624595165), tensor(1.1829634905), tensor(0.9894504547), tensor(1.0701210499), tensor(0.8846747875), tensor(1.1042523384), tensor(1.0096338987)]\n",
            "b:  [tensor(0.5980002284), tensor(1.0984710455), tensor(1.3511154652), tensor(1.0132465363), tensor(1.4814845324), tensor(0.8083783984), tensor(1.2468919754), tensor(1.1706883907), tensor(1.8094824553), tensor(0.8521284461), tensor(1.2707847357), tensor(0.6966305375), tensor(1.3108243942), tensor(1.2422738075), tensor(0.8740168214), tensor(1.0095098019), tensor(1.3150635958), tensor(1.6893628836), tensor(0.9885107875), tensor(1.1904003620)]\n",
            "c:  [tensor(-0.0155693758), tensor(0.0621780604), tensor(0.0555880889), tensor(-0.0101045221), tensor(-0.0018412380), tensor(-0.0103939651), tensor(0.0019972827), tensor(0.0017081455), tensor(0.0540347919), tensor(0.0013556955), tensor(0.0390067920), tensor(0.0027022690), tensor(0.0089094946), tensor(0.0436291546), tensor(0.0064303135), tensor(0.0408091955), tensor(-0.0014396512), tensor(0.0002946294)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8047840595,  0.0995199382, -0.5365164280,  0.1047987938,\n",
            "        -0.3203671277,  0.7824472189,  0.0868265629, -0.0198143721,\n",
            "        -0.6120978594,  0.6142684221, -0.0661034584,  0.7210754752,\n",
            "         0.3310058713,  0.1041133404,  0.3193365335,  0.1726950407,\n",
            "        -0.4660518169, -0.6273292303,  0.2635240555,  0.1232604384])\n",
            "btensor.grad: tensor([-0.4869478941,  0.1519801021,  0.2922192812,  0.0166065097,\n",
            "         0.1929107457, -0.3884576559,  0.1258240342,  0.0925337076,\n",
            "         0.5421169996, -0.2430430651,  0.0485183001, -0.5037118196,\n",
            "        -0.1456181258, -0.2967964411, -0.1430462897,  0.3290840983,\n",
            "         0.3516728282,  0.4672610760,  0.0735064745,  0.2088257074])\n",
            "ctensor.grad: tensor([  1.5285503864, -11.8128414154, -10.5609512329,   1.6499722004,\n",
            "         -0.0317425728,   1.0569069386,  -0.3649200499,  -0.4774165750,\n",
            "         -9.0052108765,   0.1433561444,  -6.5178384781,   1.2648272514,\n",
            "          0.6557717919,  -8.1802158356,   0.1196187437,  -6.1532492638,\n",
            "          0.9068784118,   0.0157648548])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.5207519531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6598240137), tensor(0.9851667285), tensor(1.0764416456), tensor(1.1996741295), tensor(1.0892145634), tensor(1.4481940269), tensor(1.0599848032), tensor(1.1699510813), tensor(0.9084960222), tensor(1.2536020279), tensor(1.0876166821), tensor(1.5250208378), tensor(0.9882622361), tensor(1.2623673677), tensor(1.1827715635), tensor(0.9893217087), tensor(1.0703158379), tensor(0.8849493265), tensor(1.1040904522), tensor(1.0095347166)]\n",
            "b:  [tensor(0.5982322693), tensor(1.0983761549), tensor(1.3509477377), tensor(1.0132161379), tensor(1.4813624620), tensor(0.8085519671), tensor(1.2468055487), tensor(1.1706035137), tensor(1.8091714382), tensor(0.8522384167), tensor(1.2707431316), tensor(0.6968717575), tensor(1.3108688593), tensor(1.2424091101), tensor(0.8740713000), tensor(1.0093114376), tensor(1.3148545027), tensor(1.6891012192), tensor(0.9884536266), tensor(1.1902681589)]\n",
            "c:  [tensor(-0.0162355714), tensor(0.0685987920), tensor(0.0613834336), tensor(-0.0109488526), tensor(-0.0018244484), tensor(-0.0109146396), tensor(0.0022024072), tensor(0.0019876433), tensor(0.0588621423), tensor(0.0012750343), tensor(0.0423680507), tensor(0.0019035686), tensor(0.0083329892), tensor(0.0480590276), tensor(0.0063629793), tensor(0.0440441594), tensor(-0.0020180654), tensor(0.0001318122)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8434034586,  0.1518543959, -0.4661411643,  0.1450783014,\n",
            "        -0.2779602408,  0.8262691498,  0.1507056057,  0.0189602375,\n",
            "        -0.5412594080,  0.6873201728,  0.0165604353,  0.7576222420,\n",
            "         0.3928774595,  0.1842368841,  0.3838098049,  0.2574354410,\n",
            "        -0.3896375895, -0.5490596294,  0.3236554861,  0.1983559132])\n",
            "btensor.grad: tensor([-0.4641031027,  0.1896689534,  0.3354538679,  0.0608732104,\n",
            "         0.2442455292, -0.3471099138,  0.1727490127,  0.1698031425,\n",
            "         0.6220336556, -0.2198884487,  0.0831135511, -0.4823856950,\n",
            "        -0.0888747275, -0.2706122398, -0.1089276671,  0.3966268301,\n",
            "         0.4182499945,  0.5234172344,  0.1143658161,  0.2644282579])\n",
            "ctensor.grad: tensor([  1.3323926926, -12.8414688110, -11.5906887054,   1.6886601448,\n",
            "         -0.0335793048,   1.0413490534,  -0.4102489650,  -0.5589957237,\n",
            "         -9.6547021866,   0.1613225639,  -6.7225151062,   1.5974005461,\n",
            "          1.1530115604,  -8.8597431183,   0.1346680820,  -6.4699254036,\n",
            "          1.1568281651,   0.3256343603])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.2225341797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6593787670), tensor(0.9850600958), tensor(1.0766350031), tensor(1.1995778084), tensor(1.0893293619), tensor(1.4477540255), tensor(1.0598719120), tensor(1.1699186563), tensor(0.9087269902), tensor(1.2532154322), tensor(1.0875610113), tensor(1.5246194601), tensor(0.9880293012), tensor(1.2622286081), tensor(1.1825420856), tensor(0.9891437888), tensor(1.0704671144), tensor(0.8851792812), tensor(1.1038933992), tensor(1.0093924999)]\n",
            "b:  [tensor(0.5984525681), tensor(1.0982593298), tensor(1.3507539034), tensor(1.0131601095), tensor(1.4812099934), tensor(0.8087021708), tensor(1.2466913462), tensor(1.1704735756), tensor(1.8088133335), tensor(0.8523352742), tensor(1.2706810236), tensor(0.6971014142), tensor(1.3108804226), tensor(1.2425289154), tensor(0.8741059899), tensor(1.0090734959), tensor(1.3146060705), tensor(1.6888059378), tensor(0.9883725643), tensor(1.1901035309)]\n",
            "c:  [tensor(-0.0167747587), tensor(0.0755257905), tensor(0.0676987842), tensor(-0.0118028214), tensor(-0.0018068202), tensor(-0.0114170620), tensor(0.0024318753), tensor(0.0023121033), tensor(0.0639909059), tensor(0.0011843517), tensor(0.0457661748), tensor(0.0008974337), tensor(0.0074249282), tensor(0.0528181158), tensor(0.0062871198), tensor(0.0473880321), tensor(-0.0027578729), tensor(-0.0002557182)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8903880119,  0.2132607698, -0.3866565228,  0.1927226782,\n",
            "        -0.2296693921,  0.8799275160,  0.2258837819,  0.0648654699,\n",
            "        -0.4619868994,  0.7731146812,  0.1114180684,  0.8026366234,\n",
            "         0.4658406973,  0.2775988579,  0.4589451551,  0.3558582067,\n",
            "        -0.3024691343, -0.4598593712,  0.3940038681,  0.2843790650])\n",
            "btensor.grad: tensor([-0.4405568838,  0.2337126732,  0.3875736594,  0.1121475697,\n",
            "         0.3050202429, -0.3004493713,  0.2283594161,  0.2599431276,\n",
            "         0.7161990404, -0.1937518716,  0.1242018938, -0.4592844546,\n",
            "        -0.0232143998, -0.2395154238, -0.0694013163,  0.4758605361,\n",
            "         0.4967476130,  0.5905050039,  0.1620906591,  0.3293044567])\n",
            "ctensor.grad: tensor([  1.0783749819, -13.8539981842, -12.6307058334,   1.7079371214,\n",
            "         -0.0352564789,   1.0048450232,  -0.4589363635,  -0.6489197016,\n",
            "        -10.2575216293,   0.1813649833,  -6.7962479591,   2.0122697353,\n",
            "          1.8161220551,  -9.5181751251,   0.1517193764,  -6.6877465248,\n",
            "          1.4796148539,   0.7750607729])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1070.8814697266, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6589064598), tensor(0.9849193692), tensor(1.0767850876), tensor(1.1994547844), tensor(1.0894181728), tensor(1.4472827911), tensor(1.0597165823), tensor(1.1698604822), tensor(0.9089154601), tensor(1.2527805567), tensor(1.0874527693), tensor(1.5241919756), tensor(0.9877554178), tensor(1.2620371580), tensor(1.1822706461), tensor(0.9889107943), tensor(1.0705703497), tensor(0.8853600621), tensor(1.1036571264), tensor(1.0092030764)]\n",
            "b:  [tensor(0.5986617804), tensor(1.0981184244), tensor(1.3505303860), tensor(1.0130759478), tensor(1.4810231924), tensor(0.8088272810), tensor(1.2465457916), tensor(1.1702928543), tensor(1.8084018230), tensor(0.8524187207), tensor(1.2705960274), tensor(0.6973194480), tensor(1.3108556271), tensor(1.2426314354), tensor(0.8741191030), tensor(1.0087909698), tensor(1.3143132925), tensor(1.6884725094), tensor(0.9882651567), tensor(1.1899029016)]\n",
            "c:  [tensor(-0.0171619039), tensor(0.0829409212), tensor(0.0745264441), tensor(-0.0126560433), tensor(-0.0017884680), tensor(-0.0118910819), tensor(0.0026864994), tensor(0.0026839576), tensor(0.0693920553), tensor(0.0010826610), tensor(0.0491294265), tensor(-0.0003582648), tensor(0.0061008874), tensor(0.0578869879), tensor(0.0062017236), tensor(0.0507798865), tensor(-0.0036988046), tensor(-0.0009453753)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9446024895,  0.2814754546, -0.3001435399,  0.2459896803,\n",
            "        -0.1775879711,  0.9423606396,  0.3106734157,  0.1162844896,\n",
            "        -0.3769009113,  0.8698451519,  0.2164993286,  0.8549798727,\n",
            "         0.5477418900,  0.3828163147,  0.5428460836,  0.4659303427,\n",
            "        -0.2064131498, -0.3616001606,  0.4724951386,  0.3789321780])\n",
            "btensor.grad: tensor([-0.4184825420,  0.2817636132,  0.4471297264,  0.1683236957,\n",
            "         0.3735148311, -0.2502390146,  0.2910782993,  0.3613550663,\n",
            "         0.8231019974, -0.1668399572,  0.1699452400, -0.4361152649,\n",
            "         0.0495816171, -0.2050092220, -0.0262635648,  0.5649573803,\n",
            "         0.5854669213,  0.6669557095,  0.2147850990,  0.4013510942])\n",
            "ctensor.grad: tensor([  0.7742915750, -14.8302669525, -13.6553249359,   1.7064441442,\n",
            "         -0.0367043316,   0.9480388761,  -0.5092481971,  -0.7437084913,\n",
            "        -10.8023061752,   0.2033814490,  -6.7265067101,   2.5113968849,\n",
            "          2.6480817795, -10.1377449036,   0.1707928330,  -6.7837080956,\n",
            "          1.8818633556,   1.3793140650])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1070.4949951172, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6584044695), tensor(0.9847429991), tensor(1.0768901110), tensor(1.1993038654), tensor(1.0894806385), tensor(1.4467769861), tensor(1.0595155954), tensor(1.1697752476), tensor(0.9090605378), tensor(1.2522933483), tensor(1.0872884989), tensor(1.5237357616), tensor(0.9874379039), tensor(1.2617884874), tensor(1.1819545031), tensor(0.9886187315), tensor(1.0706226826), tensor(0.8854888678), tensor(1.1033792496), tensor(1.0089639425)]\n",
            "b:  [tensor(0.5988623500), tensor(1.0979534388), tensor(1.3502745628), tensor(1.0129629374), tensor(1.4807997942), tensor(0.8089269996), tensor(1.2463667393), tensor(1.1700572968), tensor(1.8079319000), tensor(0.8524900079), tensor(1.2704874277), tensor(0.6975272298), tensor(1.3107924461), tensor(1.2427163124), tensor(0.8741103411), tensor(1.0084606409), tensor(1.3139725924), tensor(1.6880975962), tensor(0.9881304502), tensor(1.1896643639)]\n",
            "c:  [tensor(-0.0173796266), tensor(0.0908210650), tensor(0.0818483084), tensor(-0.0134982560), tensor(-0.0017695416), tensor(-0.0123279821), tensor(0.0029658445), tensor(0.0031030211), tensor(0.0750366822), tensor(0.0009691057), tensor(0.0523891822), tensor(-0.0019024849), tensor(0.0042858184), tensor(0.0632408410), tensor(0.0061058411), tensor(0.0541539527), tensor(-0.0048804851), tensor(-0.0020139369)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0040032864,  0.3527725339, -0.2100469470,  0.3019527197,\n",
            "        -0.1250238121,  1.0115263462,  0.4019952118,  0.1704628468,\n",
            "        -0.2901573181,  0.9744521379,  0.3285014033,  0.9125381708,\n",
            "         0.6349946260,  0.4972939491,  0.6323536634,  0.5841501951,\n",
            "        -0.1047126055, -0.2576353550,  0.5556670427,  0.4782586694])\n",
            "btensor.grad: tensor([-0.4011241198,  0.3300670981,  0.5115515590,  0.2259826660,\n",
            "         0.4467018843, -0.1993807554,  0.3581353426,  0.4710713625,\n",
            "         0.9398983717, -0.1425600052,  0.2172862291, -0.4155158997,\n",
            "         0.1264741719, -0.1696393490,  0.0175376981,  0.6607450843,\n",
            "         0.6813708544,  0.7499190569,  0.2693883777,  0.4770560265])\n",
            "ctensor.grad: tensor([  0.4354440868, -15.7602853775, -14.6437358856,   1.6844254732,\n",
            "         -0.0378526263,   0.8737993836,  -0.5586901307,  -0.8381270766,\n",
            "        -11.2892503738,   0.2271103859,  -6.5195121765,   3.0884401798,\n",
            "          3.6301376820, -10.7077121735,   0.1917653382,  -6.7481288910,\n",
            "          2.3633604050,   2.1371231079])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1070.0631103516, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6578716040), tensor(0.9845318794), tensor(1.0769505501), tensor(1.1991255283), tensor(1.0895187855), tensor(1.4462347031), tensor(1.0592677593), tensor(1.1696634293), tensor(0.9091641307), tensor(1.2517518997), tensor(1.0870668888), tensor(1.5232496262), tensor(0.9870764613), tensor(1.2614797354), tensor(1.1815928221), tensor(0.9882657528), tensor(1.0706235170), tensor(0.8855651617), tensor(1.1030597687), tensor(1.0086750984)]\n",
            "b:  [tensor(0.5990586281), tensor(1.0977666378), tensor(1.3499859571), tensor(1.0128226280), tensor(1.4805395603), tensor(0.8090028763), tensor(1.2461538315), tensor(1.1697647572), tensor(1.8074005842), tensor(0.8525527120), tensor(1.2703562975), tensor(0.6977276802), tensor(1.3106907606), tensor(1.2427847385), tensor(0.8740813136), tensor(1.0080811977), tensor(1.3135824203), tensor(1.6876798868), tensor(0.9879695177), tensor(1.1893885136)]\n",
            "c:  [tensor(-0.0174212158), tensor(0.0991445184), tensor(0.0896407738), tensor(-0.0143201509), tensor(-0.0017502217), tensor(-0.0127214845), tensor(0.0032679832), tensor(0.0035658849), tensor(0.0809025168), tensor(0.0008430398), tensor(0.0554900281), tensor(-0.0037666778), tensor(0.0019259434), tensor(0.0688548461), tensor(0.0059986650), tensor(0.0574485883), tensor(-0.0063382746), tensor(-0.0035268781)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0658061504,  0.4222566783, -0.1208984852,  0.3567271233,\n",
            "        -0.0762595236,  1.0845943689,  0.4956783354,  0.2237108946,\n",
            "        -0.2071628571,  1.0829131603,  0.4431220889,  0.9723825455,\n",
            "         0.7228496075,  0.6175271273,  0.7233846188,  0.7059158087,\n",
            "        -0.0017114878, -0.1525304317,  0.6389464736,  0.5775698423])\n",
            "btensor.grad: tensor([-0.3925966024,  0.3737205267,  0.5773137212,  0.2806176543,\n",
            "         0.5204643011, -0.1517225504,  0.4257680476,  0.5850328207,\n",
            "         1.0626946688, -0.1253557801,  0.2621480227, -0.4008851647,\n",
            "         0.2033612728, -0.1368039846,  0.0581116527,  0.7589456439,\n",
            "         0.7803527713,  0.8354704380,  0.3218783140,  0.5518008471])\n",
            "ctensor.grad: tensor([  0.0831767768, -16.6469020844, -15.5849294662,   1.6437900066,\n",
            "         -0.0386399217,   0.7870050669,  -0.6042775512,  -0.9257274270,\n",
            "        -11.7316617966,   0.2521318793,  -6.2016892433,   3.7283859253,\n",
            "          4.7197499275, -11.2280139923,   0.2143526822,  -6.5892691612,\n",
            "          2.9155788422,   3.0258824825])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1069.5815429688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6573081017), tensor(0.9842896461), tensor(1.0769693851), tensor(1.1989225149), tensor(1.0895367861), tensor(1.4456554651), tensor(1.0589742661), tensor(1.1695275307), tensor(0.9092310667), tensor(1.2511564493), tensor(1.0867890120), tensor(1.5227340460), tensor(0.9866734147), tensor(1.2611099482), tensor(1.1811870337), tensor(0.9878526330), tensor(1.0705746412), tensor(0.8855908513), tensor(1.1027011871), tensor(1.0083392859)]\n",
            "b:  [tensor(0.5992573500), tensor(1.0975630283), tensor(1.3496657610), tensor(1.0126590729), tensor(1.4802445173), tensor(0.8090586662), tensor(1.2459089756), tensor(1.1694154739), tensor(1.8068070412), tensor(0.8526127934), tensor(1.2702063322), tensor(0.6979256868), tensor(1.3105529547), tensor(1.2428399324), tensor(0.8740357757), tensor(1.0076538324), tensor(1.3131434917), tensor(1.6872203350), tensor(0.9877856374), tensor(1.1890783310)]\n",
            "c:  [tensor(-0.0172920935), tensor(0.1078964993), tensor(0.0978804976), tensor(-0.0151140625), tensor(-0.0017307100), tensor(-0.0130683603), tensor(0.0035894956), tensor(0.0040658270), tensor(0.0869793445), tensor(0.0007040916), tensor(0.0583980680), tensor(-0.0059712427), tensor(-0.0010010093), tensor(0.0747098550), tensor(0.0058796033), tensor(0.0606152639), tensor(-0.0080993287), tensor(-0.0055276491)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1269011497,  0.4844937325, -0.0377317071,  0.4059487581,\n",
            "        -0.0360135734,  1.1584045887,  0.5870232582,  0.2718687057,\n",
            "        -0.1338913441,  1.1908900738,  0.5556976795,  1.0312016010,\n",
            "         0.8060458302,  0.7396583557,  0.8114876151,  0.8262054920,\n",
            "         0.0977764726, -0.0514289141,  0.7172687650,  0.6717104912])\n",
            "btensor.grad: tensor([-0.3974460363,  0.4072055817,  0.6404570937,  0.3271601200,\n",
            "         0.5901229978, -0.1115958691,  0.4897354245,  0.6986683607,\n",
            "         1.1871628761, -0.1202053428,  0.2999242544, -0.3959984481,\n",
            "         0.2756325006, -0.1103436947,  0.0910838842,  0.8547916412,\n",
            "         0.8778333068,  0.9191794395,  0.3677667975,  0.6204364300])\n",
            "ctensor.grad: tensor([ -0.2582428455, -17.5039653778, -16.4794425964,   1.5878229141,\n",
            "         -0.0390234105,   0.6937521100,  -0.6430244446,  -0.9998840690,\n",
            "        -12.1536502838,   0.2778963745,  -5.8160824776,   4.4091300964,\n",
            "          5.8539052010, -11.7100229263,   0.2381236553,  -6.3333520889,\n",
            "          3.5221078396,   4.0015416145])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1069.0482177734, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6567158699), tensor(0.9840224385), tensor(1.0769519806), tensor(1.1986998320), tensor(1.0895411968), tensor(1.4450404644), tensor(1.0586384535), tensor(1.1693720818), tensor(0.9092690349), tensor(1.2505092621), tensor(1.0864579678), tensor(1.5221910477), tensor(0.9862335920), tensor(1.2606798410), tensor(1.1807407141), tensor(0.9873824120), tensor(1.0704799891), tensor(0.8855704665), tensor(1.1023082733), tensor(1.0079612732)]\n",
            "b:  [tensor(0.5994673371), tensor(1.0973503590), tensor(1.3493171930), tensor(1.0124787092), tensor(1.4799189568), tensor(0.8091002107), tensor(1.2456359863), tensor(1.1690115929), tensor(1.8061523438), tensor(0.8526787758), tensor(1.2700432539), tensor(0.6981278658), tensor(1.3103834391), tensor(1.2428869009), tensor(0.8739796281), tensor(1.0071818829), tensor(1.3126587868), tensor(1.6867219210), tensor(0.9875842333), tensor(1.1887392998)]\n",
            "c:  [tensor(-0.0170090329), tensor(0.1170716286), tensor(0.1065489501), tensor(-0.0158743486), tensor(-0.0017112163), tensor(-0.0133684594), tensor(0.0039257510), tensor(0.0045933612), tensor(0.0932717398), tensor(0.0005522040), tensor(0.0611050874), tensor(-0.0085235620), tensor(-0.0044796392), tensor(0.0807967186), tensor(0.0057483301), tensor(0.0636249483), tensor(-0.0101799034), tensor(-0.0080296993)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1844054461,  0.5343624949,  0.0347239375,  0.4454761744,\n",
            "        -0.0087471604,  1.2300287485,  0.6716501713,  0.3109679222,\n",
            "        -0.0759711266,  1.2944682837,  0.6620113850,  1.0858802795,\n",
            "         0.8796470761,  0.8602299690,  0.8926045895,  0.9404655695,\n",
            "         0.1892965436,  0.0408236980,  0.7858563066,  0.7559798360])\n",
            "btensor.grad: tensor([-0.4199750423,  0.4252356887,  0.6972452998,  0.3607634306,\n",
            "         0.6512009501, -0.0831177235,  0.5460261106,  0.8077132702,\n",
            "         1.3093199730, -0.1319283247,  0.3262233734, -0.4044111371,\n",
            "         0.3389209211, -0.0938912630,  0.1122851074,  0.9438428879,\n",
            "         0.9695172310,  0.9968360662,  0.4028238058,  0.6780933142])\n",
            "ctensor.grad: tensor([ -0.5661196709, -18.3502521515, -17.3369121552,   1.5205709934,\n",
            "         -0.0389874764,   0.6001988053,  -0.6725108624,  -1.0550687313,\n",
            "        -12.5847940445,   0.3037753403,  -5.4140348434,   5.1046385765,\n",
            "          6.9572596550, -12.1737279892,   0.2625461221,  -6.0193614960,\n",
            "          4.1611480713,   5.0040993690])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1068.4582519531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6560977697), tensor(0.9837384820), tensor(1.0769056082), tensor(1.1984637976), tensor(1.0895401239), tensor(1.4443917274), tensor(1.0582653284), tensor(1.1692031622), tensor(0.9092879295), tensor(1.2498137951), tensor(1.0860784054), tensor(1.5216239691), tensor(0.9857636690), tensor(1.2601914406), tensor(1.1802588701), tensor(0.9868596792), tensor(1.0703451633), tensor(0.8855102062), tensor(1.1018877029), tensor(1.0075478554)]\n",
            "b:  [tensor(0.5996990800), tensor(1.0971385241), tensor(1.3489447832), tensor(1.0122898817), tensor(1.4795688391), tensor(0.8091349602), tensor(1.2453402281), tensor(1.1685570478), tensor(1.8054392338), tensor(0.8527609706), tensor(1.2698744535), tensor(0.6983422637), tensor(1.3101885319), tensor(1.2429319620), tensor(0.8739203811), tensor(1.0066704750), tensor(1.3121327162), tensor(1.6861892939), tensor(0.9873723388), tensor(1.1883788109)]\n",
            "c:  [tensor(-0.0165972114), tensor(0.1266724914), tensor(0.1156342924), tensor(-0.0165974125), tensor(-0.0016919426), tensor(-0.0136241699), tensor(0.0042714314), tensor(0.0051373313), tensor(0.0997982621), tensor(0.0003876403), tensor(0.0636275560), tensor(-0.0114180418), tensor(-0.0084563456), tensor(0.0871178135), tensor(0.0056048036), tensor(0.0664700121), tensor(-0.0125845671), tensor(-0.0110133383)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2362000942,  0.5679181814,  0.0928260684,  0.4721208811,\n",
            "         0.0020768046,  1.2973773479,  0.7462831736,  0.3379344940,\n",
            "        -0.0377815962,  1.3908376694,  0.7590153813,  1.1340795755,\n",
            "         0.9398667216,  0.9768505096,  0.9637773037,  1.0454202890,\n",
            "         0.2695612907,  0.1205565929,  0.8410327435,  0.8268748522])\n",
            "btensor.grad: tensor([-0.4635287523,  0.4236278534,  0.7448841333,  0.3775827885,\n",
            "         0.7002132535, -0.0694600344,  0.5915772319,  0.9089893103,\n",
            "         1.4262866974, -0.1643412709,  0.3376154900, -0.4288243055,\n",
            "         0.3898669779, -0.0901998281,  0.1184753925,  1.0227738619,\n",
            "         1.0521967411,  1.0652136803,  0.4237993360,  0.7210227251])\n",
            "ctensor.grad: tensor([ -0.8236413598, -19.2017383575, -18.1706809998,   1.4461266994,\n",
            "         -0.0385473371,   0.5114215612,  -0.6913608909,  -1.0879404545,\n",
            "        -13.0530433655,   0.3291272521,  -5.0449347496,   5.7889590263,\n",
            "          7.9534125328, -12.6421918869,   0.2870532572,  -5.6901307106,\n",
            "          4.8093276024,   5.9672770500])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1067.8073730469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6554571390), tensor(0.9834469557), tensor(1.0768382549), tensor(1.1982216835), tensor(1.0895428658), tensor(1.4437118769), tensor(1.0578606129), tensor(1.1690275669), tensor(0.9092988372), tensor(1.2490744591), tensor(1.0856556892), tensor(1.5210366249), tensor(0.9852713346), tensor(1.2596471310), tensor(1.1797471046), tensor(0.9862898588), tensor(1.0701766014), tensor(0.8854172230), tensor(1.1014473438), tensor(1.0071065426)]\n",
            "b:  [tensor(0.5999640226), tensor(1.0969384909), tensor(1.3485537767), tensor(1.0121021271), tensor(1.4792011976), tensor(0.8091710806), tensor(1.2450277805), tensor(1.1680564880), tensor(1.8046708107), tensor(0.8528707623), tensor(1.2697082758), tensor(0.6985775828), tensor(1.3099751472), tensor(1.2429822683), tensor(0.8738664389), tensor(1.0061254501), tensor(1.3115705252), tensor(1.6856279373), tensor(0.9871578217), tensor(1.1880052090)]\n",
            "c:  [tensor(-0.0160857476), tensor(0.1367051154), tensor(0.1251298040), tensor(-0.0172814094), tensor(-0.0016730693), tensor(-0.0138394935), tensor(0.0046211537), tensor(0.0056862766), tensor(0.1065872014), tensor(0.0002109523), tensor(0.0660005882), tensor(-0.0146381790), tensor(-0.0128448764), tensor(0.0936853513), tensor(0.0054492457), tensor(0.0691612586), tensor(-0.0153074404), tensor(-0.0144282421)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2813242674,  0.5830153227,  0.1346127391,  0.4841582775,\n",
            "        -0.0054677129,  1.3595840931,  0.8093914986,  0.3510989547,\n",
            "        -0.0217572451,  1.4787263870,  0.8453352451,  1.1746618748,\n",
            "         0.9846425653,  1.0886362791,  1.0236049891,  1.1396037340,\n",
            "         0.3370529413,  0.1859281063,  0.8807866573,  0.8826544285])\n",
            "btensor.grad: tensor([-0.5299189091,  0.4000117779,  0.7820163965,  0.3754317164,\n",
            "         0.7352875471, -0.0722732544,  0.6248290539,  1.0010017157,\n",
            "         1.5368053913, -0.2196344137,  0.3322515488, -0.4705896080,\n",
            "         0.4267085493, -0.1006087065,  0.1079096347,  1.0899703503,\n",
            "         1.1242877245,  1.1226433516,  0.4289939404,  0.7472170591])\n",
            "ctensor.grad: tensor([ -1.0229258537, -20.0652351379, -18.9910182953,   1.3679919243,\n",
            "         -0.0377465785,   0.4306475222,  -0.6994441748,  -1.0978908539,\n",
            "        -13.5778713226,   0.3533760309,  -4.7460622787,   6.4402737617,\n",
            "          8.7770614624, -13.1350803375,   0.3111155331,  -5.3824906349,\n",
            "          5.4457459450,   6.8298068047])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1067.0921630859, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6547970772), tensor(0.9831571579), tensor(1.0767582655), tensor(1.1979808807), tensor(1.0895586014), tensor(1.4430032969), tensor(1.0574299097), tensor(1.1688523293), tensor(0.9093129039), tensor(1.2482951880), tensor(1.0851949453), tensor(1.5204327106), tensor(0.9847643971), tensor(1.2590489388), tensor(1.1792109013), tensor(0.9856780767), tensor(1.0699805021), tensor(0.8852986693), tensor(1.1009948254), tensor(1.0066448450)]\n",
            "b:  [tensor(0.6002736092), tensor(1.0967613459), tensor(1.3481492996), tensor(1.0119251013), tensor(1.4788229465), tensor(0.8092167974), tensor(1.2447048426), tensor(1.1675144434), tensor(1.8038500547), tensor(0.8530197740), tensor(1.2695531845), tensor(0.6988423467), tensor(1.3097504377), tensor(1.2430446148), tensor(0.8738261461), tensor(1.0055525303), tensor(1.3109774590), tensor(1.6850433350), tensor(0.9869485497), tensor(1.1876268387)]\n",
            "c:  [tensor(-0.0155027416), tensor(0.1471730918), tensor(0.1350303292), tensor(-0.0179257598), tensor(-0.0016547451), tensor(-0.0140190134), tensor(0.0049700351), tensor(0.0062297140), tensor(0.1136698797), tensor(2.2907479433e-05), tensor(0.0682680458), tensor(-0.0181603562), tensor(-0.0175372772), tensor(0.1005169228), tensor(0.0052820938), tensor(0.0717208162), tensor(-0.0183350816), tensor(-0.0182009097)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3201066256,  0.5795723796,  0.1600314379,  0.4816087484,\n",
            "        -0.0315301716,  1.4171500206,  0.8613749146,  0.3504300117,\n",
            "        -0.0280967951,  1.5584996939,  0.9213949442,  1.2078566551,\n",
            "         1.0138683319,  1.1962908506,  1.0723915100,  1.2235108614,\n",
            "         0.3922464252,  0.2371631265,  0.9050156474,  0.9234923124])\n",
            "btensor.grad: tensor([-0.6191209555,  0.3541733623,  0.8089555502,  0.3540764451,\n",
            "         0.7564468980, -0.0914309025,  0.6459506154,  1.0841314793,\n",
            "         1.6413954496, -0.2980191708,  0.3101500273, -0.5294717550,\n",
            "         0.4495235384, -0.1247938871,  0.0806105137,  1.1457344294,\n",
            "         1.1860456467,  1.1692286730,  0.4185011387,  0.7566853762])\n",
            "ctensor.grad: tensor([ -1.1660130024, -20.9359512329, -19.8010463715,   1.2887017727,\n",
            "         -0.0366484970,   0.3590389192,  -0.6977627277,  -1.0868744850,\n",
            "        -14.1653594971,   0.3760896027,  -4.5349116325,   7.0443544388,\n",
            "          9.3848009109, -13.6631412506,   0.3343037367,  -5.1191196442,\n",
            "          6.0552806854,   7.5453352928])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1066.3121337891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6541200876), tensor(0.9828774929), tensor(1.0766729116), tensor(1.1977478266), tensor(1.0895959139), tensor(1.4422674179), tensor(1.0569777489), tensor(1.1686836481), tensor(0.9093403816), tensor(1.2474792004), tensor(1.0847003460), tensor(1.5198152065), tensor(0.9842498302), tensor(1.2583980560), tensor(1.1786549091), tensor(0.9850284457), tensor(1.0697617531), tensor(0.8851605058), tensor(1.1005371809), tensor(1.0061691999)]\n",
            "b:  [tensor(0.6006382704), tensor(1.0966173410), tensor(1.3477355242), tensor(1.0117675066), tensor(1.4784401655), tensor(0.8092793822), tensor(1.2443765402), tensor(1.1669342518), tensor(1.8029789925), tensor(0.8532186747), tensor(1.2694166899), tensor(0.6991442442), tensor(1.3095203638), tensor(1.2431250811), tensor(0.8738070130), tensor(1.0049564838), tensor(1.3103578091), tensor(1.6844400167), tensor(0.9867514968), tensor(1.1872512102)]\n",
            "c:  [tensor(-0.0148709407), tensor(0.1580725908), tensor(0.1453279704), tensor(-0.0185306128), tensor(-0.0016370834), tensor(-0.0141670210), tensor(0.0053140656), tensor(0.0067590345), tensor(0.1210731044), tensor(-0.0001756142), tensor(0.0704709068), tensor(-0.0219586398), tensor(-0.0224181060), tensor(0.1076296419), tensor(0.0051039280), tensor(0.0741729960), tensor(-0.0216502715), tensor(-0.0222452506)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3540236950,  0.5593876243,  0.1707949042,  0.4660947323,\n",
            "        -0.0746524632,  1.4717602730,  0.9043990970,  0.3374132514,\n",
            "        -0.0549008846,  1.6318751574,  0.9891529679,  1.2351011038,\n",
            "         1.0291738510,  1.3018288612,  1.1119222641,  1.2992812395,\n",
            "         0.4373880029,  0.2763357162,  0.9153251052,  0.9512362480])\n",
            "btensor.grad: tensor([-0.7293816805,  0.2879697680,  0.8275057673,  0.3151057959,\n",
            "         0.7654466629, -0.1251227856,  0.6566695571,  1.1604059935,\n",
            "         1.7421200275, -0.3977778554,  0.2730764151, -0.6037811637,\n",
            "         0.4600859582, -0.1608926058,  0.0382356793,  1.1920968294,\n",
            "         1.2393447161,  1.2066813707,  0.3940664530,  0.7512836456])\n",
            "ctensor.grad: tensor([ -1.2636024952, -21.7989902496, -20.5952682495,   1.2097060680,\n",
            "         -0.0353235379,   0.2960146368,  -0.6880614161,  -1.0586413145,\n",
            "        -14.8064498901,   0.3970434070,  -4.4057197571,   7.5965671539,\n",
            "          9.7616558075, -14.2254447937,   0.3563312590,  -4.9043660164,\n",
            "          6.6303806305,   8.0886802673])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1065.4625244141, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6534274817), tensor(0.9826147556), tensor(1.0765880346), tensor(1.1975276470), tensor(1.0896620750), tensor(1.4415044785), tensor(1.0565068722), tensor(1.1685264111), tensor(0.9093897939), tensor(1.2466285229), tensor(1.0841746330), tensor(1.5191859007), tensor(0.9837331772), tensor(1.2576940060), tensor(1.1780824661), tensor(0.9843434095), tensor(1.0695238113), tensor(0.8850071430), tensor(1.1000798941), tensor(1.0056848526)]\n",
            "b:  [tensor(0.6010671258), tensor(1.0965149403), tensor(1.3473153114), tensor(1.0116368532), tensor(1.4780575037), tensor(0.8093645573), tensor(1.2440466881), tensor(1.1663178205), tensor(1.8020579815), tensor(0.8534765840), tensor(1.2693046331), tensor(0.6994896531), tensor(1.3092896938), tensor(1.2432280779), tensor(0.8738152385), tensor(1.0043404102), tensor(1.3097143173), tensor(1.6838212013), tensor(0.9865722060), tensor(1.1868841648)]\n",
            "c:  [tensor(-0.0142050693), tensor(0.1693894565), tensor(0.1560087204), tensor(-0.0190963745), tensor(-0.0016201653), tensor(-0.0142870005), tensor(0.0056502153), tensor(0.0072678467), tensor(0.1288125366), tensor(-0.0003837325), tensor(0.0726367459), tensor(-0.0260093957), tensor(-0.0273785032), tensor(0.1150346100), tensor(0.0049153967), tensor(0.0765356123), tensor(-0.0252356585), tensor(-0.0264733657)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3853011131,  0.5255064964,  0.1698015928,  0.4403668642,\n",
            "        -0.1322561502,  1.5258066654,  0.9417428970,  0.3145366907,\n",
            "        -0.0988055468,  1.7013528347,  1.0514934063,  1.2586005926,\n",
            "         1.0333307981,  1.4079985619,  1.1449158192,  1.3700246811,\n",
            "         0.4758637547,  0.3066864610,  0.9144862294,  0.9688014984])\n",
            "btensor.grad: tensor([-0.8577109575,  0.2047644258,  0.8404282331,  0.2614011765,\n",
            "         0.7652558684, -0.1703872681,  0.6597523689,  1.2328873873,\n",
            "         1.8419604301, -0.5157859921,  0.2240233421, -0.6907988191,\n",
            "         0.4612855911, -0.2059811354, -0.0164035708,  1.2321983576,\n",
            "         1.2870925665,  1.2377364635,  0.3585637212,  0.7341359854])\n",
            "ctensor.grad: tensor([ -1.3317427635, -22.6337337494, -21.3615036011,   1.1315248013,\n",
            "         -0.0338362008,   0.2399596423,  -0.6722995639,  -1.0176247358,\n",
            "        -15.4788770676,   0.4162366390,  -4.3316731453,   8.1015100479,\n",
            "          9.9207925797, -14.8099327087,   0.3770628273,  -4.7252283096,\n",
            "          7.1707749367,   8.4562301636])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1064.5509033203, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6527192593), tensor(0.9823740721), tensor(1.0765078068), tensor(1.1973237991), tensor(1.0897628069), tensor(1.4407135248), tensor(1.0560183525), tensor(1.1683840752), tensor(0.9094677567), tensor(1.2457437515), tensor(1.0836188793), tensor(1.5185455084), tensor(0.9832184911), tensor(1.2569352388), tensor(1.1774953604), tensor(0.9836239219), tensor(1.0692681074), tensor(0.8848412633), tensor(1.0996271372), tensor(1.0051951408)]\n",
            "b:  [tensor(0.6015673876), tensor(1.0964607000), tensor(1.3468899727), tensor(1.0115386248), tensor(1.4776779413), tensor(0.8094764948), tensor(1.2437175512), tensor(1.1656653881), tensor(1.8010859489), tensor(0.8538007140), tensor(1.2692214251), tensor(0.6998834014), tensor(1.3090615273), tensor(1.2433564663), tensor(0.8738552928), tensor(1.0037056208), tensor(1.3090480566), tensor(1.6831885576), tensor(0.9864145517), tensor(1.1865297556)]\n",
            "c:  [tensor(-0.0135113094), tensor(0.1810987890), tensor(0.1670507938), tensor(-0.0196233932), tensor(-0.0016040480), tensor(-0.0143815158), tensor(0.0059762937), tensor(0.0077517903), tensor(0.1368888915), tensor(-0.0006006556), tensor(0.0747731924), tensor(-0.0302943587), tensor(-0.0323262289), tensor(0.1227331758), tensor(0.0047171530), tensor(0.0788141787), tensor(-0.0290762335), tensor(-0.0308037587)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.4163254499,  0.4813735187,  0.1603616476,  0.4075934887,\n",
            "        -0.2013821304,  1.5817885399,  0.9769721031,  0.2846026421,\n",
            "        -0.1558742523,  1.7694714069,  1.1114403009,  1.2807283401,\n",
            "         1.0293881893,  1.5175069571,  1.1742757559,  1.4389319420,\n",
            "         0.5113766789,  0.3317363858,  0.9056062102,  0.9793628454])\n",
            "btensor.grad: tensor([-1.0005762577,  0.1085606217,  0.8507302999,  0.1963378191,\n",
            "         0.7592420578, -0.2238383293,  0.6582415104,  1.3048250675,\n",
            "         1.9440184832, -0.6482888460,  0.1664345264, -0.7874425650,\n",
            "         0.4563770592, -0.2567733526, -0.0800840259,  1.2694699764,\n",
            "         1.3324189186,  1.2653964758,  0.3152688146,  0.7088062763])\n",
            "ctensor.grad: tensor([ -1.3875195980, -23.4186592102, -22.0841484070,   1.0540370941,\n",
            "         -0.0322347581,   0.1890301555,  -0.6521568298,  -0.9678875208,\n",
            "        -16.1527042389,   0.4338461459,  -4.2728967667,   8.5699243546,\n",
            "          9.8954496384, -15.3971366882,   0.3964876831,  -4.5571255684,\n",
            "          7.6811509132,   8.6607847214])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1063.5767822266, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6519947052), tensor(0.9821590781), tensor(1.0764350891), tensor(1.1971384287), tensor(1.0899025202), tensor(1.4398926497), tensor(1.0555118322), tensor(1.1682590246), tensor(0.9095790386), tensor(1.2448246479), tensor(1.0830332041), tensor(1.5178937912), tensor(0.9827085733), tensor(1.2561190128), tensor(1.1768941879), tensor(0.9828696847), tensor(1.0689945221), tensor(0.8846640587), tensor(1.0991814137), tensor(1.0047023296)]\n",
            "b:  [tensor(0.6021447182), tensor(1.0964591503), tensor(1.3464595079), tensor(1.0114771128), tensor(1.4773027897), tensor(0.8096176982), tensor(1.2433902025), tensor(1.1649760008), tensor(1.8000606298), tensor(0.8541965485), tensor(1.2691696882), tensor(0.7003288269), tensor(1.3088374138), tensor(1.2435115576), tensor(0.8739302158), tensor(1.0030522346), tensor(1.3083591461), tensor(1.6825424433), tensor(0.9862809777), tensor(1.1861904860)]\n",
            "c:  [tensor(-0.0127888126), tensor(0.1931662261), tensor(0.1784245521), tensor(-0.0201118123), tensor(-0.0015887744), tensor(-0.0144524155), tensor(0.0062906528), tensor(0.0082080020), tensor(0.1452876627), tensor(-0.0008257236), tensor(0.0768670514), tensor(-0.0348013118), tensor(-0.0371892080), tensor(0.1307160109), tensor(0.0045098197), tensor(0.0810003728), tensor(-0.0331600383), tensor(-0.0351650193)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.4491251707,  0.4299844503,  0.1453998089,  0.3706452847,\n",
            "        -0.2793858945,  1.6417099237,  1.0130966902,  0.2500340939,\n",
            "        -0.2225093842,  1.8381356001,  1.1714012623,  1.3034213781,\n",
            "         1.0198802948,  1.6324056387,  1.2023985386,  1.5084695816,\n",
            "         0.5471194386,  0.3543913364,  0.8913481832,  0.9855675697])\n",
            "btensor.grad: tensor([-1.1546084881,  0.0031466484,  0.8609673381,  0.1229641438,\n",
            "         0.7503806949, -0.2824109793,  0.6547371149,  1.3788619041,\n",
            "         2.0507540703, -0.7916536927,  0.1034728289, -0.8908733130,\n",
            "         0.4482070506, -0.3102848530, -0.1498778462,  1.3068132401,\n",
            "         1.3779046535,  1.2921438217,  0.2670993805,  0.6784584522])\n",
            "ctensor.grad: tensor([ -1.4449930191, -24.1348762512, -22.7475185394,   0.9768383503,\n",
            "         -0.0305470526,   0.1418001056,  -0.6287179589,  -0.9124228358,\n",
            "        -16.7975559235,   0.4501359761,  -4.1877202988,   9.0139074326,\n",
            "          9.7259550095, -15.9656658173,   0.4146662951,  -4.3723869324,\n",
            "          8.1676063538,   8.7225208282])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1062.5447998047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6512521505), tensor(0.9819723964), tensor(1.0763715506), tensor(1.1969726086), tensor(1.0900846720), tensor(1.4390392303), tensor(1.0549857616), tensor(1.1681528091), tensor(0.9097270966), tensor(1.2438704967), tensor(1.0824167728), tensor(1.5172299147), tensor(0.9822054505), tensor(1.2552422285), tensor(1.1762788296), tensor(0.9820797443), tensor(1.0687018633), tensor(0.8844758272), tensor(1.0987447500), tensor(1.0042077303)]\n",
            "b:  [tensor(0.6028032303), tensor(1.0965133905), tensor(1.3460230827), tensor(1.0114552975), tensor(1.4769324064), tensor(0.8097895980), tensor(1.2430647612), tensor(1.1642477512), tensor(1.7989789248), tensor(0.8546680212), tensor(1.2691509724), tensor(0.7008282542), tensor(1.3086180687), tensor(1.2436937094), tensor(0.8740420341), tensor(1.0023791790), tensor(1.3076465130), tensor(1.6818827391), tensor(0.9861728549), tensor(1.1858677864)]\n",
            "c:  [tensor(-0.0120323906), tensor(0.2055500448), tensor(0.1900935620), tensor(-0.0205615759), tensor(-0.0015743832), tensor(-0.0145012150), tensor(0.0065918546), tensor(0.0086344881), tensor(0.1539820284), tensor(-0.0010584050), tensor(0.0788884908), tensor(-0.0395224839), tensor(-0.0419129208), tensor(0.1389645189), tensor(0.0042939805), tensor(0.0830744430), tensor(-0.0374771990), tensor(-0.0394952483)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.4850269556,  0.3733571768,  0.1269646287,  0.3316537142,\n",
            "        -0.3643812835,  1.7067326307,  1.0520684719,  0.2124363184,\n",
            "        -0.2960606813,  1.9082529545,  1.2327446938,  1.3278354406,\n",
            "         1.0063016415,  1.7536723614,  1.2307565212,  1.5799200535,\n",
            "         0.5852607489,  0.3764096498,  0.8734041452,  0.9891055822])\n",
            "btensor.grad: tensor([-1.3170363903, -0.1084876060,  0.8728007674,  0.0435252190,\n",
            "         0.7407328486, -0.3438110352,  0.6509478688,  1.4565135241,\n",
            "         2.1635153294, -0.9428988695,  0.0375171900, -0.9988956451,\n",
            "         0.4387283921, -0.3642699718, -0.2235959321,  1.3461188078,\n",
            "         1.4251489639,  1.3194795847,  0.2161917686,  0.6453438997])\n",
            "ctensor.grad: tensor([ -1.5128438473, -24.7676334381, -23.3380241394,   0.8995286822,\n",
            "         -0.0287822057,   0.0975993946,  -0.6024034619,  -0.8529728055,\n",
            "        -17.3887233734,   0.4653627872,  -4.0428733826,   9.4423446655,\n",
            "          9.4474210739, -16.4970092773,   0.4316779971,  -4.1481375694,\n",
            "          8.6343202591,   8.6604585648])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1061.4622802734, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6504898071), tensor(0.9818161726), tensor(1.0763185024), tensor(1.1968266964), tensor(1.0903123617), tensor(1.4381506443), tensor(1.0544384718), tensor(1.1680665016), tensor(0.9099145532), tensor(1.2428805828), tensor(1.0817688704), tensor(1.5165528059), tensor(0.9817109108), tensor(1.2543016672), tensor(1.1756489277), tensor(0.9812530875), tensor(1.0683884621), tensor(0.8842766881), tensor(1.0983185768), tensor(1.0037124157)]\n",
            "b:  [tensor(0.6035461426), tensor(1.0966255665), tensor(1.3455796242), tensor(1.0114756823), tensor(1.4765666723), tensor(0.8099929094), tensor(1.2427409887), tensor(1.1634787321), tensor(1.7978377342), tensor(0.8552179337), tensor(1.2691658735), tensor(0.7013832331), tensor(1.3084036112), tensor(1.2439023256), tensor(0.8741919398), tensor(1.0016851425), tensor(1.3069092035), tensor(1.6812088490), tensor(0.9860909581), tensor(1.1855623722)]\n",
            "c:  [tensor(-0.0112353014), tensor(0.2182033956), tensor(0.2020162493), tensor(-0.0209725145), tensor(-0.0015609148), tensor(-0.0145294685), tensor(0.0068784142), tensor(0.0090296324), tensor(0.1629371345), tensor(-0.0012982666), tensor(0.0807977468), tensor(-0.0444517918), tensor(-0.0464545637), tensor(0.1474535912), tensor(0.0040701819), tensor(0.0850095078), tensor(-0.0420181863), tensor(-0.0437393337)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.5245966911,  0.3124750257,  0.1061441302,  0.2919095755,\n",
            "        -0.4552719891,  1.7771205902,  1.0946943760,  0.1725345850,\n",
            "        -0.3749006987,  1.9797332287,  1.2957544327,  1.3542865515,\n",
            "         0.9890616536,  1.8812319040,  1.2598690987,  1.6533467770,\n",
            "         0.6268956661,  0.3983359337,  0.8524430394,  0.9906727076])\n",
            "btensor.grad: tensor([-1.4857670069, -0.2242377996,  0.8869658709, -0.0406633615,\n",
            "         0.7313627601, -0.4066244364,  0.6476188898,  1.5381191969,\n",
            "         2.2824969292, -1.0997809172, -0.0299181938, -1.1099978685,\n",
            "         0.4289349020, -0.4172904491, -0.2998607159,  1.3881878853,\n",
            "         1.4746878147,  1.3478600979,  0.1638219357,  0.6107240915])\n",
            "ctensor.grad: tensor([ -1.5941778421, -25.3067131042, -23.8453788757,   0.8218785524,\n",
            "         -0.0269369557,   0.0565075055,  -0.5731189251,  -0.7902895212,\n",
            "        -17.9102020264,   0.4797231555,  -3.8185162544,   9.8586177826,\n",
            "          9.0832853317, -16.9781513214,   0.4475970864,  -3.8701336384,\n",
            "          9.0819749832,   8.4881734848])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1060.3358154297, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6497058868), tensor(0.9816923738), tensor(1.0762767792), tensor(1.1967006922), tensor(1.0905880928), tensor(1.4372243881), tensor(1.0538680553), tensor(1.1680012941), tensor(0.9101436138), tensor(1.2418546677), tensor(1.0810889006), tensor(1.5158616304), tensor(0.9812270403), tensor(1.2532945871), tensor(1.1750041246), tensor(0.9803891182), tensor(1.0680522919), tensor(0.8840667605), tensor(1.0979044437), tensor(1.0032172203)]\n",
            "b:  [tensor(0.6043757200), tensor(1.0967968702), tensor(1.3451278210), tensor(1.0115401745), tensor(1.4762053490), tensor(0.8102279305), tensor(1.2424186468), tensor(1.1626671553), tensor(1.7966341972), tensor(0.8558482528), tensor(1.2692148685), tensor(0.7019948363), tensor(1.3081940413), tensor(1.2441365719), tensor(0.8743808866), tensor(1.0009685755), tensor(1.3061460257), tensor(1.6805204153), tensor(0.9860356450), tensor(1.1852748394)]\n",
            "c:  [tensor(-0.0103912735), tensor(0.2310763001), tensor(0.2141475677), tensor(-0.0213444568), tensor(-0.0015484133), tensor(-0.0145390285), tensor(0.0071486682), tensor(0.0093919393), tensor(0.1721142083), tensor(-0.0015449438), tensor(0.0825514793), tensor(-0.0495822839), tensor(-0.0507775173), tensor(0.1561542898), tensor(0.0038389331), tensor(0.0867756605), tensor(-0.0467722565), tensor(-0.0478464477)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.5678489208,  0.2475899756,  0.0833328366,  0.2520949841,\n",
            "        -0.5515054464,  1.8524646759,  1.1409329176,  0.1303986311,\n",
            "        -0.4581511021,  2.0517742634,  1.3599435091,  1.3824499846,\n",
            "         0.9677740335,  2.0142450333,  1.2895594835,  1.7279573679,\n",
            "         0.6723068953,  0.4198402166,  0.8283647299,  0.9902793169])\n",
            "btensor.grad: tensor([-1.6591498852, -0.3426648378,  0.9034969807, -0.1289392114,\n",
            "         0.7225881815, -0.4700775146,  0.6447790861,  1.6231210232,\n",
            "         2.4070096016, -1.2605835199, -0.0979607105, -1.2231698036,\n",
            "         0.4191079140, -0.4685255289, -0.3778875470,  1.4330148697,\n",
            "         1.5263080597,  1.3769428730,  0.1106427908,  0.5751376152])\n",
            "ctensor.grad: tensor([-1.6880559921e+00, -2.5745798111e+01, -2.4262638092e+01,\n",
            "         7.4388384819e-01, -2.5003012270e-02,  1.9119735807e-02,\n",
            "        -5.4050832987e-01, -7.2461348772e-01, -1.8354143143e+01,\n",
            "         4.9335443974e-01, -3.5074601173e+00,  1.0260983467e+01,\n",
            "         8.6459035873e+00, -1.7401399612e+01,  4.6249750257e-01,\n",
            "        -3.5323004723e+00,  9.5081424713e+00,  8.2142295837e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1059.1735839844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6488986015), tensor(0.9816030860), tensor(1.0762474537), tensor(1.1965943575), tensor(1.0909144878), tensor(1.4362584352), tensor(1.0532729626), tensor(1.1679583788), tensor(0.9104162455), tensor(1.2407931089), tensor(1.0803767443), tensor(1.5151557922), tensor(0.9807562232), tensor(1.2522188425), tensor(1.1743444204), tensor(0.9794878364), tensor(1.0676916838), tensor(0.8838467002), tensor(1.0975041389), tensor(1.0027234554)]\n",
            "b:  [tensor(0.6052935719), tensor(1.0970281363), tensor(1.3446668386), tensor(1.0116506815), tensor(1.4758481979), tensor(0.8104947805), tensor(1.2420976162), tensor(1.1618119478), tensor(1.7953662872), tensor(0.8565601707), tensor(1.2692978382), tensor(0.7026636600), tensor(1.3079894781), tensor(1.2443952560), tensor(0.8746094704), tensor(1.0002285242), tensor(1.3053563833), tensor(1.6798174381), tensor(0.9860071540), tensor(1.1850054264)]\n",
            "c:  [tensor(-0.0094955377), tensor(0.2441173345), tensor(0.2264405936), tensor(-0.0216773208), tensor(-0.0015369268), tensor(-0.0145321656), tensor(0.0074007600), tensor(0.0097199883), tensor(0.1814734936), tensor(-0.0017981266), tensor(0.0841071159), tensor(-0.0549044907), tensor(-0.0548480004), tensor(0.1650356501), tensor(0.0036006954), tensor(0.0883426666), tensor(-0.0517265797), tensor(-0.0517690852)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.6144803762,  0.1786259860,  0.0585910380,  0.2125880718,\n",
            "        -0.6527581215,  1.9319499731,  1.1902618408,  0.0857853889,\n",
            "        -0.5452654362,  2.1232323647,  1.4244029522,  1.4116594791,\n",
            "         0.9416725636,  2.1514616013,  1.3193043470,  1.8025143147,\n",
            "         0.7213328481,  0.4401300550,  0.8006953597,  0.9876333475])\n",
            "btensor.grad: tensor([-1.8357203007, -0.4626356363,  0.9220409393, -0.2209599614,\n",
            "         0.7143140435, -0.5337455273,  0.6420691013,  1.7104519606,\n",
            "         2.5358457565, -1.4238018990, -0.1660519838, -1.3376142979,\n",
            "         0.4091486335, -0.5174869299, -0.4571654797,  1.4801466465,\n",
            "         1.5793859959,  1.4059475660,  0.0569961071,  0.5387597084])\n",
            "ctensor.grad: tensor([-1.7914716005e+00, -2.6082054138e+01, -2.4586053848e+01,\n",
            "         6.6572785378e-01, -2.2972865030e-02, -1.3725747354e-02,\n",
            "        -5.0418323278e-01, -6.5609788895e-01, -1.8718582153e+01,\n",
            "         5.0636565685e-01, -3.1112730503e+00,  1.0644415855e+01,\n",
            "         8.1409645081e+00, -1.7762733459e+01,  4.7647547722e-01,\n",
            "        -3.1340088844e+00,  9.9086446762e+00,  7.8452739716e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1057.9820556641, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6480665207), tensor(0.9815503359), tensor(1.0762314796), tensor(1.1965074539), tensor(1.0912938118), tensor(1.4352511168), tensor(1.0526520014), tensor(1.1679391861), tensor(0.9107341170), tensor(1.2396966219), tensor(1.0796326399), tensor(1.5144352913), tensor(0.9803012609), tensor(1.2510731220), tensor(1.1736701727), tensor(0.9785500169), tensor(1.0673048496), tensor(0.8836175203), tensor(1.0971196890), tensor(1.0022321939)]\n",
            "b:  [tensor(0.6063005924), tensor(1.0973197222), tensor(1.3441958427), tensor(1.0118088722), tensor(1.4754949808), tensor(0.8107933998), tensor(1.2417781353), tensor(1.1609125137), tensor(1.7940324545), tensor(0.8573541045), tensor(1.2694146633), tensor(0.7033899426), tensor(1.3077900410), tensor(1.2446771860), tensor(0.8748781085), tensor(0.9994640350), tensor(1.3045397997), tensor(1.6791005135), tensor(0.9860055447), tensor(1.1847546101)]\n",
            "c:  [tensor(-0.0085450830), tensor(0.2572751641), tensor(0.2388479859), tensor(-0.0219711829), tensor(-0.0015265055), tensor(-0.0145115806), tensor(0.0076326937), tensor(0.0100125195), tensor(0.1909758449), tensor(-0.0020575619), tensor(0.0854250640), tensor(-0.0604058616), tensor(-0.0586342067), tensor(0.1740658432), tensor(0.0033558635), tensor(0.0896812379), tensor(-0.0568660647), tensor(-0.0554635674)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.6640869379,  0.1054997742,  0.0319532752,  0.1737289429,\n",
            "        -0.7586555481,  2.0146031380,  1.2420033216,  0.0383801460,\n",
            "        -0.6356843710,  2.1928999424,  1.4881141186,  1.4411176443,\n",
            "         0.9099310040,  2.2915027142,  1.3485100269,  1.8756760359,\n",
            "         0.7736803293,  0.4583210945,  0.7689035535,  0.9824657440])\n",
            "btensor.grad: tensor([-2.0139884949, -0.5830631256,  0.9421061873, -0.3164225817,\n",
            "         0.7063389421, -0.5972852707,  0.6390051842,  1.7988591194,\n",
            "         2.6675977707, -1.5879250765, -0.2337268591, -1.4525804520,\n",
            "         0.3988571763, -0.5638073683, -0.5372357368,  1.5290215015,\n",
            "         1.6332007647,  1.4339593649,  0.0031710863,  0.5016897917])\n",
            "ctensor.grad: tensor([-1.9009090662e+00, -2.6315660477e+01, -2.4814786911e+01,\n",
            "         5.8772504330e-01, -2.0842729136e-02, -4.1169606149e-02,\n",
            "        -4.6386775374e-01, -5.8506178856e-01, -1.9004686356e+01,\n",
            "         5.1887071133e-01, -2.6358988285e+00,  1.1002737999e+01,\n",
            "         7.5724091530e+00, -1.8060373306e+01,  4.8966389894e-01,\n",
            "        -2.6771368980e+00,  1.0278970718e+01,  7.3889651299e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1056.7703857422, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6472084522), tensor(0.9815362096), tensor(1.0762296915), tensor(1.1964395046), tensor(1.0917282104), tensor(1.4342014790), tensor(1.0520042181), tensor(1.1679452658), tensor(0.9110984802), tensor(1.2385667562), tensor(1.0788576603), tensor(1.5137002468), tensor(0.9798653722), tensor(1.2498565912), tensor(1.1729818583), tensor(0.9775769114), tensor(1.0668903589), tensor(0.8833807111), tensor(1.0967534781), tensor(1.0017448664)]\n",
            "b:  [tensor(0.6073967814), tensor(1.0976711512), tensor(1.3437142372), tensor(1.0120162964), tensor(1.4751458168), tensor(0.8111236095), tensor(1.2414605618), tensor(1.1599689722), tensor(1.7926321030), tensor(0.8582298160), tensor(1.2695648670), tensor(0.7041735649), tensor(1.3075960875), tensor(1.2449808121), tensor(0.8751869202), tensor(0.9986745119), tensor(1.3036962748), tensor(1.6783704758), tensor(0.9860308170), tensor(1.1845226288)]\n",
            "c:  [tensor(-0.0075385761), tensor(0.2704998851), tensor(0.2513233423), tensor(-0.0222263206), tensor(-0.0015171989), tensor(-0.0144803599), tensor(0.0078424206), tensor(0.0102685448), tensor(0.2005835921), tensor(-0.0023230638), tensor(0.0864697918), tensor(-0.0660706908), tensor(-0.0621064976), tensor(0.1832128614), tensor(0.0031047470), tensor(0.0907637626), tensor(-0.0621734783), tensor(-0.0588908754)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.7162555456,  0.0282471627,  0.0035291314,  0.1359091997,\n",
            "        -0.8686977625,  2.0993881226,  1.2954649925, -0.0120793581,\n",
            "        -0.7287263870,  2.2596302032,  1.5500776768,  1.4700162411,\n",
            "         0.8718028665,  2.4330072403,  1.3766328096,  1.9461607933,\n",
            "         0.8290140629,  0.4735851288,  0.7325311303,  0.9746165276])\n",
            "btensor.grad: tensor([-2.1923952103, -0.7028051615,  0.9631768465, -0.4149543643,\n",
            "         0.6984394789, -0.6603873968,  0.6350908875,  1.8870466948,\n",
            "         2.8007755280, -1.7513917685, -0.3004969358, -1.5672903061,\n",
            "         0.3880261481, -0.6071661711, -0.6176054478,  1.5790416002,\n",
            "         1.6870400906,  1.4600433111, -0.0505226851,  0.4640725851])\n",
            "ctensor.grad: tensor([-2.0130138397e+00, -2.6449449539e+01, -2.4950704575e+01,\n",
            "         5.1027607918e-01, -1.8613079563e-02, -6.2441382557e-02,\n",
            "        -4.1945451498e-01, -5.1205050945e-01, -1.9215497971e+01,\n",
            "         5.3100383282e-01, -2.0894508362e+00,  1.1329652786e+01,\n",
            "         6.9445805550e+00, -1.8294023514e+01,  5.0223302841e-01,\n",
            "        -2.1650447845e+00,  1.0614826202e+01,  6.8546152115e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1055.5460205078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6463232040), tensor(0.9815626740), tensor(1.0762429237), tensor(1.1963896751), tensor(1.0922193527), tensor(1.4331088066), tensor(1.0513292551), tensor(1.1679781675), tensor(0.9115102887), tensor(1.2374055386), tensor(1.0780529976), tensor(1.5129514933), tensor(0.9794520140), tensor(1.2485692501), tensor(1.1722801924), tensor(0.9765704870), tensor(1.0664469004), tensor(0.8831381202), tensor(1.0964078903), tensor(1.0012627840)]\n",
            "b:  [tensor(0.6085814834), tensor(1.0980814695), tensor(1.3432219028), tensor(1.0122743845), tensor(1.4748005867), tensor(0.8114849925), tensor(1.2411456108), tensor(1.1589820385), tensor(1.7911651134), tensor(0.8591861129), tensor(1.2697478533), tensor(0.7050140500), tensor(1.3074078560), tensor(1.2453044653), tensor(0.8755357862), tensor(0.9978596568), tensor(1.3028261662), tensor(1.6776288748), tensor(0.9860826731), tensor(1.1843096018)]\n",
            "c:  [tensor(-0.0064761522), tensor(0.2837443054), tensor(0.2638223767), tensor(-0.0224432293), tensor(-0.0015090548), tensor(-0.0144419000), tensor(0.0080279298), tensor(0.0104874512), tensor(0.2102612108), tensor(-0.0025945255), tensor(0.0872102231), tensor(-0.0718803853), tensor(-0.0652381480), tensor(0.1924452037), tensor(0.0028475567), tensor(0.0915647745), tensor(-0.0676296428), tensor(-0.0620173700)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.7705883980, -0.0529223382, -0.0264689028,  0.0995808840,\n",
            "        -0.9822146893,  2.1852760315,  1.3499972820, -0.0657616854,\n",
            "        -0.8235679865,  2.3224151134,  1.6093910933,  1.4975776672,\n",
            "         0.8266921043,  2.5746898651,  1.4032249451,  2.0128042698,\n",
            "         0.8870339990,  0.4852210283,  0.6912539005,  0.9641054869])\n",
            "btensor.grad: tensor([-2.3693466187, -0.8207087517,  0.9847425818, -0.5161082745,\n",
            "         0.6904475689, -0.7227133512,  0.6298603415,  1.9737546444,\n",
            "         2.9338982105, -1.9125880003, -0.3658699989, -1.6809457541,\n",
            "         0.3764854074, -0.6472722292, -0.6977360249,  1.6296763420,\n",
            "         1.7402725220,  1.4833120108, -0.1037290096,  0.4261078835])\n",
            "ctensor.grad: tensor([-2.1248478889e+00, -2.6488853455e+01, -2.4998094559e+01,\n",
            "         4.3381884694e-01, -1.6288213432e-02, -7.6919056475e-02,\n",
            "        -3.7101799250e-01, -4.3781200051e-01, -1.9355239868e+01,\n",
            "         5.4292345047e-01, -1.4808574915e+00,  1.1619388580e+01,\n",
            "         6.2633080482e+00, -1.8464694977e+01,  5.1438045502e-01,\n",
            "        -1.6020261049e+00,  1.0912330627e+01,  6.2529854774e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1054.3160400391, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6454098225), tensor(0.9816315174), tensor(1.0762717724), tensor(1.1963570118), tensor(1.0927685499), tensor(1.4319732189), tensor(1.0506267548), tensor(1.1680395603), tensor(0.9119699001), tensor(1.2362153530), tensor(1.0772204399), tensor(1.5121899843), tensor(0.9790649414), tensor(1.2472115755), tensor(1.1715662479), tensor(0.9755331874), tensor(1.0659731627), tensor(0.8828917742), tensor(1.0960854292), tensor(1.0007872581)]\n",
            "b:  [tensor(0.6098530889), tensor(1.0985492468), tensor(1.3427187204), tensor(1.0125840902), tensor(1.4744595289), tensor(0.8118769526), tensor(1.2408341169), tensor(1.1579531431), tensor(1.7896323204), tensor(0.8602210879), tensor(1.2699625492), tensor(0.7059104443), tensor(1.3072258234), tensor(1.2456463575), tensor(0.8759243488), tensor(0.9970194697), tensor(1.3019299507), tensor(1.6768773794), tensor(0.9861606956), tensor(1.1841155291)]\n",
            "c:  [tensor(-0.0053591626), tensor(0.2969648540), tensor(0.2763039768), tensor(-0.0226226281), tensor(-0.0015021171), tensor(-0.0143998200), tensor(0.0081873322), tensor(0.0106690712), tensor(0.2199755907), tensor(-0.0028719301), tensor(0.0876199156), tensor(-0.0778138712), tensor(-0.0680060536), tensor(0.2017324865), tensor(0.0025843973), tensor(0.0920615047), tensor(-0.0732136592), tensor(-0.0648151338)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.8267228603, -0.1376478076, -0.0577157736,  0.0652614832,\n",
            "        -1.0984113216,  2.2712492943,  1.4050420523, -0.1227549314,\n",
            "        -0.9192330241,  2.3803985119,  1.6652312279,  1.5230829716,\n",
            "         0.7741503716,  2.7153887749,  1.4279484749,  2.0746192932,\n",
            "         0.9474584460,  0.4926736951,  0.6448810697,  0.9511177540])\n",
            "btensor.grad: tensor([-2.5432548523, -0.9355973005,  1.0063292980, -0.6193538904,\n",
            "         0.6822241545, -0.7839521170,  0.6228773594,  2.0577852726,\n",
            "         3.0655150414, -2.0699303150, -0.4293353558, -1.7927570343,\n",
            "         0.3640899658, -0.6838717461, -0.7770693898,  1.6804150343,\n",
            "         1.7923494577,  1.5029560328, -0.1560735703,  0.3880487680])\n",
            "ctensor.grad: tensor([-2.2339789867e+00, -2.6441102982e+01, -2.4963199615e+01,\n",
            "         3.5879817605e-01, -1.3875329867e-02, -8.4160633385e-02,\n",
            "        -3.1880497932e-01, -3.6324077845e-01, -1.9428762436e+01,\n",
            "         5.5480927229e-01, -8.1938797235e-01,  1.1866968155e+01,\n",
            "         5.5358171463e+00, -1.8574560165e+01,  5.2631896734e-01,\n",
            "        -9.9346184731e-01,  1.1168036461e+01,  5.5955233574e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1053.0870361328, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6444677114), tensor(0.9817442298), tensor(1.0763167143), tensor(1.1963403225), tensor(1.0933767557), tensor(1.4307950735), tensor(1.0498967171), tensor(1.1681311131), tensor(0.9124772549), tensor(1.2349989414), tensor(1.0763620138), tensor(1.5114170313), tensor(0.9787079692), tensor(1.2457845211), tensor(1.1708409786), tensor(0.9744678140), tensor(1.0654681921), tensor(0.8826439977), tensor(1.0957887173), tensor(1.0003192425)]\n",
            "b:  [tensor(0.6112093925), tensor(1.0990724564), tensor(1.3422049284), tensor(1.0129461288), tensor(1.4741226435), tensor(0.8122988343), tensor(1.2405272722), tensor(1.1568840742), tensor(1.7880351543), tensor(0.8613320589), tensor(1.2702077627), tensor(0.7068614364), tensor(1.3070504665), tensor(1.2460047007), tensor(0.8763518929), tensor(0.9961540699), tensor(1.3010085821), tensor(1.6761182547), tensor(0.9862642884), tensor(1.1839404106)]\n",
            "c:  [tensor(-0.0041899234), tensor(0.3101224005), tensor(0.2887309790), tensor(-0.0227654465), tensor(-0.0014964254), tensor(-0.0143578602), tensor(0.0083189402), tensor(0.0108137224), tensor(0.2296963334), tensor(-0.0031553584), tensor(0.0876771212), tensor(-0.0838479623), tensor(-0.0703913197), tensor(0.2110458463), tensor(0.0023152626), tensor(0.0922342241), tensor(-0.0789031312), tensor(-0.0672621056)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.8843206167, -0.2254234850, -0.0898027420,  0.0334897041,\n",
            "        -1.2163723707,  2.3563508987,  1.4601114988, -0.1830404997,\n",
            "        -1.0146639347,  2.4328765869,  1.7168891430,  1.5458759069,\n",
            "         0.7138965726,  2.8540754318,  1.4505910873,  2.1307988167,\n",
            "         1.0100436211,  0.4955314994,  0.5933567286,  0.9359874725])\n",
            "btensor.grad: tensor([-2.7125997543, -1.0463296175,  1.0274765491, -0.7241331339,\n",
            "         0.6736802459, -0.8438190222,  0.6137420535,  2.1380362511,\n",
            "         3.1942427158, -2.2218906879, -0.4904097319, -1.9019697905,\n",
            "         0.3507349193, -0.7167605162, -0.8550472260,  1.7308249474,\n",
            "         1.8427999020,  1.5182578564, -0.2071747780,  0.3501882553])\n",
            "ctensor.grad: tensor([-2.3384780884e+00, -2.6315114975e+01, -2.4854017258e+01,\n",
            "         2.8563743830e-01, -1.1383538134e-02, -8.3920091391e-02,\n",
            "        -2.6321536303e-01, -2.8930258751e-01, -1.9441490173e+01,\n",
            "         5.6685656309e-01, -1.1441662908e-01,  1.2068184853e+01,\n",
            "         4.7705345154e+00, -1.8626710892e+01,  5.3826934099e-01,\n",
            "        -3.4544566274e-01,  1.1378938675e+01,  4.8939442635e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1051.8627929688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6434961557), tensor(0.9819020629), tensor(1.0763778687), tensor(1.1963379383), tensor(1.0940443277), tensor(1.4295752048), tensor(1.0491392612), tensor(1.1682543755), tensor(0.9130316377), tensor(1.2337592840), tensor(1.0754801035), tensor(1.5106343031), tensor(0.9783850908), tensor(1.2442896366), tensor(1.1701054573), tensor(0.9733774662), tensor(1.0649309158), tensor(0.8823972344), tensor(1.0955203772), tensor(0.9998596311)]\n",
            "b:  [tensor(0.6126473546), tensor(1.0996483564), tensor(1.3416810036), tensor(1.0133610964), tensor(1.4737902880), tensor(0.8127498627), tensor(1.2402262688), tensor(1.1557773352), tensor(1.7863757610), tensor(0.8625155687), tensor(1.2704820633), tensor(0.7078653574), tensor(1.3068822622), tensor(1.2463775873), tensor(0.8768174648), tensor(0.9952638149), tensor(1.3000630140), tensor(1.6753540039), tensor(0.9863926172), tensor(1.1837840080)]\n",
            "c:  [tensor(-0.0029714871), tensor(0.3231828511), tensor(0.3010707498), tensor(-0.0228728037), tensor(-0.0014920139), tensor(-0.0143197859), tensor(0.0084213289), tensor(0.0109222056), tensor(0.2393958420), tensor(-0.0034449932), tensor(0.0873647258), tensor(-0.0899578035), tensor(-0.0723796040), tensor(0.2203584909), tensor(0.0020400381), tensor(0.0920667350), tensor(-0.0846743286), tensor(-0.0693419129)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.9430851936, -0.3156303167, -0.1222669482,  0.0048221350,\n",
            "        -1.3351056576,  2.4396686554,  1.5148277283, -0.2465277910,\n",
            "        -1.1087300777,  2.4793002605,  1.7637689114,  1.5653756857,\n",
            "         0.6457794905,  2.9898591042,  1.4710376263,  2.1807072163,\n",
            "         1.0745792389,  0.4935329556,  0.5367514491,  0.9191650152])\n",
            "btensor.grad: tensor([-2.8759644032, -1.1518450975,  1.0477697849, -0.8298578858,\n",
            "         0.6647539735, -0.9020649195,  0.6020870805,  2.2134919167,\n",
            "         3.3187787533, -2.3670568466, -0.5486407280, -2.0078961849,\n",
            "         0.3363290727, -0.7457903624, -0.9311381578,  1.7804963589,\n",
            "         1.8912380934,  1.5285958052, -0.2566782236,  0.3128321171])\n",
            "ctensor.grad: tensor([-2.4368722439e+00, -2.6120883942e+01, -2.4679552078e+01,\n",
            "         2.1471488476e-01, -8.8228657842e-03, -7.6147846878e-02,\n",
            "        -2.0477777719e-01, -2.1696653962e-01, -1.9399028778e+01,\n",
            "         5.7926964760e-01,  6.2479126453e-01,  1.2219674110e+01,\n",
            "         3.9765753746e+00, -1.8625288010e+01,  5.5044877529e-01,\n",
            "         3.3498442173e-01,  1.1542396545e+01,  4.1596174240e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1050.6492919922, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6424947977), tensor(0.9821058512), tensor(1.0764551163), tensor(1.1963480711), tensor(1.0947711468), tensor(1.4283150434), tensor(1.0483547449), tensor(1.1684108973), tensor(0.9136317968), tensor(1.2324995995), tensor(1.0745774508), tensor(1.5098437071), tensor(0.9781001806), tensor(1.2427285910), tensor(1.1693607569), tensor(0.9722654819), tensor(1.0643604994), tensor(0.8821539283), tensor(1.0952827930), tensor(0.9994090199)]\n",
            "b:  [tensor(0.6141633987), tensor(1.1002739668), tensor(1.3411475420), tensor(1.0138291121), tensor(1.4734625816), tensor(0.8132290840), tensor(1.2399324179), tensor(1.1546356678), tensor(1.7846567631), tensor(0.8637676239), tensor(1.2707839012), tensor(0.7089203000), tensor(1.3067218065), tensor(1.2467629910), tensor(0.8773198724), tensor(0.9943493009), tensor(1.2990943193), tensor(1.6745872498), tensor(0.9865447283), tensor(1.1836458445)]\n",
            "c:  [tensor(-0.0017073796), tensor(0.3361173272), tensor(0.3132955432), tensor(-0.0229459796), tensor(-0.0014889122), tensor(-0.0142892916), tensor(0.0084933909), tensor(0.0109957773), tensor(0.2490493804), tensor(-0.0037411223), tensor(0.0866700485), tensor(-0.0961173102), tensor(-0.0739615113), tensor(0.2296458930), tensor(0.0017585020), tensor(0.0915464461), tensor(-0.0905025154), tensor(-0.0710438117)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.0027422905, -0.4075441360, -0.1545874774, -0.0202018023,\n",
            "        -1.4535422325,  2.5203752518,  1.5689196587, -0.3130358458,\n",
            "        -1.2002665997,  2.5192847252,  1.8053812981,  1.5810866356,\n",
            "         0.5698093772,  3.1220083237,  1.4892956018,  2.2239112854,\n",
            "         1.1408836842,  0.4865533113,  0.4752695560,  0.9012138844])\n",
            "btensor.grad: tensor([-3.0320632458, -1.2511498928,  1.0668265820, -0.9359200001,\n",
            "         0.6554400325, -0.9584949017,  0.5875829458,  2.2832746506,\n",
            "         3.4379260540, -2.5041527748, -0.6036140919, -2.1099228859,\n",
            "         0.3208078146, -0.7708585262, -1.0048484802,  1.8290860653,\n",
            "         1.9373618364,  1.5334779024, -0.3042516708,  0.2763046026])\n",
            "ctensor.grad: tensor([-2.5282149315e+00, -2.5868965149e+01, -2.4449611664e+01,\n",
            "         1.4635163546e-01, -6.2034381554e-03, -6.0987669975e-02,\n",
            "        -1.4412437379e-01, -1.4714343846e-01, -1.9307083130e+01,\n",
            "         5.9225809574e-01,  1.3893491030e+00,  1.2319011688e+01,\n",
            "         3.1638140678e+00, -1.8574810028e+01,  5.6307220459e-01,\n",
            "         1.0405836105e+00,  1.1656365395e+01,  3.4037952423e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1049.4499511719, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6414632797), tensor(0.9823560715), tensor(1.0765482187), tensor(1.1963685751), tensor(1.0955564976), tensor(1.4270161390), tensor(1.0475436449), tensor(1.1686021090), tensor(0.9142758250), tensor(1.2312233448), tensor(1.0736567974), tensor(1.5090473890), tensor(0.9778571129), tensor(1.2411036491), tensor(1.1686080694), tensor(0.9711354375), tensor(1.0637561083), tensor(0.8819166422), tensor(1.0950782299), tensor(0.9989676476)]\n",
            "b:  [tensor(0.6157532930), tensor(1.1009457111), tensor(1.3406053782), tensor(1.0143499374), tensor(1.4731397629), tensor(0.8137355447), tensor(1.2396475077), tensor(1.1534624100), tensor(1.7828814983), tensor(0.8650836349), tensor(1.2711113691), tensor(0.7100240588), tensor(1.3065696955), tensor(1.2471590042), tensor(0.8778577447), tensor(0.9934111834), tensor(1.2981038094), tensor(1.6738209724), tensor(0.9867195487), tensor(1.1835254431)]\n",
            "c:  [tensor(-0.0004014205), tensor(0.3489024043), tensor(0.3253826797), tensor(-0.0229863804), tensor(-0.0014871449), tensor(-0.0142699145), tensor(0.0085343691), tensor(0.0110360887), tensor(0.2586351037), tensor(-0.0040441360), tensor(0.0855848566), tensor(-0.1022994891), tensor(-0.0751324818), tensor(0.2388860285), tensor(0.0014703308), tensor(0.0906646624), tensor(-0.0963621214), tensor(-0.0723623484)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.0630493164, -0.5003983378, -0.1862378120, -0.0410840511,\n",
            "        -1.5706121922,  2.5977191925,  1.6222065687, -0.3823306561,\n",
            "        -1.2881157398,  2.5525844097,  1.8413416147,  1.5925972462,\n",
            "         0.4860979915,  3.2499198914,  1.5054565668,  2.2601432800,\n",
            "         1.2088122368,  0.4745888710,  0.4091955423,  0.8827604055])\n",
            "btensor.grad: tensor([-3.1797716618, -1.3433746099,  1.0842975378, -1.0417386293,\n",
            "         0.6457493305, -1.0129677057,  0.5699295998,  2.3466091156,\n",
            "         3.5505914688, -2.6320698261, -0.6549667120, -2.2075240612,\n",
            "         0.3041231930, -0.7919274569, -1.0757470131,  1.8762872219,\n",
            "         1.9809306860,  1.5325258970, -0.3496166468,  0.2409017086])\n",
            "ctensor.grad: tensor([-2.6119179726e+00, -2.5570182800e+01, -2.4174287796e+01,\n",
            "         8.0802135170e-02, -3.5345938522e-03, -3.8753386587e-02,\n",
            "        -8.1956066191e-02, -8.0622822046e-02, -1.9171442032e+01,\n",
            "         6.0602742434e-01,  2.1703877449e+00,  1.2364362717e+01,\n",
            "         2.3419468403e+00, -1.8480285645e+01,  5.7634240389e-01,\n",
            "         1.7635711432e+00,  1.1719217300e+01,  2.6370735168e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1048.2667236328, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6404013634), tensor(0.9826527834), tensor(1.0766565800), tensor(1.1963973045), tensor(1.0963990688), tensor(1.4256806374), tensor(1.0467063189), tensor(1.1688292027), tensor(0.9149613976), tensor(1.2299338579), tensor(1.0727211237), tensor(1.5082476139), tensor(0.9776596427), tensor(1.2394170761), tensor(1.1678482294), tensor(0.9699907899), tensor(1.0631170273), tensor(0.8816877604), tensor(1.0949087143), tensor(0.9985353947)]\n",
            "b:  [tensor(0.6174123883), tensor(1.1016595364), tensor(1.3400554657), tensor(1.0149233341), tensor(1.4728218317), tensor(0.8142682314), tensor(1.2393730879), tensor(1.1522610188), tensor(1.7810535431), tensor(0.8664585948), tensor(1.2714625597), tensor(0.7111741900), tensor(1.3064265251), tensor(1.2475634813), tensor(0.8784294724), tensor(0.9924502373), tensor(1.2970929146), tensor(1.6730582714), tensor(0.9869158268), tensor(1.1834219694)]\n",
            "c:  [tensor(0.0009424859), tensor(0.3615199327), tensor(0.3373144269), tensor(-0.0229955073), tensor(-0.0014867326), tensor(-0.0142649598), tensor(0.0085438769), tensor(0.0110451048), tensor(0.2681339085), tensor(-0.0043545263), tensor(0.0841051117), tensor(-0.1084768474), tensor(-0.0758929700), tensor(0.2480594069), tensor(0.0011751035), tensor(0.0894164816), tensor(-0.1022270918), tensor(-0.0732972398)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.1237878799, -0.5933713913, -0.2166610956, -0.0573716164,\n",
            "        -1.6852320433,  2.6710324287,  1.6746155024, -0.4541056156,\n",
            "        -1.3711525202,  2.5790863037,  1.8713624477,  1.5995815992,\n",
            "         0.3948810697,  3.3731322289,  1.5197155476,  2.2893249989,\n",
            "         1.2782514095,  0.4577419162,  0.3389247060,  0.8644890785])\n",
            "btensor.grad: tensor([-3.3181347847, -1.4277502298,  1.0998744965, -1.1467304230,\n",
            "         0.6357468367, -1.0653853416,  0.5488678217,  2.4028754234,\n",
            "         3.6558134556, -2.7498695850, -0.7024004459, -2.3002619743,\n",
            "         0.2862445414, -0.8089950085, -1.1434520483,  1.9218375683,\n",
            "         2.0217819214,  1.5255005360, -0.3925323486,  0.2069276571])\n",
            "ctensor.grad: tensor([-2.6878125668e+00, -2.5235027313e+01, -2.3863481522e+01,\n",
            "         1.8252693117e-02, -8.2436908269e-04, -9.9094472826e-03,\n",
            "        -1.9016342238e-02, -1.8031967804e-02, -1.8997596741e+01,\n",
            "         6.2078028917e-01,  2.9594879150e+00,  1.2354719162e+01,\n",
            "         1.5209810734e+00, -1.8346754074e+01,  5.9045463800e-01,\n",
            "         2.4963636398e+00,  1.1729936600e+01,  1.8697898388e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1047.1021728516, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6393089294), tensor(0.9829956293), tensor(1.0767792463), tensor(1.1964316368), tensor(1.0972971916), tensor(1.4243108034), tensor(1.0458432436), tensor(1.1690932512), tensor(0.9156855345), tensor(1.2286344767), tensor(1.0717735291), tensor(1.5074467659), tensor(0.9775114059), tensor(1.2376713753), tensor(1.1670820713), tensor(0.9688350558), tensor(1.0624424219), tensor(0.8814696670), tensor(1.0947762728), tensor(0.9981118441)]\n",
            "b:  [tensor(0.6191355586), tensor(1.1024113894), tensor(1.3394988775), tensor(1.0155484676), tensor(1.4725090265), tensor(0.8148260713), tensor(1.2391110659), tensor(1.1510351896), tensor(1.7791771889), tensor(0.8678870201), tensor(1.2718354464), tensor(0.7123680711), tensor(1.3062930107), tensor(1.2479745150), tensor(0.8790333271), tensor(0.9914674759), tensor(1.2960630655), tensor(1.6723021269), tensor(0.9871322513), tensor(1.1833347082)]\n",
            "c:  [tensor(0.0023204661), tensor(0.3739566803), tensor(0.3490777612), tensor(-0.0229749177), tensor(-0.0014876932), tensor(-0.0142774405), tensor(0.0085219070), tensor(0.0110250060), tensor(0.2775294781), tensor(-0.0046728803), tensor(0.0822309256), tensor(-0.1146216467), tensor(-0.0762481391), tensor(0.2571491003), tensor(0.0008723093), tensor(0.0878008977), tensor(-0.1080710441), tensor(-0.0738529861)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.1847486496, -0.6856590509, -0.2453452647, -0.0686831474,\n",
            "        -1.7963585854,  2.7397418022,  1.7261313200, -0.5280289650,\n",
            "        -1.4483275414,  2.5987887383,  1.8952358961,  1.6017887592,\n",
            "         0.2964664698,  3.4912974834,  1.5323171616,  2.3114981651,\n",
            "         1.3491029739,  0.4361734390,  0.2648959160,  0.8470757008])\n",
            "btensor.grad: tensor([-3.4463803768, -1.5036642551,  1.1132873297, -1.2503569126,\n",
            "         0.6254978180, -1.1157170534,  0.5241627693,  2.4515562057,\n",
            "         3.7527413368, -2.8567981720, -0.7456667423, -2.3878116608,\n",
            "         0.2671461403, -0.8221306801, -1.2076644897,  1.9655010700,\n",
            "         2.0597918034,  1.5122903585, -0.4328358173,  0.1746293306])\n",
            "ctensor.grad: tensor([-2.7559599876e+00, -2.4873516083e+01, -2.3526681900e+01,\n",
            "        -4.1177645326e-02,  1.9210369792e-03,  2.4960840121e-02,\n",
            "         4.3940111995e-02,  4.0197730064e-02, -1.8791116714e+01,\n",
            "         6.3670790195e-01,  3.7483699322e+00,  1.2289591789e+01,\n",
            "         7.1033102274e-01, -1.8179409027e+01,  6.0558825731e-01,\n",
            "         3.2311611176e+00,  1.1687902451e+01,  1.1114978790e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1045.9564208984, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6381860971), tensor(0.9833838344), tensor(1.0769151449), tensor(1.1964689493), tensor(1.0982487202), tensor(1.4229091406), tensor(1.0449548960), tensor(1.1693950891), tensor(0.9164448977), tensor(1.2273285389), tensor(1.0708171129), tensor(1.5066472292), tensor(0.9774158001), tensor(1.2358692884), tensor(1.1663103104), tensor(0.9676716328), tensor(1.0617318153), tensor(0.8812646270), tensor(1.0946824551), tensor(0.9976962209)]\n",
            "b:  [tensor(0.6209174991), tensor(1.1031967402), tensor(1.3389366865), tensor(1.0162245035), tensor(1.4722014666), tensor(0.8154080510), tensor(1.2388632298), tensor(1.1497890949), tensor(1.7772568464), tensor(0.8693631887), tensor(1.2722277641), tensor(0.7136030197), tensor(1.3061696291), tensor(1.2483901978), tensor(0.8796674013), tensor(0.9904639125), tensor(1.2950155735), tensor(1.6715556383), tensor(0.9873674512), tensor(1.1832625866)]\n",
            "c:  [tensor(0.0037288188), tensor(0.3862040639), tensor(0.3606640697), tensor(-0.0229262020), tensor(-0.0014900418), tensor(-0.0143100312), tensor(0.0084688226), tensor(0.0109780831), tensor(0.2868079841), tensor(-0.0049998760), tensor(0.0799663514), tensor(-0.1207061931), tensor(-0.0762077942), tensor(0.2661406696), tensor(0.0005613538), tensor(0.0858206823), tensor(-0.1138675734), tensor(-0.0740386844)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.2457418442, -0.7764682770, -0.2717888653, -0.0747324228,\n",
            "        -1.9029884338,  2.8033528328,  1.7768111229, -0.6037298441,\n",
            "        -1.5186805725,  2.6117949486,  1.9128406048,  1.5990495682,\n",
            "         0.1912473440,  3.6041650772,  1.5435761213,  2.3268585205,\n",
            "         1.4212871790,  0.4101267457,  0.1876339316,  0.8311980963])\n",
            "btensor.grad: tensor([-3.5639114380, -1.5706001520,  1.1243001223, -1.3521120548,\n",
            "         0.6151248813, -1.1639649868,  0.4956269264,  2.4922852516,\n",
            "         3.8406631947, -2.9522941113, -0.7845868468, -2.4699342251,\n",
            "         0.2468225062, -0.8314234018, -1.2681514025,  2.0070946217,\n",
            "         2.0948982239,  1.4929132462, -0.4704068899,  0.1442464590])\n",
            "ctensor.grad: tensor([-2.8167054653e+00, -2.4494764328e+01, -2.3172622681e+01,\n",
            "        -9.7430475056e-02,  4.6974490397e-03,  6.5181508660e-02,\n",
            "         1.0616982728e-01,  9.3845136464e-02, -1.8557022095e+01,\n",
            "         6.5399134159e-01,  4.5291485786e+00,  1.2169090271e+01,\n",
            "        -8.0694168806e-02, -1.7983123779e+01,  6.2191110849e-01,\n",
            "         3.9604241848e+00,  1.1593060493e+01,  3.7139117718e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1044.8310546875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6370327473), tensor(0.9838163853), tensor(1.0770629644), tensor(1.1965066195), tensor(1.0992507935), tensor(1.4214783907), tensor(1.0440415144), tensor(1.1697355509), tensor(0.9172356129), tensor(1.2260193825), tensor(1.0698550940), tensor(1.5058516264), tensor(0.9773759842), tensor(1.2340135574), tensor(1.1655334234), tensor(0.9665037990), tensor(1.0609844923), tensor(0.8810746670), tensor(1.0946285725), tensor(0.9972874522)]\n",
            "b:  [tensor(0.6227526665), tensor(1.1040108204), tensor(1.3383703232), tensor(1.0169502497), tensor(1.4718991518), tensor(0.8160131574), tensor(1.2386317253), tensor(1.1485266685), tensor(1.7752974033), tensor(0.8708811998), tensor(1.2726372480), tensor(0.7148762345), tensor(1.3060569763), tensor(1.2488087416), tensor(0.8803297877), tensor(0.9894406796), tensor(1.2939519882), tensor(1.6708219051), tensor(0.9876200557), tensor(1.1832046509)]\n",
            "c:  [tensor(0.0051640719), tensor(0.3982574940), tensor(0.3720686138), tensor(-0.0228509512), tensor(-0.0014937933), tensor(-0.0143650379), tensor(0.0083853407), tensor(0.0109066321), tensor(0.2959581017), tensor(-0.0053362744), tensor(0.0773193836), tensor(-0.1267030686), tensor(-0.0757860541), tensor(0.2750220597), tensor(0.0002415667), tensor(0.0834823623), tensor(-0.1195904240), tensor(-0.0738676488)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.3065834045, -0.8650689721, -0.2955466509, -0.0753289461,\n",
            "        -2.0042109489,  2.8614578247,  1.8267452717, -0.6808242798,\n",
            "        -1.5813817978,  2.6182701588,  1.9241080284,  1.5912626982,\n",
            "         0.0796492100,  3.7115693092,  1.5538309813,  2.3356909752,\n",
            "         1.4947326183,  0.3798699975,  0.1076736450,  0.8174829483])\n",
            "btensor.grad: tensor([-3.6703026295, -1.6282026768,  1.1327071190, -1.4515171051,\n",
            "         0.6047484875, -1.2101818323,  0.4630945623,  2.5248074532,\n",
            "         3.9189865589, -3.0359783173, -0.8190369606, -2.5464770794,\n",
            "         0.2252689600, -0.8370071650, -1.3247568607,  2.0464484692,\n",
            "         2.1270592213,  1.4675099850, -0.5051974058,  0.1159626245])\n",
            "ctensor.grad: tensor([-2.8705062866e+00, -2.4106861115e+01, -2.2809082031e+01,\n",
            "        -1.5050250292e-01,  7.5028701685e-03,  1.1001282185e-01,\n",
            "         1.6696321964e-01,  1.4290235937e-01, -1.8300205231e+01,\n",
            "         6.7279648781e-01,  5.2939333916e+00,  1.1993740082e+01,\n",
            "        -8.4347903728e-01, -1.7762763977e+01,  6.3957405090e-01,\n",
            "         4.6766467094e+00,  1.1445704460e+01, -3.4206771851e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1043.7252197266, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6358492374), tensor(0.9842917919), tensor(1.0772210360), tensor(1.1965417862), tensor(1.1003004313), tensor(1.4200215340), tensor(1.0431034565), tensor(1.1701149940), tensor(0.9180535078), tensor(1.2247101068), tensor(1.0688905716), tensor(1.5050624609), tensor(0.9773949385), tensor(1.2321068048), tensor(1.1647516489), tensor(0.9653345942), tensor(1.0601998568), tensor(0.8809018135), tensor(1.0946158171), tensor(0.9968842268)]\n",
            "b:  [tensor(0.6246353388), tensor(1.1048489809), tensor(1.3378010988), tensor(1.0177242756), tensor(1.4716018438), tensor(0.8166403770), tensor(1.2384184599), tensor(1.1472522020), tensor(1.7733037472), tensor(0.8724350333), tensor(1.2730617523), tensor(0.7161849141), tensor(1.3059557676), tensor(1.2492282391), tensor(0.8810184598), tensor(0.9883989692), tensor(1.2928738594), tensor(1.6701037884), tensor(0.9878886342), tensor(1.1831597090)]\n",
            "c:  [tensor(0.0066230525), tensor(0.4101157784), tensor(0.3832899332), tensor(-0.0227507334), tensor(-0.0014989621), tensor(-0.0144443763), tensor(0.0082725110), tensor(0.0108128507), tensor(0.3049706817), tensor(-0.0056829113), tensor(0.0743017048), tensor(-0.1325853467), tensor(-0.0750012174), tensor(0.2837834358), tensor(-8.7789201643e-05), tensor(0.0807960257), tensor(-0.1252137423), tensor(-0.0733572394)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.3670899868, -0.9507836103, -0.3162257969, -0.0703798532,\n",
            "        -2.0991988182,  2.9137430191,  1.8760336637, -0.7589070797,\n",
            "        -1.6357510090,  2.6184670925,  1.9290499687,  1.5784034729,\n",
            "        -0.0378508568,  3.8134176731,  1.5634577274,  2.3383746147,\n",
            "         1.5693683624,  0.3456863761,  0.0255997181,  0.8064932823])\n",
            "btensor.grad: tensor([-3.7652947903, -1.6762379408,  1.1383535862, -1.5481290817,\n",
            "         0.5945198536, -1.2544432878,  0.4264664352,  2.5489985943,\n",
            "         3.9872584343, -3.1076343060, -0.8489513397, -2.6173753738,\n",
            "         0.2024935186, -0.8390378952, -1.3773868084,  2.0834329128,\n",
            "         2.1562788486,  1.4363526106, -0.5372037888,  0.0899461508])\n",
            "ctensor.grad: tensor([-2.9179611206e+00, -2.3716575623e+01, -2.2442655563e+01,\n",
            "        -2.0043526590e-01,  1.0337361135e-02,  1.5867735445e-01,\n",
            "         2.2565981746e-01,  1.8756340444e-01, -1.8025161743e+01,\n",
            "         6.9327390194e-01,  6.0353612900e+00,  1.1764569283e+01,\n",
            "        -1.5696734190e+00, -1.7522777557e+01,  6.5871179104e-01,\n",
            "         5.3726668358e+00,  1.1246630669e+01, -1.0208184719e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1042.6381835938, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6346356869), tensor(0.9848083258), tensor(1.0773878098), tensor(1.1965717077), tensor(1.1013940573), tensor(1.4185415506), tensor(1.0421410799), tensor(1.1705337763), tensor(0.9188941717), tensor(1.2234038115), tensor(1.0679267645), tensor(1.5042822361), tensor(0.9774753451), tensor(1.2301520109), tensor(1.1639652252), tensor(0.9641669393), tensor(1.0593773127), tensor(0.8807478547), tensor(1.0946447849), tensor(0.9964848757)]\n",
            "b:  [tensor(0.6265597343), tensor(1.1057063341), tensor(1.3372305632), tensor(1.0185450315), tensor(1.4713095427), tensor(0.8172888160), tensor(1.2382255793), tensor(1.1459697485), tensor(1.7712811232), tensor(0.8740186691), tensor(1.2734988928), tensor(0.7175262570), tensor(1.3058664799), tensor(1.2496471405), tensor(0.8817314506), tensor(0.9873399734), tensor(1.2917826176), tensor(1.6694039106), tensor(0.9881718755), tensor(1.1831265688)]\n",
            "c:  [tensor(0.0081028733), tensor(0.4217805266), tensor(0.3943293095), tensor(-0.0226270799), tensor(-0.0015055635), tensor(-0.0145495664), tensor(0.0081316801), tensor(0.0106987506), tensor(0.3138387203), tensor(-0.0060406877), tensor(0.0709287226), tensor(-0.1383267939), tensor(-0.0738753229), tensor(0.2924171090), tensor(-0.0004275095), tensor(0.0777753443), tensor(-0.1307121962), tensor(-0.0725283325)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.4270894527, -1.0330405235, -0.3335140049, -0.0599198341,\n",
            "        -2.1872377396,  2.9599721432,  1.9247709513, -0.8375936747,\n",
            "        -1.6812887192,  2.6126656532,  1.9277050495,  1.5605031252,\n",
            "        -0.1607596278,  3.9096524715,  1.5728241205,  2.3353495598,\n",
            "         1.6450998783,  0.3078752756, -0.0580057502,  0.7986915112])\n",
            "btensor.grad: tensor([-3.8487834930, -1.7145968676,  1.1411014795, -1.6415600777,\n",
            "         0.5845832825, -1.2968646288,  0.3856636584,  2.5648498535,\n",
            "         4.0451412201, -3.1672334671, -0.8743131757, -2.6826488972,\n",
            "         0.1785227656, -0.8377093077, -1.4260179996,  2.1179385185,\n",
            "         2.1825747490,  1.3998174667, -0.5664871931,  0.0662869215])\n",
            "ctensor.grad: tensor([-2.9596416950e+00, -2.3329524994e+01, -2.2078777313e+01,\n",
            "        -2.4730786681e-01,  1.3202850707e-02,  2.1037992835e-01,\n",
            "         2.8166097403e-01,  2.2819997370e-01, -1.7736106873e+01,\n",
            "         7.1555292606e-01,  6.7459697723e+00,  1.1482878685e+01,\n",
            "        -2.2517864704e+00, -1.7267345428e+01,  6.7944061756e-01,\n",
            "         6.0413575172e+00,  1.0996905327e+01, -1.6578066349e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1041.5717773438, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6333924532), tensor(0.9853640199), tensor(1.0775613785), tensor(1.1965937614), tensor(1.1025279760), tensor(1.4170415401), tensor(1.0411545038), tensor(1.1709920168), tensor(0.9197530150), tensor(1.2221032381), tensor(1.0669666529), tensor(1.5035134554), tensor(0.9776196480), tensor(1.2281519175), tensor(1.1631740332), tensor(0.9630033970), tensor(1.0585163832), tensor(0.8806145191), tensor(1.0947160721), tensor(0.9960876703)]\n",
            "b:  [tensor(0.6285201311), tensor(1.1065779924), tensor(1.3366601467), tensor(1.0194107294), tensor(1.4710220098), tensor(0.8179575801), tensor(1.2380552292), tensor(1.1446834803), tensor(1.7692348957), tensor(0.8756260872), tensor(1.2739465237), tensor(0.7188974619), tensor(1.3057898283), tensor(1.2500637770), tensor(0.8824667931), tensor(0.9862650037), tensor(1.2906795740), tensor(1.6687246561), tensor(0.9884684682), tensor(1.1831040382)]\n",
            "c:  [tensor(0.0096009821), tensor(0.4332553744), tensor(0.4051900804), tensor(-0.0224814676), tensor(-0.0015136149), tensor(-0.0146817323), tensor(0.0079644602), tensor(0.0105660828), tensor(0.3225570321), tensor(-0.0064105601), tensor(0.0672191754), tensor(-0.1439020783), tensor(-0.0724340305), tensor(0.3009172082), tensor(-0.0007784383), tensor(0.0744372681), tensor(-0.1360612661), tensor(-0.0714052320)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.4864077568, -1.1113476753, -0.3471470773, -0.0440796614,\n",
            "        -2.2677292824,  3.0000019073,  1.9730441570, -0.9164781570,\n",
            "        -1.7176558971,  2.6011974812,  1.9201858044,  1.5376658440,\n",
            "        -0.2885570526,  4.0002861023,  1.5823202133,  2.3271112442,\n",
            "         1.7218208313,  0.2667265534, -0.1425405741,  0.7944482565])\n",
            "btensor.grad: tensor([-3.9208021164, -1.7432960272,  1.1408810616, -1.7314214706,\n",
            "         0.5751144886, -1.3375488520,  0.3406952024,  2.5724873543,\n",
            "         4.0924568176, -3.2148766518, -0.8951525092, -2.7423629761,\n",
            "         0.1533970237, -0.8332031965, -1.4706697464,  2.1498892307,\n",
            "         2.2060074806,  1.3583946228, -0.5931335688,  0.0450832844])\n",
            "ctensor.grad: tensor([-2.9962172508e+00, -2.2949689865e+01, -2.1721527100e+01,\n",
            "        -2.9122421145e-01,  1.6102766618e-02,  2.6433160901e-01,\n",
            "         3.3443918824e-01,  2.6533538103e-01, -1.7436647415e+01,\n",
            "         7.3974519968e-01,  7.4190988541e+00,  1.1150554657e+01,\n",
            "        -2.8825919628e+00, -1.7000221252e+01,  7.0185744762e-01,\n",
            "         6.6761560440e+00,  1.0698146820e+01, -2.2462024689e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1040.5239257812, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6321200132), tensor(0.9859566689), tensor(1.0777398348), tensor(1.1966053247), tensor(1.1036981344), tensor(1.4155246019), tensor(1.0401440859), tensor(1.1714895964), tensor(0.9206253886), tensor(1.2208110094), tensor(1.0660133362), tensor(1.5027583838), tensor(0.9778299928), tensor(1.2261092663), tensor(1.1623778343), tensor(0.9618462920), tensor(1.0576167107), tensor(0.8805032372), tensor(1.0948297977), tensor(0.9956907034)]\n",
            "b:  [tensor(0.6305108666), tensor(1.1074591875), tensor(1.3360912800), tensor(1.0203194618), tensor(1.4707388878), tensor(0.8186458945), tensor(1.2379094362), tensor(1.1433974504), tensor(1.7671703100), tensor(0.8772515059), tensor(1.2744022608), tensor(0.7202957869), tensor(1.3057262897), tensor(1.2504765987), tensor(0.8832225204), tensor(0.9851753712), tensor(1.2895662785), tensor(1.6680682898), tensor(0.9887771010), tensor(1.1830908060)]\n",
            "c:  [tensor(0.0111150900), tensor(0.4445453286), tensor(0.4158769250), tensor(-0.0223153122), tensor(-0.0015231357), tensor(-0.0148416106), tensor(0.0077726892), tensor(0.0104162833), tensor(0.3311221600), tensor(-0.0067935260), tensor(0.0631951839), tensor(-0.1492869109), tensor(-0.0707060099), tensor(0.3092796504), tensor(-0.0011414550), tensor(0.0708019882), tensor(-0.1412373483), tensor(-0.0700149760)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.5448703766, -1.1853387356, -0.3569613397, -0.0231115818,\n",
            "        -2.3402178288,  3.0337581635,  2.0208652020, -0.9951857328,\n",
            "        -1.7447352409,  2.5844049454,  1.9066218138,  1.5100497007,\n",
            "        -0.4207191467,  4.0853371620,  1.5922904015,  2.3141694069,\n",
            "         1.7993807793,  0.2225151658, -0.2274150848,  0.7939921618])\n",
            "btensor.grad: tensor([-3.9815173149, -1.7624772787,  1.1376376152, -1.8173799515,\n",
            "         0.5662704706, -1.3766288757,  0.2916120291,  2.5721304417,\n",
            "         4.1291365623, -3.2508258820, -0.9115469456, -2.7966589928,\n",
            "         0.1271696091, -0.8257415295, -1.5114347935,  2.1792285442,\n",
            "         2.2266318798,  1.3126587868, -0.6172804832,  0.0263652802])\n",
            "ctensor.grad: tensor([-3.0282158852e+00, -2.2579919815e+01, -2.1373687744e+01,\n",
            "        -3.3231148124e-01,  1.9041649997e-02,  3.1975710392e-01,\n",
            "         3.8354176283e-01,  2.9959827662e-01, -1.7130279541e+01,\n",
            "         7.6593178511e-01,  8.0479803085e+00,  1.0769660950e+01,\n",
            "        -3.4560444355e+00, -1.6724901199e+01,  7.2603356838e-01,\n",
            "         7.2705521584e+00,  1.0352158546e+01, -2.7805075645e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1039.4952392578, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6308188438), tensor(0.9865840077), tensor(1.0779212713), tensor(1.1966040134), tensor(1.1049003601), tensor(1.4139939547), tensor(1.0391099453), tensor(1.1720262766), tensor(0.9215067029), tensor(1.2195296288), tensor(1.0650696754), tensor(1.5020194054), tensor(0.9781083465), tensor(1.2240267992), tensor(1.1615762711), tensor(0.9606977701), tensor(1.0566779375), tensor(0.8804154992), tensor(1.0949858427), tensor(0.9952920079)]\n",
            "b:  [tensor(0.6325264573), tensor(1.1083453894), tensor(1.3355256319), tensor(1.0212689638), tensor(1.4704598188), tensor(0.8193529844), tensor(1.2377901077), tensor(1.1421153545), tensor(1.7650927305), tensor(0.8788892627), tensor(1.2748640776), tensor(0.7217186689), tensor(1.3056763411), tensor(1.2508844137), tensor(0.8839967251), tensor(0.9840723872), tensor(1.2884440422), tensor(1.6674365997), tensor(0.9890966415), tensor(1.1830856800)]\n",
            "c:  [tensor(0.0126431994), tensor(0.4556561410), tensor(0.4263952971), tensor(-0.0221299622), tensor(-0.0015341480), tensor(-0.0150295710), tensor(0.0075583905), tensor(0.0102504352), tensor(0.3395320475), tensor(-0.0071906112), tensor(0.0588817298), tensor(-0.1544584036), tensor(-0.0687228441), tensor(0.3175017536), tensor(-0.0015174649), tensor(0.0668925121), tensor(-0.1462180465), tensor(-0.0683872253)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.6023256779e+00, -1.2547265291e+00, -3.6284241080e-01,\n",
            "         2.6457309723e-03, -2.4043564796e+00,  3.0612711906e+00,\n",
            "         2.0682253838e+00, -1.0733300447e+00, -1.7625777721e+00,\n",
            "         2.5626597404e+00,  1.8872103691e+00,  1.4778752327e+00,\n",
            "        -5.5668282509e-01,  4.1648669243e+00,  1.6030873060e+00,\n",
            "         2.2970600128e+00,  1.8776112795e+00,  1.7551499605e-01,\n",
            "        -3.1202781200e-01,  7.9744708538e-01])\n",
            "btensor.grad: tensor([-4.0312023163, -1.7723674774,  1.1313798428, -1.8990952969,\n",
            "         0.5582236052, -1.4142017365,  0.2385594547,  2.5641508102,\n",
            "         4.1552753448, -3.2754557133, -0.9235981703, -2.8457119465,\n",
            "         0.0999324322, -0.8155233860, -1.5484143496,  2.2059478760,\n",
            "         2.2445561886,  1.2632884979, -0.6390870810,  0.0101546049])\n",
            "ctensor.grad: tensor([-3.0562176704e+00, -2.2221603394e+01, -2.1036769867e+01,\n",
            "        -3.7070071697e-01,  2.2024644539e-02,  3.7592113018e-01,\n",
            "         4.2859730124e-01,  3.3169621229e-01, -1.6819759369e+01,\n",
            "         7.9417061806e-01,  8.6269054413e+00,  1.0342993736e+01,\n",
            "        -3.9663376808e+00, -1.6444196701e+01,  7.5201976299e-01,\n",
            "         7.8189530373e+00,  9.9613838196e+00, -3.2555019855e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1038.4863281250, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6294895411), tensor(0.9872437119), tensor(1.0781036615), tensor(1.1965876818), tensor(1.1061303616), tensor(1.4124525785), tensor(1.0380524397), tensor(1.1726015806), tensor(0.9223924279), tensor(1.2182614803), tensor(1.0641386509), tensor(1.5012986660), tensor(0.9784563184), tensor(1.2219073772), tensor(1.1607687473), tensor(0.9595596194), tensor(1.0556998253), tensor(0.8803524971), tensor(1.0951837301), tensor(0.9948896170)]\n",
            "b:  [tensor(0.6345615983), tensor(1.1092320681), tensor(1.3349645138), tensor(1.0222570896), tensor(1.4701842070), tensor(0.8200781941), tensor(1.2376992702), tensor(1.1408408880), tensor(1.7630071640), tensor(0.8805338740), tensor(1.2753298283), tensor(0.7231635451), tensor(1.3056404591), tensor(1.2512857914), tensor(0.8847876191), tensor(0.9829573631), tensor(1.2873140574), tensor(1.6668311357), tensor(0.9894260168), tensor(1.1830874681)]\n",
            "c:  [tensor(0.0141834980), tensor(0.4665935934), tensor(0.4367508888), tensor(-0.0219266955), tensor(-0.0015466765), tensor(-0.0152456323), tensor(0.0073237345), tensor(0.0100692585), tensor(0.3477859199), tensor(-0.0076028518), tensor(0.0543066636), tensor(-0.1593951285), tensor(-0.0665181205), tensor(0.3255822062), tensor(-0.0019073812), tensor(0.0627346709), tensor(-0.1509822607), tensor(-0.0665533543)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.6586086750, -1.3193600178, -0.3647898734,  0.0327327251,\n",
            "        -2.4599595070,  3.0826370716,  2.1150143147, -1.1505633593,\n",
            "        -1.7714712620,  2.5363254547,  1.8621610403,  1.4414231777,\n",
            "        -0.6958921552,  4.2389388084,  1.6149989367,  2.2763032913,\n",
            "         1.9562635422,  0.1259549856, -0.3958006501,  0.8047649860])\n",
            "btensor.grad: tensor([-4.0702457428e+00, -1.7733236551e+00,  1.1221528053e+00,\n",
            "        -1.9762828350e+00,  5.5111932755e-01, -1.4503912926e+00,\n",
            "         1.8173888326e-01,  2.5489892960e+00,  4.1710815430e+00,\n",
            "        -3.2892799377e+00, -9.3145734072e-01, -2.8897457123e+00,\n",
            "         7.1776509285e-02, -8.0276775360e-01, -1.5817821026e+00,\n",
            "         2.2300407887e+00,  2.2598848343e+00,  1.2110037804e+00,\n",
            "        -6.5874695778e-01, -3.5793781281e-03])\n",
            "ctensor.grad: tensor([ -3.0805966854, -21.8749103546, -20.7111778259,  -0.4065352976,\n",
            "          0.0250569936,   0.4321217239,   0.4693123102,   0.3623524904,\n",
            "        -16.5077190399,   0.8244807720,   9.1501302719,   9.8734359741,\n",
            "         -4.4094481468, -16.1609058380,   0.7798327208,   8.3156757355,\n",
            "          9.5284318924,  -3.6677439213])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1037.4968261719, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6281327009), tensor(0.9879332781), tensor(1.0782850981), tensor(1.1965543032), tensor(1.1073838472), tensor(1.4109035730), tensor(1.0369719267), tensor(1.1732147932), tensor(0.9232783318), tensor(1.2170085907), tensor(1.0632227659), tensor(1.5005981922), tensor(0.9788751602), tensor(1.2197535038), tensor(1.1599545479), tensor(0.9584333897), tensor(1.0546822548), tensor(0.8803154230), tensor(1.0954227448), tensor(0.9944817424)]\n",
            "b:  [tensor(0.6366111636), tensor(1.1101149321), tensor(1.3344094753), tensor(1.0232814550), tensor(1.4699116945), tensor(0.8208208084), tensor(1.2376384735), tensor(1.1395772696), tensor(1.7609187365), tensor(0.8821803331), tensor(1.2757974863), tensor(0.7246280313), tensor(1.3056190014), tensor(1.2516796589), tensor(0.8855934739), tensor(0.9818315506), tensor(1.2861776352), tensor(1.6662528515), tensor(0.9897642136), tensor(1.1830948591)]\n",
            "c:  [tensor(0.0157344323), tensor(0.4773631096), tensor(0.4469490051), tensor(-0.0217067245), tensor(-0.0015607483), tensor(-0.0154894963), tensor(0.0070709931), tensor(0.0098731071), tensor(0.3558837771), tensor(-0.0080312788), tensor(0.0494997203), tensor(-0.1640776247), tensor(-0.0641274825), tensor(0.3335205615), tensor(-0.0023121154), tensor(0.0583562553), tensor(-0.1555106789), tensor(-0.0645464435)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.7135963440, -1.3791325092, -0.3628249764,  0.0666583776,\n",
            "        -2.5069375038,  3.0980658531,  2.1611030102, -1.2265104055,\n",
            "        -1.7718374729,  2.5058133602,  1.8317632675,  1.4010468721,\n",
            "        -0.8377224803,  4.3076686859,  1.6283119917,  2.2524321079,\n",
            "         2.0350692272,  0.0741050243, -0.4781351686,  0.8158007860])\n",
            "btensor.grad: tensor([-4.0990929604, -1.7657530308,  1.1100691557, -2.0486207008,\n",
            "         0.5451282859, -1.4852671623,  0.1214743257,  2.5272548199,\n",
            "         4.1769294739, -3.2928848267, -0.9352685213, -2.9289908409,\n",
            "         0.0428580046, -0.7876634598, -1.6116874218,  2.2515716553,\n",
            "         2.2727866173,  1.1566220522, -0.6764247417, -0.0148336887])\n",
            "ctensor.grad: tensor([ -3.1018674374, -21.5390129089, -20.3962268829,  -0.4399414361,\n",
            "          0.0281435698,   0.4877276421,   0.5054826736,   0.3923023343,\n",
            "        -16.1957397461,   0.8568537235,   9.6138820648,   9.3649921417,\n",
            "         -4.7812690735, -15.8767309189,   0.8094683290,   8.7568321228,\n",
            "          9.0568485260,  -4.0138235092])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1036.5264892578, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6267491579), tensor(0.9886503220), tensor(1.0784636736), tensor(1.1965024471), tensor(1.1086565256), tensor(1.4093496799), tensor(1.0358687639), tensor(1.1738651991), tensor(0.9241605401), tensor(1.2157728672), tensor(1.0623246431), tensor(1.4999196529), tensor(0.9793659449), tensor(1.2175679207), tensor(1.1591329575), tensor(0.9573204517), tensor(1.0536254644), tensor(0.8803053498), tensor(1.0957020521), tensor(0.9940666556)]\n",
            "b:  [tensor(0.6386703253), tensor(1.1109900475), tensor(1.3338618279), tensor(1.0243394375), tensor(1.4696415663), tensor(0.8215802908), tensor(1.2376093864), tensor(1.1383274794), tensor(1.7588320971), tensor(0.8838238120), tensor(1.2762651443), tensor(0.7261099219), tensor(1.3056123257), tensor(1.2520648241), tensor(0.8864126801), tensor(0.9806962609), tensor(1.2850359678), tensor(1.6657023430), tensor(0.9901103973), tensor(1.1831067801)]\n",
            "c:  [tensor(0.0172944944), tensor(0.4879691899), tensor(0.4569942057), tensor(-0.0214711968), tensor(-0.0015763924), tensor(-0.0157605633), tensor(0.0068025077), tensor(0.0096620144), tensor(0.3638267517), tensor(-0.0084768934), tensor(0.0444930233), tensor(-0.1684882641), tensor(-0.0615868643), tensor(0.3413175046), tensor(-0.0027325535), tensor(0.0537873767), tensor(-0.1597856283), tensor(-0.0623997077)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.7671303749, -1.4341154099, -0.3571146429,  0.1037894487,\n",
            "        -2.5453872681,  3.1078026295,  2.2062196732, -1.3008760214,\n",
            "        -1.7643699646,  2.4714715481,  1.7962728739,  1.3571155071,\n",
            "        -0.9815949202,  4.3711361885,  1.6432110071,  2.2258992195,\n",
            "         2.1136484146,  0.0201427937, -0.5585138798,  0.8302192688])\n",
            "btensor.grad: tensor([-4.1183333397, -1.7502310276,  1.0952504873, -2.1158676147,\n",
            "         0.5403357148, -1.5189398527,  0.0581177473,  2.4995727539,\n",
            "         4.1732678413, -3.2870054245, -0.9352614284, -2.9637522697,\n",
            "         0.0133006573, -0.7704393864, -1.6383676529,  2.2705707550,\n",
            "         2.2833986282,  1.1009197235, -0.6923654079, -0.0237284899])\n",
            "ctensor.grad: tensor([ -3.1201231480, -21.2121410370, -20.0903759003,  -0.4710564017,\n",
            "          0.0312882625,   0.5421354771,   0.5369706154,   0.4221849442,\n",
            "        -15.8859500885,   0.8912282586,  10.0133943558,   8.8212900162,\n",
            "         -5.0812358856, -15.5938653946,   0.8408762217,   9.1377544403,\n",
            "          8.5498971939,  -4.2934694290])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1035.5783691406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6253396273), tensor(0.9893924594), tensor(1.0786375999), tensor(1.1964306831), tensor(1.1099442244), tensor(1.4077935219), tensor(1.0347436666), tensor(1.1745518446), tensor(0.9250354171), tensor(1.2145559788), tensor(1.0614465475), tensor(1.4992645979), tensor(0.9799293280), tensor(1.2153531313), tensor(1.1583030224), tensor(0.9562218189), tensor(1.0525295734), tensor(0.8803231716), tensor(1.0960202217), tensor(0.9936428070)]\n",
            "b:  [tensor(0.6407345533), tensor(1.1118537188), tensor(1.3333228827), tensor(1.0254282951), tensor(1.4693731070), tensor(0.8223559856), tensor(1.2376133204), tensor(1.1370940208), tensor(1.7567516565), tensor(0.8854599595), tensor(1.2767308950), tensor(0.7276070118), tensor(1.3056206703), tensor(1.2524404526), tensor(0.8872436285), tensor(0.9795526266), tensor(1.2838900089), tensor(1.6651799679), tensor(0.9904637337), tensor(1.1831219196)]\n",
            "c:  [tensor(0.0188625436), tensor(0.4984151423), tensor(0.4668899477), tensor(-0.0212212186), tensor(-0.0015936394), tensor(-0.0160580110), tensor(0.0065206285), tensor(0.0094356816), tensor(0.3716157973), tensor(-0.0089406595), tensor(0.0393186584), tensor(-0.1726123989), tensor(-0.0589338616), tensor(0.3489736021), tensor(-0.0031695548), tensor(0.0490583107), tensor(-0.1637921333), tensor(-0.0601477437)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.8191695213, -1.4842897654, -0.3477342129,  0.1435997486,\n",
            "        -2.5753927231,  3.1122579575,  2.2501733303, -1.3732346296,\n",
            "        -1.7497248650,  2.4337742329,  1.7561104298,  1.3101364374,\n",
            "        -1.1267189980,  4.4295449257,  1.6599519253,  2.1972665787,\n",
            "         2.1916935444, -0.0355906487, -0.6362730861,  0.8477046490])\n",
            "btensor.grad: tensor([-4.1285076141, -1.7272340059,  1.0779688358, -2.1776726246,\n",
            "         0.5369476676, -1.5513714552, -0.0077642798,  2.4668083191,\n",
            "         4.1607980728, -3.2723197937, -0.9315581918, -2.9942350388,\n",
            "        -0.0166113973, -0.7512224913, -1.6619457006,  2.2872312069,\n",
            "         2.2920157909,  1.0448215008, -0.7066757679, -0.0302194357])\n",
            "ctensor.grad: tensor([ -3.1360998154, -20.8918972015, -19.7915096283,  -0.4999571443,\n",
            "          0.0344939530,   0.5948948860,   0.5637583137,   0.4526648819,\n",
            "        -15.5781126022,   0.9275323153,  10.3487291336,   8.2482595444,\n",
            "         -5.3060064316, -15.3122062683,   0.8740024567,   9.4581327438,\n",
            "          8.0130023956,  -4.5039291382])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1034.6516113281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6239048243), tensor(0.9901574850), tensor(1.0788052082), tensor(1.1963380575), tensor(1.1112428904), tensor(1.4062376022), tensor(1.0335974693), tensor(1.1752735376), tensor(0.9258999228), tensor(1.2133594751), tensor(1.0605908632), tensor(1.4986343384), tensor(0.9805656075), tensor(1.2131116390), tensor(1.1574637890), tensor(0.9551384449), tensor(1.0513952971), tensor(0.8803697228), tensor(1.0963758230), tensor(0.9932090044)]\n",
            "b:  [tensor(0.6427997947), tensor(1.1127024889), tensor(1.3327937126), tensor(1.0265452862), tensor(1.4691057205), tensor(0.8231473565), tensor(1.2376512289), tensor(1.1358791590), tensor(1.7546815872), tensor(0.8870849013), tensor(1.2771931887), tensor(0.7291174531), tensor(1.3056440353), tensor(1.2528055906), tensor(0.8880850077), tensor(0.9784018993), tensor(1.2827407122), tensor(1.6646854877), tensor(0.9908235669), tensor(1.1831392050)]\n",
            "c:  [tensor(0.0204371065), tensor(0.5087028742), tensor(0.4766384363), tensor(-0.0209578145), tensor(-0.0016125201), tensor(-0.0163807366), tensor(0.0062277168), tensor(0.0091936421), tensor(0.3792534173), tensor(-0.0094234571), tensor(0.0340116993), tensor(-0.1764368713), tensor(-0.0562026538), tensor(0.3564909399), tensor(-0.0036239072), tensor(0.0442022532), tensor(-0.1675165743), tensor(-0.0578217879)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.8695111275, -1.5300159454, -0.3351180851,  0.1852527857,\n",
            "        -2.5974102020,  3.1117248535,  2.2924532890, -1.4434443712,\n",
            "        -1.7290560007,  2.3929703236,  1.7114694118,  1.2604689598,\n",
            "        -1.2726125717,  4.4829154015,  1.6784992218,  2.1667842865,\n",
            "         2.2685999870, -0.0930840969, -0.7111034989,  0.8675456047])\n",
            "btensor.grad: tensor([-4.1304397583, -1.6976139545,  1.0583187342, -2.2339763641,\n",
            "         0.5348650813, -1.5827740431, -0.0757714510,  2.4296064377,\n",
            "         4.1400609016, -3.2498409748, -0.9245572686, -3.0208799839,\n",
            "        -0.0468361974, -0.7303290367, -1.6827994585,  2.3015048504,\n",
            "         2.2987122536,  0.9889971614, -0.7197130919, -0.0345796347])\n",
            "ctensor.grad: tensor([ -3.1491246223, -20.5754776001, -19.4969520569,  -0.5268065333,\n",
            "          0.0377613083,   0.6454520226,   0.5858232975,   0.4840795994,\n",
            "        -15.2752265930,   0.9655948877,  10.6139154434,   7.6489453316,\n",
            "         -5.4624137878, -15.0346975327,   0.9087049961,   9.7121124268,\n",
            "          7.4488706589,  -4.6519098282])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1033.7474365234, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6224457026), tensor(0.9909430146), tensor(1.0789647102), tensor(1.1962238550), tensor(1.1125485897), tensor(1.4046840668), tensor(1.0324308872), tensor(1.1760289669), tensor(0.9267513156), tensor(1.2121845484), tensor(1.0597592592), tensor(1.4980298281), tensor(0.9812746048), tensor(1.2108458281), tensor(1.1566141844), tensor(0.9540708065), tensor(1.0502232313), tensor(0.8804455400), tensor(1.0967668295), tensor(0.9927642345)]\n",
            "b:  [tensor(0.6448620558), tensor(1.1135332584), tensor(1.3322752714), tensor(1.0276873112), tensor(1.4688384533), tensor(0.8239537477), tensor(1.2377237082), tensor(1.1346845627), tensor(1.7526254654), tensor(0.8886949420), tensor(1.2776502371), tensor(0.7306392789), tensor(1.3056824207), tensor(1.2531594038), tensor(0.8889353871), tensor(0.9772449136), tensor(1.2815886736), tensor(1.6642181873), tensor(0.9911892414), tensor(1.1831574440)]\n",
            "c:  [tensor(0.0220178161), tensor(0.5188328028), tensor(0.4862404168), tensor(-0.0206820667), tensor(-0.0016330654), tensor(-0.0167276133), tensor(0.0059260060), tensor(0.0089350641), tensor(0.3867392838), tensor(-0.0099261245), tensor(0.0286019370), tensor(-0.1799539775), tensor(-0.0534323342), tensor(0.3638691008), tensor(-0.0040963707), tensor(0.0392476656), tensor(-0.1709503382), tensor(-0.0554575697)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.9183437824, -1.5710262060, -0.3190943599,  0.2284785509,\n",
            "        -2.6114301682,  3.1069588661,  2.3330957890, -1.5108164549,\n",
            "        -1.7028353214,  2.3498034477,  1.6631171703,  1.2089365721,\n",
            "        -1.4180037975,  4.5316948891,  1.6992700100,  2.1352415085,\n",
            "         2.3442282677, -0.1516107321, -0.7820273042,  0.8895814419])\n",
            "btensor.grad: tensor([-4.1245002747, -1.6616352797,  1.0369094610, -2.2841572762,\n",
            "         0.5345478654, -1.6127885580, -0.1449275613,  2.3892743587,\n",
            "         4.1122417450, -3.2200396061, -0.9141192436, -3.0436689854,\n",
            "        -0.0767541528, -0.7076265812, -1.7007776499,  2.3139402866,\n",
            "         2.3041074276,  0.9346001744, -0.7312947512, -0.0364916921])\n",
            "ctensor.grad: tensor([ -3.1614205837, -20.2598876953, -19.2039489746,  -0.5514972806,\n",
            "          0.0410907380,   0.6937550902,   0.6034211516,   0.5171563029,\n",
            "        -14.9717493057,   1.0053349733,  10.8195238113,   7.0342054367,\n",
            "         -5.5406360626, -14.7563495636,   0.9449272156,   9.9091730118,\n",
            "          6.8675179482,  -4.7284374237])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1032.8681640625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6209632158), tensor(0.9917473197), tensor(1.0791151524), tensor(1.1960879564), tensor(1.1138579845), tensor(1.4031351805), tensor(1.0312454700), tensor(1.1768169403), tensor(0.9275879264), tensor(1.2110325098), tensor(1.0589538813), tensor(1.4974521399), tensor(0.9820562005), tensor(1.2085580826), tensor(1.1557533741), tensor(0.9530197382), tensor(1.0490145683), tensor(0.8805515170), tensor(1.0971915722), tensor(0.9923080802)]\n",
            "b:  [tensor(0.6469181180), tensor(1.1143437624), tensor(1.3317686319), tensor(1.0288517475), tensor(1.4685708284), tensor(0.8247748613), tensor(1.2378313541), tensor(1.1335116625), tensor(1.7505867481), tensor(0.8902872801), tensor(1.2781008482), tensor(0.7321710587), tensor(1.3057358265), tensor(1.2535014153), tensor(0.8897938132), tensor(0.9760829806), tensor(1.2804348469), tensor(1.6637773514), tensor(0.9915603995), tensor(1.1831759214)]\n",
            "c:  [tensor(0.0236022528), tensor(0.5288038254), tensor(0.4956952333), tensor(-0.0203948263), tensor(-0.0016553039), tensor(-0.0170970466), tensor(0.0056178039), tensor(0.0086594122), tensor(0.3940790892), tensor(-0.0104493061), tensor(0.0231299270), tensor(-0.1831535697), tensor(-0.0506459810), tensor(0.3711132407), tensor(-0.0045875399), tensor(0.0342330560), tensor(-0.1740825176), tensor(-0.0530759022)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.9650855064, -1.6085689068, -0.3009222746,  0.2718085051,\n",
            "        -2.6187434196,  3.0978903770,  2.3708760738, -1.5758867264,\n",
            "        -1.6732473373,  2.3039665222,  1.6106505394,  1.1554503441,\n",
            "        -1.5632016659,  4.5755062103,  1.7215363979,  2.1021628380,\n",
            "         2.4173672199, -0.2119745016, -0.8495869040,  0.9122503996])\n",
            "btensor.grad: tensor([-4.1121463776, -1.6209453344,  1.0133507252, -2.3288879395,\n",
            "         0.5351846218, -1.6422179937, -0.2153103948,  2.3458614349,\n",
            "         4.0773987770, -3.1846718788, -0.9012898207, -3.0635552406,\n",
            "        -0.1069153547, -0.6839573383, -1.7168526649,  2.3238575459,\n",
            "         2.3077235222,  0.8816345930, -0.7423458099, -0.0369235277])\n",
            "ctensor.grad: tensor([ -3.1688740253, -19.9420948029, -18.9096126556,  -0.5744794607,\n",
            "          0.0444770232,   0.7388662100,   0.6164041162,   0.5513045192,\n",
            "        -14.6795864105,   1.0463638306,  10.9440193176,   6.3991694450,\n",
            "         -5.5727052689, -14.4882736206,   0.9823384285,  10.0292186737,\n",
            "          6.2643599510,  -4.7633318901])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1032.0129394531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6194577217), tensor(0.9925675988), tensor(1.0792545080), tensor(1.1959296465), tensor(1.1151669025), tensor(1.4015918970), tensor(1.0300418139), tensor(1.1776351929), tensor(0.9284072518), tensor(1.2099037170), tensor(1.0581756830), tensor(1.4969011545), tensor(0.9829085469), tensor(1.2062501907), tensor(1.1548798084), tensor(0.9519848228), tensor(1.0477700233), tensor(0.8806872368), tensor(1.0976470709), tensor(0.9918395281)]\n",
            "b:  [tensor(0.6489643455), tensor(1.1151307821), tensor(1.3312737942), tensor(1.0300347805), tensor(1.4683014154), tensor(0.8256094456), tensor(1.2379735708), tensor(1.1323604584), tensor(1.7485675812), tensor(0.8918585777), tensor(1.2785431147), tensor(0.7337107062), tensor(1.3058034182), tensor(1.2538303137), tensor(0.8906585574), tensor(0.9749162793), tensor(1.2792789936), tensor(1.6633610725), tensor(0.9919360280), tensor(1.1831928492)]\n",
            "c:  [tensor(0.0251932554), tensor(0.5386136174), tensor(0.5050010085), tensor(-0.0200974420), tensor(-0.0016792653), tensor(-0.0174882170), tensor(0.0053048595), tensor(0.0083650341), tensor(0.4012635648), tensor(-0.0109937536), tensor(0.0176078156), tensor(-0.1860418916), tensor(-0.0478964858), tensor(0.3782145679), tensor(-0.0050981170), tensor(0.0291705802), tensor(-0.1769175380), tensor(-0.0507254601)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.0109925270, -1.6405042410, -0.2786998153,  0.3165690899,\n",
            "        -2.6178855896,  3.0864689350,  2.4074079990, -1.6364219189,\n",
            "        -1.6386678219,  2.2575716972,  1.5563871861,  1.1020740271,\n",
            "        -1.7047508955,  4.6158871651,  1.7470370531,  2.0698370934,\n",
            "         2.4891414642, -0.2714231014, -0.9109839201,  0.9370596409])\n",
            "btensor.grad: tensor([-4.0924482346, -1.5740954876,  0.9896355867, -2.3660011292,\n",
            "         0.5387487411, -1.6691898108, -0.2844382524,  2.3023278713,\n",
            "         4.0383472443, -3.1426367760, -0.8844624758, -3.0792727470,\n",
            "        -0.1351484656, -0.6579029560, -1.7294425964,  2.3334429264,\n",
            "         2.3116521835,  0.8325371742, -0.7512003183, -0.0339143276])\n",
            "ctensor.grad: tensor([ -3.1820039749, -19.6196308136, -18.6115474701,  -0.5947674513,\n",
            "          0.0479229018,   0.7823422551,   0.6258884668,   0.5887567997,\n",
            "        -14.3689556122,   1.0888954401,  11.0442237854,   5.7766423225,\n",
            "         -5.4989900589, -14.2026729584,   1.0211538076,  10.1249504089,\n",
            "          5.6700429916,  -4.7008843422])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1031.1860351562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6179312468), tensor(0.9934040308), tensor(1.0793836117), tensor(1.1957508326), tensor(1.1164740324), tensor(1.4000569582), tensor(1.0288230181), tensor(1.1784836054), tensor(0.9292105436), tensor(1.2088001966), tensor(1.0574276447), tensor(1.4963783026), tensor(0.9838326573), tensor(1.2039252520), tensor(1.1539940834), tensor(0.9509679079), tensor(1.0464924574), tensor(0.8808550835), tensor(1.0981328487), tensor(0.9913604856)]\n",
            "b:  [tensor(0.6509994268), tensor(1.1158943176), tensor(1.3307927847), tensor(1.0312348604), tensor(1.4680311680), tensor(0.8264585137), tensor(1.2381515503), tensor(1.1312330961), tensor(1.7465718985), tensor(0.8934084177), tensor(1.2789772749), tensor(0.7352581024), tensor(1.3058862686), tensor(1.2541470528), tensor(0.8915300965), tensor(0.9737472534), tensor(1.2781230211), tensor(1.6629692316), tensor(0.9923171997), tensor(1.1832090616)]\n",
            "c:  [tensor(0.0267812926), tensor(0.5482583642), tensor(0.5141544342), tensor(-0.0197899677), tensor(-0.0017049671), tensor(-0.0178980324), tensor(0.0049898867), tensor(0.0080530141), tensor(0.4083185494), tensor(-0.0115595609), tensor(0.0121124918), tensor(-0.1885960400), tensor(-0.0451527536), tensor(0.3851971328), tensor(-0.0056282170), tensor(0.0241328105), tensor(-0.1794325262), tensor(-0.0483766943)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.0529808998, -1.6728879213, -0.2581417859,  0.3575135469,\n",
            "        -2.6142113209,  3.0698781013,  2.4375672340, -1.6969406605,\n",
            "        -1.6066329479,  2.2069244385,  1.4961684942,  1.0457488298,\n",
            "        -1.8482027054,  4.6498489380,  1.7714123726,  2.0337862968,\n",
            "         2.5551092625, -0.3357055187, -0.9715790749,  0.9580835104])\n",
            "btensor.grad: tensor([-4.0702176094, -1.5270972252,  0.9620934725, -2.4002351761,\n",
            "         0.5404873490, -1.6981877089, -0.3558914661,  2.2546305656,\n",
            "         3.9913930893, -3.0996327400, -0.8683931828, -3.0948429108,\n",
            "        -0.1658041477, -0.6334437132, -1.7430256605,  2.3380486965,\n",
            "         2.3118872643,  0.7836805582, -0.7623511553, -0.0325435400])\n",
            "ctensor.grad: tensor([ -3.1760737896, -19.2895107269, -18.3068695068,  -0.6149493456,\n",
            "          0.0514035895,   0.8196295500,   0.6299452782,   0.6240397692,\n",
            "        -14.1099548340,   1.1316146851,  10.9906473160,   5.1083059311,\n",
            "         -5.4874634743, -13.9651479721,   1.0601994991,  10.0755367279,\n",
            "          5.0299720764,  -4.6975331306])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.3842773438, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6163817644), tensor(0.9942495823), tensor(1.0794965029), tensor(1.1955478191), tensor(1.1177719831), tensor(1.3985283375), tensor(1.0275863409), tensor(1.1793560982), tensor(0.9299912453), tensor(1.2077187300), tensor(1.0567065477), tensor(1.4958801270), tensor(0.9848207831), tensor(1.2015823126), tensor(1.1530913115), tensor(0.9499646425), tensor(1.0451800823), tensor(0.8810495734), tensor(1.0986410379), tensor(0.9908663630)]\n",
            "b:  [tensor(0.6530172825), tensor(1.1166279316), tensor(1.3303221464), tensor(1.0324444771), tensor(1.4677549601), tensor(0.8273173571), tensor(1.2383605242), tensor(1.1301249266), tensor(1.7445973158), tensor(0.8949304819), tensor(1.2793982029), tensor(0.7368085384), tensor(1.3059796095), tensor(1.2544475794), tensor(0.8924034834), tensor(0.9725721478), tensor(1.2769631147), tensor(1.6625961065), tensor(0.9926995635), tensor(1.1832191944)]\n",
            "c:  [tensor(0.0283892564), tensor(0.5577344298), tensor(0.5231522918), tensor(-0.0194759630), tensor(-0.0017324495), tensor(-0.0183295161), tensor(0.0046723280), tensor(0.0077169985), tensor(0.4151771069), tensor(-0.0121480711), tensor(0.0065457071), tensor(-0.1908806562), tensor(-0.0425887592), tensor(0.3919982314), tensor(-0.0061791041), tensor(0.0190289840), tensor(-0.1816866398), tensor(-0.0461915545)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.0989246368, -1.6910676956, -0.2258045375,  0.4061040282,\n",
            "        -2.5960018635,  3.0573329926,  2.4732906818, -1.7449682951,\n",
            "        -1.5614144802,  2.1630260944,  1.4421827793,  0.9963046312,\n",
            "        -1.9762299061,  4.6858396530,  1.8055595160,  2.0065207481,\n",
            "         2.6248586178, -0.3890373707, -1.0163302422,  0.9882358313])\n",
            "btensor.grad: tensor([-4.0357532501, -1.4671279192,  0.9413562417, -2.4191703796,\n",
            "         0.5524404049, -1.7177329063, -0.4179705977,  2.2162418365,\n",
            "         3.9492754936, -3.0441527367, -0.8418177366, -3.1008911133,\n",
            "        -0.1867820323, -0.6009581089, -1.7467654943,  2.3502402306,\n",
            "         2.3197586536,  0.7463502288, -0.7646842003, -0.0201593041])\n",
            "ctensor.grad: tensor([ -3.2159290314, -18.9521331787, -17.9957351685,  -0.6280083656,\n",
            "          0.0549646877,   0.8629676104,   0.6351174712,   0.6720316410,\n",
            "        -13.7171163559,   1.1770205498,  11.1335687637,   4.5692367554,\n",
            "         -5.1279897690, -13.6021928787,   1.1017740965,  10.2076530457,\n",
            "          4.5082130432,  -4.3702783585])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1029.6110839844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6148177385), tensor(0.9951171875), tensor(1.0796059370), tensor(1.1953327656), tensor(1.1190704107), tensor(1.3970152140), tensor(1.0263456106), tensor(1.1802626848), tensor(0.9307655096), tensor(1.2066698074), tensor(1.0560249090), tensor(1.4954154491), tensor(0.9858869314), tensor(1.1992312670), tensor(1.1521813869), tensor(0.9489869475), tensor(1.0438444614), tensor(0.8812859058), tensor(1.0991832018), tensor(0.9903709888)]\n",
            "b:  [tensor(0.6550260782), tensor(1.1173434258), tensor(1.3298720121), tensor(1.0336723328), tensor(1.4674837589), tensor(0.8281965256), tensor(1.2386102676), tensor(1.1290478706), tensor(1.7426567078), tensor(0.8964354992), tensor(1.2798161507), tensor(0.7383702397), tensor(1.3060944080), tensor(1.2547409534), tensor(0.8932886124), tensor(0.9714035392), tensor(1.2758100033), tensor(1.6622499228), tensor(0.9930936694), tensor(1.1832346916)]\n",
            "c:  [tensor(0.0299533345), tensor(0.5670342445), tensor(0.5319877267), tensor(-0.0191491414), tensor(-0.0017616719), tensor(-0.0187683534), tensor(0.0043602367), tensor(0.0073719560), tensor(0.4220206141), tensor(-0.0127568068), tensor(0.0012750127), tensor(-0.1927347630), tensor(-0.0398078524), tensor(0.3987876177), tensor(-0.0067485655), tensor(0.0142032746), tensor(-0.1835274547), tensor(-0.0437994599)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.1279499531, -1.7352476120, -0.2188549340,  0.4300354719,\n",
            "        -2.5967988968,  3.0262317657,  2.4815542698, -1.8131152391,\n",
            "        -1.5485007763,  2.0978665352,  1.3633118868,  0.9293572903,\n",
            "        -2.1322717667,  4.7021417618,  1.8198506832,  1.9553375244,\n",
            "         2.6711382866, -0.4726973772, -1.0843623877,  0.9907087088])\n",
            "btensor.grad: tensor([-4.0176153183, -1.4310208559,  0.9002360702, -2.4557504654,\n",
            "         0.5423405170, -1.7583059072, -0.4993840456,  2.1542043686,\n",
            "         3.8811659813, -3.0100078583, -0.8357770443, -3.1234002113,\n",
            "        -0.2295383811, -0.5866969824, -1.7702488899,  2.3371865749,\n",
            "         2.3061199188,  0.6922534704, -0.7882394791, -0.0309585333])\n",
            "ctensor.grad: tensor([ -3.1281549931, -18.5996189117, -17.6708507538,  -0.6536413431,\n",
            "          0.0584446974,   0.8776751757,   0.6241829395,   0.6900845170,\n",
            "        -13.6870174408,   1.2174711227,  10.5413885117,   3.7082021236,\n",
            "         -5.5618114471, -13.5787439346,   1.1389223337,   9.6514186859,\n",
            "          3.6816418171,  -4.7841892242])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1028.8669433594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6132186651), tensor(0.9959639907), tensor(1.0796732903), tensor(1.1950738430), tensor(1.1203328371), tensor(1.3954943419), tensor(1.0250686407), tensor(1.1811673641), tensor(0.9314874411), tensor(1.2056239843), tensor(1.0553507805), tensor(1.4949578047), tensor(0.9869825244), tensor(1.1968511343), tensor(1.1512330770), tensor(0.9480004907), tensor(1.0424602032), tensor(0.8815207481), tensor(1.0997158289), tensor(0.9898377061)]\n",
            "b:  [tensor(0.6569952965), tensor(1.1180025339), tensor(1.3294140100), tensor(1.0348813534), tensor(1.4671838284), tensor(0.8290639520), tensor(1.2388690710), tensor(1.1279661655), tensor(1.7407183647), tensor(0.8978882432), tensor(1.2801983356), tensor(0.7399154902), tensor(1.3061969280), tensor(1.2549998760), tensor(0.8941542506), tensor(0.9702079296), tensor(1.2746332884), tensor(1.6618990898), tensor(0.9934682846), tensor(1.1832195520)]\n",
            "c:  [tensor(0.0316546224), tensor(0.5761600733), tensor(0.5406627059), tensor(-0.0188321825), tensor(-0.0017928196), tensor(-0.0192516930), tensor(0.0040352754), tensor(0.0069702170), tensor(0.4283264279), tensor(-0.0133935213), tensor(-0.0046616350), tensor(-0.1946488470), tensor(-0.0380137041), tensor(0.4050756693), tensor(-0.0073437495), tensor(0.0087575503), tensor(-0.1854140460), tensor(-0.0423249155)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.1982321739, -1.6935521364, -0.1346679032,  0.5177706480,\n",
            "        -2.5248029232,  3.0417871475,  2.5538399220, -1.8092954159,\n",
            "        -1.4438149929,  2.0915694237,  1.3482650518,  0.9151967764,\n",
            "        -2.1911966801,  4.7602458000,  1.8966541290,  1.9728912115,\n",
            "         2.7684035301, -0.4696478844, -1.0653202534,  1.0665152073])\n",
            "btensor.grad: tensor([-3.9384200573, -1.3182299137,  0.9160519838, -2.4181084633,\n",
            "         0.5998961329, -1.7348586321, -0.5175611973,  2.1633224487,\n",
            "         3.8766150475, -2.9054951668, -0.7644642591, -3.0904831886,\n",
            "        -0.2050992250, -0.5178694725, -1.7312508821,  2.3911688328,\n",
            "         2.3533735275,  0.7015560269, -0.7492781878,  0.0303396583])\n",
            "ctensor.grad: tensor([ -3.4025778770, -18.2515983582, -17.3499507904,  -0.6339170933,\n",
            "          0.0622954778,   0.9666810632,   0.6499229670,   0.8034776449,\n",
            "        -12.6116199493,   1.2734280825,  11.8732948303,   3.8281633854,\n",
            "         -3.5882952213, -12.5761175156,   1.1903676987,  10.8914480209,\n",
            "          3.7731759548,  -2.9490909576])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1028.1547851562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6116482019), tensor(0.9969110489), tensor(1.0798101425), tensor(1.1948684454), tensor(1.1216580868), tensor(1.3940365314), tensor(1.0238574743), tensor(1.1821683645), tensor(0.9322935939), tensor(1.2046685219), tensor(1.0547811985), tensor(1.4945827723), tensor(0.9882408977), tensor(1.1945102215), tensor(1.1503357887), tensor(0.9471063614), tensor(1.0411109924), tensor(0.8818815351), tensor(1.1003561020), tensor(0.9893783927)]\n",
            "b:  [tensor(0.6590083241), tensor(1.1187146902), tensor(1.3290318251), tensor(1.0361680984), tensor(1.4669518471), tensor(0.8300110698), tensor(1.2392243147), tensor(1.1269811392), tensor(1.7388796806), tensor(0.8993885517), tensor(1.2806376219), tensor(0.7415217161), tensor(1.3063830137), tensor(1.2553033829), tensor(0.8950892091), tensor(0.9690857530), tensor(1.2735220194), tensor(1.6616277695), tensor(0.9939142466), tensor(1.1832767725)]\n",
            "c:  [tensor(0.0329547375), tensor(0.5850788951), tensor(0.5491468310), tensor(-0.0184592362), tensor(-0.0018253545), tensor(-0.0196585394), tensor(0.0037597469), tensor(0.0066572092), tensor(0.4356451035), tensor(-0.0140355695), tensor(-0.0083083175), tensor(-0.1951983422), tensor(-0.0336674042), tensor(0.4123156071), tensor(-0.0079439888), tensor(0.0054573934), tensor(-0.1860067248), tensor(-0.0384600312)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.1409201622, -1.8940823078, -0.2736863792,  0.4107121825,\n",
            "        -2.6505718231,  2.9155406952,  2.4222733974, -2.0018899441,\n",
            "        -1.6123101711,  1.9109642506,  1.1391310692,  0.7500283122,\n",
            "        -2.5167064667,  4.6818099022,  1.7945876122,  1.7882968187,\n",
            "         2.6984095573, -0.7216227055, -1.2806155682,  0.9185904264])\n",
            "btensor.grad: tensor([-4.0260763168, -1.4241983891,  0.7644318938, -2.5734586716,\n",
            "         0.4640207291, -1.8942868710, -0.7105358839,  1.9701691866,\n",
            "         3.6773033142, -3.0006003380, -0.8786913157, -3.2124264240,\n",
            "        -0.3722704053, -0.6070479155, -1.8699059486,  2.2443308830,\n",
            "         2.2225997448,  0.5426215529, -0.8919490576, -0.1144527197])\n",
            "ctensor.grad: tensor([ -2.6002311707, -17.8376216888, -16.9683036804,  -0.7458925843,\n",
            "          0.0650698021,   0.8136937618,   0.5510569811,   0.6260153055,\n",
            "        -14.6373653412,   1.2840969563,   7.2933654785,   1.0989778042,\n",
            "         -8.6925964355, -14.4798974991,   1.2004787922,   6.6003136635,\n",
            "          1.1853450537,  -7.7297716141])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1027.5009765625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6099159718), tensor(0.9975817800), tensor(1.0796746016), tensor(1.1944245100), tensor(1.1227356195), tensor(1.3924267292), tensor(1.0224090815), tensor(1.1829560995), tensor(0.9327695966), tensor(1.2035360336), tensor(1.0540217161), tensor(1.4940555096), tensor(0.9892414808), tensor(1.1920074224), tensor(1.1492141485), tensor(0.9459937811), tensor(1.0395511389), tensor(0.8819761872), tensor(1.1007344723), tensor(0.9886556268)]\n",
            "b:  [tensor(0.6608029008), tensor(1.1191415787), tensor(1.3284679651), tensor(1.0372242928), tensor(1.4664896727), tensor(0.8307556510), tensor(1.2394018173), tensor(1.1257789135), tensor(1.7368447781), tensor(0.9006268978), tensor(1.2808467150), tensor(0.7429469824), tensor(1.3063551188), tensor(1.2554085255), tensor(0.8958176970), tensor(0.9677317739), tensor(1.2722032070), tensor(1.6611696482), tensor(0.9941518903), tensor(1.1830874681)]\n",
            "c:  [tensor(0.0355086029), tensor(0.5938482881), tensor(0.5574924350), tensor(-0.0182432756), tensor(-0.0018610364), tensor(-0.0203693062), tensor(0.0033385681), tensor(0.0059672655), tensor(0.4392052889), tensor(-0.0147519866), tensor(-0.0187029615), tensor(-0.1988431364), tensor(-0.0377475321), tensor(0.4160333276), tensor(-0.0086126570), tensor(-0.0041543772), tensor(-0.1894960999), tensor(-0.0424713567)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.4644975662, -1.3415043354,  0.2709878683,  0.8879508376,\n",
            "        -2.1550097466,  3.2195565701,  2.8968868256, -1.5755463839,\n",
            "        -0.9520435333,  2.2649130821,  1.5190398693,  1.0545170307,\n",
            "        -2.0012164116,  5.0055050850,  2.2432188988,  2.2252192497,\n",
            "         3.1196951866, -0.1893132925, -0.7567689419,  1.4455713034])\n",
            "btensor.grad: tensor([-3.5891480446, -0.8538141251,  1.1278109550, -2.1123199463,\n",
            "         0.9243994951, -1.4892103672, -0.3550342321,  2.4043922424,\n",
            "         4.0696926117, -2.4766530991, -0.4182788134, -2.8505573273,\n",
            "         0.0558806658, -0.2101715803, -1.4569833279,  2.7079005241,\n",
            "         2.6376867294,  0.9161980152, -0.4752922058,  0.3785312176])\n",
            "ctensor.grad: tensor([ -5.1077299118, -17.5387954712, -16.6911869049,  -0.4319201708,\n",
            "          0.0713636577,   1.4215321541,   0.8423573375,   1.3798873425,\n",
            "         -7.1204004288,   1.4328333139,  20.7892856598,   7.2895970345,\n",
            "          8.1602535248,  -7.4354434013,   1.3373370171,  19.2235393524,\n",
            "          6.9787621498,   8.0226488113])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1027.1931152344, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6086374521), tensor(0.9991831779), tensor(1.0803664923), tensor(1.1946841478), tensor(1.1245539188), tensor(1.3913588524), tensor(1.0217128992), tensor(1.1845105886), tensor(0.9342566133), tensor(1.2030910254), tensor(1.0540262461), tensor(1.4941257238), tensor(0.9913298488), tensor(1.1900012493), tensor(1.1487566233), tensor(0.9456675649), tensor(1.0385814905), tensor(0.8830716014), tensor(1.1020261049), tensor(0.9887635708)]\n",
            "b:  [tensor(0.6631969810), tensor(1.1203507185), tensor(1.3285386562), tensor(1.0390062332), tensor(1.4667379856), tensor(0.8321844339), tensor(1.2402613163), tensor(1.1253504753), tensor(1.7355583906), tensor(0.9025752544), tensor(1.2817301750), tensor(0.7449496984), tensor(1.3070532084), tensor(1.2560845613), tensor(0.8972072005), tensor(0.9671167731), tensor(1.2715444565), tensor(1.6613557339), tensor(0.9950642586), tensor(1.1836603880)]\n",
            "c:  [tensor(0.0339285173), tensor(0.6019382477), tensor(0.5652122498), tensor(-0.0174644589), tensor(-0.0018937930), tensor(-0.0200787149), tensor(0.0034415179), tensor(0.0064838752), tensor(0.4546297193), tensor(-0.0153198782), tensor(-0.0062263208), tensor(-0.1911919117), tensor(-0.0147328097), tensor(0.4308848381), tensor(-0.0091453465), tensor(0.0076722908), tensor(-0.1823323369), tensor(-0.0211091284)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.5569982529, -3.2027804852, -1.3837468624, -0.5192688704,\n",
            "        -3.6365368366,  2.1356420517,  1.3923346996, -3.1090562344,\n",
            "        -2.9740259647,  0.8901088834, -0.0090630651, -0.1403770745,\n",
            "        -4.1766881943,  4.0124483109,  0.9150629044,  0.6524410248,\n",
            "         1.9391887188, -2.1908535957, -2.5832097530, -0.2158932686])\n",
            "btensor.grad: tensor([-4.7882156372, -2.4183778763, -0.1413978934, -3.5639445782,\n",
            "        -0.4966780245, -2.8575859070, -1.7191007137,  0.8568336964,\n",
            "         2.5728468895, -3.8967404366, -1.7669634819, -4.0053906441,\n",
            "        -1.3961911201, -1.3521825075, -2.7790467739,  1.2300018072,\n",
            "         1.3175907135, -0.3720663786, -1.8246803284, -1.1457422972])\n",
            "ctensor.grad: tensor([  3.1601684093, -16.1798629761, -15.4396686554,  -1.5576343536,\n",
            "          0.0655132607,  -0.5811823010,  -0.2058994025,  -1.0332190990,\n",
            "        -30.8488693237,   1.1357823610, -24.9532794952, -15.3024339676,\n",
            "        -46.0294418335, -29.7030200958,   1.0653786659, -23.6533355713,\n",
            "        -14.3275279999, -42.7244529724])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.3706054688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6059157848), tensor(0.9978895187), tensor(1.0784643888), tensor(1.1927064657), tensor(1.1239418983), tensor(1.3886657953), tensor(1.0187199116), tensor(1.1836619377), tensor(0.9325892329), tensor(1.2006431818), tensor(1.0518218279), tensor(1.4923939705), tensor(0.9902806878), tensor(1.1864908934), tensor(1.1462160349), tensor(0.9429817796), tensor(1.0357034206), tensor(0.8811990619), tensor(1.1005243063), tensor(0.9862905145)]\n",
            "b:  [tensor(0.6633843780), tensor(1.1188049316), tensor(1.3265594244), tensor(1.0382962227), tensor(1.4646146297), tensor(0.8313565254), tensor(1.2389601469), tensor(1.1224852800), tensor(1.7319722176), tensor(0.9019439220), tensor(1.2802780867), tensor(0.7449674010), tensor(1.3053759336), tensor(1.2547978163), tensor(0.8963547945), tensor(0.9640974402), tensor(1.2687503099), tensor(1.6594443321), tensor(0.9937203526), tensor(1.1816835403)]\n",
            "c:  [tensor(0.0450200140), tensor(0.6079573035), tensor(0.5709856749), tensor(-0.0183565393), tensor(-0.0019425271), tensor(-0.0228511337), tensor(0.0019565620), tensor(0.0032267100), tensor(0.4347020090), tensor(-0.0164242070), tensor(-0.0618097335), tensor(-0.2180072218), tensor(-0.0772117972), tensor(0.4124656916), tensor(-0.0101748658), tensor(-0.0443470404), tensor(-0.2075728029), tensor(-0.0798378512)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([5.4434309006, 2.5872945786, 3.8041667938, 3.9553794861, 1.2239912748,\n",
            "        5.3862133026, 5.9860229492, 1.6972131729, 3.3347558975, 4.8956365585,\n",
            "        4.4088749886, 3.4635016918, 2.0983750820, 7.0207200050, 5.0812888145,\n",
            "        5.3715882301, 5.7562179565, 3.7450604439, 3.0035698414, 4.9461631775])\n",
            "btensor.grad: tensor([-0.3748040199,  3.0914547443,  3.9585196972,  1.4200757742,\n",
            "         4.2466526031,  1.6558133364,  2.6023201942,  5.7304449081,\n",
            "         7.1722755432,  1.2627186775,  2.9041187763, -0.0354067460,\n",
            "         3.3545949459,  2.5734708309,  1.7048175335,  6.0387220383,\n",
            "         5.5882573128,  3.8228285313,  2.6877975464,  3.9536092281])\n",
            "ctensor.grad: tensor([-2.2182994843e+01, -1.2038138390e+01, -1.1546864510e+01,\n",
            "         1.7841604948e+00,  9.7468353808e-02,  5.5448360443e+00,\n",
            "         2.9699115753e+00,  6.5143299103e+00,  3.9855407715e+01,\n",
            "         2.2086577415e+00,  1.1116682434e+02,  5.3630630493e+01,\n",
            "         1.2495796967e+02,  3.6838291168e+01,  2.0590388775e+00,\n",
            "         1.0403865814e+02,  5.0480918884e+01,  1.1745743561e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1067.1362304688, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6088179350), tensor(1.0074377060), tensor(1.0866229534), tensor(1.1991893053), tensor(1.1318651438), tensor(1.3923398256), tensor(1.0252077579), tensor(1.1909658909), tensor(0.9435004592), tensor(1.2064185143), tensor(1.0586988926), tensor(1.4971433878), tensor(1.0014686584), tensor(1.1896342039), tensor(1.1521372795), tensor(0.9498628974), tensor(1.0402617455), tensor(0.8912643194), tensor(1.1093808413), tensor(0.9939053655)]\n",
            "b:  [tensor(0.6692758799), tensor(1.1250559092), tensor(1.3309692144), tensor(1.0444903374), tensor(1.4696465731), tensor(0.8371263742), tensor(1.2443054914), tensor(1.1273823977), tensor(1.7359597683), tensor(0.9082133174), tensor(1.2855491638), tensor(0.7505679131), tensor(1.3108805418), tensor(1.2595186234), tensor(0.9018911123), tensor(0.9687342644), tensor(1.2729295492), tensor(1.6639927626), tensor(0.9990296960), tensor(1.1874169111)]\n",
            "c:  [tensor(0.0041893609), tensor(0.5815058351), tensor(0.5464119315), tensor(-0.0110157561), tensor(-0.0019236685), tensor(-0.0108299330), tensor(0.0077733854), tensor(0.0171883944), tensor(0.5521947145), tensor(-0.0158637799), tensor(0.1321518719), tensor(-0.1330621988), tensor(0.1313376278), tensor(0.5255616307), tensor(-0.0096454183), tensor(0.1430369616), tensor(-0.1248794273), tensor(0.1213145033)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ -5.8042173386, -19.0963497162, -16.3171653748, -12.9656600952,\n",
            "        -15.8465423584,  -7.3481588364, -12.9755973816, -14.6079950333,\n",
            "        -21.8224868774, -11.5506048203, -13.7541961670,  -9.4987964630,\n",
            "        -22.3758220673,  -6.2867197990, -11.8424806595, -13.7622528076,\n",
            "         -9.1167516708, -20.1305217743, -17.7130832672, -15.2297096252])\n",
            "btensor.grad: tensor([-11.7830066681, -12.5020551682,  -8.8196773529, -12.3882417679,\n",
            "        -10.0638408661, -11.5396499634, -10.6906614304,  -9.7941904068,\n",
            "         -7.9751243591, -12.5388383865, -10.5421304703, -11.2010641098,\n",
            "        -11.0092687607,  -9.4415664673, -11.0726861954,  -9.2736330032,\n",
            "         -8.3584108353,  -9.0967979431, -10.6186933517, -11.4666614532])\n",
            "ctensor.grad: tensor([ 8.1661300659e+01,  5.2902915955e+01,  4.9147468567e+01,\n",
            "        -1.4681565285e+01, -3.7717241794e-02, -2.4042400360e+01,\n",
            "        -1.1633646011e+01, -2.7923368454e+01, -2.3498545837e+02,\n",
            "        -1.1208525896e+00, -3.8792318726e+02, -1.6989004517e+02,\n",
            "        -4.1709881592e+02, -2.2619181824e+02, -1.0588949919e+00,\n",
            "        -3.7476800537e+02, -1.6538674927e+02, -4.0230468750e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1147.1953125000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6034736633), tensor(0.9994277954), tensor(1.0797923803), tensor(1.1924591064), tensor(1.1247031689), tensor(1.3864768744), tensor(1.0181118250), tensor(1.1837929487), tensor(0.9356951714), tensor(1.1999527216), tensor(1.0519229174), tensor(1.4911848307), tensor(0.9939953089), tensor(1.1835858822), tensor(1.1456849575), tensor(0.9424901009), tensor(1.0334517956), tensor(0.8833975792), tensor(1.1021890640), tensor(0.9863470197)]\n",
            "b:  [tensor(0.6584005356), tensor(1.1145427227), tensor(1.3217852116), tensor(1.0343071222), tensor(1.4599848986), tensor(0.8272224069), tensor(1.2347776890), tensor(1.1180619001), tensor(1.7269333601), tensor(0.8971533775), tensor(1.2756019831), tensor(0.7405662537), tensor(1.3014876842), tensor(1.2500370741), tensor(0.8920819759), tensor(0.9595317245), tensor(1.2637588978), tensor(1.6544651985), tensor(0.9892358184), tensor(1.1773743629)]\n",
            "c:  [tensor(0.0331563242), tensor(0.5192448497), tensor(0.4845755696), tensor(-0.0115730725), tensor(-0.0019357146), tensor(-0.0120669948), tensor(0.0070799594), tensor(0.0156218410), tensor(0.5433073640), tensor(-0.0162005369), tensor(0.1069274694), tensor(-0.1503460705), tensor(0.0897718742), tensor(0.5166423321), tensor(-0.0099906037), tensor(0.1180879101), tensor(-0.1419396251), tensor(0.0795002505)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([10.6884679794, 16.0198440552, 13.6611318588, 13.4603834152,\n",
            "        14.3240613937, 11.7259082794, 14.1918792725, 14.3457822800,\n",
            "        15.6105213165, 12.9315490723, 13.5519828796, 11.9171133041,\n",
            "        14.9467430115, 12.0965318680, 12.9046840668, 14.7456235886,\n",
            "        13.6198749542, 15.7334794998, 14.3834991455, 15.1167287827])\n",
            "btensor.grad: tensor([21.7506942749, 21.0264587402, 18.3679637909, 20.3665370941,\n",
            "        19.3233699799, 19.8079624176, 19.0555362701, 18.6409187317,\n",
            "        18.0527839661, 22.1199073792, 19.8942584991, 20.0032634735,\n",
            "        18.7857913971, 18.9630546570, 19.6182308197, 18.4050941467,\n",
            "        18.3413085938, 19.0550689697, 19.5877246857, 20.0851001740])\n",
            "ctensor.grad: tensor([-5.7933925629e+01,  1.2452193451e+02,  1.2367274475e+02,\n",
            "         1.1146321297e+00,  2.4092152715e-02,  2.4741237164e+00,\n",
            "         1.3868522644e+00,  3.1331062317e+00,  1.7774753571e+01,\n",
            "         6.7351257801e-01,  5.0448806763e+01,  3.4567726135e+01,\n",
            "         8.3131500244e+01,  1.7838596344e+01,  6.9036996365e-01,\n",
            "         4.9898097992e+01,  3.4120391846e+01,  8.3628501892e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1112.9948730469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5989743471), tensor(0.9926198721), tensor(1.0739426613), tensor(1.1867651939), tensor(1.1187568903), tensor(1.3815243244), tensor(1.0120371580), tensor(1.1777538061), tensor(0.9289559722), tensor(1.1945266724), tensor(1.0462108850), tensor(1.4861767292), tensor(0.9876203537), tensor(1.1785137653), tensor(1.1402324438), tensor(0.9362011552), tensor(1.0276769400), tensor(0.8766370416), tensor(1.0960654020), tensor(0.9799081683)]\n",
            "b:  [tensor(0.6501582861), tensor(1.1061627865), tensor(1.3145540953), tensor(1.0262811184), tensor(1.4522415400), tensor(0.8195829988), tensor(1.2273352146), tensor(1.1106120348), tensor(1.7195456028), tensor(0.8884145021), tensor(1.2677400112), tensor(0.7331439853), tensor(1.2940053940), tensor(1.2428236008), tensor(0.8844640255), tensor(0.9521470070), tensor(1.2565134764), tensor(1.6468940973), tensor(0.9815890789), tensor(1.1693911552)]\n",
            "c:  [tensor(0.0585149266), tensor(0.4729063809), tensor(0.4389792085), tensor(-0.0150193004), tensor(-0.0020208510), tensor(-0.0201060958), tensor(0.0024517216), tensor(0.0051965276), tensor(0.5291966200), tensor(-0.0167520661), tensor(0.0676501840), tensor(-0.1768718362), tensor(0.0257215425), tensor(0.5026617646), tensor(-0.0105474675), tensor(0.0796972215), tensor(-0.1677945554), tensor(0.0160967633)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 8.9986648560, 13.6158361435, 11.6995506287, 11.3878440857,\n",
            "        11.8926582336,  9.9050931931, 12.1494483948, 12.0782337189,\n",
            "        13.4784078598, 10.8520822525, 11.4240789413, 10.0161561966,\n",
            "        12.7499570847, 10.1441650391, 10.9050683975, 12.5779371262,\n",
            "        11.5497674942, 13.5210275650, 12.2472496033, 12.8776502609])\n",
            "btensor.grad: tensor([16.4844589233, 16.7598743439, 14.4621515274, 16.0518970490,\n",
            "        15.4867649078, 15.2788419724, 14.8848571777, 14.8997764587,\n",
            "        14.7754268646, 17.4776992798, 15.7239122391, 14.8445339203,\n",
            "        14.9644699097, 14.4269943237, 15.2359218597, 14.7694158554,\n",
            "        14.4908666611, 15.1422290802, 15.2935085297, 15.9664497375])\n",
            "ctensor.grad: tensor([-50.7172012329,  92.6769409180,  91.1927413940,   6.8924555779,\n",
            "          0.1702725738,  16.0781993866,   9.2564754486,  20.8506259918,\n",
            "         28.2214889526,   1.1030595303,  78.5545654297,  53.0515251160,\n",
            "        128.1006622314,  27.9611225128,   1.1137282848,  76.7813796997,\n",
            "         51.7098503113, 126.8069686890])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1069.0694580078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5954755545), tensor(0.9878664613), tensor(1.0695779324), tensor(1.1826061010), tensor(1.1149090528), tensor(1.3776822090), tensor(1.0073709488), tensor(1.1736021042), tensor(0.9239937067), tensor(1.1904643774), tensor(1.0419932604), tensor(1.4825106859), tensor(0.9829615355), tensor(1.1745425463), tensor(1.1361577511), tensor(0.9314450622), tensor(1.0233906507), tensor(0.8716083169), tensor(1.0915570259), tensor(0.9751022458)]\n",
            "b:  [tensor(0.6458666325), tensor(1.1008566618), tensor(1.3099392653), tensor(1.0214167833), tensor(1.4471242428), tensor(0.8151502609), tensor(1.2227834463), tensor(1.1054215431), tensor(1.7140536308), tensor(0.8832538128), tensor(1.2628631592), tensor(0.7293348908), tensor(1.2891860008), tensor(1.2387077808), tensor(0.8800072670), tensor(0.9469743371), tensor(1.2517074347), tensor(1.6420040131), tensor(0.9769598842), tensor(1.1642681360)]\n",
            "c:  [tensor(0.0813961029), tensor(0.4549460709), tensor(0.4218934476), tensor(-0.0185366198), tensor(-0.0021332805), tensor(-0.0291708056), tensor(-0.0029142085), tensor(-0.0068110814), tensor(0.5059258342), tensor(-0.0176860057), tensor(0.0060237013), tensor(-0.2149826884), tensor(-0.0658034980), tensor(0.4803988338), tensor(-0.0114482054), tensor(0.0212680921), tensor(-0.2037672848), tensor(-0.0709516332)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 6.9974708557,  9.5067911148,  8.7293586731,  8.3181257248,\n",
            "         7.6956181526,  7.6843261719,  9.3324289322,  8.3034172058,\n",
            "         9.9244995117,  8.1245107651,  8.4353599548,  7.3320736885,\n",
            "         9.3175888062,  7.9425487518,  8.1494522095,  9.5122261047,\n",
            "         8.5726318359, 10.0574159622,  9.0167179108,  9.6119022369])\n",
            "btensor.grad: tensor([ 8.5833005905, 10.6123514175,  9.2297639847,  9.7285871506,\n",
            "        10.2345743179,  8.8655090332,  9.1035242081, 10.3809814453,\n",
            "        10.9838237762, 10.3213272095,  9.7538175583,  7.6181707382,\n",
            "         9.6386728287,  8.2315893173,  8.9134950638, 10.3452987671,\n",
            "         9.6120567322,  9.7801342010,  9.2583761215, 10.2460365295])\n",
            "ctensor.grad: tensor([-45.7623558044,  35.9206047058,  34.1715393066,   7.0346374512,\n",
            "          0.2248592079,  18.1294193268,  10.7318592072,  24.0152168274,\n",
            "         46.5415763855,   1.8678772449, 123.2529602051,  76.2217102051,\n",
            "        183.0500793457,  44.5258712769,   1.8014764786, 116.8582534790,\n",
            "         71.9454650879, 174.0967864990])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1031.6270751953, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5946233273), tensor(0.9890748858), tensor(1.0701962709), tensor(1.1830012798), tensor(1.1163712740), tensor(1.3768429756), tensor(1.0070414543), tensor(1.1747606993), tensor(0.9252303839), tensor(1.1901522875), tensor(1.0420176983), tensor(1.4825170040), tensor(0.9841000438), tensor(1.1732096672), tensor(1.1359272003), tensor(0.9313287735), tensor(1.0230723619), tensor(0.8723357916), tensor(1.0924063921), tensor(0.9754846692)]\n",
            "b:  [tensor(0.6481548548), tensor(1.1021703482), tensor(1.3101527691), tensor(1.0229692459), tensor(1.4475649595), tensor(0.8165500760), tensor(1.2234916687), tensor(1.1052690744), tensor(1.7130534649), tensor(0.8852065206), tensor(1.2638400793), tensor(0.7311977744), tensor(1.2899032831), tensor(1.2394962311), tensor(0.8814055324), tensor(0.9469438791), tensor(1.2514717579), tensor(1.6422680616), tensor(0.9780532122), tensor(1.1650350094)]\n",
            "c:  [tensor(0.0793268308), tensor(0.4679242969), tensor(0.4343915582), tensor(-0.0166797992), tensor(-0.0021956039), tensor(-0.0283859223), tensor(-0.0030203692), tensor(-0.0058987625), tensor(0.5158318281), tensor(-0.0181239825), tensor(0.0114074517), tensor(-0.2124442160), tensor(-0.0521714762), tensor(0.4898771346), tensor(-0.0118538309), tensor(0.0263561867), tensor(-0.2014995217), tensor(-0.0585296340)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.7043612003, -2.4168543816, -1.2366399765, -0.7903659344,\n",
            "        -2.9244060516,  1.6785229445,  0.6588804722, -2.3170900345,\n",
            "        -2.4733238220,  0.6241879463, -0.0488258600, -0.0125589967,\n",
            "        -2.2770247459,  2.6657149792,  0.4612069726,  0.2325896025,\n",
            "         0.6366059780, -1.4549574852, -1.6986489296, -0.7649062872])\n",
            "btensor.grad: tensor([-4.5764856339, -2.6273593903, -0.4269677103, -3.1048271656,\n",
            "        -0.8815472126, -2.7996134758, -1.4164291620,  0.3048311472,\n",
            "         2.0002851486, -3.9053592682, -1.9538714886, -3.7257254124,\n",
            "        -1.4345564842, -1.5768092871, -2.7965660095,  0.0609114766,\n",
            "         0.4714625776, -0.5279976726, -2.1866393089, -1.5337295532])\n",
            "ctensor.grad: tensor([  4.1385402679, -25.9564437866, -24.9962463379,  -3.7136397362,\n",
            "          0.1246469393,  -1.5697673559,   0.2123217285,  -1.8246375322,\n",
            "        -19.8120365143,   0.8759521246, -10.7674999237,  -5.0769534111,\n",
            "        -27.2640380859, -18.9565753937,   0.8112499714, -10.1761884689,\n",
            "         -4.5355305672, -24.8439960480])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.9434814453, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5931396484), tensor(0.9891596437), tensor(1.0697472095), tensor(1.1824346781), tensor(1.1169108152), tensor(1.3753180504), tensor(1.0057220459), tensor(1.1749750376), tensor(0.9251666665), tensor(1.1890165806), tensor(1.0411424637), tensor(1.4818038940), tensor(0.9840250611), tensor(1.1712166071), tensor(1.1348479986), tensor(0.9302409291), tensor(1.0219532251), tensor(0.8718659282), tensor(1.0921454430), tensor(0.9747563601)]\n",
            "b:  [tensor(0.6495790482), tensor(1.1023774147), tensor(1.3095438480), tensor(1.0235329866), tensor(1.4470489025), tensor(0.8170421124), tensor(1.2233617306), tensor(1.1041305065), tensor(1.7111276388), tensor(0.8861283660), tensor(1.2638810873), tensor(0.7322830558), tensor(1.2896933556), tensor(1.2395142317), tensor(0.8819013238), tensor(0.9459229112), tensor(1.2503883839), tensor(1.6417019367), tensor(0.9782449007), tensor(1.1647952795)]\n",
            "c:  [tensor(0.0834039226), tensor(0.4799382091), tensor(0.4459426403), tensor(-0.0161700957), tensor(-0.0022707391), tensor(-0.0300430637), tensor(-0.0043337159), tensor(-0.0079677179), tensor(0.5140401125), tensor(-0.0187705979), tensor(-0.0055772346), tensor(-0.2207437754), tensor(-0.0661045909), tensor(0.4885747731), tensor(-0.0124477427), tensor(0.0108879218), tensor(-0.2092225850), tensor(-0.0714317262)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.9672443867, -0.1694659889,  0.8982241154,  1.1332654953,\n",
            "        -1.0791518688,  3.0497722626,  2.6387231350, -0.4286589622,\n",
            "         0.1273891926,  2.2714352608,  1.7505259514,  1.4262806177,\n",
            "         0.1499941349,  3.9860861301,  2.1583971977,  2.1756768227,\n",
            "         2.2383441925,  0.9396894574,  0.5220110416,  1.4566313028])\n",
            "btensor.grad: tensor([-2.8483664989, -0.4141802788,  1.2179280519, -1.1273779869,\n",
            "         1.0321629047, -0.9840751886,  0.2598292232,  2.2770524025,\n",
            "         3.8516993523, -1.8436574936, -0.0821070671, -2.1705334187,\n",
            "         0.4197408557, -0.0359860659, -0.9915295839,  2.0419211388,\n",
            "         2.1668033600,  1.1321687698, -0.3834278584,  0.4795525670])\n",
            "ctensor.grad: tensor([ -8.1541776657, -24.0278282166, -23.1021556854,  -1.0194083452,\n",
            "          0.1502700895,   3.3142812252,   2.6266937256,   4.1379113197,\n",
            "          3.5833747387,   1.2932289839,  33.9693717957,  16.5991077423,\n",
            "         27.8662281036,   2.6047322750,   1.1878244877,  30.9365291595,\n",
            "         15.4461145401,  25.8041839600])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.5032958984, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5924413204), tensor(0.9908884168), tensor(1.0707796812), tensor(1.1831684113), tensor(1.1187902689), tensor(1.3746925592), tensor(1.0057225227), tensor(1.1765717268), tensor(0.9269556999), tensor(1.1890112162), tensor(1.0415375233), tensor(1.4820992947), tensor(0.9857984781), tensor(1.1700127125), tensor(1.1349050999), tensor(0.9304972291), tensor(1.0218740702), tensor(0.8731282353), tensor(1.0935001373), tensor(0.9755455256)]\n",
            "b:  [tensor(0.6522132754), tensor(1.1040644646), tensor(1.3100274801), tensor(1.0254777670), tensor(1.4478024244), tensor(0.8187881708), tensor(1.2243990898), tensor(1.1042885780), tensor(1.7104109526), tensor(0.8884544969), tensor(1.2651765347), tensor(0.7344602346), tensor(1.2907428741), tensor(1.2405477762), tensor(0.8836329579), tensor(0.9461672306), tensor(1.2503852844), tensor(1.6422578096), tensor(0.9796498418), tensor(1.1658978462)]\n",
            "c:  [tensor(0.0789969042), tensor(0.4914247394), tensor(0.4570365846), tensor(-0.0138656795), tensor(-0.0023284105), tensor(-0.0283247083), tensor(-0.0038662700), tensor(-0.0057542338), tensor(0.5289009213), tensor(-0.0191648118), tensor(0.0093390690), tensor(-0.2131473571), tensor(-0.0402641967), tensor(0.5026301742), tensor(-0.0128170252), tensor(0.0247416906), tensor(-0.2022401541), tensor(-0.0477700830)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3966341019e+00, -3.4576051235e+00, -2.0649142265e+00,\n",
            "        -1.4674042463e+00, -3.7590205669e+00,  1.2509185076e+00,\n",
            "        -8.7088346481e-04, -3.1934423447e+00, -3.5780582428e+00,\n",
            "         1.0767683387e-02, -7.9003411531e-01, -5.9076237679e-01,\n",
            "        -3.5468373299e+00,  2.4077618122e+00, -1.1426830292e-01,\n",
            "        -5.1256310940e-01,  1.5838927031e-01, -2.5246539116e+00,\n",
            "        -2.7095055580e+00, -1.5783400536e+00])\n",
            "btensor.grad: tensor([-5.2685027122, -3.3741497993, -0.9671895504, -3.8895373344,\n",
            "        -1.5069777966, -3.4921226501, -2.0748004913, -0.3162440062,\n",
            "         1.4333081245, -4.6522169113, -2.5909302235, -4.3544001579,\n",
            "        -2.0990543365, -2.0670146942, -3.4633059502, -0.4886845946,\n",
            "         0.0062038600, -1.1117950678, -2.8098258972, -2.2050652504])\n",
            "ctensor.grad: tensor([  8.8140373230, -22.9730510712, -22.1879062653,  -4.6088318825,\n",
            "          0.1153430119,  -3.4367105961,  -0.9348918200,  -4.4269680977,\n",
            "        -29.7216110229,   0.7884261012, -29.8326053619, -15.1928462982,\n",
            "        -51.6807823181, -28.1108093262,   0.7385643721, -27.7075366974,\n",
            "        -13.9648561478, -47.3232879639])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.9755859375, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5905061960), tensor(0.9902164340), tensor(1.0696007013), tensor(1.1819359064), tensor(1.1186878681), tensor(1.3726919889), tensor(1.0037201643), tensor(1.1761423349), tensor(0.9260129333), tensor(1.1873198748), tensor(1.0400683880), tensor(1.4808938503), tensor(0.9849717021), tensor(1.1675397158), tensor(1.1332336664), tensor(0.9287331700), tensor(1.0201685429), tensor(0.8718634844), tensor(1.0925109386), tensor(0.9740410447)]\n",
            "b:  [tensor(0.6529722214), tensor(1.1034368277), tensor(1.3088086843), tensor(1.0253216028), tensor(1.4465789795), tensor(0.8186228275), tensor(1.2236722708), tensor(1.1024385691), tensor(1.7078133821), tensor(0.8885813951), tensor(1.2645106316), tensor(0.7349643111), tensor(1.2898489237), tensor(1.2399818897), tensor(0.8834545016), tensor(0.9444061518), tensor(1.2486722469), tensor(1.6410887241), tensor(0.9791637063), tensor(1.1649091244)]\n",
            "c:  [tensor(0.0872368217), tensor(0.5011657476), tensor(0.4664258659), tensor(-0.0142977303), tensor(-0.0024132000), tensor(-0.0316612683), tensor(-0.0059804819), tensor(-0.0098583270), tensor(0.5194423199), tensor(-0.0199690331), tensor(-0.0226586163), tensor(-0.2289175391), tensor(-0.0737072378), tensor(0.4941733181), tensor(-0.0135577638), tensor(-0.0046812966), tensor(-0.2168914825), tensor(-0.0787640512)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([3.8702173233, 1.3439991474, 2.3580491543, 2.4650804996, 0.2047111988,\n",
            "        4.0011081696, 4.0047798157, 0.8586784601, 1.8855166435, 3.3827672005,\n",
            "        2.9382026196, 2.4109199047, 1.6535604000, 4.9458837509, 3.3429265022,\n",
            "        3.5281534195, 3.4110865593, 2.5295557976, 1.9783828259, 3.0089361668])\n",
            "btensor.grad: tensor([-1.5179110765,  1.2553243637,  2.4376766682,  0.3123806715,\n",
            "         2.4469926357,  0.3306835294,  1.4536017179,  3.7001113892,\n",
            "         5.1952266693, -0.2537439167,  1.3317108154, -1.0081274509,\n",
            "         1.7880172729,  1.1317261457,  0.3569608927,  3.5221173763,\n",
            "         3.4260470867,  2.3381667137,  0.9723187089,  1.9774634838])\n",
            "ctensor.grad: tensor([-16.4798412323, -19.4820289612, -18.7785682678,   0.8641020060,\n",
            "          0.1695789099,   6.6731166840,   4.2284235954,   8.2081861496,\n",
            "         18.9172573090,   1.6084426641,  63.9953651428,  31.5403652191,\n",
            "         66.8860855103,  16.9137268066,   1.4814776182,  58.8459701538,\n",
            "         29.3026580811,  61.9879379272])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1032.7231445312, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5904875994), tensor(0.9934070706), tensor(1.0719461441), tensor(1.1837905645), tensor(1.1217138767), tensor(1.3728654385), tensor(1.0049127340), tensor(1.1789079905), tensor(0.9294649959), tensor(1.1883450747), tensor(1.0416315794), tensor(1.4820663929), tensor(0.9884717464), tensor(1.1670502424), tensor(1.1343181133), tensor(0.9302002788), tensor(1.0209825039), tensor(0.8747035265), tensor(1.0952955484), tensor(0.9761661291)]\n",
            "b:  [tensor(0.6564707160), tensor(1.1062407494), tensor(1.3101590872), tensor(1.0283087492), tensor(1.4483201504), tensor(0.8213359714), tensor(1.2256498337), tensor(1.1036521196), tensor(1.7081013918), tensor(0.8919309974), tensor(1.2667561769), tensor(0.7379627824), tensor(1.2918967009), tensor(1.2418047190), tensor(0.8861244917), tensor(0.9456496835), tensor(1.2495414019), tensor(1.6425174475), tensor(0.9814981222), tensor(1.1670641899)]\n",
            "c:  [tensor(0.0749813467), tensor(0.5088124275), tensor(0.4740359187), tensor(-0.0103359669), tensor(-0.0024553719), tensor(-0.0267909300), tensor(-0.0038450765), tensor(-0.0037147901), tensor(0.5503509641), tensor(-0.0201582536), tensor(0.0224169306), tensor(-0.2071478665), tensor(-0.0130431727), tensor(0.5231807232), tensor(-0.0137440404), tensor(0.0373024307), tensor(-0.1965156347), tensor(-0.0225700401)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0372445583, -6.3812875748, -4.6909699440, -3.7092883587,\n",
            "        -6.0520563126, -0.3467842340, -2.3851280212, -5.5313978195,\n",
            "        -6.9040837288, -2.0502860546, -3.1263110638, -2.3451182842,\n",
            "        -7.0000772476,  0.9789950848, -2.1689696312, -2.9342558384,\n",
            "        -1.6279813051, -5.6800751686, -5.5693235397, -4.2501659393])\n",
            "btensor.grad: tensor([-6.9970335960, -5.6078128815, -2.7007544041, -5.9742136002,\n",
            "        -3.4824306965, -5.4262990952, -3.9550108910, -2.4270856380,\n",
            "        -0.5760648847, -6.6992182732, -4.4910621643, -5.9969215393,\n",
            "        -4.0955724716, -3.6457109451, -5.3399572372, -2.4870941639,\n",
            "        -1.7382879257, -2.8574428558, -4.6688237190, -4.3100242615])\n",
            "ctensor.grad: tensor([ 2.4510940552e+01, -1.5293357849e+01, -1.5220109940e+01,\n",
            "        -7.9235272408e+00,  8.4343791008e-02, -9.7406768799e+00,\n",
            "        -4.2708106041e+00, -1.2287073135e+01, -6.1817333221e+01,\n",
            "         3.7844270468e-01, -9.0151092529e+01, -4.3539340973e+01,\n",
            "        -1.2132812500e+02, -5.8014781952e+01,  3.7255316973e-01,\n",
            "        -8.3967453003e+01, -4.0751701355e+01, -1.1238801575e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1041.1617431641, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5876078606), tensor(0.9906399846), tensor(1.0689780712), tensor(1.1809453964), tensor(1.1197727919), tensor(1.3697795868), tensor(1.0013651848), tensor(1.1765998602), tensor(0.9263028502), tensor(1.1853265762), tensor(1.0386620760), tensor(1.4795522690), tensor(0.9855862260), tensor(1.1636362076), tensor(1.1312795877), tensor(0.9267567396), tensor(1.0178744793), tensor(0.8713662028), tensor(1.0923808813), tensor(0.9727908969)]\n",
            "b:  [tensor(0.6550450921), tensor(1.1033016443), tensor(1.3072685003), tensor(1.0259549618), tensor(1.4451707602), tensor(0.8192079663), tensor(1.2231479883), tensor(1.1000028849), tensor(1.7038538456), tensor(0.8896244168), tensor(1.2640681267), tensor(0.7365953326), tensor(1.2890672684), tensor(1.2395985126), tensor(0.8839448690), tensor(0.9420402050), tensor(1.2462650537), tensor(1.6396025419), tensor(0.9790500402), tensor(1.1640338898)]\n",
            "c:  [tensor(0.0929041654), tensor(0.5092692971), tensor(0.4747276604), tensor(-0.0128703183), tensor(-0.0025627012), tensor(-0.0340636633), tensor(-0.0080519961), tensor(-0.0129393637), tensor(0.5261859298), tensor(-0.0212187469), tensor(-0.0401261039), tensor(-0.2409766316), tensor(-0.0927370042), tensor(0.5006314516), tensor(-0.0147382757), tensor(-0.0210439377), tensor(-0.2281037271), tensor(-0.0971414596)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([5.7594547272, 5.5341873169, 5.9361963272, 5.6902766228, 3.8821120262,\n",
            "        6.1717934608, 7.0950236320, 4.6161909103, 6.3243346214, 6.0370006561,\n",
            "        5.9389648438, 5.0281963348, 5.7710614204, 6.8281865120, 6.0770945549,\n",
            "        6.8870315552, 6.2160091400, 6.6746034622, 5.8293442726, 6.7504863739])\n",
            "btensor.grad: tensor([2.8511900902, 5.8783044815, 5.7812371254, 4.7075400352, 6.2987265587,\n",
            "        4.2559871674, 5.0037865639, 7.2985248566, 8.4951992035, 4.6131634712,\n",
            "        5.3761029243, 2.7349281311, 5.6589250565, 4.4124932289, 4.3592987061,\n",
            "        7.2190089226, 6.5527358055, 5.8297853470, 4.8961839676, 6.0605497360])\n",
            "ctensor.grad: tensor([-35.8456420898,  -0.9137427807,  -1.3834879398,   5.0687026978,\n",
            "          0.2146584392,  14.5454683304,   8.4138383865,  18.4491462708,\n",
            "         48.3300704956,   2.1209850311, 125.0860595703,  67.6575088501,\n",
            "        159.3876495361,  45.0984878540,   1.9884709120, 116.6927337646,\n",
            "         63.1761703491, 149.1428222656])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1044.3209228516, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5889601707), tensor(0.9963839054), tensor(1.0737509727), tensor(1.1848934889), tensor(1.1247540712), tensor(1.3714727163), tensor(1.0048902035), tensor(1.1812558174), tensor(0.9328169823), tensor(1.1882816553), tensor(1.0423855782), tensor(1.4822424650), tensor(0.9920439720), tensor(1.1647071838), tensor(1.1343719959), tensor(0.9304768443), tensor(1.0204327106), tensor(0.8770329952), tensor(1.0976283550), tensor(0.9774072170)]\n",
            "b:  [tensor(0.6597726941), tensor(1.1078734398), tensor(1.3100513220), tensor(1.0304942131), tensor(1.4485398531), tensor(0.8234252334), tensor(1.2266185284), tensor(1.1030004025), tensor(1.7058686018), tensor(0.8945213556), tensor(1.2678210735), tensor(0.7408211231), tensor(1.2927246094), tensor(1.2427500486), tensor(0.8880577683), tensor(0.9450230002), tensor(1.2486814260), tensor(1.6424440145), tensor(0.9828689694), tensor(1.1679207087)]\n",
            "c:  [tensor(0.0654654205), tensor(0.5061430931), tensor(0.4723602533), tensor(-0.0056499694), tensor(-0.0025749558), tensor(-0.0231925230), tensor(-0.0029475619), tensor(0.0003719991), tensor(0.5852168202), tensor(-0.0210427716), tensor(0.0558526926), tensor(-0.1976616234), tensor(0.0210171416), tensor(0.5563542247), tensor(-0.0145858657), tensor(0.0695595145), tensor(-0.1868510097), tensor(0.0099535510)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ -2.7046003342, -11.4878530502,  -9.5456867218,  -7.8962593079,\n",
            "         -9.9626474380,  -3.3863682747,  -7.0501179695,  -9.3119535446,\n",
            "        -13.0282526016,  -5.9100685120,  -7.4469242096,  -5.3803143501,\n",
            "        -12.9154720306,  -2.1420254707,  -6.1847290993,  -7.4401783943,\n",
            "         -5.1165709496, -11.3335704803, -10.4950408936,  -9.2326631546])\n",
            "btensor.grad: tensor([-9.4552211761, -9.1436548233, -5.5656290054, -9.0785417557,\n",
            "        -6.7382659912, -8.4345407486, -6.9410829544, -5.9951314926,\n",
            "        -4.0294346809, -9.7938804626, -7.5058507919, -8.4515542984,\n",
            "        -7.3145918846, -6.3030328751, -8.2257795334, -5.9655842781,\n",
            "        -4.8326301575, -5.6830244064, -7.6378040314, -7.7735962868])\n",
            "ctensor.grad: tensor([ 5.4877490997e+01,  6.2523579597e+00,  4.7348079681e+00,\n",
            "        -1.4440696716e+01,  2.4509198964e-02, -2.1742279053e+01,\n",
            "        -1.0208868027e+01, -2.6622724533e+01, -1.1806182098e+02,\n",
            "        -3.5195010900e-01, -1.9195758057e+02, -8.6630004883e+01,\n",
            "        -2.2750828552e+02, -1.1144554138e+02, -3.0481961370e-01,\n",
            "        -1.8120690918e+02, -8.2505439758e+01, -2.1419001770e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1067.7021484375, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5853368044), tensor(0.9914597869), tensor(1.0691925287), tensor(1.1805614233), tensor(1.1207730770), tensor(1.3674774170), tensor(1.0000543594), tensor(1.1769226789), tensor(0.9276247025), tensor(1.1840846539), tensor(1.0380092859), tensor(1.4784381390), tensor(0.9872134924), tensor(1.1605917215), tensor(1.1301485300), tensor(0.9255234599), tensor(1.0160189867), tensor(0.8717712164), tensor(1.0929419994), tensor(0.9723888636)]\n",
            "b:  [tensor(0.6552688479), tensor(1.1023393869), tensor(1.3052256107), tensor(1.0254144669), tensor(1.4432170391), tensor(0.8187749982), tensor(1.2218778133), tensor(1.0975691080), tensor(1.7001208067), tensor(0.8890840411), tensor(1.2627165318), tensor(0.7367935181), tensor(1.2876714468), tensor(1.2384414673), tensor(0.8833700418), tensor(0.9396100044), tensor(1.2437294722), tensor(1.6373866796), tensor(0.9780135155), tensor(1.1625918150)]\n",
            "c:  [tensor(0.0907267556), tensor(0.4876856804), tensor(0.4548295438), tensor(-0.0097017772), tensor(-0.0026966881), tensor(-0.0333259031), tensor(-0.0088143963), tensor(-0.0128192874), tensor(0.5596680641), tensor(-0.0220401920), tensor(-0.0111302771), tensor(-0.2384271473), tensor(-0.0770580098), tensor(0.5317366719), tensor(-0.0155555541), tensor(0.0056683272), tensor(-0.2255317718), tensor(-0.0839076415)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 7.2467346191,  9.8481864929,  9.1168508530,  8.6640415192,\n",
            "         7.9619479179,  7.9905090332,  9.6716012955,  8.6663579941,\n",
            "        10.3845682144,  8.3940649033,  8.7525644302,  7.6085710526,\n",
            "         9.6610040665,  8.2310009003,  8.4469242096,  9.9067115784,\n",
            "         8.8273963928, 10.5235137939,  9.3726119995, 10.0366582870])\n",
            "btensor.grad: tensor([ 9.0076608658, 11.0679931641,  9.6515140533, 10.1595497131,\n",
            "        10.6457366943,  9.3004989624,  9.4815139771, 10.8625249863,\n",
            "        11.4956283569, 10.8746089935, 10.2091655731,  8.0551834106,\n",
            "        10.1062927246,  8.6170825958,  9.3755092621, 10.8259935379,\n",
            "         9.9038791656, 10.1145658493,  9.7108831406, 10.6579046249])\n",
            "ctensor.grad: tensor([-50.5226707458,  36.9148330688,  35.0613937378,   8.1036148071,\n",
            "          0.2434643358,  20.2667617798,  11.7336673737,  26.3825721741,\n",
            "         51.0974845886,   1.9948394299, 133.9659271240,  81.5310516357,\n",
            "        196.1502990723,  49.2351303101,   1.9393770695, 127.7823638916,\n",
            "         77.3615112305, 187.7223815918])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1029.6997070312, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5847885609), tensor(0.9936008453), tensor(1.0705767870), tensor(1.1816005707), tensor(1.1229704618), tensor(1.3670144081), tensor(1.0003966093), tensor(1.1788450480), tensor(0.9298927188), tensor(1.1843531132), tensor(1.0387221575), tensor(1.4789396524), tensor(0.9893763065), tensor(1.1595467329), tensor(1.1304794550), tensor(0.9261662364), tensor(1.0162914991), tensor(0.8734557629), tensor(1.0946815014), tensor(0.9736059308)]\n",
            "b:  [tensor(0.6581572890), tensor(1.1043444872), tensor(1.3059029579), tensor(1.0276217461), tensor(1.4442343712), tensor(0.8207512498), tensor(1.2231028080), tensor(1.0979607105), tensor(1.6996219158), tensor(0.8917193413), tensor(1.2642685175), tensor(0.7391542792), tensor(1.2889754772), tensor(1.2396323681), tensor(0.8853515983), tensor(0.9401324391), tensor(1.2439273596), tensor(1.6381609440), tensor(0.9796832800), tensor(1.1639779806)]\n",
            "c:  [tensor(0.0841606483), tensor(0.4976598918), tensor(0.4643343687), tensor(-0.0069797449), tensor(-0.0027501762), tensor(-0.0307707619), tensor(-0.0078346655), tensor(-0.0092567671), tensor(0.5765265822), tensor(-0.0223932005), tensor(0.0091354791), tensor(-0.2274813056), tensor(-0.0427740850), tensor(0.5478476882), tensor(-0.0158808008), tensor(0.0248546340), tensor(-0.2153026015), tensor(-0.0520934761)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0965615511, -4.2821197510, -2.7686219215, -2.0784130096,\n",
            "        -4.3946714401,  0.9260416031, -0.6845658422, -3.8446598053,\n",
            "        -4.5360817909, -0.5368509293, -1.4258394241, -1.0031001568,\n",
            "        -4.3255825043,  2.0899062157, -0.6617993116, -1.2856005430,\n",
            "        -0.5450305939, -3.3690838814, -3.4791164398, -2.4341218472])\n",
            "btensor.grad: tensor([-5.7768526077, -4.0101537704, -1.3547667265, -4.4146656990,\n",
            "        -2.0345542431, -3.9524984360, -2.4498836994, -0.7831187844,\n",
            "         0.9977214932, -5.2705612183, -3.1038937569, -4.7215003967,\n",
            "        -2.6079878807, -2.3818526268, -3.9630692005, -1.0448385477,\n",
            "        -0.3957275152, -1.5485804081, -3.3395488262, -2.7723782063])\n",
            "ctensor.grad: tensor([ 13.1322154999, -19.9484367371, -19.0096206665,  -5.4440646172,\n",
            "          0.1069764942,  -5.1102814674,  -1.9594615698,  -7.1250400543,\n",
            "        -33.7170257568,   0.7060182691, -40.5315093994, -21.8916740417,\n",
            "        -68.5678482056, -32.2219772339,   0.6504938602, -38.3726119995,\n",
            "        -20.4583358765, -63.6283264160])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.7706298828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5826878548), tensor(0.9926021099), tensor(1.0690922737), tensor(1.1800879240), tensor(1.1225831509), tensor(1.3648113012), tensor(0.9981536269), tensor(1.1781029701), tensor(0.9285645485), tensor(1.1824549437), tensor(1.0370260477), tensor(1.4775153399), tensor(0.9881436229), tensor(1.1569315195), tensor(1.1286041737), tensor(0.9241608381), tensor(1.0144706964), tensor(0.8718143106), tensor(1.0933641195), tensor(0.9718016982)]\n",
            "b:  [tensor(0.6585710049), tensor(1.1033527851), tensor(1.3043959141), tensor(1.0271013975), tensor(1.4427199364), tensor(0.8202530146), tensor(1.2220753431), tensor(1.0958062410), tensor(1.6967433691), tensor(0.8914698362), tensor(1.2632802725), tensor(0.7393259406), tensor(1.2877843380), tensor(1.2387824059), tensor(0.8848372102), tensor(0.9381039143), tensor(1.2419869900), tensor(1.6367387772), tensor(0.9789057374), tensor(1.1626873016)]\n",
            "c:  [tensor(0.0944814980), tensor(0.5057606101), tensor(0.4722733796), tensor(-0.0078203706), tensor(-0.0028400803), tensor(-0.0348774828), tensor(-0.0103045162), tensor(-0.0141808707), tensor(0.5638284087), tensor(-0.0232490320), tensor(-0.0282112490), tensor(-0.2456091642), tensor(-0.0825809538), tensor(0.5362605453), tensor(-0.0166707020), tensor(-0.0097155459), tensor(-0.2322027087), tensor(-0.0891095847)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([4.2014141083, 1.9974749088, 2.9689536095, 3.0252792835, 0.7745881677,\n",
            "        4.4062924385, 4.4859318733, 1.4840477705, 2.6563773155, 3.7963016033,\n",
            "        3.3922691345, 2.8486452103, 2.4653820992, 5.2305212021, 3.7505216599,\n",
            "        4.0107774734, 3.6415867805, 3.2828660011, 2.6347663403, 3.6084263325])\n",
            "btensor.grad: tensor([-0.8274103999,  1.9833717346,  3.0141525269,  1.0407001972,\n",
            "         3.0289661884,  0.9965283871,  2.0549983978,  4.3089113235,\n",
            "         5.7571654320,  0.4989523292,  1.9765146971, -0.3433725536,\n",
            "         2.3822805882,  1.6999986172,  1.0287418365,  4.0569944382,\n",
            "         3.8806819916,  2.8444125652,  1.5550559759,  2.5814728737])\n",
            "ctensor.grad: tensor([-20.6417007446, -16.2013893127, -15.8780269623,   1.6812510490,\n",
            "          0.1798080951,   8.2134408951,   4.9397010803,   9.8482074738,\n",
            "         25.3963470459,   1.7116637230,  74.6934509277,  36.2557029724,\n",
            "         79.6137313843,  23.1743392944,   1.5798006058,  69.1403579712,\n",
            "         33.8002090454,  74.0322036743])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1032.3039550781, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5827677250), tensor(0.9960724115), tensor(1.0716773272), tensor(1.1821525097), tensor(1.1258015633), tensor(1.3650963306), tensor(0.9996064305), tensor(1.1810874939), tensor(0.9323543310), tensor(1.1836811304), tensor(1.0388269424), tensor(1.4788243771), tensor(0.9919142127), tensor(1.1565536261), tensor(1.1298782825), tensor(0.9259353876), tensor(1.0155539513), tensor(0.8749686480), tensor(1.0964106321), tensor(0.9742590785)]\n",
            "b:  [tensor(0.6622034311), tensor(1.1063436270), tensor(1.3058612347), tensor(1.0302326679), tensor(1.4446195364), tensor(0.8230905533), tensor(1.2241485119), tensor(1.0971653461), tensor(1.6971729994), tensor(0.8949998021), tensor(1.2656773329), tensor(0.7424094081), tensor(1.2899817228), tensor(1.2406737804), tensor(0.8876552582), tensor(0.9395309687), tensor(1.2429690361), tensor(1.6382898092), tensor(0.9813994765), tensor(1.1650110483)]\n",
            "c:  [tensor(0.0804229081), tensor(0.5118694305), tensor(0.4781907797), tensor(-0.0035241744), tensor(-0.0028793821), tensor(-0.0293198135), tensor(-0.0077416403), tensor(-0.0068982225), tensor(0.5949385166), tensor(-0.0234188624), tensor(0.0189379025), tensor(-0.2221914083), tensor(-0.0176850632), tensor(0.5657057166), tensor(-0.0168335084), tensor(0.0346609727), tensor(-0.2101254016), tensor(-0.0286379047)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.1598504782, -6.9405937195, -5.1701273918, -4.1292276382,\n",
            "        -6.4368004799, -0.5701365471, -2.9055554867, -5.9690098763,\n",
            "        -7.5795059204, -2.4524736404, -3.6017832756, -2.6180584431,\n",
            "        -7.5412044525,  0.7558519840, -2.5481672287, -3.5490567684,\n",
            "        -2.1664311886, -6.3087220192, -6.0930147171, -4.9147520065])\n",
            "btensor.grad: tensor([-7.2648506165, -5.9815878868, -2.9305703640, -6.2626500130,\n",
            "        -3.7991352081, -5.6751308441, -4.1463613510, -2.7181363106,\n",
            "        -0.8592553735, -7.0599298477, -4.7940397263, -6.1669259071,\n",
            "        -4.3948078156, -3.7828106880, -5.6361474991, -2.8541126251,\n",
            "        -1.9642050266, -3.1020741463, -4.9874210358, -4.6473822594])\n",
            "ctensor.grad: tensor([ 2.8117172241e+01, -1.2217666626e+01, -1.1834791183e+01,\n",
            "        -8.5923919678e+00,  7.8603580594e-02, -1.1115337372e+01,\n",
            "        -5.1257510185e+00, -1.4565296173e+01, -6.2220245361e+01,\n",
            "         3.3966118097e-01, -9.4298301697e+01, -4.6835506439e+01,\n",
            "        -1.2979177856e+02, -5.8890365601e+01,  3.2561257482e-01,\n",
            "        -8.8753036499e+01, -4.4154605865e+01, -1.2094335175e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1040.1361083984, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5799117088), tensor(0.9932891726), tensor(1.0686893463), tensor(1.1792891026), tensor(1.1238386631), tensor(1.3620185852), tensor(0.9960866570), tensor(1.1787371635), tensor(0.9291365147), tensor(1.1806923151), tensor(1.0358811617), tensor(1.4763127565), tensor(0.9889726043), tensor(1.1531941891), tensor(1.1268706322), tensor(0.9225064516), tensor(1.0125377178), tensor(0.8715779781), tensor(1.0934752226), tensor(0.9708686471)]\n",
            "b:  [tensor(0.6606941223), tensor(1.1033864021), tensor(1.3029545546), tensor(1.0278370380), tensor(1.4414856434), tensor(0.8209187984), tensor(1.2216337919), tensor(1.0935409069), tensor(1.6929533482), tensor(0.8926278949), tensor(1.2629628181), tensor(0.7409536242), tensor(1.2871506214), tensor(1.2384369373), tensor(0.8854196668), tensor(0.9359695315), tensor(1.2397557497), tensor(1.6354010105), tensor(0.9789378643), tensor(1.1620017290)]\n",
            "c:  [tensor(0.0989709347), tensor(0.5119199753), tensor(0.4786007106), tensor(-0.0061544520), tensor(-0.0029897909), tensor(-0.0368230268), tensor(-0.0120451171), tensor(-0.0163443610), tensor(0.5711084008), tensor(-0.0244670846), tensor(-0.0421431661), tensor(-0.2548493147), tensor(-0.0949665010), tensor(0.5432989001), tensor(-0.0178221054), tensor(-0.0226485841), tensor(-0.2407644242), tensor(-0.1013212502)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([5.7119913101, 5.5664687157, 5.9760427475, 5.7267041206, 3.9259183407,\n",
            "        6.1554117203, 7.0395197868, 4.7007651329, 6.4356298447, 5.9775753021,\n",
            "        5.8916182518, 5.0231361389, 5.8832287788, 6.7188167572, 6.0153732300,\n",
            "        6.8578872681, 6.0324854851, 6.7813076973, 5.8707842827, 6.7808599472])\n",
            "btensor.grad: tensor([3.0185976028, 5.9145550728, 5.8134112358, 4.7912769318, 6.2677445412,\n",
            "        4.3434987068, 5.0294990540, 7.2489366531, 8.4392795563, 4.7438240051,\n",
            "        5.4290928841, 2.9115414619, 5.6622948647, 4.4735860825, 4.4711661339,\n",
            "        7.1228537560, 6.4266452789, 5.7776751518, 4.9231915474, 6.0186042786])\n",
            "ctensor.grad: tensor([-3.7096057892e+01, -1.0112693906e-01, -8.1988674402e-01,\n",
            "         5.2605547905e+00,  2.2081771493e-01,  1.5006423950e+01,\n",
            "         8.6069526672e+00,  1.8892274857e+01,  4.7660255432e+01,\n",
            "         2.0964446068e+00,  1.2216213226e+02,  6.5315788269e+01,\n",
            "         1.5456286621e+02,  4.4813579559e+01,  1.9771935940e+00,\n",
            "         1.1461911011e+02,  6.1278049469e+01,  1.4536668396e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1039.7465820312, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5809189081), tensor(0.9984884262), tensor(1.0729173422), tensor(1.1827746630), tensor(1.1283656359), tensor(1.3633421659), tensor(0.9991362691), tensor(1.1830149889), tensor(0.9350005388), tensor(1.1832354069), tensor(1.0391535759), tensor(1.4786603451), tensor(0.9947834611), tensor(1.1538618803), tensor(1.1294963360), tensor(0.9258293509), tensor(1.0147895813), tensor(0.8766731024), tensor(1.0981955528), tensor(0.9750441313)]\n",
            "b:  [tensor(0.6651593447), tensor(1.1075881720), tensor(1.3054057360), tensor(1.0320327282), tensor(1.4444960356), tensor(0.8247826099), tensor(1.2247251272), tensor(1.0961300135), tensor(1.6945768595), tensor(0.8972185254), tensor(1.2663922310), tensor(0.7448711991), tensor(1.2904466391), tensor(1.2412221432), tensor(0.8892311454), tensor(0.9385849237), tensor(1.2417837381), tensor(1.6379047632), tensor(0.9824422002), tensor(1.1655057669)]\n",
            "c:  [tensor(0.0743472949), tensor(0.5110353231), tensor(0.4779073596), tensor(0.0003864160), tensor(-0.0030086371), tensor(-0.0270925127), tensor(-0.0073999353), tensor(-0.0040343367), tensor(0.6206640005), tensor(-0.0243874472), tensor(0.0388250574), tensor(-0.2170226872), tensor(0.0056501031), tensor(0.5902436972), tensor(-0.0177553315), tensor(0.0539609678), tensor(-0.2047750503), tensor(-0.0068269297)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ -2.0144267082, -10.3985309601,  -8.4560747147,  -6.9711713791,\n",
            "         -9.0539693832,  -2.6471672058,  -6.0992565155,  -8.5555467606,\n",
            "        -11.7280921936,  -5.0860714912,  -6.5448536873,  -4.6951289177,\n",
            "        -11.6216716766,  -1.3354723454,  -5.2514548302,  -6.6458411217,\n",
            "         -4.5036377907, -10.1902780533,  -9.4406223297,  -8.3510141373])\n",
            "btensor.grad: tensor([-8.9304761887, -8.4035987854, -4.9024286270, -8.3914136887,\n",
            "        -6.0206813812, -7.7276115417, -6.1826896667, -5.1783227921,\n",
            "        -3.2469148636, -9.1812705994, -6.8587718010, -7.8351106644,\n",
            "        -6.5919919014, -5.5703353882, -7.6230025291, -5.2307758331,\n",
            "        -4.0558795929, -5.0075106621, -7.0086236000, -7.0081501007])\n",
            "ctensor.grad: tensor([ 4.9247280121e+01,  1.7692841291e+00,  1.3867273331e+00,\n",
            "        -1.3081735611e+01,  3.7692688406e-02, -1.9461029053e+01,\n",
            "        -9.2903633118e+00, -2.4620046616e+01, -9.9111228943e+01,\n",
            "        -1.5927459300e-01, -1.6193643188e+02, -7.5653251648e+01,\n",
            "        -2.0123320007e+02, -9.3889587402e+01, -1.3354629278e-01,\n",
            "        -1.5321910095e+02, -7.1978744507e+01, -1.8898863220e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1056.7131347656, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5775495768), tensor(0.9941679239), tensor(1.0687786341), tensor(1.1788548231), tensor(1.1249703169), tensor(1.3596303463), tensor(0.9946956635), tensor(1.1792244911), tensor(0.9303061366), tensor(1.1794165373), tensor(1.0352021456), tensor(1.4752409458), tensor(0.9904512763), tensor(1.1500215530), tensor(1.1256455183), tensor(0.9213054180), tensor(1.0108298063), tensor(0.8718923926), tensor(1.0939776897), tensor(0.9704625010)]\n",
            "b:  [tensor(0.6615595818), tensor(1.1028443575), tensor(1.3011978865), tensor(1.0277729034), tensor(1.4398776293), tensor(0.8209033608), tensor(1.2207100391), tensor(1.0912883282), tensor(1.6893185377), tensor(0.8926872611), tensor(1.2620269060), tensor(0.7416270971), tensor(1.2860827446), tensor(1.2375916243), tensor(0.8852883577), tensor(0.9337700605), tensor(1.2374628782), tensor(1.6335698366), tensor(0.9783319235), tensor(1.1609256268)]\n",
            "c:  [tensor(0.0985474363), tensor(0.4992865622), tensor(0.4670966268), tensor(-0.0034505622), tensor(-0.0031323282), tensor(-0.0369259827), tensor(-0.0130571183), tensor(-0.0167333782), tensor(0.5943261385), tensor(-0.0254269727), tensor(-0.0290878713), tensor(-0.2566027939), tensor(-0.0894610956), tensor(0.5648990870), tensor(-0.0187633447), tensor(-0.0107664503), tensor(-0.2423072159), tensor(-0.0976810902)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([6.7385444641, 8.6410522461, 8.2773237228, 7.8396015167, 6.7905721664,\n",
            "        7.4237356186, 8.8811988831, 7.5809602737, 9.3888225555, 7.6377215385,\n",
            "        7.9028110504, 6.8388328552, 8.6643981934, 7.6805992126, 7.7015814781,\n",
            "        9.0478630066, 7.9194550514, 9.5614757538, 8.4358196259, 9.1633005142])\n",
            "btensor.grad: tensor([ 7.1994886398,  9.4876089096,  8.4156541824,  8.5196323395,\n",
            "         9.2367553711,  7.7584891319,  8.0302248001,  9.6833324432,\n",
            "        10.5165920258,  9.0625591278,  8.7305326462,  6.4882020950,\n",
            "         8.7278985977,  7.2609281540,  7.8855743408,  9.6297369003,\n",
            "         8.6418209076,  8.6698932648,  8.2206096649,  9.1602725983])\n",
            "ctensor.grad: tensor([-48.4002838135,  23.4975013733,  21.6214523315,   7.6739559174,\n",
            "          0.2473820150,  19.6669387817,  11.3143653870,  25.3980827332,\n",
            "         52.6757583618,   2.0790524483, 135.8258514404,  79.1602096558,\n",
            "        190.2223815918,  50.6891632080,   2.0160253048, 129.4548339844,\n",
            "         75.0643157959, 181.7083129883])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1032.1826171875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5776752234), tensor(0.9976541400), tensor(1.0714073181), tensor(1.1809755564), tensor(1.1281807423), tensor(1.3599518538), tensor(0.9962444305), tensor(1.1822247505), tensor(0.9341432452), tensor(1.1807032824), tensor(1.0370602608), tensor(1.4765684605), tensor(0.9941898584), tensor(1.1497139931), tensor(1.1269773245), tensor(0.9231860042), tensor(1.0120384693), tensor(0.8750920892), tensor(1.0970501900), tensor(0.9730312228)]\n",
            "b:  [tensor(0.6651839018), tensor(1.1058744192), tensor(1.3026868105), tensor(1.0309059620), tensor(1.4418232441), tensor(0.8237379789), tensor(1.2227692604), tensor(1.0926927328), tensor(1.6897976398), tensor(0.8962433338), tensor(1.2644557953), tensor(0.7446783781), tensor(1.2883074284), tensor(1.2394751310), tensor(0.8881242275), tensor(0.9352810383), tensor(1.2384859324), tensor(1.6351494789), tensor(0.9808633327), tensor(1.1632890701)]\n",
            "c:  [tensor(0.0839663967), tensor(0.5051922202), tensor(0.4725255966), tensor(0.0009405375), tensor(-0.0031709650), tensor(-0.0311855674), tensor(-0.0103925485), tensor(-0.0090963766), tensor(0.6239817142), tensor(-0.0255960692), tensor(0.0163489804), tensor(-0.2337009609), tensor(-0.0257535353), tensor(0.5931127071), tensor(-0.0189170539), tensor(0.0322372504), tensor(-0.2207319438), tensor(-0.0382985026)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.2511926293, -6.9724817276, -5.2573680878, -4.2415118217,\n",
            "        -6.4207682610, -0.6429505348, -3.0975131989, -6.0004677773,\n",
            "        -7.6742238998, -2.5734565258, -3.7161183357, -2.6550953388,\n",
            "        -7.4771146774,  0.6151074171, -2.6635208130, -3.7611894608,\n",
            "        -2.4173493385, -6.3993577957, -6.1450958252, -5.1374874115])\n",
            "btensor.grad: tensor([-7.2485957146, -6.0601334572, -2.9779605865, -6.2660875320,\n",
            "        -3.8911406994, -5.6692466736, -4.1184420586, -2.8087377548,\n",
            "        -0.9583070278, -7.1121959686, -4.8576846123, -6.1025419235,\n",
            "        -4.4492912292, -3.7670726776, -5.6717152596, -3.0219449997,\n",
            "        -2.0460653305, -3.1593885422, -5.0627961159, -4.7269549370])\n",
            "ctensor.grad: tensor([ 2.9162073135e+01, -1.1811336517e+01, -1.0857935905e+01,\n",
            "        -8.7821989059e+00,  7.7273711562e-02, -1.1480831146e+01,\n",
            "        -5.3291401863e+00, -1.5274001122e+01, -5.9311115265e+01,\n",
            "         3.3819195628e-01, -9.0873695374e+01, -4.5803668976e+01,\n",
            "        -1.2741510773e+02, -5.6427227020e+01,  3.0741882324e-01,\n",
            "        -8.6007400513e+01, -4.3150535583e+01, -1.1876516724e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1037.6469726562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5749861002), tensor(0.9951550364), tensor(1.0686639547), tensor(1.1783354282), tensor(1.1264637709), tensor(1.3570497036), tensor(0.9929767251), tensor(1.1801146269), tensor(0.9311960340), tensor(1.1779315472), tensor(1.0343514681), tensor(1.4742544889), tensor(0.9914710522), tensor(1.1465557814), tensor(1.1242024899), tensor(0.9200232625), tensor(1.0093179941), tensor(0.8719555140), tensor(1.0943653584), tensor(0.9699054360)]\n",
            "b:  [tensor(0.6639509797), tensor(1.1032596827), tensor(1.3000372648), tensor(1.0288125277), tensor(1.4390062094), tensor(0.8218333125), tensor(1.2205208540), tensor(1.0893720388), tensor(1.6858673096), tensor(0.8942098022), tensor(1.2620431185), tensor(0.7434474230), tensor(1.2857834101), tensor(1.2374812365), tensor(0.8861583471), tensor(0.9320563674), tensor(1.2355711460), tensor(1.6325567961), tensor(0.9787055850), tensor(1.1606158018)]\n",
            "c:  [tensor(0.1014265567), tensor(0.5067353845), tensor(0.4745934904), tensor(-0.0013974556), tensor(-0.0032789351), tensor(-0.0381700657), tensor(-0.0144132795), tensor(-0.0178308487), tensor(0.6022374630), tensor(-0.0266005471), tensor(-0.0395987220), tensor(-0.2630542219), tensor(-0.0949195698), tensor(0.5725672245), tensor(-0.0198645201), tensor(-0.0204416849), tensor(-0.2483372092), tensor(-0.1034340784)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([5.3783574104, 4.9982061386, 5.4867920876, 5.2803478241, 3.4339559078,\n",
            "        5.8041834831, 6.5353870392, 4.2201681137, 5.8943872452, 5.5434331894,\n",
            "        5.4176135063, 4.6278848648, 5.4375562668, 6.3165211678, 5.5497360229,\n",
            "        6.3254618645, 5.4410147667, 6.2730979919, 5.3695564270, 6.2515511513])\n",
            "btensor.grad: tensor([2.4657971859, 5.2294740677, 5.2990431786, 4.1868796349, 5.6340808868,\n",
            "        3.8093066216, 4.4967365265, 6.6413831711, 7.8607573509, 4.0671072006,\n",
            "        4.8254070282, 2.4618692398, 5.0480089188, 3.9876708984, 3.9317147732,\n",
            "        6.4493718147, 5.8295493126, 5.1853456497, 4.3154497147, 5.3465800285])\n",
            "ctensor.grad: tensor([-34.9203186035,  -3.0863800049,  -4.1357598305,   4.6759858131,\n",
            "          0.2159403861,  13.9689931870,   8.0414619446,  17.4689407349,\n",
            "         43.4885520935,   2.0089576244, 111.8954010010,  58.7065429688,\n",
            "        138.3320617676,  41.0909500122,   1.8949333429, 105.3578643799,\n",
            "         55.2105445862, 130.2711486816])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1035.4984130859, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5755916834), tensor(0.9995714426), tensor(1.0721633434), tensor(1.1812052727), tensor(1.1303685904), tensor(1.3579272032), tensor(0.9953733087), tensor(1.1838272810), tensor(0.9361336231), tensor(1.1799272299), tensor(1.0370059013), tensor(1.4761490822), tensor(0.9963370562), tensor(1.1467838287), tensor(1.1262426376), tensor(0.9227455258), tensor(1.0111374855), tensor(0.8762129545), tensor(1.0983451605), tensor(0.9733970165)]\n",
            "b:  [tensor(0.6680276394), tensor(1.1069412231), tensor(1.3020627499), tensor(1.0325309038), tensor(1.4415482283), tensor(0.8252250552), tensor(1.2231372595), tensor(1.0914456844), tensor(1.6869993210), tensor(0.8983399868), tensor(1.2650306225), tensor(0.7469516993), tensor(1.2886049747), tensor(1.2398343086), tensor(0.8895384073), tensor(0.9342017770), tensor(1.2371443510), tensor(1.6346492767), tensor(0.9817819595), tensor(1.1636126041)]\n",
            "c:  [tensor(0.0811881572), tensor(0.5090249181), tensor(0.4764807820), tensor(0.0042028218), tensor(-0.0033068529), tensor(-0.0301467590), tensor(-0.0105811739), tensor(-0.0074007064), tensor(0.6415137053), tensor(-0.0266416334), tensor(0.0238529108), tensor(-0.2322306782), tensor(-0.0116545931), tensor(0.6098798513), tensor(-0.0199008882), tensor(0.0396476723), tensor(-0.2191873789), tensor(-0.0256476477)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.2111589909, -8.8327941895, -6.9988417625, -5.7396650314,\n",
            "        -7.8095445633, -1.7549892664, -4.7932085991, -7.4253520966,\n",
            "        -9.8752117157, -3.9913113117, -5.3088135719, -3.7891337872,\n",
            "        -9.7320489883, -0.4560550451, -4.0802865028, -5.4445710182,\n",
            "        -3.6389393806, -8.5148277283, -7.9596138000, -6.9831595421])\n",
            "btensor.grad: tensor([-8.1533298492, -7.3631052971, -4.0508575439, -7.4367899895,\n",
            "        -5.0841150284, -6.7835063934, -5.2328324318, -4.1472215652,\n",
            "        -2.2639632225, -8.2603902817, -5.9749760628, -7.0085783005,\n",
            "        -5.6431465149, -4.7061624527, -6.7601647377, -4.2908701897,\n",
            "        -3.1465122700, -4.1848745346, -6.1527309418, -5.9935584068])\n",
            "ctensor.grad: tensor([ 4.0476791382e+01, -4.5790896416e+00, -3.7746095657e+00,\n",
            "        -1.1200553894e+01,  5.5835399777e-02, -1.6046613693e+01,\n",
            "        -7.6642107964e+00, -2.0860282898e+01, -7.8552513123e+01,\n",
            "         8.2170769572e-02, -1.2690325928e+02, -6.1647094727e+01,\n",
            "        -1.6652995300e+02, -7.4625198364e+01,  7.2735771537e-02,\n",
            "        -1.2017871094e+02, -5.8299648285e+01, -1.5557286072e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1045.3863525391, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5725731850), tensor(0.9961361885), tensor(1.0686858892), tensor(1.1779022217), tensor(1.1277965307), tensor(1.3546247482), tensor(0.9915137887), tensor(1.1808493137), tensor(0.9322458506), tensor(1.1766383648), tensor(1.0336757898), tensor(1.4732836485), tensor(0.9927646518), tensor(1.1433126926), tensor(1.1229305267), tensor(0.9188831449), tensor(1.0078209639), tensor(0.8722018003), tensor(1.0948557854), tensor(0.9695020318)]\n",
            "b:  [tensor(0.6656168103), tensor(1.1032691002), tensor(1.2986603975), tensor(1.0293698311), tensor(1.4378645420), tensor(0.8223555684), tensor(1.2200487852), tensor(1.0873974562), tensor(1.6824299097), tensor(0.8950604200), tensor(1.2616579533), tensor(0.7447257042), tensor(1.2851828337), tensor(1.2370544672), tensor(0.8865898848), tensor(0.9302122593), tensor(1.2335901260), tensor(1.6312391758), tensor(0.9786699414), tensor(1.1600193977)]\n",
            "c:  [tensor(0.1024455428), tensor(0.5046308637), tensor(0.4730742872), tensor(0.0010160049), tensor(-0.0034250617), tensor(-0.0387782194), tensor(-0.0155303199), tensor(-0.0183985122), tensor(0.6166393161), tensor(-0.0276803523), tensor(-0.0395288579), tensor(-0.2672171891), tensor(-0.0953618586), tensor(0.5859816074), tensor(-0.0209002253), tensor(-0.0206921510), tensor(-0.2523308396), tensor(-0.1053076014)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([6.0369272232, 6.8704910278, 6.9549136162, 6.6060042381, 5.1440491676,\n",
            "        6.6049337387, 7.7190089226, 5.9559140205, 7.7755031586, 6.5776681900,\n",
            "        6.6603307724, 5.7309169769, 7.1447849274, 6.9422235489, 6.6241812706,\n",
            "        7.7247414589, 6.6331110001, 8.0222539902, 6.9788160324, 7.7899193764])\n",
            "btensor.grad: tensor([4.8216972351, 7.3443431854, 6.8047585487, 6.3221492767, 7.3673391342,\n",
            "        5.7390251160, 6.1770596504, 8.0964832306, 9.1388511658, 6.5591568947,\n",
            "        6.7452230453, 4.4520320892, 6.8443384171, 5.5596656799, 5.8970985413,\n",
            "        7.9790019989, 7.1083545685, 6.8203091621, 6.2240586281, 7.1865153313])\n",
            "ctensor.grad: tensor([-42.5147781372,   8.7881097794,   6.8130121231,   6.3736333847,\n",
            "          0.2364177853,  17.2629203796,   9.8982925415,  21.9956130981,\n",
            "         49.7488174438,   2.0774381161, 126.7635269165,  69.9730377197,\n",
            "        167.4145202637,  47.7964706421,   1.9986734390, 120.6796417236,\n",
            "         66.2868957520, 159.3199005127])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Score table after training\n",
            "                 Arsenal          Birmingham       Blackburn        Fulham           Leicester        Man United       Portsmouth       Charlton         Leeds            Liverpool        Bolton           Chelsea          Everton          Man City         Newcastle        Southampton      Tottenham        Wolves           Aston Villa      Middlesbrough    \n",
            "Arsenal          1.277            1.812            2.269            1.859            2.506            1.598            2.156            1.880            2.729            1.729            2.429            1.372            2.205            2.415            1.656            1.765            2.228            2.812            1.693            2.200            \n",
            "Birmingham       0.710            0.920            1.143            0.990            1.275            0.915            1.104            0.970            1.356            0.959            1.351            0.729            1.116            1.395            0.879            0.969            1.148            1.466            0.837            1.216            \n",
            "Blackburn        0.828            1.051            1.321            1.152            1.475            1.056            1.286            1.113            1.549            1.113            1.569            0.843            1.274            1.608            1.023            1.124            1.345            1.680            0.969            1.417            \n",
            "Fulham           0.939            1.262            1.596            1.322            1.763            1.194            1.515            1.328            1.887            1.283            1.798            0.992            1.558            1.812            1.200            1.294            1.575            1.986            1.183            1.605            \n",
            "Leicester        0.864            1.123            1.409            1.211            1.565            1.102            1.361            1.184            1.664            1.167            1.643            0.890            1.366            1.676            1.077            1.178            1.419            1.786            1.036            1.480            \n",
            "Man United       1.148            1.624            2.040            1.676            2.253            1.432            1.945            1.686            2.446            1.549            2.179            1.230            1.971            2.164            1.487            1.584            2.012            2.521            1.523            1.979            \n",
            "Portsmouth       0.789            0.994            1.269            1.087            1.409            1.012            1.221            1.063            1.469            1.076            1.508            0.812            1.232            1.541            0.987            1.079            1.278            1.597            0.932            1.346            \n",
            "Charlton         0.890            1.188            1.475            1.260            1.642            1.132            1.424            1.241            1.764            1.199            1.689            0.922            1.432            1.721            1.115            1.216            1.479            1.870            1.088            1.528            \n",
            "Leeds            0.653            0.819            1.018            0.902            1.141            0.848            0.993            0.870            1.198            0.879            1.241            0.661            0.993            1.294            0.798            0.888            1.035            1.318            0.742            1.119            \n",
            "Liverpool        1.022            1.381            1.755            1.475            1.945            1.277            1.692            1.453            2.064            1.375            1.936            1.078            1.680            1.932            1.308            1.402            1.761            2.176            1.303            1.759            \n",
            "Bolton           0.923            1.223            1.569            1.318            1.737            1.156            1.510            1.296            1.826            1.245            1.751            0.970            1.502            1.751            1.179            1.266            1.576            1.939            1.164            1.585            \n",
            "Chelsea          1.160            1.606            1.995            1.675            2.215            1.456            1.919            1.664            2.403            1.561            2.198            1.221            1.928            2.206            1.478            1.590            1.989            2.500            1.481            1.996            \n",
            "Everton          0.756            0.955            1.189            1.057            1.334            0.963            1.171            1.007            1.401            1.005            1.423            0.760            1.140            1.467            0.923            1.020            1.225            1.526            0.869            1.298            \n",
            "Man City         1.014            1.407            1.785            1.472            1.971            1.265            1.705            1.472            2.115            1.368            1.924            1.082            1.718            1.912            1.310            1.397            1.768            2.199            1.332            1.746            \n",
            "Newcastle        0.944            1.234            1.570            1.337            1.740            1.189            1.518            1.305            1.835            1.272            1.790            0.980            1.505            1.803            1.191            1.289            1.585            1.961            1.160            1.619            \n",
            "Southampton      0.828            1.064            1.375            1.166            1.521            1.042            1.327            1.136            1.584            1.119            1.572            0.860            1.314            1.580            1.048            1.131            1.390            1.702            1.018            1.418            \n",
            "Tottenham        0.826            1.039            1.337            1.139            1.481            1.056            1.284            1.115            1.539            1.127            1.579            0.854            1.295            1.606            1.037            1.131            1.345            1.673            0.983            1.409            \n",
            "Wolves           0.681            0.855            1.082            0.939            1.207            0.874            1.049            0.911            1.259            0.922            1.296            0.694            1.047            1.333            0.844            0.927            1.098            1.371            0.793            1.162            \n",
            "Aston Villa      0.834            1.053            1.319            1.159            1.475            1.066            1.288            1.115            1.549            1.119            1.578            0.845            1.273            1.623            1.026            1.131            1.347            1.685            0.965            1.428            \n",
            "Middlesbrough    0.861            1.127            1.457            1.210            1.607            1.086            1.389            1.202            1.686            1.172            1.642            0.906            1.408            1.647            1.101            1.184            1.451            1.793            1.082            1.472            \n",
            "\n",
            "\n",
            "\n",
            "=============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Historic Bookies Estimate:\n",
        "==========================\n",
        "\n",
        "The historic bookies estimate for goals landed by team i against team j is\n",
        "\n",
        "a_i b_j\n",
        "\n",
        "where\n",
        "\n",
        "a_i = A_i C^{-1/2}\n",
        "b_j = B_j C^{-1/2}\n",
        "\n",
        "A_i = goals landed per game by team i\n",
        "B_j = goals conceded per game by team j\n",
        "C   = average goals per game of all teams\n",
        "\n",
        "\n",
        "The historic 1980's max likelihood models\n",
        "=========================================\n",
        "\n",
        "Starting with Maher, the 1980's max likelihood model starts with this guess and\n",
        "perfects it by max likelihood, when team i lands k goals agaist team j\n",
        "loss was minus the log of the predicted probability by Poisson\n",
        "\n",
        "- log ( e^{-a_ib_j}(a_ib_j)^k /k!)\n",
        "\n",
        "\n",
        "A gitgub user said chi squared shows HST,AST,HR,AR have a significant effect\n",
        "\n",
        "HST = home shots on target\n",
        "AST = away shots on target\n",
        "HR  = home red cards\n",
        "AR  = away red cards\n",
        "HS  = home shots\n",
        "AS  = away shots\n",
        "HC  = home quarter kicks\n",
        "AC  = away corner kicks\n",
        "HF  = home fouls\n",
        "AF  = away fouls\n",
        "\n",
        "\n",
        "New Cross-entropy AI model with a neural layer\n",
        "==========================================\n",
        "\n",
        "Since gradient descent generalizes max likelihood we can replace a_ib_j by\n",
        "\n",
        "a_i b_j  +  (c_0 sigma ( c_3 HST + c_4 HR +c_5 HS + c_6 HC _c_7 HF)\n",
        "             + ...\n",
        "             +c_2 sigma(c_13 HST + c_14 HR + c_15 HS + c_16 HCC + c_17 HF) )b_j\n",
        "\n",
        "when i is the home team and\n",
        "\n",
        "a_i b_j  +  c_0 sigma ( c_1 AST + c_2 AR  ..... +AF  ) b_j\n",
        "\n",
        "when i is the away team, with sigma being the sigmoid function\n",
        "\n",
        "\n",
        "   sigma(x)=softmax(0,x) = e^x/(e^0+e^x) = 1/(1+e^{-x})\n",
        "\n",
        "\n",
        "\n",
        "This puts a *neural layer* behind the standard max likelihood model from the 1980s\n",
        "\n",
        "The weights are now the a_i,  b_i,   and c_i\n",
        "\n",
        "Since the c_i are shared by all teams the training rate for the c_i should be lower\n",
        "\n",
        "As in the model which this generalizes, the training is cross entropy versus the Poisson distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A comment about the way the databases are stored, variables like HST and AST refer to 'home'\n",
        "and 'away' team but we do not make a distinction between home versus away.\n",
        "\n",
        "This means that each row of a data table is interpreted as if it were two rows,\n",
        "one giving information about team i against team j, the other giving information\n",
        "about team j against team i.\n",
        "\n",
        "For instance to calculate the average goals scored by any team over all games\n",
        "each row gives goals scored by a home team and goals scored by an away team\n",
        "and we have to add 2 to total games.\n",
        "\n",
        "That is when we say total games it really means the sum over all teams\n",
        "of the number of games that team played in, which is twice the number\n",
        "of games.\n",
        "\n",
        "That explains the line  totalgames=totalgames+2 each time a row is read in.\n",
        "\n",
        "There is no need to change this architecture to include things causing a home\n",
        "team advantage. This starts with a constant taking the value 1 in the first line\n",
        "\n",
        "loss -=  ...\n",
        "\n",
        "and taking the value 0 in the second line\n",
        "\n",
        "loss -= ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import torch as t\n",
        "import torch.nn as n\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def sigma(x):\n",
        "  return t.exp(x)/(t.exp(t.tensor(1.0))+t.exp(x))-t.tensor(0.5)\n",
        "\n",
        "\n",
        "\n",
        "t.set_printoptions(precision=10)\n",
        "#print(\"beep boop\")\n",
        "#print(\"Aston Villa loses\")\n",
        "print(\"=============================================================\")\n",
        "\n",
        "#GITHUB LOCATION:\n",
        "#https://github.com/Pavlos01232/Match_Outcome_Prediction\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0304.csv?raw=true')\n",
        "\n",
        "\n",
        "#\"DEEP learning\" just means \"hidden\" layers\n",
        "\n",
        "\n",
        "#df.to_csv(r'C:\\Users\\Pavlos\\Desktop\\export_dataframe.csv', sep='\\t', encoding='utf-8')\n",
        "#print (df[2])\n",
        "#file_list = os.listdir('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/')\n",
        "#df = pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0405.csv?raw=true',sep='\\t', lineterminator='\\r')\n",
        "#print(df)\n",
        "\n",
        "#read function\n",
        "\n",
        "first = \"https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL\"\n",
        "last = \".csv?raw=true\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Starting with an array of data frames,\n",
        "and an array of column names, make a\n",
        "single array with the chosen columns\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine(dataFrames,columnNames):\n",
        " t=[]\n",
        " for i in range(0,len(dataFrames)):\n",
        "    theseColumns=dataFrames[i].columns.values[0].split(\",\")\n",
        "    for j in range(0 ,len(dataFrames[i])):\n",
        "      row=dataFrames[i].values[j][0].split(\",\")\n",
        "      newEntry=[]\n",
        "      for k in range(0, len(columnNames)):\n",
        "         for m in range(0, len(theseColumns)):\n",
        "             if(columnNames[k]==theseColumns[m] and m<=len(row)):\n",
        "                newEntry.append(row[m])\n",
        "      t.append(newEntry)\n",
        " return t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "can read years 3 to 23, 13*********************, there's something wrong with 14\n",
        "since the first two files in the training data are formatted incorrectly\n",
        "converts csv to dataframe\n",
        "'''\n",
        "\n",
        "df=[]\n",
        "\n",
        "for i in range(3, 4):\n",
        "  result = first + str('{:02.0f}'.format(i)) + str('{:02.0f}'.format(i+1)) + last\n",
        "  x = pd.read_csv(result, sep='\\t', encoding = 'unicode_escape', lineterminator='\\r')\n",
        "  df.append(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get an array with each (home) team listed once\n",
        "from an array of data frames with column \"HomeTeam\"\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getTeams(df):\n",
        " teams=[]\n",
        " homeTeams=combine(df,[\"HomeTeam\"])\n",
        " for i in range(len(homeTeams)):\n",
        "  if(len(homeTeams[i])>0):\n",
        "   found=False\n",
        "   for j in range(len(teams)):\n",
        "    if homeTeams[i][0] == teams[j]:\n",
        "      found=True\n",
        "      break\n",
        "   if found:\n",
        "    continue\n",
        "   teams.append(homeTeams[i][0])\n",
        " return teams\n",
        "\n",
        "\n",
        "'''\n",
        "Get the list of teams from the array of data frames called df\n",
        "and print it to the console\n",
        "'''\n",
        "\n",
        "\n",
        "teams=getTeams(df)\n",
        "print(\"\\nteams:\")\n",
        "print(teams)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Create an array called Data with just the team names and scores\n",
        "from the data framees in the array of frames df, and print it\n",
        "\n",
        "The list [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\"] can be\n",
        "made longer if other columnts may be useful to use\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Data=combine(df, [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\", \"HS\",\"AS\",\"HC\",\"AC\",\"HF\",\"AF\"])\n",
        "#print(\"\\n\\ndata: (team names respective goals scored, respective shots on target, respective red cards, respective shots, respective corner-kicks, respective fouls)\")\n",
        "#print(Data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get the numerical index of a team name x in the array teams\n",
        "otherwise just return x\n",
        "'''\n",
        "\n",
        "def getIndex(x,teams):\n",
        "  for i in range(len(teams)):\n",
        "   if(teams[i]==x):\n",
        "    return i\n",
        "  return x\n",
        "\n",
        "\n",
        "#print(\"\\n\\nIndex assigned to Everton:\")\n",
        "#print(getIndex(\"Everton\",teams))\n",
        "\n",
        "\n",
        "'''\n",
        "Replace any occurrence of names from the array teams\n",
        "which occur anywhere in A by their actual  numbers\n",
        "'''\n",
        "\n",
        "def teamsToNumbers(A,teams):\n",
        "  B=[]\n",
        "  for i in range(len(A)):\n",
        "    B.append([])\n",
        "    for j in range(len(A[i])):\n",
        "      B[i].append(getIndex(A[i][j],teams))\n",
        "  return B\n",
        "\n",
        "\n",
        "'''\n",
        "Create Data2 which is a copy of Data but with team names\n",
        "replaced by their index\n",
        "'''\n",
        "\n",
        "#print(\"\\n\\ndata2, using the team's index number instead of name\")\n",
        "Data2=teamsToNumbers(Data,teams)\n",
        "print(len(Data2))\n",
        "\n",
        "\n",
        "'''\n",
        "remove game 12 from Data2\n",
        "Caution: these indices must be high enough not to disrupt the indices of team names\n",
        "'''\n",
        "\n",
        "excluded=[137, 141, 144, 150, 153, 163, 191, 191, 201]\n",
        "if(len(excluded)>0):\n",
        "  print(\"excluded: \"+str(excluded))\n",
        "  print(\"\\nCAUTION: check that the teams list was not disrupted by exclusions:\\n\"+str(teams))\n",
        "validateGame=[]\n",
        "for i in range(len(excluded)):\n",
        " validateGame.append(Data2.pop(i))\n",
        "\n",
        "'''\n",
        "\n",
        "Assume Data2 has team indices in column 0 and 1 and scores\n",
        "in cols 2 and 3\n",
        "\n",
        "A[i] is array of average goals landed per game by tean i\n",
        "B[i] is array of average goals conceded per game by team i\n",
        "C is total games played by all teams (twide the number of games)\n",
        "games[i]=total games played by team i\n",
        "a[i]*b[j]=first approx of expected goals landed by i when playing\n",
        "    against j\n",
        "'''\n",
        "\n",
        "A=[0]*len(teams)\n",
        "B=[0]*len(teams)\n",
        "games=[0]*len(teams)\n",
        "a=[0]*len(teams)\n",
        "b=[0]*len(teams)\n",
        "C=0\n",
        "totalGames=0\n",
        "\n",
        "for i in range(len(Data2)):\n",
        "  if(len(Data2[i])<2):\n",
        "    continue\n",
        "  games[Data2[i][0]]+=1\n",
        "  games[Data2[i][1]]+=1\n",
        "  A[Data2[i][0]]+=int(Data2[i][2])\n",
        "  B[Data2[i][0]]+=int(Data2[i][3])\n",
        "  A[Data2[i][1]]+=int(Data2[i][3])\n",
        "  B[Data2[i][1]]+=int(Data2[i][2])\n",
        "  C+=int(Data2[i][2])+int(Data2[i][3])\n",
        "  totalGames+=2\n",
        "\n",
        "\n",
        "'''\n",
        "Initial estimates of a,b,c\n",
        "'''\n",
        "\n",
        "for i in range(len(A)):\n",
        "  a[i]=A[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "for i in range(len(B)):\n",
        "  b[i]=B[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "#c is another set of hidden weights for our weightrix. they are initally nonzero to avoid a stationary point.\n",
        "\n",
        "c=[0.001,0.002,0.004,-0.001,-0.002,-0.004,0.0005,0.0001,0.01,0.002,0.005,0.007,0.009,0.003,0.007,0.009,0.002,-0.001]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "AI training function\n",
        "\n",
        "The training is by gradient descent, the loss\n",
        "function will be cross entropy loss function against Poisson\n",
        "using loss.backward()\n",
        "\n",
        "The hidden weights at the moment are the entries of a,b,c\n",
        "the array c  is shared for all teams.\n",
        "These enter into the calculation of mu (which we\n",
        "call muHome or muAway during training) and are\n",
        "hidden as they have no direct meaning.\n",
        "\n",
        "Thus mu as a function of the entries of a,b,c is\n",
        "learned by training, the weights are the entries\n",
        "of the three arrays.\n",
        "\n",
        "\n",
        "\n",
        "tau is the training rate for c which should be small\n",
        "compared to eta\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def elementOf(i,A):\n",
        "  for j in range(len(A)):\n",
        "    if(A[j]==i):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def train(eta,tau):\n",
        "  loss=t.tensor(0.0)\n",
        "\n",
        "\n",
        "\n",
        "  atensor=t.tensor(a,requires_grad=True)\n",
        "  btensor=t.tensor(b,requires_grad=True)\n",
        "  ctensor=t.tensor(c,requires_grad=True)\n",
        "\n",
        "\n",
        "  for i in range(len(Data2)):\n",
        "    if(len(Data2[i])==0):\n",
        "      continue\n",
        "    homeTeam=int(Data2[i][0])\n",
        "    awayTeam=int(Data2[i][1])\n",
        "    homeGoals=int(Data2[i][2])\n",
        "    awayGoals=int(Data2[i][3])\n",
        "    HST=t.tensor(float(Data2[i][4]))\n",
        "    AST=t.tensor(float(Data2[i][5]))\n",
        "    HR=t.tensor(float(Data2[i][6]))\n",
        "    AR=t.tensor(float(Data2[i][7]))\n",
        "    HS=t.tensor(float(Data2[i][8]))\n",
        "    AS=t.tensor(float(Data2[i][9]))\n",
        "    HC=t.tensor(float(Data2[i][10]))\n",
        "    AC=t.tensor(float(Data2[i][11]))\n",
        "    HF=t.tensor(float(Data2[i][12]))\n",
        "    AF=t.tensor(float(Data2[i][13]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    The reason there are two lines of code for the loss is that\n",
        "    each game can be thought of as  two 'rows' of data where we label\n",
        "    the home team as team i  or team j.\n",
        "\n",
        "    Thus teams are interpreted symmetrically and there is not yet any\n",
        "    home team advantage but this can be put in\n",
        "    without modifying the architecture as a constant which is 1 in the\n",
        "    first line and 0 in the second\n",
        "    '''\n",
        "\n",
        "\n",
        "    muHome=atensor[homeTeam]*btensor[awayTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*HST+ctensor[4]*HR+ctensor[5]*HS+ctensor[6]*HC+ctensor[7]*HF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*HST+ctensor[9]*HR+ctensor[10]*HS+ctensor[11]*HC+ctensor[12]*HF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*HST+ctensor[14]*HR+ctensor[15]*HS+ctensor[16]*HC+ctensor[17]*HF)\n",
        "    muHome=muHome+neural*btensor[awayTeam]\n",
        "\n",
        "    muAway=atensor[awayTeam]*btensor[homeTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*AST+ctensor[4]*AR+ctensor[5]*AS+ctensor[6]*AC+ctensor[7]*AF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*AST+ctensor[9]*AR+ctensor[10]*AS+ctensor[11]*AC+ctensor[12]*AF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*AST+ctensor[14]*AR+ctensor[15]*AS+ctensor[16]*AC+ctensor[17]*AF)\n",
        "    muAway=muAway+neural*btensor[homeTeam]\n",
        "\n",
        "\n",
        "    loss-=t.log(t.exp(-muHome)*t.pow(muHome,homeGoals)/math.factorial(homeGoals))\n",
        "    loss-=t.log(t.exp(-muAway)*t.pow(muAway,awayGoals)/math.factorial(awayGoals))\n",
        "\n",
        "  loss.backward()\n",
        "  for i in range(len(c)):\n",
        "    c[i]=c[i]-tau*ctensor.grad[i]\n",
        "  for i in range(len(a)):\n",
        "    a[i]=a[i]-eta*atensor.grad[i]\n",
        "  for i in range(len(b)):\n",
        "    b[i]=b[i]-eta*btensor.grad[i]\n",
        "  print(\"\\n\\nCross-entropy loss vs Poisson: \"+str(loss))\n",
        "  print(\"\\n\\nWeights:\\n\\na:  \"+str(a))\n",
        "  print(\"b:  \"+str(b))\n",
        "  print(\"c:  \"+str(c))\n",
        "  print(\"\\n\\n\\n\")\n",
        "  print(\"Partial derivatives of loss w/r to hidden weights:\")\n",
        "  print(\"\\natensor.grad: \"+str(atensor.grad))\n",
        "  print(\"btensor.grad: \"+str(btensor.grad))\n",
        "  print(\"ctensor.grad: \"+str(ctensor.grad))\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Use weights to construct predicted expected goals scored by i\n",
        "against j and then find probability of k goals scored using\n",
        "Poisson when given values of R S ST  C F are provided\n",
        "'''\n",
        "\n",
        "# non-tensor version of sigma for using in the field\n",
        "\n",
        "def mathsigma(x):\n",
        "  return math.exp(x)/(math.exp(0)+math.exp(x))-0.5\n",
        "\n",
        "def goalProb(i,j,k,ST,R,S,C,F):\n",
        "  mu=a[i]*b[j]\n",
        "  neural=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  neural+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  neural+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  mu+=neural*b[j]\n",
        "  return math.exp(-mu)*mu**k/math.factorial(k)\n",
        "\n",
        "\n",
        "def goalProb2(i,j,k):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  #print(\"\\nNaive prediction of ST, R, S, C, F: \"+str(ST)+\", \"+str(R)+\", \"+str(S)+\", \"+str(C)+\", \"+str(F))\n",
        "  return goalProb(i,j,k,ST,R,S,C,F)\n",
        "\n",
        "\n",
        "'''\n",
        "Pre-training estimates of ST, R, S, C, F\n",
        "'''\n",
        "\n",
        "def preTrain(i,j,u,v):\n",
        "  X=0\n",
        "  Y=0\n",
        "  Z=0\n",
        "  for s in range(len(Data2)):\n",
        "    if(len(Data2[s])<2):\n",
        "      continue\n",
        "    if(Data2[s][0]==i):\n",
        "      X+=int(Data2[s][u])\n",
        "    if(Data2[s][0]==j):\n",
        "      Y+=int(Data2[s][u])\n",
        "    if(Data2[s][1]==i):\n",
        "      X+=int(Data2[s][v])\n",
        "    if(Data2[s][1]==j):\n",
        "      Y+=int(Data2[s][v])\n",
        "    Z+=int(Data2[s][u])+int(Data2[s][v])\n",
        "  return (X/games[i])*(Y/games[j])/(Z/totalGames)\n",
        "\n",
        "def preTrainST(i,j):\n",
        "  return preTrain(i,j,4,5)\n",
        "\n",
        "def preTrainR(i,j):\n",
        "  return preTrain(i,j,6,7)\n",
        "\n",
        "def preTrainS(i,j):\n",
        "  return preTrain(i,j,8,9)\n",
        "\n",
        "def preTrainC(i,j):\n",
        "  return preTrain(i,j,10,11)\n",
        "\n",
        "def preTrainF(i,j):\n",
        "  return preTrain(i,j,12,13)\n",
        "\n",
        "\n",
        "def expectedGoals(i,j):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  mu=a[i]*b[j]\n",
        "  neural=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  neural+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  neural+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  mu+=neural*b[j]\n",
        "  return mu\n",
        "\n",
        "'''\n",
        "Test training a bit\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "If training iterations is set to zero this is used\n",
        "'''\n",
        "\n",
        "\n",
        "import numpy\n",
        "\n",
        "def readWeights():\n",
        " aLocal = numpy.fromfile('a.bin', dtype=numpy.float32)\n",
        " bLocal = numpy.fromfile('b.bin', dtype=numpy.float32)\n",
        " cLocal = numpy.fromfile('c.bin', dtype=numpy.float32)\n",
        " for i in range(len(a)):\n",
        "  a[i]=aLocal[i]\n",
        " for i in range(len(b)):\n",
        "  b[i]=bLocal[i]\n",
        " for i in range(len(c)):\n",
        "  c[i]=cLocal[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n Pre-training score table\")\n",
        "st=\"{:<17}\".format(\"\")\n",
        "for i in range(len(teams)):\n",
        "  st+=\"{:<17}\".format(teams[i])\n",
        "print (st)\n",
        "for i in range(len(teams)):\n",
        "  st=\"{:<17}\".format(teams[i])\n",
        "  for j in range(len(teams)):\n",
        "    st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "  print(st)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def  validate():\n",
        "  MSE1=0.0\n",
        "  MSE2=0.0\n",
        "  loss1=0.0\n",
        "  loss2=0.0\n",
        "  for s in range(len(validateGame)):\n",
        "    ii=validateGame[s][0]\n",
        "    jj=validateGame[s][1]\n",
        "    print(\"\\nvalidating excluded game:\"+teams[ii]+\" vs \" +teams[jj])\n",
        "    print(\"pre-training prediction: \"+str(expectedGoals(ii,jj))+\" to \"+str(expectedGoals(jj,ii)))\n",
        "    MSE1+=(float(validateGame[s][2])-expectedGoals(ii,jj))**2\n",
        "    MSE1+=(float(validateGame[s][3])-expectedGoals(jj,ii))**2\n",
        "    loss1-=math.log(goalProb2(ii,jj,int(validateGame[s][2])))\n",
        "    print(\"*\"+str(loss1))\n",
        "    loss1-=math.log(goalProb2(jj,ii,int(validateGame[s][3])))\n",
        "    print(\"*\"+str(loss1))\n",
        "\n",
        "  readWeights()\n",
        "  print(\"\\n\\n\")\n",
        "  for s in range(len(validateGame)):\n",
        "    ii=validateGame[s][0]\n",
        "    jj=validateGame[s][1]\n",
        "    print(\"\\nblinded saved-weights prediction for \"+teams[ii]+ \" vs \"+teams[jj]+\": \"+str(expectedGoals(ii,jj))+\" to \"+str(expectedGoals(jj,ii)))\n",
        "    print(\"actual score of that game: \" +str(validateGame[s][2])+ \" to \"+str(validateGame[s][3]))\n",
        "    MSE2+=(float(validateGame[s][2])-expectedGoals(jj,ii))**2\n",
        "    MSE2+=(float(validateGame[s][3])-expectedGoals(jj,ii))**2\n",
        "    loss2-=math.log(goalProb2(ii,jj,int(validateGame[s][2])))\n",
        "    loss2-=math.log(goalProb2(jj,ii,int(validateGame[s][3])))\n",
        "  print(\"\\n\\npre-training MSE: \"+str(MSE1/len(validateGame))+\", post-training MSE: \"+str(MSE2/len(validateGame)))\n",
        "  print(\"\\n\\npre-training loss: \"+str(loss1)+\", post-training loss: \"+str(loss2))\n",
        "\n",
        "# TRAINING\n",
        "#NUMBER OF ITRATIONS, (If iterations=0, the weightrixes a,b,c are read from a file containing the previously trained weights instead of training the new weights)\n",
        "#(caution: the last time you save the code, do not leave iterations at 0, set it to something else to avoid errors upon re-loading the program)\n",
        "#TRAINING RATE FOR ETA, TAU.\n",
        "\n",
        "iterations=100\n",
        "\n",
        "\n",
        "\n",
        "for i in range(iterations):\n",
        "    train(0.0005,0.0005)\n",
        "\n",
        "\n",
        "if(iterations>0):\n",
        "    print(\"\\n\\n Score table after training\")\n",
        "    st=\"{:<17}\".format(\"\")\n",
        "    for i in range(len(teams)):\n",
        "       st+=\"{:<17}\".format(teams[i])\n",
        "    print (st)\n",
        "    for i in range(len(teams)):\n",
        "      st=\"{:<17}\".format(teams[i])\n",
        "      for j in range(len(teams)):\n",
        "          st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "      print(st)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "'''\n",
        "save weights\n",
        "'''\n",
        "\n",
        "\n",
        "def writeWeights():\n",
        " float_array = numpy.array(a, dtype=numpy.float32)\n",
        " float_array.tofile('a.bin')\n",
        "\n",
        "\n",
        " float_array = numpy.array(b, dtype=numpy.float32)\n",
        " float_array.tofile('b.bin')\n",
        "\n",
        " float_array = numpy.array(c, dtype=numpy.float32)\n",
        " float_array.tofile('c.bin')\n",
        "\n",
        "\n",
        "if(iterations == 0):\n",
        "  validate()\n",
        "\n",
        "if(iterations>0):\n",
        "  writeWeights()\n",
        "\n",
        "print(\"=============================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting actual goals for each match in the dataset\n",
        "# Match[2] = home goals\n",
        "# Match[3] = away goals\n",
        "goals_true = []\n",
        "\n",
        "for match in Data2:\n",
        "  if len(match) >=4:\n",
        "    goals_true.append([int(match[2]),int(match[3])])\n",
        "\n",
        "# Extracting values from tensors a, b, and c\n",
        "a_values = []\n",
        "b_values = []\n",
        "c_values = []\n",
        "\n",
        "for value in a:\n",
        "    a_values.append(float(value))\n",
        "\n",
        "for value in b:\n",
        "    b_values.append(float(value))\n",
        "\n",
        "for value in c:\n",
        "    c_values.append(float(value))\n",
        "\n",
        "\n",
        "def predict_goals(home_team, away_team, a_values, b_values, c_values, teams):\n",
        "    # Indices of the home and away teams\n",
        "    home_team_index = getIndex(home_team, teams)\n",
        "    away_team_index = getIndex(away_team, teams)\n",
        "    predicted_home_goals=expectedGoals(home_team_index,away_team_index)\n",
        "    predicted_away_goals=expectedGoals(away_team_index,home_team_index)\n",
        "    return [predicted_home_goals, predicted_away_goals]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_goals = []\n",
        "actual_goals = []\n",
        "\n",
        "# Go over each match in Data2\n",
        "for match in Data2:\n",
        "    if len(match) >= 4:\n",
        "        home_team_index = match[0]\n",
        "        away_team_index = match[1]\n",
        "\n",
        "        # Call the predict_goals function to get the predicted goals\n",
        "        predicted_match_goals = predict_goals(teams[home_team_index], teams[away_team_index], a_values, b_values, c_values, teams)\n",
        "\n",
        "        predicted_goals.append(predicted_match_goals)\n",
        "        actual_goals.append([int(match[2]), int(match[3])])\n",
        "\n",
        "\n",
        "total_matches = len(actual_goals)\n",
        "correct_predictions = 0\n",
        "\n",
        "\n",
        "absolute_errors = []\n",
        "\n",
        "# Iterate over each match to calculate absolute errors\n",
        "for i in range(total_matches):\n",
        "    absolute_error_match = [] # Each match\n",
        "    print(\"Actual goals: \", actual_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][0] - predicted_goals[i][0]))\n",
        "\n",
        "    print(\"Predicted goals: \", predicted_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][1] - predicted_goals[i][1]))\n",
        "\n",
        "    absolute_errors.append(absolute_error_match)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = 0\n",
        "for i in range(total_matches):\n",
        "    if actual_goals[i] == predicted_goals[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_matches\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "\n",
        "# Iterate over each match to calculate sum of absolute errors\n",
        "sum_abs_errors = 0\n",
        "for error_p_match in absolute_errors:\n",
        "    sum_abs_errors += sum(error_p_match)\n",
        "\n",
        "# Calculate mean absolute error\n",
        "mean_absolute_error = sum_abs_errors / total_matches\n",
        "\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten\n",
        "true_goals_flat = []\n",
        "for match in actual_goals:\n",
        "    for goal in match:\n",
        "        true_goals_flat.append(goal)\n",
        "\n",
        "predicted_goals_flat = []\n",
        "for match in predicted_goals:\n",
        "    for goal in match:\n",
        "        predicted_goals_flat.append(goal)\n",
        "\n",
        "\n",
        "# Histogram of Predicted Goals vs. Actual Goals\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist([true_goals_flat, predicted_goals_flat], bins=10, label=['True Goals', 'Predicted Goals'])\n",
        "plt.xlabel('Goals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Predicted Goals vs. True Goals')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e3tJxBFgMOLT",
        "outputId": "4c1b094c-3570-4d27-86ec-99288e88b227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.7476479021245026, 1.636438271158316]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.8996602842276948, 1.51587515563425]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [2.2683393411680552, 1.3699209742590284]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [1.973845546718576, 1.817382758136728]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.387354434006895, 1.8365314011509075]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.6667736401305344, 1.322583519433513]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.9591705801776538, 1.7703826430133471]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.5348146558530886, 1.443351864887055]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [1.4633075203929837, 2.4341585841213162]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.4873944802248404, 1.6855979843103812]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [1.1489495149555522, 2.387426923841508]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [1.8473096502205557, 2.0619754207512564]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [2.0180312205636506, 1.5886875201547153]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.3862237385154395, 2.116012161099973]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.8181163322209235, 1.9339714299916144]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [1.7966754484125107, 1.7968609380670517]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.0844058814160404, 1.2238741340748844]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.015754326934882, 1.4264682379914004]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.7865872993012646, 1.223475755645869]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [2.0984342667504774, 1.60185152024474]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.6704312401085901, 2.004364951160531]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [2.464365574369604, 1.2929973330594842]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [1.3874922951054744, 2.0574497540784056]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [2.194478932153197, 1.5927868516554553]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.7608189267540704, 1.3754756135369073]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [1.5631937778569305, 2.0012680090046966]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.5767078609540088, 2.170338890083037]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.2792156808397526, 2.5043435566446535]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.2782963307474287, 1.829015949544731]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [2.477768080268717, 1.1681599784239296]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [1.4668415732103928, 2.084181899519597]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.7380960279694178, 1.7577752166977503]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.4723050141886376, 2.0432119513109828]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [2.3801961384227837, 1.2610375724247063]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.3772097545625195, 1.9815792757035717]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.048925832220164, 1.3372567448101909]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.5123731555010538, 1.8077045144106607]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [1.668890782662468, 1.875822492905693]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [2.42030347106129, 1.844116814109386]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.7190175570813895, 1.657752446111846]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.9876755307319285, 1.72124872905041]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.509310471768671, 2.192383515591913]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.248202821372787, 1.5053212848890223]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.9605316571222775, 1.4374950579630368]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.8384310106975856, 1.8260094027263942]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [1.4886409697525966, 1.6880914030068386]\n",
            "Actual goals:  [0, 5]\n",
            "Predicted goals:  [1.1415014885790502, 2.9544005416758212]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.4125849863096398, 1.8261572381063622]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.776926816932667, 1.663512886993397]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.9294515598188666, 1.2533412325750617]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.714746199905579, 1.6154042113420415]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [2.2344309168223733, 1.620924718892991]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.969347587898648, 1.310458175018704]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [1.4250159163470335, 2.5188714761729307]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.6078338592380201, 1.4446014135211624]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.6084609749743433, 2.0677989247271507]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.557032934039811, 1.8366105469167031]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [2.191653142840862, 1.6928851708483692]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.9945631941280593, 1.790649028631233]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.228587527651329, 1.6509037212319564]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.7206693212834256, 2.348610810503091]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.2902573663727686, 1.9387696702019617]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [2.0336843510969125, 1.315145709167286]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.5965958351019292, 1.3380532378080754]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.6246041028916829, 1.948013183397054]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [1.8354711984377228, 1.7466167663317047]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.5980281609049953, 2.4519500511637844]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.9042037242441836, 1.5388489478617373]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.2276749410410241, 2.252902428128274]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.2212186674982597, 2.1430653552919914]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.685687537810171, 1.4800059157818204]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.4570334378849035, 1.397855681575394]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.2657214482254306, 2.886046596331052]\n",
            "Actual goals:  [6, 2]\n",
            "Predicted goals:  [2.009703642645808, 1.8148617809209544]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.3780577375358984, 1.8470553948349404]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.4117534054301748, 1.9800257576488027]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.4696150938189418, 1.7093821684039028]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.444889689337878, 1.7177589186545894]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.9131915580989998, 2.0006619703533333]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.6924486701834422, 2.0498940294134838]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [1.5581996244794454, 1.6794185442334737]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.9220552356292726, 1.4708655388404848]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.6626969262633207, 1.765155999330032]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.362312078513181, 1.3747994900122045]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [2.566142961444211, 1.3333666176217316]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [1.9304470354159482, 1.482833021267236]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [1.9099372413531819, 1.403510251320986]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.7407804919964145, 1.531106014419205]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [1.7688739765793913, 2.339909123292648]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.273318450692897, 2.2866913487947036]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.7147847072758264, 1.7445298092045374]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.224886263115403, 2.429525578720566]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [1.1090968929446536, 3.262126752010663]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [2.2091132358864223, 1.3393274502113734]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.1235045742907563, 1.5317173552432097]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.6288976578308083, 1.4787880091262795]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.677158697042261, 1.6346993283365114]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.8172956725063643, 1.8283994217815718]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.5676425072793527, 1.7389433953408762]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.9744731404079818, 2.064337752033653]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.584903143368889, 1.7868688330280076]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.525389449314974, 1.1794105336235983]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.794842385513214, 1.47288300917807]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.508477733095137, 1.7013665890292802]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.7091429686400665, 1.8179805731627716]\n",
            "Actual goals:  [6, 1]\n",
            "Predicted goals:  [2.2436677284854625, 1.640831954063771]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.4518041661706351, 2.120738392206707]\n",
            "Actual goals:  [5, 0]\n",
            "Predicted goals:  [1.8262431789838902, 1.3444397021734347]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.4925118138051432, 1.7437664319730204]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [2.223514887117806, 1.9014179432625204]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.8915737806390358, 1.8000855384896828]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [1.147975870435265, 2.2770919883895187]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.122020270282377, 1.6272206563297402]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.686819185017839, 2.3098453176014573]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.736938710332159, 2.2077708197834527]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.3295480090503866, 1.3901518064861473]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.38469347244137, 1.9125824261271305]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [1.9311204131436523, 1.554827349979895]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.1942162751790593, 1.9270791053411074]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.5181046845364188, 1.8886783244061087]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.9644752212962824, 1.5450936027965265]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.5533637090908068, 1.4281233235584279]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.8594437926599632, 1.8608478396722923]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.8642241907426302, 1.7631019306489413]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [2.5183702855171455, 1.5202038435997258]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.9754278521332547, 1.8777583699021145]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.2772568666205408, 2.3846897088751797]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.1555527739772034, 1.2821842723139956]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.7313471161038976, 1.5245836101682286]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.8285505197933671, 1.3877919934681413]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.8929509910831954, 1.7323976549397018]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [1.7999369130978053, 1.6823828445324367]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.0184491589668188, 1.585357399285117]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.1786385798528873, 3.0608710074499332]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.2360397142021156, 2.836581842288206]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [1.8690547419455104, 1.409257657613998]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.7145709874650195, 1.6953503968735888]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.5271942768606976, 1.5697952662024335]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [2.19882585203469, 1.6054173866492534]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.737533859660794, 2.0354253572727936]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.543712968816737, 1.6258375596904553]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [2.400557445028903, 1.277417705992343]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.7257029152442454, 1.9313226080466745]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.6507592311029473, 1.3449246993188897]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [2.232797541295459, 1.4787388226141231]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.5948552605251987, 1.885825705204459]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [1.9448383376283946, 1.4188924953623119]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.8125581416125205, 1.7148613604540732]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.617041750868043, 1.2085299334127506]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [2.31013535564027, 1.3604904920347705]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.4516028313066767, 2.543103948059301]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.5611158789791395, 1.9498037280239393]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.1926697240765902, 2.545477305501303]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.5455202482765897, 1.7704265736168607]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.9318045185789214, 1.9443737076505767]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.3759618888434628, 2.0352100620803095]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [1.6591435696661478, 1.472288372713328]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.3532479078186868, 2.2505259186638704]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [2.5367945511792183, 1.6624742825482368]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [3.146422054121184, 1.0751601117345564]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.7391203062629161, 1.8089554922675548]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.7677524876268933, 1.8001715731295178]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [1.3641400371588517, 2.154699865440254]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.6386986831308632, 1.4693205813051489]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.411216842060419, 2.3890007604411565]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.4949917188481037, 2.164347111304158]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [2.0307790001182324, 1.4438531358985671]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [2.2985633660172007, 1.3163359809731032]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.7820338731356282, 1.7714691882677693]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [1.6328334135574571, 1.6724546474975817]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [2.031540690392653, 1.930422721833388]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [2.33510015425409, 1.247797648284089]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.57661548531055, 1.7695082751512483]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.5618769678147477, 2.000004247096364]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.315054690628822, 2.131528651505287]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [2.0090926881563274, 1.4580065603983012]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.644898975621941, 1.98546477628875]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.9995371172906005, 2.0220434735299193]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.1195531769825864, 2.037601081950848]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.8547657706688587, 1.5009599902580921]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.3699209742590284, 2.2683393411680552]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.8365314011509075, 1.387354434006895]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.1492606042523286, 2.581714773048868]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.817382758136728, 1.973845546718576]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.51587515563425, 1.8996602842276948]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.4665032192588923, 1.3226984306694147]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.868027730364019, 1.5711948112432303]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [1.636438271158316, 1.7476479021245026]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.6526959828622927, 2.2715129546411625]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [2.387426923841508, 1.1489495149555522]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.443351864887055, 1.5348146558530886]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [1.8751164380183742, 1.8881473246684597]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.4341585841213162, 1.4633075203929837]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.0383198441555908, 1.514445442109387]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.671214919219992, 2.273424248825094]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.6855979843103812, 1.4873944802248404]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [1.7703826430133471, 1.9591705801776538]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [1.322583519433513, 2.6667736401305344]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.735432581340732, 1.4463178676719088]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.7968609380670517, 1.7966754484125107]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.5886875201547153, 2.0180312205636506]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [2.0619754207512564, 1.8473096502205557]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [1.9339714299916144, 1.8181163322209235]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.116012161099973, 1.3862237385154395]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.4264682379914004, 2.015754326934882]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.223475755645869, 2.7865872993012646]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.2238741340748844, 2.0844058814160404]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.1430653552919914, 1.2212186674982597]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.6794185442334737, 1.5581996244794454]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.287330960560505, 2.4800678842551154]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.3754756135369073, 1.7608189267540704]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [2.004364951160531, 1.6704312401085901]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.0012680090046966, 1.5631937778569305]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [1.5927868516554553, 2.194478932153197]\n",
            "Actual goals:  [0, 5]\n",
            "Predicted goals:  [1.60185152024474, 2.0984342667504774]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.0574497540784056, 1.3874922951054744]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.829015949544731, 1.2782963307474287]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.170338890083037, 1.5767078609540088]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.5043435566446535, 1.2792156808397526]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [1.2929973330594842, 2.464365574369604]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.3890007604411565, 1.411216842060419]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.4438531358985671, 2.0307790001182324]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [1.3163359809731032, 2.2985633660172007]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.8001715731295178, 1.7677524876268933]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [2.164347111304158, 1.4949917188481037]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.4693205813051489, 1.6386986831308632]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [1.7714691882677693, 1.7820338731356282]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [1.0751601117345564, 3.146422054121184]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.154699865440254, 1.3641400371588517]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.8089554922675548, 1.7391203062629161]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.037601081950848, 1.1195531769825864]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [2.0220434735299193, 1.9995371172906005]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.930422721833388, 2.031540690392653]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [1.7695082751512483, 1.57661548531055]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.4580065603983012, 2.0090926881563274]\n",
            "Actual goals:  [2, 4]\n",
            "Predicted goals:  [1.98546477628875, 1.644898975621941]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.6724546474975817, 1.6328334135574571]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.000004247096364, 1.5618769678147477]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [2.131528651505287, 1.315054690628822]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.247797648284089, 2.33510015425409]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [1.8148617809209544, 2.009703642645808]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [2.0498940294134838, 1.6924486701834422]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.4800059157818204, 1.685687537810171]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [2.886046596331052, 1.2657214482254306]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.8470553948349404, 1.3780577375358984]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [1.7177589186545894, 1.444889689337878]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.397855681575394, 2.4570334378849035]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.7093821684039028, 1.4696150938189418]\n",
            "Actual goals:  [4, 4]\n",
            "Predicted goals:  [2.0006619703533333, 1.9131915580989998]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.2866913487947036, 1.273318450692897]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.531106014419205, 1.7407804919964145]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.4708655388404848, 1.9220552356292726]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.482833021267236, 1.9304470354159482]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.339909123292648, 1.7688739765793913]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.3747994900122045, 2.362312078513181]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.3333666176217316, 2.566142961444211]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.403510251320986, 1.9099372413531819]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.6642348127677422, 1.5868599968268853]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.765155999330032, 1.6626969262633207]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.7445298092045374, 1.7147847072758264]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.9313226080466745, 1.7257029152442454]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.2085299334127506, 2.617041750868043]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.277417705992343, 2.400557445028903]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.885825705204459, 1.5948552605251987]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.7148613604540732, 1.8125581416125205]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.543103948059301, 1.4516028313066767]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [1.4787388226141231, 2.232797541295459]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.3449246993188897, 1.6507592311029473]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.4188924953623119, 1.9448383376283946]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [1.3604904920347705, 2.31013535564027]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [1.9800257576488027, 1.4117534054301748]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.545477305501303, 1.1926697240765902]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.9498037280239393, 1.5611158789791395]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.0352100620803095, 1.3759618888434628]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.9443737076505767, 1.9318045185789214]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.4800678842551154, 1.287330960560505]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [2.2505259186638704, 1.3532479078186868]\n",
            "Actual goals:  [5, 3]\n",
            "Predicted goals:  [1.5868599968268853, 1.6642348127677422]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.7704265736168607, 1.5455202482765897]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.472288372713328, 1.6591435696661478]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.6624742825482368, 2.5367945511792183]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [2.192383515591913, 1.509310471768671]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.8260094027263942, 1.8384310106975856]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.657752446111846, 1.7190175570813895]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [2.9544005416758212, 1.1415014885790502]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.663512886993397, 1.776926816932667]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.72124872905041, 1.9876755307319285]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.6880914030068386, 1.4886409697525966]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.8261572381063622, 1.4125849863096398]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.4374950579630368, 1.9605316571222775]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.5053212848890223, 2.248202821372787]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.8077045144106607, 1.5123731555010538]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.7577752166977503, 1.7380960279694178]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [1.9815792757035717, 1.3772097545625195]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.2610375724247063, 2.3801961384227837]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [1.3372567448101909, 2.048925832220164]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.875822492905693, 1.668890782662468]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [2.084181899519597, 1.4668415732103928]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.844116814109386, 2.42030347106129]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [1.9387696702019617, 1.2902573663727686]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.7466167663317047, 1.8354711984377228]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.315145709167286, 2.0336843510969125]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [2.348610810503091, 1.7206693212834256]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.5388489478617373, 1.9042037242441836]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.948013183397054, 1.6246041028916829]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.252902428128274, 1.2276749410410241]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.6509037212319564, 2.228587527651329]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [2.4519500511637844, 1.5980281609049953]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.2533412325750617, 1.9294515598188666]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [1.310458175018704, 1.969347587898648]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [2.0677989247271507, 1.6084609749743433]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.8366105469167031, 1.557032934039811]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.4446014135211624, 1.6078338592380201]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.6154042113420415, 1.714746199905579]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.790649028631233, 1.9945631941280593]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.620924718892991, 2.2344309168223733]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.6928851708483692, 2.191653142840862]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.5188714761729307, 1.4250159163470335]\n",
            "Actual goals:  [5, 0]\n",
            "Predicted goals:  [3.262126752010663, 1.1090968929446536]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [2.064337752033653, 1.9744731404079818]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.8283994217815718, 1.8172956725063643]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.7868688330280076, 1.584903143368889]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.429525578720566, 1.224886263115403]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.7389433953408762, 1.5676425072793527]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [1.6346993283365114, 1.677158697042261]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.3393274502113734, 2.2091132358864223]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.5317173552432097, 2.1235045742907563]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [1.4787880091262795, 1.6288976578308083]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [2.0432119513109828, 1.4723050141886376]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.8000855384896828, 1.8915737806390358]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.8179805731627716, 1.7091429686400665]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.9014179432625204, 2.223514887117806]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.7437664319730204, 1.4925118138051432]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.47288300917807, 1.794842385513214]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.7013665890292802, 1.508477733095137]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [2.120738392206707, 1.4518041661706351]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.640831954063771, 2.2436677284854625]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.3444397021734347, 1.8262431789838902]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [1.1794105336235983, 2.525389449314974]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [2.2770919883895187, 1.147975870435265]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.3901518064861473, 2.3295480090503866]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [2.2077708197834527, 1.736938710332159]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [1.9270791053411074, 1.1942162751790593]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.554827349979895, 1.9311204131436523]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.5450936027965265, 1.9644752212962824]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.6272206563297402, 2.122020270282377]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.8886783244061087, 1.5181046845364188]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [2.3098453176014573, 1.686819185017839]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [1.9125824261271305, 1.38469347244137]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.1681599784239296, 2.477768080268717]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [1.3877919934681413, 1.8285505197933671]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [1.7631019306489413, 1.8642241907426302]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [1.5202038435997258, 2.5183702855171455]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [1.8777583699021145, 1.9754278521332547]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.5245836101682286, 1.7313471161038976]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.7323976549397018, 1.8929509910831954]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.4281233235584279, 1.5533637090908068]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [1.8608478396722923, 1.8594437926599632]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [1.2821842723139956, 2.1555527739772034]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [2.3846897088751797, 1.2772568666205408]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [1.3380532378080754, 1.5965958351019292]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [2.836581842288206, 1.2360397142021156]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.409257657613998, 1.8690547419455104]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.6823828445324367, 1.7999369130978053]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.585357399285117, 2.0184491589668188]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [1.6258375596904553, 1.543712968816737]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [3.0608710074499332, 1.1786385798528873]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [1.5697952662024335, 1.5271942768606976]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [2.0354253572727936, 1.737533859660794]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [1.6953503968735888, 1.7145709874650195]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [1.6054173866492534, 2.19882585203469]\n",
            "Accuracy:  0.0\n",
            "Mean Absolute Error:  2.1009595092810676\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}