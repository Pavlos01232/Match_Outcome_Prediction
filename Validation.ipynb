{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavlos01232/Match_Outcome_Prediction/blob/main/Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsKoaXp4LRvc",
        "outputId": "e451cd62-8b53-48fd-ad0a-cea8d459b6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================================\n",
            "\n",
            "teams:\n",
            "['Arsenal', 'Birmingham', 'Blackburn', 'Fulham', 'Leicester', 'Man United', 'Portsmouth', 'Charlton', 'Leeds', 'Liverpool', 'Bolton', 'Chelsea', 'Everton', 'Man City', 'Newcastle', 'Southampton', 'Tottenham', 'Wolves', 'Aston Villa', 'Middlesbrough']\n",
            "381\n",
            "excluded: [137, 141, 144, 150, 153, 163, 191, 191, 201]\n",
            "\n",
            "CAUTION: check that the teams list was not disrupted by exclusions:\n",
            "['Arsenal', 'Birmingham', 'Blackburn', 'Fulham', 'Leicester', 'Man United', 'Portsmouth', 'Charlton', 'Leeds', 'Liverpool', 'Bolton', 'Chelsea', 'Everton', 'Man City', 'Newcastle', 'Southampton', 'Tottenham', 'Wolves', 'Aston Villa', 'Middlesbrough']\n",
            "\n",
            "\n",
            " Pre-training score table\n",
            "                 Arsenal          Birmingham       Blackburn        Fulham           Leicester        Man United       Portsmouth       Charlton         Leeds            Liverpool        Bolton           Chelsea          Everton          Man City         Newcastle        Southampton      Tottenham        Wolves           Aston Villa      Middlesbrough    \n",
            "Arsenal          0.984            1.839            2.265            1.692            2.479            1.338            2.086            1.954            3.033            1.418            2.125            1.150            2.184            2.069            1.456            1.692            2.204            2.833            1.653            1.993            \n",
            "Birmingham       0.580            1.085            1.336            0.998            1.462            0.789            1.230            1.152            1.789            0.836            1.253            0.678            1.288            1.220            0.859            0.998            1.300            1.671            0.975            1.175            \n",
            "Blackburn        0.627            1.172            1.443            1.078            1.579            0.852            1.329            1.245            1.932            0.903            1.354            0.732            1.391            1.318            0.928            1.078            1.404            1.805            1.053            1.269            \n",
            "Fulham           0.707            1.321            1.627            1.216            1.781            0.961            1.498            1.404            2.179            1.019            1.527            0.826            1.569            1.486            1.046            1.216            1.583            2.035            1.187            1.431            \n",
            "Leicester        0.638            1.192            1.467            1.096            1.606            0.867            1.351            1.266            1.965            0.919            1.377            0.745            1.415            1.341            0.943            1.096            1.428            1.836            1.071            1.291            \n",
            "Man United       0.859            1.606            1.978            1.478            2.165            1.169            1.821            1.706            2.649            1.238            1.856            1.004            1.907            1.807            1.272            1.478            1.924            2.474            1.443            1.740            \n",
            "Portsmouth       0.624            1.166            1.436            1.073            1.571            0.848            1.322            1.239            1.923            0.899            1.347            0.729            1.384            1.312            0.923            1.073            1.397            1.796            1.048            1.263            \n",
            "Charlton         0.688            1.286            1.584            1.184            1.734            0.936            1.459            1.367            2.122            0.992            1.486            0.804            1.528            1.447            1.018            1.184            1.541            1.982            1.156            1.394            \n",
            "Leeds            0.527            0.985            1.213            0.906            1.328            0.717            1.117            1.047            1.625            0.760            1.138            0.616            1.170            1.108            0.780            0.906            1.180            1.518            0.885            1.067            \n",
            "Liverpool        0.742            1.387            1.708            1.276            1.870            1.009            1.573            1.474            2.288            1.070            1.603            0.867            1.647            1.561            1.098            1.276            1.662            2.137            1.247            1.503            \n",
            "Bolton           0.638            1.192            1.468            1.096            1.606            0.867            1.351            1.266            1.965            0.919            1.377            0.745            1.415            1.341            0.944            1.096            1.428            1.836            1.071            1.291            \n",
            "Chelsea          0.904            1.690            2.081            1.555            2.278            1.230            1.916            1.796            2.787            1.303            1.953            1.056            2.007            1.901            1.338            1.555            2.025            2.603            1.519            1.831            \n",
            "Everton          0.584            1.092            1.344            1.004            1.471            0.794            1.238            1.160            1.800            0.842            1.261            0.682            1.296            1.228            0.864            1.004            1.308            1.682            0.981            1.183            \n",
            "Man City         0.742            1.387            1.708            1.276            1.870            1.009            1.573            1.474            2.288            1.070            1.603            0.867            1.647            1.561            1.098            1.276            1.662            2.137            1.247            1.503            \n",
            "Newcastle        0.698            1.305            1.607            1.200            1.759            0.949            1.479            1.386            2.152            1.006            1.507            0.816            1.549            1.468            1.033            1.200            1.563            2.010            1.172            1.413            \n",
            "Southampton      0.582            1.088            1.340            1.001            1.467            0.792            1.234            1.156            1.794            0.839            1.257            0.680            1.292            1.224            0.861            1.001            1.304            1.676            0.978            1.179            \n",
            "Tottenham        0.624            1.166            1.436            1.073            1.571            0.848            1.322            1.239            1.923            0.899            1.347            0.729            1.384            1.312            0.923            1.073            1.397            1.796            1.048            1.263            \n",
            "Wolves           0.513            0.959            1.180            0.882            1.292            0.697            1.087            1.018            1.581            0.739            1.108            0.599            1.138            1.078            0.759            0.882            1.149            1.477            0.861            1.038            \n",
            "Aston Villa      0.651            1.218            1.499            1.120            1.641            0.886            1.381            1.294            2.008            0.939            1.407            0.761            1.446            1.370            0.964            1.120            1.459            1.876            1.094            1.319            \n",
            "Middlesbrough    0.594            1.110            1.367            1.021            1.496            0.808            1.259            1.179            1.830            0.856            1.282            0.694            1.318            1.249            0.879            1.021            1.330            1.710            0.997            1.202            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.1118164062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6664271355), tensor(0.9850926995), tensor(1.0699005127), tensor(1.1999126673), tensor(1.0854712725), tensor(1.4545227289), tensor(1.0594611168), tensor(1.1691074371), tensor(0.9013012648), tensor(1.2575557232), tensor(1.0850569010), tensor(1.5309759378), tensor(0.9900228381), tensor(1.2614544630), tensor(1.1842513084), tensor(0.9889701009), tensor(1.0642963648), tensor(0.8774982691), tensor(1.1052327156), tensor(1.0089190006)]\n",
            "b:  [tensor(0.5934676528), tensor(1.0991032124), tensor(1.3528553247), tensor(1.0124939680), tensor(1.4821271896), tensor(0.8041849136), tensor(1.2470684052), tensor(1.1696966887), tensor(1.8123178482), tensor(0.8497199416), tensor(1.2706383467), tensor(0.6920222044), tensor(1.3083268404), tensor(1.2393925190), tensor(0.8721671700), tensor(1.0108259916), tensor(1.3166310787), tensor(1.6922575235), tensor(0.9883502126), tensor(1.1910165548)]\n",
            "c:  [tensor(0.0006175477), tensor(0.0027526792), tensor(0.0045260028), tensor(-0.0009359902), tensor(-0.0020010551), tensor(-0.0039534150), tensor(0.0004909860), tensor(9.2386362667e-05), tensor(0.0101514030), tensor(0.0019977042), tensor(0.0051151915), tensor(0.0069832145), tensor(0.0089936294), tensor(0.0032852963), tensor(0.0069956831), tensor(0.0092156576), tensor(0.0019680320), tensor(-0.0010110710)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0013159513,  0.1090390980, -1.0619471073,  0.1389383078,\n",
            "        -0.5141189694,  1.0078197718, -0.0203450322, -0.0190247297,\n",
            "        -1.2154382467,  0.6287783980, -0.4312559962,  0.9570941925,\n",
            "         0.3964468837, -0.1509771347,  0.2631532550, -0.0469359159,\n",
            "        -0.9873890877, -1.2938888073,  0.2418425083, -0.0718551278])\n",
            "btensor.grad: tensor([-0.9869332314,  0.2287415862,  0.4156872630, -0.0434427857,\n",
            "         0.1952003390, -0.7560182810,  0.1243171990, -0.1368646622,\n",
            "         0.4651151896, -0.3225972652,  0.1185947657, -0.8735825419,\n",
            "        -0.3567301631, -0.3229476213, -0.2276645601,  0.2901619673,\n",
            "         0.3365422487,  0.5435038805,  0.0770481825,  0.1835277081])\n",
            "ctensor.grad: tensor([ 0.7649046183, -1.5053580999, -1.0520049334, -0.1280196607,\n",
            "         0.0021100282, -0.0931706205,  0.0180280395,  0.0152272647,\n",
            "        -0.3028057218,  0.0045918403, -0.2303827852,  0.0335717201,\n",
            "         0.0127398307, -0.5705927014,  0.0086340727, -0.4313150644,\n",
            "         0.0639361590,  0.0221418776])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0505371094, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6620829105), tensor(0.9846364260), tensor(1.0741184950), tensor(1.1993253231), tensor(1.0874966383), tensor(1.4502321482), tensor(1.0595101118), tensor(1.1691665649), tensor(0.9059343338), tensor(1.2549444437), tensor(1.0867471695), tensor(1.5268847942), tensor(0.9884753227), tensor(1.2620650530), tensor(1.1831549406), tensor(0.9891112447), tensor(1.0681921244), tensor(0.8823415637), tensor(1.1042230129), tensor(1.0091680288)]\n",
            "b:  [tensor(0.5965512395), tensor(1.0981541872), tensor(1.3511629105), tensor(1.0126210451), tensor(1.4812902212), tensor(0.8068836331), tensor(1.2465205193), tensor(1.1702260971), tensor(1.8103061914), tensor(0.8508624434), tensor(1.2701659203), tensor(0.6949420571), tensor(1.3097518682), tensor(1.2407063246), tensor(0.8730008602), tensor(1.0096663237), tensor(1.3152707815), tensor(1.6899441481), tensor(0.9880091548), tensor(1.1902430058)]\n",
            "c:  [tensor(0.0003115111), tensor(0.0035784757), tensor(0.0051491284), tensor(-0.0008965571), tensor(-0.0020017093), tensor(-0.0039248480), tensor(0.0004853229), tensor(8.7180575065e-05), tensor(0.0103594121), tensor(0.0019945297), tensor(0.0052728560), tensor(0.0069596577), tensor(0.0089824172), tensor(0.0036080221), tensor(0.0069907759), tensor(0.0094588725), tensor(0.0019312880), tensor(-0.0010269645)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8688437939,  0.0912604332, -0.8436011076,  0.1174608469,\n",
            "        -0.4050633907,  0.8581132889, -0.0097893178, -0.0118221045,\n",
            "        -0.9266175032,  0.5222594738, -0.3380604386,  0.8182313442,\n",
            "         0.3095042109, -0.1221137047,  0.2192682624, -0.0282275677,\n",
            "        -0.7791621685, -0.9686642885,  0.2019480467, -0.0498149395])\n",
            "btensor.grad: tensor([-0.6167185307,  0.1898061633,  0.3384777904, -0.0254043341,\n",
            "         0.1673960984, -0.5397441387,  0.1095810533, -0.1058856249,\n",
            "         0.4023422003, -0.2285051346,  0.0944784880, -0.5839745402,\n",
            "        -0.2850140631, -0.2627710104, -0.1667385995,  0.2319360375,\n",
            "         0.2720490992,  0.4626779556,  0.0682075024,  0.1547214985])\n",
            "ctensor.grad: tensor([ 6.1207324266e-01, -1.6515927315e+00, -1.2462507486e+00,\n",
            "        -7.8866168857e-02,  1.3086247491e-03, -5.7134114206e-02,\n",
            "         1.1326209642e-02,  1.0411570780e-02, -4.1601753235e-01,\n",
            "         6.3487128355e-03, -3.1532916427e-01,  4.7113399953e-02,\n",
            "         2.2425143048e-02, -6.4545184374e-01,  9.8146377131e-03,\n",
            "        -4.8642903566e-01,  7.3487915099e-02,  3.1786855310e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0117187500, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6583418846), tensor(0.9842984080), tensor(1.0775148869), tensor(1.1988687515), tensor(1.0891337395), tensor(1.4466158152), tensor(1.0595637560), tensor(1.1692402363), tensor(0.9095178246), tensor(1.2528150082), tensor(1.0881124735), tensor(1.5234211683), tensor(0.9873058200), tensor(1.2625976801), tensor(1.1822776794), tensor(0.9892289639), tensor(1.0713117123), tensor(0.8860217929), tensor(1.1034209728), tensor(1.0093787909)]\n",
            "b:  [tensor(0.5985087752), tensor(1.0974012613), tensor(1.3498152494), tensor(1.0127185583), tensor(1.4806051254), tensor(0.8088396192), tensor(1.2460728884), tensor(1.1706677675), tensor(1.8085925579), tensor(0.8517006636), tensor(1.2698225975), tensor(0.6969261765), tensor(1.3109219074), tensor(1.2418104410), tensor(0.8736397624), tensor(1.0087718964), tensor(1.3142021894), tensor(1.6880017519), tensor(0.9877431989), tensor(1.1896247864)]\n",
            "c:  [tensor(3.7377292756e-05), tensor(0.0044528190), tensor(0.0058365273), tensor(-0.0008766638), tensor(-0.0020020402), tensor(-0.0039104433), tensor(0.0004824631), tensor(8.4437713667e-05), tensor(0.0106301792), tensor(0.0019903935), tensor(0.0054781395), tensor(0.0069290847), tensor(0.0089666005), tensor(0.0039761853), tensor(0.0069851801), tensor(0.0097366059), tensor(0.0018896955), tensor(-0.0010462826)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7482151985,  0.0676011443, -0.6792782545,  0.0913127661,\n",
            "        -0.3274271488,  0.7232652903, -0.0107269883, -0.0147366524,\n",
            "        -0.7167031765,  0.4258777201, -0.2730640769,  0.6927300692,\n",
            "         0.2338984609, -0.1065243483,  0.1754459143, -0.0235415697,\n",
            "        -0.6239112616, -0.7360494137,  0.1603975892, -0.0421459675])\n",
            "btensor.grad: tensor([-0.3915081024,  0.1505839229,  0.2695238590, -0.0194988847,\n",
            "         0.1370308995, -0.3911962509,  0.0895336270, -0.0883390903,\n",
            "         0.3427343965, -0.1676390767,  0.0686662197, -0.3968208432,\n",
            "        -0.2340193838, -0.2208305597, -0.1277759820,  0.1788946986,\n",
            "         0.2137207389,  0.3884884119,  0.0531902313,  0.1236412525])\n",
            "ctensor.grad: tensor([ 5.4826754332e-01, -1.7486867905e+00, -1.3747975826e+00,\n",
            "        -3.9786539972e-02,  6.6148955375e-04, -2.8809331357e-02,\n",
            "         5.7195983827e-03,  5.4857246578e-03, -5.4153484106e-01,\n",
            "         8.2722995430e-03, -4.1056671739e-01,  6.1145763844e-02,\n",
            "         3.1632959843e-02, -7.3632615805e-01,  1.1192102917e-02,\n",
            "        -5.5546718836e-01,  8.3185054362e-02,  3.8636069745e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9858398438, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6551346779), tensor(0.9840752482), tensor(1.0802773237), tensor(1.1985378265), tensor(1.0904823542), tensor(1.4435878992), tensor(1.0596425533), tensor(1.1693447828), tensor(0.9123213291), tensor(1.2511010170), tensor(1.0892395973), tensor(1.5205069780), tensor(0.9864476919), tensor(1.2630828619), tensor(1.1815966368), tensor(0.9893519282), tensor(1.0738370419), tensor(0.8888521194), tensor(1.1028079987), tensor(1.0095813274)]\n",
            "b:  [tensor(0.5997686982), tensor(1.0968238115), tensor(1.3487609625), tensor(1.0128126144), tensor(1.4800627232), tensor(0.8102751374), tensor(1.2457262278), tensor(1.1710548401), tensor(1.8071461916), tensor(0.8523348570), tensor(1.2695962191), tensor(0.6982942820), tensor(1.3119015694), tensor(1.2427583933), tensor(0.8741471767), tensor(1.0081014633), tensor(1.3133817911), tensor(1.6863850355), tensor(0.9875550866), tensor(1.1891499758)]\n",
            "c:  [tensor(-0.0002269825), tensor(0.0053680609), tensor(0.0065754992), tensor(-0.0008742744), tensor(-0.0020020800), tensor(-0.0039087110), tensor(0.0004821218), tensor(8.4102633991e-05), tensor(0.0109680789), tensor(0.0019852405), tensor(0.0057348507), tensor(0.0068914588), tensor(0.0089466404), tensor(0.0043952689), tensor(0.0069788289), tensor(0.0100536421), tensor(0.0018432208), tensor(-0.0010680435)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.6414321661,  0.0446346998, -0.5524797440,  0.0661758184,\n",
            "        -0.2697290778,  0.6055836678, -0.0157670081, -0.0209152699,\n",
            "        -0.5606993437,  0.3427889943, -0.2254343629,  0.5828495026,\n",
            "         0.1716217399, -0.0970429182,  0.1362179518, -0.0245879889,\n",
            "        -0.5050697327, -0.5660712719,  0.1225908995, -0.0405011773])\n",
            "btensor.grad: tensor([-0.2519799471,  0.1155009866,  0.2108570337, -0.0188150406,\n",
            "         0.1084774211, -0.2871026993,  0.0693337619, -0.0774135590,\n",
            "         0.2892783880, -0.1268333793,  0.0452769995, -0.2736228108,\n",
            "        -0.1959214807, -0.1896016598, -0.1014881879,  0.1340959072,\n",
            "         0.1640759706,  0.3233529329,  0.0376279354,  0.0949704647])\n",
            "ctensor.grad: tensor([ 5.2871960402e-01, -1.8304836750e+00, -1.4779436588e+00,\n",
            "        -4.7789597884e-03,  7.9449273471e-05, -3.4647635184e-03,\n",
            "         6.8257120438e-04,  6.7015696550e-04, -6.7579919100e-01,\n",
            "         1.0305974633e-02, -5.1342272758e-01,  7.5251743197e-02,\n",
            "         3.9920352399e-02, -8.3816719055e-01,  1.2701979838e-02,\n",
            "        -6.3407248259e-01,  9.2949561775e-02,  4.3521948159e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9664306641, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6523928642), tensor(0.9839510918), tensor(1.0825415850), tensor(1.1983166933), tensor(1.0916093588), tensor(1.4410643578), tensor(1.0597504377), tensor(1.1694813967), tensor(0.9145355821), tensor(1.2497354746), tensor(1.0901851654), tensor(1.5180653334), tensor(0.9858379364), tensor(1.2635340691), tensor(1.1810817719), tensor(0.9894893169), tensor(1.0758987665), tensor(0.8910515904), tensor(1.1023563147), tensor(1.0097862482)]\n",
            "b:  [tensor(0.6005911827), tensor(1.0963944197), tensor(1.3479496241), tensor(1.0129131079), tensor(1.4796459675), tensor(0.8113406301), tensor(1.2454707623), tensor(1.1714037657), tensor(1.8059326410), tensor(0.8528282046), tensor(1.2694680691), tensor(0.6992515326), tensor(1.3127334118), tensor(1.2435841560), tensor(0.8745617270), tensor(1.0076124668), tensor(1.3127657175), tensor(1.6850477457), tensor(0.9874363542), tensor(1.1887983084)]\n",
            "c:  [tensor(-0.0004924968), tensor(0.0063253399), tensor(0.0073637250), tensor(-0.0008888036), tensor(-0.0020018385), tensor(-0.0039192638), tensor(0.0004841791), tensor(8.6151012511e-05), tensor(0.0113769500), tensor(0.0019790239), tensor(0.0060463618), tensor(0.0068467711), tensor(0.0089230603), tensor(0.0048697898), tensor(0.0069716685), tensor(0.0104138916), tensor(0.0017918089), tensor(-0.0010914771)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.5483577251,  0.0248332918, -0.4528605938,  0.0442361832,\n",
            "        -0.2254034877,  0.5047174692, -0.0215684772, -0.0273212194,\n",
            "        -0.4428555965,  0.2731123567, -0.1891153455,  0.4883369207,\n",
            "         0.1219455600, -0.0902421474,  0.1029730439, -0.0274778605,\n",
            "        -0.4123501778, -0.4398957491,  0.0903398395, -0.0409920812])\n",
            "btensor.grad: tensor([-0.1644966602,  0.0858705044,  0.1622599363, -0.0200934410,\n",
            "         0.0833401754, -0.2130999565,  0.0510841012, -0.0697914362,\n",
            "         0.2426982522, -0.0986727476,  0.0256305933, -0.1914558411,\n",
            "        -0.1663685143, -0.1651411057, -0.0829089358,  0.0977969766,\n",
            "         0.1232037395,  0.2674685717,  0.0237470865,  0.0703246593])\n",
            "ctensor.grad: tensor([ 5.3102856874e-01, -1.9145584106e+00, -1.5764518976e+00,\n",
            "         2.9058411717e-02, -4.8269989202e-04,  2.1105218679e-02,\n",
            "        -4.1145179421e-03, -4.0967622772e-03, -8.1774294376e-01,\n",
            "         1.2433157302e-02, -6.2302219868e-01,  8.9374952018e-02,\n",
            "         4.7159649432e-02, -9.4904148579e-01,  1.4320677146e-02,\n",
            "        -7.2049891949e-01,  1.0282377154e-01,  4.6867154539e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9525146484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6500531435), tensor(0.9839073420), tensor(1.0844095945), tensor(1.1981865168), tensor(1.0925620794), tensor(1.4389684200), tensor(1.0598841906), tensor(1.1696453094), tensor(0.9162995815), tensor(1.2486572266), tensor(1.0909880400), tensor(1.5160260201), tensor(0.9854223728), tensor(1.2639567852), tensor(1.1807035208), tensor(0.9896418452), tensor(1.0775939226), tensor(0.8927774429), tensor(1.1020373106), tensor(1.0099958181)]\n",
            "b:  [tensor(0.6011375785), tensor(1.0960865021), tensor(1.3473365307), tensor(1.0130225420), tensor(1.4793362617), tensor(0.8121406436), tensor(1.2452937365), tensor(1.1717232466), tensor(1.8049191236), tensor(0.8532223105), tensor(1.2694190741), tensor(0.6999325156), tensor(1.3134475946), tensor(1.2443108559), tensor(0.8749083281), tensor(1.0072669983), tensor(1.3123146296), tensor(1.6839473248), tensor(0.9873754382), tensor(1.1885488033)]\n",
            "c:  [tensor(-0.0007646017), tensor(0.0073304507), tensor(0.0082043232), tensor(-0.0009203641), tensor(-0.0020013147), tensor(-0.0039422270), tensor(0.0004886105), tensor(9.0604400611e-05), tensor(0.0118607748), tensor(0.0019716949), tensor(0.0064161192), tensor(0.0067949668), tensor(0.0088963928), tensor(0.0054041194), tensor(0.0069636456), tensor(0.0108210845), tensor(0.0017353592), tensor(-0.0011159166)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.4679369926,  0.0087521970, -0.3736093640,  0.0260329247,\n",
            "        -0.1905372143,  0.4191800356, -0.0267463923, -0.0327773094,\n",
            "        -0.3528056145,  0.2156602144, -0.1605769992,  0.4078617394,\n",
            "         0.0831183195, -0.0845466852,  0.0756602287, -0.0305031538,\n",
            "        -0.3390341997, -0.3451761007,  0.0638093352, -0.0419086218])\n",
            "btensor.grad: tensor([-0.1092758179,  0.0615916252,  0.1226149201, -0.0218809247,\n",
            "         0.0619450584, -0.1600003242,  0.0354157388, -0.0639020205,\n",
            "         0.2027062774, -0.0788248777,  0.0097904205, -0.1361954063,\n",
            "        -0.1428434402, -0.1453281641, -0.0693204179,  0.0691001415,\n",
            "         0.0902182609,  0.2200852633,  0.0121806860,  0.0499037504])\n",
            "ctensor.grad: tensor([ 5.4420971870e-01, -2.0102214813e+00, -1.6811956167e+00,\n",
            "         6.3120931387e-02, -1.0475722374e-03,  4.5926462859e-02,\n",
            "        -8.8627859950e-03, -8.9067723602e-03, -9.6765011549e-01,\n",
            "         1.4658017084e-02, -7.3951482773e-01,  1.0360831022e-01,\n",
            "         5.3335811943e-02, -1.0686588287e+00,  1.6045838594e-02,\n",
            "        -8.1438595057e-01,  1.1289943010e-01,  4.8879072070e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9403076172, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6480592489), tensor(0.9839266539), tensor(1.0859596729), tensor(1.1981296539), tensor(1.0933755636), tensor(1.4372328520), tensor(1.0600386858), tensor(1.1698302031), tensor(0.9177168012), tensor(1.2478132248), tensor(1.0916764736), tensor(1.5143272877), tensor(0.9851564169), tensor(1.2643532753), tensor(1.1804352999), tensor(0.9898068905), tensor(1.0789965391), tensor(0.8941450715), tensor(1.1018249989), tensor(1.0102087259)]\n",
            "b:  [tensor(0.6015094519), tensor(1.0958763361), tensor(1.3468837738), tensor(1.0131405592), tensor(1.4791160822), tensor(0.8127492070), tensor(1.2451821566), tensor(1.1720182896), tensor(1.8040760756), tensor(0.8535456657), tensor(1.2694325447), tensor(0.7004270554), tensor(1.3140666485), tensor(1.2449557781), tensor(0.8752042055), tensor(1.0070331097), tensor(1.3119950294), tensor(1.6830466986), tensor(0.9873610735), tensor(1.1883821487)]\n",
            "c:  [tensor(-0.0010461423), tensor(0.0083917752), tensor(0.0091033662), tensor(-0.0009694026), tensor(-0.0020005014), tensor(-0.0039779614), tensor(0.0004954475), tensor(9.7522839496e-05), tensor(0.0124240639), tensor(0.0019631970), tensor(0.0068479571), tensor(0.0067359214), tensor(0.0088671781), tensor(0.0060029342), tensor(0.0069547025), tensor(0.0112791620), tensor(0.0016737219), tensor(-0.0011407230)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.3987807035, -0.0038594306, -0.3100122213,  0.0113661289,\n",
            "        -0.1626850814,  0.3471040726, -0.0308910310, -0.0369833708,\n",
            "        -0.2834403515,  0.1687996984, -0.1376749873,  0.3397442102,\n",
            "         0.0531919003, -0.0792921782,  0.0536494255, -0.0330108404,\n",
            "        -0.2805294991, -0.2735227346,  0.0424549580, -0.0425879955])\n",
            "btensor.grad: tensor([-0.0743712187,  0.0420252085,  0.0905436277, -0.0236144662,\n",
            "         0.0440410823, -0.1217104197,  0.0223104656, -0.0590167046,\n",
            "         0.1686193347, -0.0646722317, -0.0026898384, -0.0989027694,\n",
            "        -0.1238161623, -0.1289955378, -0.0591756329,  0.0467665792,\n",
            "         0.0639096349,  0.1801230907,  0.0028680563,  0.0333213806])\n",
            "ctensor.grad: tensor([ 5.6308126450e-01, -2.1226480007e+00, -1.7980849743e+00,\n",
            "         9.8076947033e-02, -1.6265419545e-03,  7.1468785405e-02,\n",
            "        -1.3674032874e-02, -1.3836873695e-02, -1.1265777349e+00,\n",
            "         1.6995524988e-02, -8.6367565393e-01,  1.1809073389e-01,\n",
            "         5.8430105448e-02, -1.1976296902e+00,  1.7886221409e-02,\n",
            "        -9.1615450382e-01,  1.2327446789e-01,  4.9612704664e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9291992188, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6463619471), tensor(0.9839944243), tensor(1.0872530937), tensor(1.1981308460), tensor(1.0940767527), tensor(1.4357998371), tensor(1.0602085590), tensor(1.1700303555), tensor(0.9188655019), tensor(1.2471588850), tensor(1.0922715664), tensor(1.5129159689), tensor(0.9850047231), tensor(1.2647244930), tensor(1.1802546978), tensor(0.9899810553), tensor(1.0801643133), tensor(0.8952403069), tensor(1.1016974449), tensor(1.0104229450)]\n",
            "b:  [tensor(0.6017715931), tensor(1.0957444906), tensor(1.3465602398), tensor(1.0132660866), tensor(1.4789701700), tensor(0.8132196069), tensor(1.2451248169), tensor(1.1722922325), tensor(1.8033777475), tensor(0.8538185358), tensor(1.2694945335), tensor(0.7007959485), tensor(1.3146080971), tensor(1.2455329895), tensor(0.8754619360), tensor(1.0068854094), tensor(1.3117796183), tensor(1.6823142767), tensor(0.9873836637), tensor(1.1882822514)]\n",
            "c:  [tensor(-0.0013388376), tensor(0.0095193041), tensor(0.0100686718), tensor(-0.0010365301), tensor(-0.0019993884), tensor(-0.0040269350), tensor(0.0005047568), tensor(0.0001069973), tensor(0.0130720949), tensor(0.0019534638), tensor(0.0073463037), tensor(0.0066694384), tensor(0.0088359909), tensor(0.0066714850), tensor(0.0069447742), tensor(0.0117925154), tensor(0.0016067037), tensor(-0.0011652248)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.3946979046e-01, -1.3556659222e-02, -2.5868338346e-01,\n",
            "        -2.3913383484e-04, -1.4022995532e-01,  2.8660309315e-01,\n",
            "        -3.3982157707e-02, -4.0019869804e-02, -2.2973442078e-01,\n",
            "         1.3086038828e-01, -1.1902248859e-01,  2.8226944804e-01,\n",
            "         3.0343711376e-02, -7.4235200882e-02,  3.6131978035e-02,\n",
            "        -3.4827709198e-02, -2.3355805874e-01, -2.1904516220e-01,\n",
            "         2.5506436825e-02, -4.2844116688e-02])\n",
            "btensor.grad: tensor([-0.0524269342,  0.0263620615,  0.0647019744, -0.0251159072,\n",
            "         0.0291772038, -0.0940847397,  0.0114687085, -0.0547983646,\n",
            "         0.1396576166, -0.0545682907, -0.0124046803, -0.0737826005,\n",
            "        -0.1082958132, -0.1154426336, -0.0515471399,  0.0295503139,\n",
            "         0.0430806130,  0.1464734077, -0.0045167208,  0.0199900866])\n",
            "ctensor.grad: tensor([ 5.8539044857e-01, -2.2550580502e+00, -1.9306101799e+00,\n",
            "         1.3425526023e-01, -2.2256320808e-03,  9.7947306931e-02,\n",
            "        -1.8618565053e-02, -1.8948914483e-02, -1.2960612774e+00,\n",
            "         1.9466714934e-02, -9.9669259787e-01,  1.3296613097e-01,\n",
            "         6.2374569476e-02, -1.3371014595e+00,  1.9856650382e-02,\n",
            "        -1.0267072916e+00,  1.3403643668e-01,  4.9003586173e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9185791016, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6449185610), tensor(0.9840990305), tensor(1.0883386135), tensor(1.1981774569), tensor(1.0946869850), tensor(1.4346201420), tensor(1.0603893995), tensor(1.1702407598), tensor(0.9198055863), tensor(1.2466572523), tensor(1.0927898884), tensor(1.5117466450), tensor(0.9849395752), tensor(1.2650710344), tensor(1.1801431179), tensor(0.9901609421), tensor(1.0811427832), tensor(0.8961278200), tensor(1.1016365290), tensor(1.0106363297)]\n",
            "b:  [tensor(0.6019657850), tensor(1.0956752300), tensor(1.3463406563), tensor(1.0133979321), tensor(1.4788858891), tensor(0.8135907054), tensor(1.2451121807), tensor(1.1725476980), tensor(1.8028023243), tensor(0.8540555835), tensor(1.2695941925), tensor(0.7010810375), tensor(1.3150860071), tensor(1.2460540533), tensor(0.8756911159), tensor(1.0068035126), tensor(1.3116462231), tensor(1.6817235947), tensor(0.9874354005), tensor(1.1882356405)]\n",
            "c:  [tensor(-0.0016439559), tensor(0.0107242353), tensor(0.0111092981), tensor(-0.0011224488), tensor(-0.0019979642), tensor(-0.0040896689), tensor(0.0005166302), tensor(0.0001191460), tensor(0.0138110779), tensor(0.0019424155), tensor(0.0079163276), tensor(0.0065952521), tensor(0.0088034682), tensor(0.0074157682), tensor(0.0069337864), tensor(0.0123661458), tensor(0.0015340738), tensor(-0.0011886724)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.2886794806, -0.0209207535, -0.2170950174, -0.0093159676,\n",
            "        -0.1220381707,  0.2359453440, -0.0361674726, -0.0420860052,\n",
            "        -0.1880160570,  0.1003244519, -0.1036715508,  0.2338733077,\n",
            "         0.0130348206, -0.0692996979,  0.0223207474, -0.0359784365,\n",
            "        -0.1956853867, -0.1775064468,  0.0121893883, -0.0426844358])\n",
            "btensor.grad: tensor([-0.0388332605,  0.0138583183,  0.0439056158, -0.0263773799,\n",
            "         0.0168626010, -0.0742192268,  0.0025384724, -0.0510950089,\n",
            "         0.1150788665, -0.0474148393, -0.0199433565, -0.0570132434,\n",
            "        -0.0955903232, -0.1042085886, -0.0458322391,  0.0163807869,\n",
            "         0.0266714692,  0.1181246042, -0.0103431940,  0.0093291998])\n",
            "ctensor.grad: tensor([ 0.6102368236, -2.4098632336, -2.0812525749,  0.1718374044,\n",
            "        -0.0028485248,  0.1254680753, -0.0237470064, -0.0242974423,\n",
            "        -1.4779651165,  0.0220964868, -1.1400482655,  0.1483726799,\n",
            "         0.0650462061, -1.4885663986,  0.0219757278, -1.1472615004,\n",
            "         0.1452597082,  0.0468952134])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9074707031, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6436923742), tensor(0.9842314124), tensor(1.0892550945), tensor(1.1982592344), tensor(1.0952234268), tensor(1.4336521626), tensor(1.0605775118), tensor(1.1704578400), tensor(0.9205833673), tensor(1.2462778091), tensor(1.0932445526), tensor(1.5107808113), tensor(0.9849395752), tensor(1.2653934956), tensor(1.1800854206), tensor(0.9903436899), tensor(1.0819680691), tensor(0.8968567252), tensor(1.1016274691), tensor(1.0108472109)]\n",
            "b:  [tensor(0.6021190882), tensor(1.0956560373), tensor(1.3462048769), tensor(1.0135351419), tensor(1.4788526297), tensor(0.8138909936), tensor(1.2451362610), tensor(1.1727867126), tensor(1.8023310900), tensor(0.8542679548), tensor(1.2697232962), tensor(0.7013111711), tensor(1.3155119419), tensor(1.2465288639), tensor(0.8758991957), tensor(1.0067716837), tensor(1.3115773201), tensor(1.6812525988), tensor(0.9875102043), tensor(1.1882315874)]\n",
            "c:  [tensor(-0.0019626722), tensor(0.0120188724), tensor(0.0122353695), tensor(-0.0012279213), tensor(-0.0019962152), tensor(-0.0041667153), tensor(0.0005311809), tensor(0.0001341135), tensor(0.0146482904), tensor(0.0019299590), tensor(0.0085640578), tensor(0.0065130298), tensor(0.0087703355), tensor(0.0082426658), tensor(0.0069216541), tensor(0.0130057903), tensor(0.0014555687), tensor(-0.0012101987)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.4522745609e-01, -2.6475727558e-02, -1.8329668045e-01,\n",
            "        -1.6364455223e-02, -1.0727865994e-01,  1.9360327721e-01,\n",
            "        -3.7618011236e-02, -4.3420314789e-02, -1.5555942059e-01,\n",
            "         7.5890280306e-02, -9.0926229954e-02,  1.9316524267e-01,\n",
            "         5.9604644775e-06, -6.4485907555e-02,  1.1533975601e-02,\n",
            "        -3.6549806595e-02, -1.6504836082e-01, -1.4578449726e-01,\n",
            "         1.8172264099e-03, -4.2175292969e-02])\n",
            "btensor.grad: tensor([-0.0306664705,  0.0038459301,  0.0271549821, -0.0274426341,\n",
            "         0.0066554397, -0.0600519180, -0.0048274398, -0.0478147268,\n",
            "         0.0942392945, -0.0424737930, -0.0258132219, -0.0460242331,\n",
            "        -0.0851789415, -0.0949623585, -0.0416169688,  0.0063669086,\n",
            "         0.0137920678,  0.0942046642, -0.0149614811,  0.0008113384])\n",
            "ctensor.grad: tensor([ 0.6374323964, -2.5892751217, -2.2521429062,  0.2109449655,\n",
            "        -0.0034979635,  0.1540931016, -0.0291013662, -0.0299349073,\n",
            "        -1.6744241714,  0.0249128416, -1.2954601049,  0.1644448191,\n",
            "         0.0662663504, -1.6537950039,  0.0242648944, -1.2792882919,\n",
            "         0.1570103616,  0.0430527814])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8948974609, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6426519156), tensor(0.9843846560), tensor(1.0900338888), tensor(1.1983681917), tensor(1.0956999063), tensor(1.4328607321), tensor(1.0607699156), tensor(1.1706789732), tensor(0.9212347269), tensor(1.2459954023), tensor(1.0936456919), tensor(1.5099860430), tensor(0.9849880934), tensor(1.2656923532), tensor(1.1800693274), tensor(0.9905267954), tensor(1.0826690197), tensor(0.8974643946), tensor(1.1016583443), tensor(1.0110540390)]\n",
            "b:  [tensor(0.6022493839), tensor(1.0956771374), tensor(1.3461366892), tensor(1.0136770010), tensor(1.4788616896), tensor(0.8141413331), tensor(1.2451908588), tensor(1.1730110645), tensor(1.8019480705), tensor(0.8544640541), tensor(1.2698754072), tensor(0.7015064955), tensor(1.3158951998), tensor(1.2469661236), tensor(0.8760921359), tensor(1.0067776442), tensor(1.3115586042), tensor(1.6808825731), tensor(0.9876034856), tensor(1.1882615089)]\n",
            "c:  [tensor(-0.0022961514), tensor(0.0134166870), tensor(0.0134581113), tensor(-0.0013537637), tensor(-0.0019941269), tensor(-0.0042586518), tensor(0.0005485420), tensor(0.0001520721), tensor(0.0155921998), tensor(0.0019159856), tensor(0.0092964871), tensor(0.0064223669), tensor(0.0087374244), tensor(0.0091600670), tensor(0.0069082798), tensor(0.0137180267), tensor(0.0013708909), tensor(-0.0012287893)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.2080981731, -0.0306461155, -0.1557598114, -0.0218021870,\n",
            "        -0.0953007042,  0.1582759619, -0.0384866893, -0.0442159176,\n",
            "        -0.1302745342,  0.0564866811, -0.0802316070,  0.1589589417,\n",
            "        -0.0097073317, -0.0597805977,  0.0032078028, -0.0366256237,\n",
            "        -0.1401920319, -0.1215372086, -0.0061821938, -0.0413666964])\n",
            "btensor.grad: tensor([-0.0260622501, -0.0042110682,  0.0136465430, -0.0283613205,\n",
            "        -0.0018237233, -0.0500705242, -0.0109237731, -0.0448757410,\n",
            "         0.0766040683, -0.0392160416, -0.0304164886, -0.0390689820,\n",
            "        -0.0766490400, -0.0874474049, -0.0385936499, -0.0011831522,\n",
            "         0.0037451237,  0.0739969015, -0.0186505318, -0.0059818029])\n",
            "ctensor.grad: tensor([ 0.6669582725, -2.7956290245, -2.4454834461,  0.2516847253,\n",
            "        -0.0041764989,  0.1838725656, -0.0347221717, -0.0359172523,\n",
            "        -1.8878190517,  0.0279469043, -1.4648593664,  0.1813252866,\n",
            "         0.0658216327, -1.8348025084,  0.0267483220, -1.4244731665,\n",
            "         0.1693555564,  0.0371812806])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8801269531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6417697668), tensor(0.9845533967), tensor(1.0907001495), tensor(1.1984980106), tensor(1.0961278677), tensor(1.4322164059), tensor(1.0609643459), tensor(1.1709021330), tensor(0.9217874408), tensor(1.2457891703), tensor(1.0940014124), tensor(1.5093346834), tensor(0.9850723147), tensor(1.2659680843), tensor(1.1800848246), tensor(0.9907079935), tensor(1.0832686424), tensor(0.8979793191), tensor(1.1017196178), tensor(1.0112555027)]\n",
            "b:  [tensor(0.6023684144), tensor(1.0957307816), tensor(1.3461229801), tensor(1.0138229132), tensor(1.4789060354), tensor(0.8143572807), tensor(1.2452707291), tensor(1.1732220650), tensor(1.8016393185), tensor(0.8546502590), tensor(1.2700457573), tensor(0.7016812563), tensor(1.3162435293), tensor(1.2473733425), tensor(0.8762747645), tensor(1.0068116188), tensor(1.3115787506), tensor(1.6805980206), tensor(0.9877116084), tensor(1.1883183718)]\n",
            "c:  [tensor(-0.0026455936), tensor(0.0149324723), tensor(0.0147899678), tensor(-0.0015008418), tensor(-0.0019916836), tensor(-0.0043660766), tensor(0.0005688675), tensor(0.0001732252), tensor(0.0166525934), tensor(0.0019003690), tensor(0.0101216715), tensor(0.0063227792), tensor(0.0087056886), tensor(0.0101769995), tensor(0.0068935533), tensor(0.0145103838), tensor(0.0012797061), tensor(-0.0012432566)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1764341593, -0.0337509513, -0.1332464218, -0.0259608030,\n",
            "        -0.0855901390,  0.1288722754, -0.0388854742, -0.0446228981,\n",
            "        -0.1105464697,  0.0412458330, -0.0711371899,  0.1302633882,\n",
            "        -0.0168499947, -0.0551578999, -0.0030994415, -0.0362379551,\n",
            "        -0.1199357510, -0.1029815674, -0.0122604370, -0.0402892232])\n",
            "btensor.grad: tensor([-0.0238062143, -0.0107327700,  0.0027366877, -0.0291715860,\n",
            "        -0.0088707358, -0.0431859493, -0.0159808099, -0.0421955585,\n",
            "         0.0617552996, -0.0372397900, -0.0340778828, -0.0349565446,\n",
            "        -0.0696556568, -0.0814374685, -0.0365199298, -0.0067917705,\n",
            "        -0.0040220022,  0.0569128990, -0.0216234922, -0.0113828182])\n",
            "ctensor.grad: tensor([ 0.6988844275, -3.0315694809, -2.6637134552,  0.2941560745,\n",
            "        -0.0048866714,  0.2148495913, -0.0406509750, -0.0423062220,\n",
            "        -2.1207885742,  0.0312333144, -1.6503689289,  0.1991750896,\n",
            "         0.0634723231, -2.0338642597,  0.0294531677, -1.5847138166,\n",
            "         0.1823693663,  0.0289345980])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8631591797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6410222054), tensor(0.9847334027), tensor(1.0912737846), tensor(1.1986434460), tensor(1.0965163708), tensor(1.4316939116), tensor(1.0611586571), tensor(1.1711258888), tensor(0.9222629070), tensor(1.2456417084), tensor(1.0943175554), tensor(1.5088033676), tensor(0.9851820469), tensor(1.2662208080), tensor(1.1801233292), tensor(0.9908849597), tensor(1.0837852955), tensor(0.8984229565), tensor(1.1018033028), tensor(1.0114500523)]\n",
            "b:  [tensor(0.6024839282), tensor(1.0958110094), tensor(1.3461533785), tensor(1.0139722824), tensor(1.4789795876), tensor(0.8145501018), tensor(1.2453715801), tensor(1.1734203100), tensor(1.8013924360), tensor(0.8548315167), tensor(1.2702308893), tensor(0.7018455267), tensor(1.3165630102), tensor(1.2477569580), tensor(0.8764507174), tensor(1.0068657398), tensor(1.3116282225), tensor(1.6803854704), tensor(0.9878317714), tensor(1.1883964539)]\n",
            "c:  [tensor(-0.0030121882), tensor(0.0165825579), tensor(0.0162448101), tensor(-0.0016700694), tensor(-0.0019888680), tensor(-0.0044896053), tensor(0.0005923343), tensor(0.0001978121), tensor(0.0178407077), tensor(0.0018829635), tensor(0.0110488245), tensor(0.0062136804), tensor(0.0086761890), tensor(0.0113037648), tensor(0.0068773483), tensor(0.0153914448), tensor(0.0011816313), tensor(-0.0012522234)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1495145559, -0.0360058844, -0.1147361398, -0.0290913582,\n",
            "        -0.0777021497,  0.1045080423, -0.0388669670, -0.0447424650,\n",
            "        -0.0950988531,  0.0295034125, -0.0632369518,  0.1062594950,\n",
            "        -0.0219466090, -0.0505452156, -0.0076953173, -0.0353906155,\n",
            "        -0.1033208370, -0.0887290239, -0.0167385936, -0.0389155746])\n",
            "btensor.grad: tensor([-0.0231075287, -0.0160461664, -0.0060682297, -0.0298799872,\n",
            "        -0.0147024095, -0.0385620594, -0.0201613307, -0.0396598577,\n",
            "         0.0493810773, -0.0362503529, -0.0370271206, -0.0328522623,\n",
            "        -0.0638890862, -0.0767350197, -0.0351871401, -0.0108160973,\n",
            "        -0.0098994672,  0.0425050259, -0.0240277052, -0.0156228542])\n",
            "ctensor.grad: tensor([ 0.7331890464, -3.3001718521, -2.9096837044,  0.3384552002,\n",
            "        -0.0056311218,  0.2470576465, -0.0469336659, -0.0491738021,\n",
            "        -2.3762295246,  0.0348109603, -1.8543058634,  0.2181975693,\n",
            "         0.0589986891, -2.2535300255,  0.0324101970, -1.7621219158,\n",
            "         0.1961496025,  0.0179334413])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8430175781, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6403884888), tensor(0.9849210978), tensor(1.0917706490), tensor(1.1988002062), tensor(1.0968726873), tensor(1.4312715530), tensor(1.0613508224), tensor(1.1713489294), tensor(0.9226773977), tensor(1.2455378771), tensor(1.0945984125), tensor(1.5083719492), tensor(0.9853088856), tensor(1.2664499283), tensor(1.1801773310), tensor(0.9910550714), tensor(1.0842329264), tensor(0.8988113403), tensor(1.1019024849), tensor(1.0116360188)]\n",
            "b:  [tensor(0.6026012301), tensor(1.0959128141), tensor(1.3462190628), tensor(1.0141246319), tensor(1.4790769815), tensor(0.8147280216), tensor(1.2454894781), tensor(1.1736060381), tensor(1.8011960983), tensor(0.8550115824), tensor(1.2704279423), tensor(0.7020063996), tensor(1.3168582916), tensor(1.2481226921), tensor(0.8766227961), tensor(1.0069333315), tensor(1.3116990328), tensor(1.6802332401), tensor(0.9879615307), tensor(1.1884907484)]\n",
            "c:  [tensor(-0.0033970510), tensor(0.0183850452), tensor(0.0178381633), tensor(-0.0018624011), tensor(-0.0019856617), tensor(-0.0046298630), tensor(0.0006191448), tensor(0.0002261139), tensor(0.0191693678), tensor(0.0018636015), tensor(0.0120883975), tensor(0.0060943500), tensor(0.0086500766), tensor(0.0125520909), tensor(0.0068595209), tensor(0.0163709614), tensor(0.0010762142), tensor(-0.0012541184)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1267478466, -0.0375436842, -0.0993823409, -0.0313611031,\n",
            "        -0.0712598860,  0.0844633579, -0.0384302735, -0.0446183681,\n",
            "        -0.0828992128,  0.0207717642, -0.0561671853,  0.0862874389,\n",
            "        -0.0253632665, -0.0458273888, -0.0107930899, -0.0340249538,\n",
            "        -0.0895186663, -0.0776814222, -0.0198400617, -0.0371826291])\n",
            "btensor.grad: tensor([-0.0234558582, -0.0203680992, -0.0131250620, -0.0304729939,\n",
            "        -0.0194692165, -0.0355830193, -0.0235772729, -0.0371413231,\n",
            "         0.0392736793, -0.0360190868, -0.0394166708, -0.0321755558,\n",
            "        -0.0590659976, -0.0731556416, -0.0344192386, -0.0135135055,\n",
            "        -0.0141671300,  0.0304361582, -0.0259491205, -0.0188579559])\n",
            "ctensor.grad: tensor([ 0.7697256804, -3.6049747467, -3.1867046356,  0.3846635520,\n",
            "        -0.0064124884,  0.2805148661, -0.0536209717, -0.0566034690,\n",
            "        -2.6573212147,  0.0387239121, -2.0791466236,  0.2386610359,\n",
            "         0.0522241555, -2.4966518879,  0.0356545076, -1.9590318203,\n",
            "         0.2108342350,  0.0037900079])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8188476562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6398501396), tensor(0.9851131439), tensor(1.0922029018), tensor(1.1989644766), tensor(1.0972023010), tensor(1.4309306145), tensor(1.0615384579), tensor(1.1715701818), tensor(0.9230428934), tensor(1.2454643250), tensor(1.0948461294), tensor(1.5080227852), tensor(0.9854454994), tensor(1.2666541338), tensor(1.1802397966), tensor(0.9912151694), tensor(1.0846220255), tensor(0.8991560936), tensor(1.1020108461), tensor(1.0118107796)]\n",
            "b:  [tensor(0.6027237177), tensor(1.0960320234), tensor(1.3463125229), tensor(1.0142791271), tensor(1.4791932106), tensor(0.8148968220), tensor(1.2456208467), tensor(1.1737782955), tensor(1.8010394573), tensor(0.8551933169), tensor(1.2706346512), tensor(0.7021690011), tensor(1.3171328306), tensor(1.2484753132), tensor(0.8767930865), tensor(1.0070083141), tensor(1.3117839098), tensor(1.6801308393), tensor(0.9880986214), tensor(1.1885966063)]\n",
            "c:  [tensor(-0.0038011260), tensor(0.0203600731), tensor(0.0195874758), tensor(-0.0020788205), tensor(-0.0019820451), tensor(-0.0047874670), tensor(0.0006495301), tensor(0.0002584616), tensor(0.0206531230), tensor(0.0018420903), tensor(0.0132521437), tensor(0.0059638801), tensor(0.0086285258), tensor(0.0139352903), tensor(0.0068399077), tensor(0.0174599458), tensor(0.0009629001), tensor(-0.0012471952)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1076815128, -0.0384093821, -0.0864389539, -0.0328627825,\n",
            "        -0.0659188032,  0.0681805611, -0.0375207663, -0.0442483425,\n",
            "        -0.0731039047,  0.0147153139, -0.0495514274,  0.0698300302,\n",
            "        -0.0273191929, -0.0408527851, -0.0125000477, -0.0320230722,\n",
            "        -0.0778276920, -0.0689474344, -0.0216810107, -0.0349535942])\n",
            "btensor.grad: tensor([-0.0244972706, -0.0238415003, -0.0186901689, -0.0308972597,\n",
            "        -0.0232539773, -0.0337634087, -0.0262807012, -0.0344626904,\n",
            "         0.0313231945, -0.0363507867, -0.0413382053, -0.0325168818,\n",
            "        -0.0549128354, -0.0705201626, -0.0340526700, -0.0150074363,\n",
            "        -0.0169813037,  0.0204731226, -0.0274231434, -0.0211622715])\n",
            "ctensor.grad: tensor([ 0.8081498742, -3.9500551224, -3.4986245632,  0.4328386486,\n",
            "        -0.0072333119,  0.3152076602, -0.0607706122, -0.0646955371,\n",
            "        -2.9675097466,  0.0430225544, -2.3274922371,  0.2609397769,\n",
            "         0.0431021526, -2.7663986683,  0.0392265581, -2.1779682636,\n",
            "         0.2266282588, -0.0138464812])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7884521484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6393903494), tensor(0.9853060246), tensor(1.0925791264), tensor(1.1991325617), tensor(1.0975090265), tensor(1.4306544065), tensor(1.0617185831), tensor(1.1717880964), tensor(0.9233677983), tensor(1.2454086542), tensor(1.0950611830), tensor(1.5077403784), tensor(0.9855850339), tensor(1.2668311596), tensor(1.1803039312), tensor(0.9913611412), tensor(1.0849601030), tensor(0.8994649053), tensor(1.1021223068), tensor(1.0119709969)]\n",
            "b:  [tensor(0.6028537154), tensor(1.0961647034), tensor(1.3464270830), tensor(1.0144344568), tensor(1.4793236256), tensor(0.8150604367), tensor(1.2457622290), tensor(1.1739354134), tensor(1.8009119034), tensor(0.8553787470), tensor(1.2708487511), tensor(0.7023369074), tensor(1.3173885345), tensor(1.2488185167), tensor(0.8769627213), tensor(1.0070850849), tensor(1.3118759394), tensor(1.6800683737), tensor(0.9882407784), tensor(1.1887092590)]\n",
            "c:  [tensor(-0.0042250841), tensor(0.0225300789), tensor(0.0215124078), tensor(-0.0023203176), tensor(-0.0019779971), tensor(-0.0049630050), tensor(0.0006837543), tensor(0.0002952460), tensor(0.0223083626), tensor(0.0018182078), tensor(0.0145531269), tensor(0.0058210972), tensor(0.0086126197), tensor(0.0154684233), tensor(0.0068183211), tensor(0.0186707694), tensor(0.0008409800), tensor(-0.0012295750)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0919607878, -0.0385729969, -0.0752431154, -0.0336226225,\n",
            "        -0.0613531470,  0.0552519560, -0.0360175073, -0.0435714722,\n",
            "        -0.0649771690,  0.0111448616, -0.0430215597,  0.0564895570,\n",
            "        -0.0279014707, -0.0353978872, -0.0128333569, -0.0291976929,\n",
            "        -0.0676137209, -0.0617671013, -0.0222927928, -0.0320527554])\n",
            "btensor.grad: tensor([-0.0260040760, -0.0265471935, -0.0229206681, -0.0310699940,\n",
            "        -0.0260724276, -0.0327175856, -0.0282660425, -0.0314277411,\n",
            "         0.0255209804, -0.0370818377, -0.0428187847, -0.0335812569,\n",
            "        -0.0511494577, -0.0686503649, -0.0339242667, -0.0153443813,\n",
            "        -0.0184002668,  0.0124861002, -0.0284324884, -0.0225391388])\n",
            "ctensor.grad: tensor([ 0.8479157686, -4.3400135040, -3.8498649597,  0.4829942286,\n",
            "        -0.0080958297,  0.3510759473, -0.0684483349, -0.0735687464,\n",
            "        -3.3104791641,  0.0477650650, -2.6019668579,  0.2855658829,\n",
            "         0.0318118781, -3.0662651062,  0.0431733355, -2.4216482639,\n",
            "         0.2438402027, -0.0352402963])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7526855469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6389936209), tensor(0.9854956269), tensor(1.0929050446), tensor(1.1993005276), tensor(1.0977952480), tensor(1.4304274321), tensor(1.0618872643), tensor(1.1720005274), tensor(0.9236571193), tensor(1.2453585863), tensor(1.0952420235), tensor(1.5075103045), tensor(0.9857203960), tensor(1.2669770718), tensor(1.1803624630), tensor(0.9914875627), tensor(1.0852514505), tensor(0.8997423053), tensor(1.1022303104), tensor(1.0121121407)]\n",
            "b:  [tensor(0.6029927135), tensor(1.0963071585), tensor(1.3465564251), tensor(1.0145888329), tensor(1.4794628620), tensor(0.8152210116), tensor(1.2459095716), tensor(1.1740742922), tensor(1.8008021116), tensor(0.8555690646), tensor(1.2710678577), tensor(0.7025126219), tensor(1.3176259995), tensor(1.2491554022), tensor(0.8771320581), tensor(1.0071573257), tensor(1.3119678497), tensor(1.6800361872), tensor(0.9883853793), tensor(1.1888238192)]\n",
            "c:  [tensor(-0.0046691610), tensor(0.0249200556), tensor(0.0236351322), tensor(-0.0025878588), tensor(-0.0019734963), tensor(-0.0051570018), tensor(0.0007221194), tensor(0.0003369295), tensor(0.0241533965), tensor(0.0017916983), tensor(0.0160056669), tensor(0.0056644417), tensor(0.0086031575), tensor(0.0171684492), tensor(0.0067945463), tensor(0.0200171880), tensor(0.0007095090), tensor(-0.0011993536)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0793430805, -0.0379225612, -0.0651893020, -0.0335910320,\n",
            "        -0.0572439879,  0.0454043150, -0.0337394476, -0.0424822569,\n",
            "        -0.0578665733,  0.0100160018, -0.0361717343,  0.0460054874,\n",
            "        -0.0270754099, -0.0291928053, -0.0116995573, -0.0252883434,\n",
            "        -0.0582690239, -0.0554759502, -0.0216049552, -0.0282220840])\n",
            "btensor.grad: tensor([-0.0278052092, -0.0284875631, -0.0258738399, -0.0308708549,\n",
            "        -0.0278588980, -0.0321170092, -0.0294650197, -0.0277674198,\n",
            "         0.0219682455, -0.0380623937, -0.0438327789, -0.0351466238,\n",
            "        -0.0474826097, -0.0673730373, -0.0338726044, -0.0144506693,\n",
            "        -0.0183735788,  0.0064429045, -0.0289144516, -0.0229232311])\n",
            "ctensor.grad: tensor([ 0.8881536126, -4.7799525261, -4.2454471588,  0.5350823402,\n",
            "        -0.0090017822,  0.3879938722, -0.0767301992, -0.0833668709,\n",
            "        -3.6900684834,  0.0530190505, -2.9050800800,  0.3133109510,\n",
            "         0.0189246666, -3.4000518322,  0.0475499481, -2.6928360462,\n",
            "         0.2629420757, -0.0604427196])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7082519531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6386451721), tensor(0.9856770039), tensor(1.0931835175), tensor(1.1994638443), tensor(1.0980615616), tensor(1.4302350283), tensor(1.0620394945), tensor(1.1722046137), tensor(0.9239129424), tensor(1.2453014851), tensor(1.0953847170), tensor(1.5073192120), tensor(0.9858438969), tensor(1.2670865059), tensor(1.1804070473), tensor(0.9915873408), tensor(1.0854974985), tensor(0.8999895453), tensor(1.1023275852), tensor(1.0122277737)]\n",
            "b:  [tensor(0.6031415462), tensor(1.0964552164), tensor(1.3466939926), tensor(1.0147395134), tensor(1.4796053171), tensor(0.8153793812), tensor(1.2460583448), tensor(1.1741901636), tensor(1.8006978035), tensor(0.8557648063), tensor(1.2712893486), tensor(0.7026978731), tensor(1.3178439140), tensor(1.2494878769), tensor(0.8773006797), tensor(1.0072182417), tensor(1.3120516539), tensor(1.6800241470), tensor(0.9885290861), tensor(1.1889346838)]\n",
            "c:  [tensor(-0.0051329983), tensor(0.0275577288), tensor(0.0259806067), tensor(-0.0028823419), tensor(-0.0019685202), tensor(-0.0053698737), tensor(0.0007649714), tensor(0.0003840606), tensor(0.0262084641), tensor(0.0017622665), tensor(0.0176251438), tensor(0.0054917959), tensor(0.0086003412), tensor(0.0190543476), tensor(0.0067683356), tensor(0.0215143189), tensor(0.0005671850), tensor(-0.0011547764)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0696942806, -0.0362806320, -0.0556845665, -0.0326528549,\n",
            "        -0.0532539636,  0.0384918451, -0.0304371417, -0.0408192873,\n",
            "        -0.0511629581,  0.0114151537, -0.0285467505,  0.0382125080,\n",
            "        -0.0247024298, -0.0218819380, -0.0089175701, -0.0199544430,\n",
            "        -0.0492069721, -0.0494509935, -0.0194547176, -0.0231351256])\n",
            "btensor.grad: tensor([-0.0297679901, -0.0296074152, -0.0275199413, -0.0301474333,\n",
            "        -0.0284979939, -0.0316796303, -0.0297556520, -0.0231670141,\n",
            "         0.0208666921, -0.0391498804, -0.0442925692, -0.0370465964,\n",
            "        -0.0435885191, -0.0664916039, -0.0337248445, -0.0121927857,\n",
            "        -0.0167599171,  0.0024100542, -0.0287437439, -0.0221685171])\n",
            "ctensor.grad: tensor([ 0.9276749492, -5.2753467560, -4.6909499168,  0.5889659524,\n",
            "        -0.0099521335,  0.4257441461, -0.0857038945, -0.0942623615,\n",
            "        -4.1101365089,  0.0588635989, -3.2389528751,  0.3452912569,\n",
            "         0.0056329295, -3.7717967033,  0.0524213687, -2.9942629337,\n",
            "         0.2846478820, -0.0891544074])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.6544189453, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6383302212), tensor(0.9858438373), tensor(1.0934140682), tensor(1.1996169090), tensor(1.0983066559), tensor(1.4300625324), tensor(1.0621682405), tensor(1.1723964214), tensor(0.9241341949), tensor(1.2452236414), tensor(1.0954828262), tensor(1.5071538687), tensor(0.9859464765), tensor(1.2671515942), tensor(1.1804280281), tensor(0.9916511178), tensor(1.0856965780), tensor(0.9002048969), tensor(1.1024054289), tensor(1.0123096704)]\n",
            "b:  [tensor(0.6033004522), tensor(1.0966041088), tensor(1.3468326330), tensor(1.0148829222), tensor(1.4797441959), tensor(0.8155350089), tensor(1.2462030649), tensor(1.1742763519), tensor(1.8005850315), tensor(0.8559657335), tensor(1.2715095282), tensor(0.7028934360), tensor(1.3180394173), tensor(1.2498168945), tensor(0.8774670362), tensor(1.0072597265), tensor(1.3121181726), tensor(1.6800212860), tensor(0.9886679053), tensor(1.1890349388)]\n",
            "c:  [tensor(-0.0056154151), tensor(0.0304736774), tensor(0.0285768267), tensor(-0.0032045376), tensor(-0.0019630468), tensor(-0.0056018680), tensor(0.0008127072), tensor(0.0004372929), tensor(0.0284955930), tensor(0.0017295708), tensor(0.0194276143), tensor(0.0053002327), tensor(0.0086032832), tensor(0.0211471636), tensor(0.0067394031), tensor(0.0231784768), tensor(0.0004121683), tensor(-0.0010945490)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0629903078, -0.0333714783, -0.0461210608, -0.0306181908,\n",
            "        -0.0490235239,  0.0344965458, -0.0257594585, -0.0383538008,\n",
            "        -0.0442544222,  0.0155671313, -0.0196317434,  0.0330774486,\n",
            "        -0.0205121040, -0.0130230188, -0.0042071342, -0.0127512217,\n",
            "        -0.0398153067, -0.0430685282, -0.0155747533, -0.0163818598])\n",
            "btensor.grad: tensor([-0.0317827463, -0.0297757387, -0.0277369022, -0.0286870599,\n",
            "        -0.0277654678, -0.0311262608, -0.0289431810, -0.0172394514,\n",
            "         0.0225484371, -0.0401895642, -0.0440428257, -0.0391185135,\n",
            "        -0.0391053557, -0.0657954216, -0.0332740396, -0.0083057880,\n",
            "        -0.0133087337,  0.0005648136, -0.0277584791, -0.0200508833])\n",
            "ctensor.grad: tensor([ 0.9648335576, -5.8318972588, -5.1924395561,  0.6443913579,\n",
            "        -0.0109467153,  0.4639890492, -0.0954717174, -0.1064645499,\n",
            "        -4.5742588043,  0.0653914884, -3.6049418449,  0.3831261098,\n",
            "        -0.0058848448, -4.1856303215,  0.0578645915, -3.3283174038,\n",
            "         0.3100334406, -0.1204549223])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.5870361328, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6380336285), tensor(0.9859879613), tensor(1.0935934782), tensor(1.1997530460), tensor(1.0985274315), tensor(1.4298948050), tensor(1.0622646809), tensor(1.1725703478), tensor(0.9243168235), tensor(1.2451094389), tensor(1.0955269337), tensor(1.5070004463), tensor(0.9860171080), tensor(1.2671619654), tensor(1.1804138422), tensor(0.9916667938), tensor(1.0858438015), tensor(0.9003833532), tensor(1.1024534702), tensor(1.0123469830)]\n",
            "b:  [tensor(0.6034691930), tensor(1.0967481136), tensor(1.3469641209), tensor(1.0150140524), tensor(1.4798710346), tensor(0.8156859279), tensor(1.2463368177), tensor(1.1743239164), tensor(1.8004475832), tensor(0.8561708331), tensor(1.2717239857), tensor(0.7030996680), tensor(1.3182075024), tensor(1.2501420975), tensor(0.8776285648), tensor(1.0072720051), tensor(1.3121564388), tensor(1.6800153255), tensor(0.9887965322), tensor(1.1891162395)]\n",
            "c:  [tensor(-0.0061141565), tensor(0.0337012485), tensor(0.0314549506), tensor(-0.0035550147), tensor(-0.0019570549), tensor(-0.0058529875), tensor(0.0008657828), tensor(0.0004974039), tensor(0.0310383029), tensor(0.0016932149), tensor(0.0214291476), tensor(0.0050856671), tensor(0.0086092921), tensor(0.0234699268), tensor(0.0067074173), tensor(0.0250268169), tensor(0.0002418205), tensor(-0.0010183145)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0593154430, -0.0288293958, -0.0358803272, -0.0272194147,\n",
            "        -0.0441586375,  0.0335352421, -0.0192836225, -0.0347867012,\n",
            "        -0.0365202427,  0.0228473991, -0.0088283420,  0.0306764543,\n",
            "        -0.0141218305, -0.0020827055,  0.0028306246, -0.0031337738,\n",
            "        -0.0294435024, -0.0356925726, -0.0095989704, -0.0074613094])\n",
            "btensor.grad: tensor([-0.0337508917, -0.0288025141, -0.0263078809, -0.0262264609,\n",
            "        -0.0253680944, -0.0301790237, -0.0267585814, -0.0095083714,\n",
            "         0.0274822712, -0.0410149693, -0.0428799391, -0.0412469953,\n",
            "        -0.0336130857, -0.0650460720, -0.0323011279, -0.0024590492,\n",
            "        -0.0076607615,  0.0011882782, -0.0257238150, -0.0162668228])\n",
            "ctensor.grad: tensor([ 0.9974830747, -6.4551386833, -5.7562484741,  0.7009543777,\n",
            "        -0.0119838258,  0.5022390485, -0.1061511934, -0.1202219576,\n",
            "        -5.0854182243,  0.0727117434, -4.0030670166,  0.4291313589,\n",
            "        -0.0120175518, -4.6455254555,  0.0639712214, -3.6966793537,\n",
            "         0.3406955898, -0.1524688900])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.5063476562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6377393007), tensor(0.9860989451), tensor(1.0937148333), tensor(1.1998634338), tensor(1.0987185240), tensor(1.4297155142), tensor(1.0623170137), tensor(1.1727191210), tensor(0.9244531989), tensor(1.2449406385), tensor(1.0955041647), tensor(1.5068445206), tensor(0.9860422611), tensor(1.2671040297), tensor(1.1803503036), tensor(0.9916188717), tensor(1.0859307051), tensor(0.9005165100), tensor(1.1024587154), tensor(1.0123258829)]\n",
            "b:  [tensor(0.6036470532), tensor(1.0968801975), tensor(1.3470788002), tensor(1.0151263475), tensor(1.4799755812), tensor(0.8158286810), tensor(1.2464511395), tensor(1.1743209362), tensor(1.8002661467), tensor(0.8563780189), tensor(1.2719265223), tensor(0.7033160925), tensor(1.3183406591), tensor(1.2504619360), tensor(0.8777812123), tensor(1.0072430372), tensor(1.3121531010), tensor(1.6799918413), tensor(0.9889083505), tensor(1.1891683340)]\n",
            "c:  [tensor(-0.0066256030), tensor(0.0372762159), tensor(0.0346492603), tensor(-0.0039340430), tensor(-0.0019505251), tensor(-0.0061228964), tensor(0.0009247212), tensor(0.0005653183), tensor(0.0338609703), tensor(0.0016527389), tensor(0.0236447677), tensor(0.0048423754), tensor(0.0086128311), tensor(0.0260473657), tensor(0.0066719921), tensor(0.0270766709), tensor(5.2330506151e-05), tensor(-0.0009273979)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0588649511, -0.0222002566, -0.0242730379, -0.0220887661,\n",
            "        -0.0382165611,  0.0358507633, -0.0104554296, -0.0297571421,\n",
            "        -0.0272800922,  0.0337642580,  0.0045547485,  0.0311964154,\n",
            "        -0.0050270557,  0.0115957260,  0.0127077103,  0.0095835924,\n",
            "        -0.0173795223, -0.0266257524, -0.0010447502,  0.0042292476])\n",
            "btensor.grad: tensor([-0.0355706215, -0.0264261961, -0.0229295492, -0.0224502683,\n",
            "        -0.0209186822, -0.0285474062, -0.0228545964,  0.0005990267,\n",
            "         0.0362768769, -0.0414332747, -0.0405170918, -0.0432871580,\n",
            "        -0.0266262591, -0.0639747381, -0.0305343568,  0.0058025718,\n",
            "         0.0006629080,  0.0046871901, -0.0223608017, -0.0104153156])\n",
            "ctensor.grad: tensor([ 1.0228925943e+00, -7.1499304771e+00, -6.3886198997e+00,\n",
            "         7.5805616379e-01, -1.3059720397e-02,  5.3981810808e-01,\n",
            "        -1.1787662655e-01, -1.3582886755e-01, -5.6453375816e+00,\n",
            "         8.0951996148e-02, -4.4312396049e+00,  4.8658385873e-01,\n",
            "        -7.0783495903e-03, -5.1548771858e+00,  7.0850342512e-02,\n",
            "        -4.0997085571e+00,  3.7897989154e-01, -1.8183329701e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.4066162109, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6374295950), tensor(0.9861635566), tensor(1.0937676430), tensor(1.1999374628), tensor(1.0988720655), tensor(1.4295064211), tensor(1.0623102188), tensor(1.1728332043), tensor(0.9245322943), tensor(1.2446957827), tensor(1.0953977108), tensor(1.5066697598), tensor(0.9860053062), tensor(1.2669601440), tensor(1.1802201271), tensor(0.9914879799), tensor(1.0859448910), tensor(0.9005920291), tensor(1.1024053097), tensor(1.0122290850)]\n",
            "b:  [tensor(0.6038327813), tensor(1.0969917774), tensor(1.3471647501), tensor(1.0152112246), tensor(1.4800451994), tensor(0.8159582615), tensor(1.2465351820), tensor(1.1742522717), tensor(1.8000177145), tensor(0.8565842509), tensor(1.2721096277), tensor(0.7035416961), tensor(1.3184286356), tensor(1.2507733107), tensor(0.8779196739), tensor(1.0071579218), tensor(1.3120918274), tensor(1.6799339056), tensor(0.9889950156), tensor(1.1891784668)]\n",
            "c:  [tensor(-0.0071444507), tensor(0.0412360393), tensor(0.0381968096), tensor(-0.0043414813), tensor(-0.0019434411), tensor(-0.0064108139), tensor(0.0009901186), tensor(0.0006421273), tensor(0.0369878709), tensor(0.0016076087), tensor(0.0260869097), tensor(0.0045623709), tensor(0.0086041111), tensor(0.0289053358), tensor(0.0066326763), tensor(0.0293444842), tensor(-0.0001617899), tensor(-0.0008258614)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0619438887, -0.0129274726, -0.0105727911, -0.0148003101,\n",
            "        -0.0307017267,  0.0418179035,  0.0013593435, -0.0228171349,\n",
            "        -0.0158244371,  0.0489647239,  0.0212978125,  0.0349446535,\n",
            "         0.0073905587,  0.0287756920,  0.0260436535,  0.0261803865,\n",
            "        -0.0028419495, -0.0151077509,  0.0106796622,  0.0193648934])\n",
            "btensor.grad: tensor([-0.0371409655, -0.0223244429, -0.0171968937, -0.0169792771,\n",
            "        -0.0139345080, -0.0259207487, -0.0168021023,  0.0137366056,\n",
            "         0.0496822000, -0.0412510633, -0.0366218090, -0.0451198816,\n",
            "        -0.0175939500, -0.0622748137, -0.0276899785,  0.0170122385,\n",
            "         0.0122457743,  0.0115790367, -0.0173311234, -0.0020310879])\n",
            "ctensor.grad: tensor([ 1.0376957655, -7.9196486473, -7.0950951576,  0.8148766160,\n",
            "        -0.0141680064,  0.5758343935, -0.1307948679, -0.1536179483,\n",
            "        -6.2537970543,  0.0902603418, -4.8842844963,  0.5600086451,\n",
            "         0.0174401850, -5.7159409523,  0.0786312670, -4.5356254578,\n",
            "         0.4282408655, -0.2030730397])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.2847900391, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6370848417), tensor(0.9861655831), tensor(1.0937377214), tensor(1.1999616623), tensor(1.0989775658), tensor(1.4292469025), tensor(1.0622256994), tensor(1.1729005575), tensor(0.9245393276), tensor(1.2443498373), tensor(1.0951864719), tensor(1.5064581633), tensor(0.9858861566), tensor(1.2667086124), tensor(1.1800024509), tensor(0.9912503362), tensor(1.0858699083), tensor(0.9005936980), tensor(1.1022741795), tensor(1.0120357275)]\n",
            "b:  [tensor(0.6040247083), tensor(1.0970724821), tensor(1.3472080231), tensor(1.0152583122), tensor(1.4800645113), tensor(0.8160681725), tensor(1.2465758324), tensor(1.1740990877), tensor(1.7996748686), tensor(0.8567855954), tensor(1.2722636461), tensor(0.7037748098), tensor(1.3184581995), tensor(1.2510714531), tensor(0.8780369163), tensor(1.0069991350), tensor(1.3119530678), tensor(1.6798214912), tensor(0.9890463948), tensor(1.1891313791)]\n",
            "c:  [tensor(-0.0076634684), tensor(0.0456186198), tensor(0.0421366282), tensor(-0.0047766482), tensor(-0.0019357916), tensor(-0.0067153946), tensor(0.0010626467), tensor(0.0007290998), tensor(0.0404417180), tensor(0.0015572059), tensor(0.0287633389), tensor(0.0042346437), tensor(0.0085673099), tensor(0.0320698395), tensor(0.0065889433), tensor(0.0318442434), tensor(-0.0004083705), tensor(-0.0007219713)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0689407587, -0.0004030764,  0.0059874654, -0.0048439503,\n",
            "        -0.0210924000,  0.0519053936,  0.0168996751, -0.0134691000,\n",
            "        -0.0014027357,  0.0691851526,  0.0422466993,  0.0423095822,\n",
            "         0.0238329768,  0.0503103733,  0.0435304642,  0.0475337505,\n",
            "         0.0150060654, -0.0003297329,  0.0262328386,  0.0386636257])\n",
            "btensor.grad: tensor([-0.0383870602, -0.0161460638, -0.0086528659, -0.0094247460,\n",
            "        -0.0038624257, -0.0219800472, -0.0081309378,  0.0306383371,\n",
            "         0.0685591698, -0.0402688980, -0.0308033228, -0.0466245115,\n",
            "        -0.0059241354, -0.0596207380, -0.0234513134,  0.0317565799,\n",
            "         0.0277440101,  0.0224756002, -0.0102770329,  0.0094155073])\n",
            "ctensor.grad: tensor([ 1.0380352736, -8.7651586533, -7.8796367645,  0.8703342676,\n",
            "        -0.0152989598,  0.6091616750, -0.1450563818, -0.1739450395,\n",
            "        -6.9076957703,  0.1008055732, -5.3528594971,  0.6554548144,\n",
            "         0.0736031830, -6.3290052414,  0.0874658376, -4.9995160103,\n",
            "         0.4931611121, -0.2077800035])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.1354980469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6366833448), tensor(0.9860855341), tensor(1.0936068296), tensor(1.1999201775), tensor(1.0990220308), tensor(1.4289135933), tensor(1.0620409250), tensor(1.1729065180), tensor(0.9244557023), tensor(1.2438738346), tensor(1.0948451757), tensor(1.5061894655), tensor(0.9856609702), tensor(1.2663232088), tensor(1.1796730757), tensor(0.9908775687), tensor(1.0856847763), tensor(0.9005009532), tensor(1.1020425558), tensor(1.0117216110)]\n",
            "b:  [tensor(0.6042210460), tensor(1.0971101522), tensor(1.3471920490), tensor(1.0152552128), tensor(1.4800151587), tensor(0.8161503077), tensor(1.2465575933), tensor(1.1738388538), tensor(1.7992056608), tensor(0.8569771647), tensor(1.2723770142), tensor(0.7040134072), tensor(1.3184132576), tensor(1.2513499260), tensor(0.8781245351), tensor(1.0067458153), tensor(1.3117138147), tensor(1.6796313524), tensor(0.9890506864), tensor(1.1890091896)]\n",
            "c:  [tensor(-0.0081732981), tensor(0.0504604429), tensor(0.0465083644), tensor(-0.0052381908), tensor(-0.0019275722), tensor(-0.0070346221), tensor(0.0011430448), tensor(0.0008276766), tensor(0.0442418009), tensor(0.0015008188), tensor(0.0316745751), tensor(0.0038442989), tensor(0.0084784739), tensor(0.0355655588), tensor(0.0065401788), tensor(0.0345852859), tensor(-0.0006983916), tensor(-0.0006300722)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0803016424,  0.0160105824,  0.0261893868,  0.0083053112,\n",
            "        -0.0088889003,  0.0666638613,  0.0369435549, -0.0011897087,\n",
            "         0.0167295933,  0.0952090696,  0.0682564378,  0.0537480116,\n",
            "         0.0450397134,  0.0770924091,  0.0658856630,  0.0745511055,\n",
            "         0.0370222330,  0.0185514688,  0.0463138819,  0.0628209710])\n",
            "btensor.grad: tensor([-0.0392713547, -0.0075370073,  0.0032066703,  0.0006103516,\n",
            "         0.0098795146, -0.0164291859,  0.0036568642,  0.0520534515,\n",
            "         0.0938315988, -0.0383116603, -0.0226789713, -0.0477177799,\n",
            "         0.0089828372, -0.0556901693, -0.0175231248,  0.0506560206,\n",
            "         0.0478509665,  0.0380324125, -0.0008600950,  0.0244420767])\n",
            "ctensor.grad: tensor([ 1.0196595192, -9.6836481094, -8.7434759140,  0.9230846167,\n",
            "        -0.0164388698,  0.6384549141, -0.1607961804, -0.1971535534,\n",
            "        -7.6001634598,  0.1127743423, -5.8224725723,  0.7806894183,\n",
            "         0.1776725054, -6.9914383888,  0.0975290686, -5.4820876122,\n",
            "         0.5800421238, -0.1837982684])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.9547119141, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6362010241), tensor(0.9859012365), tensor(1.0933532715), tensor(1.1997945309), tensor(1.0989903212), tensor(1.4284805059), tensor(1.0617297888), tensor(1.1728340387), tensor(0.9242597222), tensor(1.2432352304), tensor(1.0943447351), tensor(1.5058408976), tensor(0.9853026867), tensor(1.2657734156), tensor(1.1792043447), tensor(0.9903373718), tensor(1.0853648186), tensor(0.9002894759), tensor(1.1016849279), tensor(1.0112596750)]\n",
            "b:  [tensor(0.6044202447), tensor(1.0970914364), tensor(1.3470978737), tensor(1.0151882172), tensor(1.4798760414), tensor(0.8161955476), tensor(1.2464625835), tensor(1.1734455824), tensor(1.7985738516), tensor(0.8571537137), tensor(1.2724366188), tensor(0.7042554021), tensor(1.3182749748), tensor(1.2516011000), tensor(0.8781729937), tensor(1.0063747168), tensor(1.3113479614), tensor(1.6793370247), tensor(0.9889948368), tensor(1.1887918711)]\n",
            "c:  [tensor(-0.0086625367), tensor(0.0557941906), tensor(0.0513502508), tensor(-0.0057239663), tensor(-0.0019187875), tensor(-0.0073657301), tensor(0.0012320942), tensor(0.0009394264), tensor(0.0484017842), tensor(0.0014376375), tensor(0.0348111168), tensor(0.0033717514), tensor(0.0083034346), tensor(0.0394138992), tensor(0.0064856708), tensor(0.0375696681), tensor(-0.0010468703), tensor(-0.0005727539)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.0964561701, 0.0368554890, 0.0507053733, 0.0251226425, 0.0063325614,\n",
            "        0.0866268873, 0.0622230470, 0.0144886971, 0.0391937494, 0.1277323216,\n",
            "        0.1000806689, 0.0697143376, 0.0716543198, 0.1099700928, 0.0937503576,\n",
            "        0.1080400944, 0.0639895201, 0.0422980785, 0.0715355873, 0.0923920274])\n",
            "btensor.grad: tensor([-0.0398448706,  0.0037434101,  0.0188233852,  0.0134109259,\n",
            "         0.0278196931, -0.0090497732,  0.0189998150,  0.0786505938,\n",
            "         0.1263666749, -0.0353049636, -0.0119316578, -0.0483944267,\n",
            "         0.0276518464, -0.0502244234, -0.0096956789,  0.0742151141,\n",
            "         0.0731771141,  0.0588641167,  0.0111669302,  0.0434751511])\n",
            "ctensor.grad: tensor([  0.9784778357, -10.6674947739,  -9.6837692261,   0.9715511799,\n",
            "         -0.0175693911,   0.6622161865,  -0.1780989468,  -0.2234996259,\n",
            "         -8.3199672699,   0.1263626069,  -6.2730855942,   0.9450950623,\n",
            "          0.3500778079,  -7.6966767311,   0.1090161353,  -5.9687614441,\n",
            "          0.6969572306,  -0.1146366149])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.7353515625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6356124878), tensor(0.9855889678), tensor(1.0929533243), tensor(1.1995649338), tensor(1.0988664627), tensor(1.4279195070), tensor(1.0612633228), tensor(1.1726646423), tensor(0.9239280224), tensor(1.2423992157), tensor(1.0936536789), tensor(1.5053882599), tensor(0.9847822189), tensor(1.2650254965), tensor(1.1785666943), tensor(0.9895946383), tensor(1.0848824978), tensor(0.8999320865), tensor(1.1011734009), tensor(1.0106215477)]\n",
            "b:  [tensor(0.6046218872), tensor(1.0970028639), tensor(1.3469054699), tensor(1.0150429010), tensor(1.4796246290), tensor(0.8161945939), tensor(1.2462718487), tensor(1.1728912592), tensor(1.7977398634), tensor(0.8573104739), tensor(1.2724287510), tensor(0.7044993043), tensor(1.3180229664), tensor(1.2518166304), tensor(0.8781726360), tensor(1.0058611631), tensor(1.3108273745), tensor(1.6789100170), tensor(0.9888656139), tensor(1.1884583235)]\n",
            "c:  [tensor(-0.0091181332), tensor(0.0616460294), tensor(0.0566964410), tensor(-0.0062309722), tensor(-0.0019094539), tensor(-0.0077051939), tensor(0.0013305683), tensor(0.0010659499), tensor(0.0529275984), tensor(0.0013667574), tensor(0.0381510183), tensor(0.0027921947), tensor(0.0079962956), tensor(0.0436306819), tensor(0.0064246040), tensor(0.0407892540), tensor(-0.0014736657), tensor(-0.0005829979)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1177111864, 0.0624504089, 0.0799868703, 0.0459159613, 0.0247783959,\n",
            "        0.1122090816, 0.0932925940, 0.0338897705, 0.0663439035, 0.1672042608,\n",
            "        0.1382119060, 0.0905373394, 0.1040894389, 0.1495902538, 0.1275192499,\n",
            "        0.1485453844, 0.0964732170, 0.0714826584, 0.1023045778, 0.1276156306])\n",
            "btensor.grad: tensor([-4.0330529213e-02,  1.7719268799e-02,  3.8470864296e-02,\n",
            "         2.9068470001e-02,  5.0290763378e-02,  1.9598007202e-04,\n",
            "         3.8145661354e-02,  1.1086630821e-01,  1.6680473089e-01,\n",
            "        -3.1357765198e-02,  1.5760660172e-03, -4.8785388470e-02,\n",
            "         5.0399720669e-02, -4.3112874031e-02,  7.0303678513e-05,\n",
            "         1.0269904137e-01,  1.0411087424e-01,  8.5392951965e-02,\n",
            "         2.5840520859e-02,  6.6699743271e-02])\n",
            "ctensor.grad: tensor([  0.9111928344, -11.7036743164, -10.6923789978,   1.0140117407,\n",
            "         -0.0186673123,   0.6789273024,  -0.1969481558,  -0.2530470788,\n",
            "         -9.0516300201,   0.1417601556,  -6.6798028946,   1.1591134071,\n",
            "          0.6142776012,  -8.4335622787,   0.1221336424,  -6.4391694069,\n",
            "          0.8535907865,   0.0204879493])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.4761962891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6348918676), tensor(0.9851255417), tensor(1.0923829079), tensor(1.1992115974), tensor(1.0986349583), tensor(1.4272017479), tensor(1.0606117249), tensor(1.1723794937), tensor(0.9234375954), tensor(1.2413311005), tensor(1.0927402973), tensor(1.5048067570), tensor(0.9840706587), tensor(1.2640444040), tensor(1.1777307987), tensor(0.9886140823), tensor(1.0842092037), tensor(0.8994007111), tensor(1.1004804373), tensor(1.0097804070)]\n",
            "b:  [tensor(0.6048278809), tensor(1.0968327522), tensor(1.3465949297), tensor(1.0148065090), tensor(1.4792382717), tensor(0.8161393404), tensor(1.2459667921), tensor(1.1721476316), tensor(1.7966631651), tensor(0.8574448228), tensor(1.2723405361), tensor(0.7047455907), tensor(1.3176372051), tensor(1.2519891262), tensor(0.8781150579), tensor(1.0051814318), tensor(1.3101243973), tensor(1.6783216000), tensor(0.9886513352), tensor(1.1879891157)]\n",
            "c:  [tensor(-0.0095263161), tensor(0.0680331662), tensor(0.0625740737), tensor(-0.0067553530), tensor(-0.0018996014), tensor(-0.0080488250), tensor(0.0014391488), tensor(0.0012087132), tensor(0.0578159653), tensor(0.0012871947), tensor(0.0416587070), tensor(0.0020756721), tensor(0.0074992715), tensor(0.0482238829), tensor(0.0063560628), tensor(0.0442232899), tensor(-0.0020039408), tensor(-0.0007056522)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1441168785, 0.0926862657, 0.1140853167, 0.0706707239, 0.0463045985,\n",
            "        0.1435534954, 0.1303223073, 0.0570284128, 0.0980850458, 0.2136177123,\n",
            "        0.1826872230, 0.1163052022, 0.1423117518, 0.1962127686, 0.1671679020,\n",
            "        0.1961171627, 0.1346507072, 0.1062734127, 0.1385831237, 0.1682237387])\n",
            "btensor.grad: tensor([-0.0411964655,  0.0340236425,  0.0621085763,  0.0472779274,\n",
            "         0.0772647262,  0.0110560656,  0.0610138476,  0.1487197876,\n",
            "         0.2153411508, -0.0268751979,  0.0176548958, -0.0492521971,\n",
            "         0.0771538317, -0.0345057249,  0.0115099400,  0.1359518170,\n",
            "         0.1406019628,  0.1176754236,  0.0428591967,  0.0938444138])\n",
            "ctensor.grad: tensor([  0.8163664937, -12.7742748260, -11.7552700043,   1.0487616062,\n",
            "         -0.0197048243,   0.6872622371,  -0.2171609998,  -0.2855265141,\n",
            "         -9.7767353058,   0.1591255963,  -7.0153803825,   1.4330447912,\n",
            "          0.9940480590,  -9.1864051819,   0.1370820552,  -6.8680701256,\n",
            "          1.0605499744,   0.2453086674])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.1689453125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6340153217), tensor(0.9844914079), tensor(1.0916206837), tensor(1.1987171173), tensor(1.0982837677), tensor(1.4262998104), tensor(1.0597472191), tensor(1.1719621420), tensor(0.9227694273), tensor(1.2399996519), tensor(1.0915759802), tensor(1.5040732622), tensor(0.9831425548), tensor(1.2627968788), tensor(1.1766705513), tensor(0.9873635769), tensor(1.0833188295), tensor(0.8986694217), tensor(1.0995818377), tensor(1.0087139606)]\n",
            "b:  [tensor(0.6050441265), tensor(1.0965741873), tensor(1.3461489677), tensor(1.0144704580), tensor(1.4786975384), tensor(0.8160250783), tensor(1.2455316782), tensor(1.1711896658), tensor(1.7953057289), tensor(0.8575583100), tensor(1.2721624374), tensor(0.7049978375), tensor(1.3171008825), tensor(1.2521138191), tensor(0.8779954314), tensor(1.0043154955), tensor(1.3092144728), tensor(1.6775456667), tensor(0.9883441925), tensor(1.1873688698)]\n",
            "c:  [tensor(-0.0098740207), tensor(0.0749624446), tensor(0.0690006763), tensor(-0.0072925230), tensor(-0.0018892760), tensor(-0.0083920034), tensor(0.0015583120), tensor(0.0013688090), tensor(0.0630543604), tensor(0.0011979177), tensor(0.0452860147), tensor(0.0011880809), tensor(0.0067448202), tensor(0.0531920642), tensor(0.0062790485), tensor(0.0478372462), tensor(-0.0026679300), tensor(-0.0009974070)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1753202677, 0.1268221140, 0.1524453163, 0.0988883972, 0.0702263415,\n",
            "        0.1803839207, 0.1729048491, 0.0834681988, 0.1336365938, 0.2663015425,\n",
            "        0.2328668833, 0.1467011273, 0.1856251955, 0.2494981289, 0.2120412588,\n",
            "        0.2501043081, 0.1780746579, 0.1462568045, 0.1797119379, 0.2132823467])\n",
            "btensor.grad: tensor([-0.0432548523,  0.0517205000,  0.0891969204,  0.0672082305,\n",
            "         0.1081474125,  0.0228582621,  0.0870119333,  0.1915948391,\n",
            "         0.2714820504, -0.0227009654,  0.0356091261, -0.0504451692,\n",
            "         0.1072736084, -0.0249438286,  0.0239230394,  0.1731985211,\n",
            "         0.1819855571,  0.1551804543,  0.0614266396,  0.1240564585])\n",
            "ctensor.grad: tensor([  0.6954086423, -13.8585529327, -12.8532114029,   1.0743396282,\n",
            "         -0.0206508227,   0.6863561273,  -0.2383264154,  -0.3201918304,\n",
            "        -10.4767847061,   0.1785538048,  -7.2546148300,   1.7751824856,\n",
            "          1.5089024305,  -9.9363613129,   0.1540282369,  -7.2279148102,\n",
            "          1.3279783726,   0.5835096836])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1070.8103027344, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6329630613), tensor(0.9836746454), tensor(1.0906518698), tensor(1.1980699301), tensor(1.0978075266), tensor(1.4251904488), tensor(1.0586477518), tensor(1.1714011431), tensor(0.9219123721), tensor(1.2383805513), tensor(1.0901393890), tensor(1.5031687021), tensor(0.9819799066), tensor(1.2612547874), tensor(1.1753667593), tensor(0.9858186245), tensor(1.0821909904), tensor(0.8977181315), tensor(1.0984604359), tensor(1.0074086189)]\n",
            "b:  [tensor(0.6052826047), tensor(1.0962282419), tensor(1.3455560207), tensor(1.0140335560), tensor(1.4779893160), tensor(0.8158528805), tensor(1.2449570894), tensor(1.1699991226), tensor(1.7936363220), tensor(0.8576592207), tensor(1.2718917131), tensor(0.7052646279), tensor(1.3164037466), tensor(1.2521910667), tensor(0.8778150082), tensor(1.0032509565), tensor(1.3080804348), tensor(1.6765624285), tensor(0.9879432917), tensor(1.1865903139)]\n",
            "c:  [tensor(-0.0101507185), tensor(0.0824306831), tensor(0.0759826973), tensor(-0.0078374166), tensor(-0.0018785392), tensor(-0.0087300381), tensor(0.0016881995), tensor(0.0015466767), tensor(0.0686228275), tensor(0.0010978961), tensor(0.0489759408), tensor(9.3243550509e-05), tensor(0.0056606075), tensor(0.0585241541), tensor(0.0061925133), tensor(0.0515840054), tensor(-0.0034997247), tensor(-0.0015243541)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.2104636431, 0.1633517146, 0.1937686801, 0.1294457912, 0.0952369869,\n",
            "        0.2218812704, 0.2198937237, 0.1122096777, 0.1714054346, 0.3238152862,\n",
            "        0.2873291969, 0.1809031665, 0.2325319052, 0.3084107637, 0.2607517242,\n",
            "        0.3089927435, 0.2255590558, 0.1902573705, 0.2242763638, 0.2610700130])\n",
            "btensor.grad: tensor([-0.0477014780,  0.0691817999,  0.1185938120,  0.0873804688,\n",
            "         0.1416463852,  0.0344430208,  0.1149286628,  0.2381186485,\n",
            "         0.3338875175, -0.0201788545,  0.0541384220, -0.0533635467,\n",
            "         0.1394225955, -0.0154496431,  0.0360885561,  0.2129191160,\n",
            "         0.2268157303,  0.1966565847,  0.0801767111,  0.1557042599])\n",
            "ctensor.grad: tensor([  0.5533960462, -14.9364805222, -13.9640417099,   1.0897877216,\n",
            "         -0.0214734375,   0.6760685444,  -0.2597749829,  -0.3557354212,\n",
            "        -11.1369323730,   0.2000432760,  -7.3798542023,   2.1896746159,\n",
            "          2.1684255600, -10.6641788483,   0.1730707884,  -7.4935150146,\n",
            "          1.6635894775,   1.0538940430])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1070.3986816406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6317220926), tensor(0.9826745391), tensor(1.0894716978), tensor(1.1972668171), tensor(1.0972104073), tensor(1.4238569736), tensor(1.0573008060), tensor(1.1706926823), tensor(0.9208674431), tensor(1.2364606857), tensor(1.0884196758), tensor(1.5020809174), tensor(0.9805761576), tensor(1.2593986988), tensor(1.1738107204), tensor(0.9839662910), tensor(1.0808151960), tensor(0.8965364099), tensor(1.0971099138), tensor(1.0058625937)]\n",
            "b:  [tensor(0.6055628657), tensor(1.0958076715), tensor(1.3448132277), tensor(1.0135051012), tensor(1.4771106243), tensor(0.8156320453), tensor(1.2442424297), tensor(1.1685682535), tensor(1.7916344404), tensor(0.8577649593), tensor(1.2715351582), tensor(0.7055611610), tensor(1.3155460358), tensor(1.2522286177), tensor(0.8775835633), tensor(1.0019863844), tensor(1.3067159653), tensor(1.6753619909), tensor(0.9874571562), tensor(1.1856580973)]\n",
            "c:  [tensor(-0.0103502702), tensor(0.0904271454), tensor(0.0835159272), tensor(-0.0083848462), tensor(-0.0018674672), tensor(-0.0090586031), tensor(0.0018285074), tensor(0.0017418462), tensor(0.0744977817), tensor(0.0009861602), tensor(0.0526690707), tensor(-0.0012441161), tensor(0.0041770637), tensor(0.0642011762), tensor(0.0060954108), tensor(0.0554076955), tensor(-0.0045349970), tensor(-0.0023567560)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.2481977940, 0.2000249624, 0.2360408902, 0.1606258154, 0.1194225699,\n",
            "        0.2667007446, 0.2693936825, 0.1416932344, 0.2089884281, 0.3839809597,\n",
            "        0.3439317942, 0.2175657153, 0.2807509303, 0.3712171316, 0.3111968040,\n",
            "        0.3704633713, 0.2751543522, 0.2363479137, 0.2700961232, 0.3092061281])\n",
            "btensor.grad: tensor([-0.0560556650,  0.0841061473,  0.1485503912,  0.1056920290,\n",
            "         0.1757303476,  0.0441640615,  0.1429348290,  0.2861791849,\n",
            "         0.4003784060, -0.0211461782,  0.0713194609, -0.0593060926,\n",
            "         0.1715481281, -0.0075199604,  0.0462891459,  0.2529075742,\n",
            "         0.2729043961,  0.2400982380,  0.0972230434,  0.1864460707])\n",
            "ctensor.grad: tensor([  0.3991034627, -15.9929180145, -15.0664653778,   1.0948599577,\n",
            "         -0.0221439954,   0.6571301818,  -0.2806157768,  -0.3903388977,\n",
            "        -11.7499008179,   0.2234717309,  -7.3862571716,   2.6747190952,\n",
            "          2.9670875072, -11.3540420532,   0.1942048520,  -7.6473817825,\n",
            "          2.0705447197,   1.6648037434])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1069.9299316406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6302880049), tensor(0.9815042019), tensor(1.0880881548), tensor(1.1963155270), tensor(1.0965081453), tensor(1.4222912788), tensor(1.0557059050), tensor(1.1698429585), tensor(0.9196502566), tensor(1.2342396975), tensor(1.0864193439), tensor(1.5008059740), tensor(0.9789388180), tensor(1.2572200298), tensor(1.1720066071), tensor(0.9818078876), tensor(1.0791933537), tensor(0.8951261640), tensor(1.0955375433), tensor(1.0040880442)]\n",
            "b:  [tensor(0.6059128642), tensor(1.0953390598), tensor(1.3439288139), tensor(1.0129070282), tensor(1.4760715961), tensor(0.8153815866), tensor(1.2433986664), tensor(1.1669025421), tensor(1.7892936468), tensor(0.8579037786), tensor(1.2711112499), tensor(0.7059096694), tensor(1.3145405054), tensor(1.2522436380), tensor(0.8773211837), tensor(1.0005338192), tensor(1.3051283360), tensor(1.6739473343), tensor(0.9869054556), tensor(1.1845908165)]\n",
            "c:  [tensor(-0.0104722613), tensor(0.0989375114), tensor(0.0915879682), tensor(-0.0089299064), tensor(-0.0018561464), tensor(-0.0093741482), tensor(0.0019784358), tensor(0.0019527961), tensor(0.0806568787), tensor(0.0008618645), tensor(0.0563111231), tensor(-0.0028550969), tensor(0.0022362587), tensor(0.0701997280), tensor(0.0059867604), tensor(0.0592498630), tensor(-0.0058079949), tensor(-0.0035614811)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.2868261337, 0.2340675294, 0.2767121196, 0.1902500391, 0.1404560059,\n",
            "        0.3131276369, 0.3189868331, 0.1699420214, 0.2434356213, 0.4441930950,\n",
            "        0.4000738263, 0.2549933195, 0.3274671435, 0.4357297421, 0.3608332276,\n",
            "        0.4316854477, 0.3243801594, 0.2820438743, 0.3144794106, 0.3549062610])\n",
            "btensor.grad: tensor([-0.0699998140,  0.0937275290,  0.1768724322,  0.1196129322,\n",
            "         0.2078136355,  0.0500974655,  0.1687560081,  0.3331537247,\n",
            "         0.4681512713, -0.0277668834,  0.0847867727, -0.0696984828,\n",
            "         0.2010956109, -0.0029962063,  0.0524733812,  0.2905110717,\n",
            "         0.3175347447,  0.2829297781,  0.1103378534,  0.2134625912])\n",
            "ctensor.grad: tensor([  0.2439824194, -17.0207309723, -16.1440792084,   1.0901206732,\n",
            "         -0.0226416048,   0.6310896277,  -0.2998566329,  -0.4218997657,\n",
            "        -12.3181905746,   0.2485915422,  -7.2841043472,   3.2219617367,\n",
            "          3.8816101551, -11.9971017838,   0.2173003852,  -7.6843323708,\n",
            "          2.5459949970,   2.4094500542])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1069.4017333984, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6286648512), tensor(0.9801910520), tensor(1.0865225792), tensor(1.1952352524), tensor(1.0957283974), tensor(1.4204941988), tensor(1.0538754463), tensor(1.1688684225), tensor(0.9182918668), tensor(1.2317303419), tensor(1.0841535330), tensor(1.4993489981), tensor(0.9770898819), tensor(1.2547214031), tensor(1.1699711084), tensor(0.9793588519), tensor(1.0773402452), tensor(0.8935025930), tensor(1.0937643051), tensor(1.0021107197)]\n",
            "b:  [tensor(0.6063683629), tensor(1.0948630571), tensor(1.3429223299), tensor(1.0122741461), tensor(1.4748959541), tensor(0.8151298165), tensor(1.2424484491), tensor(1.1650205851), tensor(1.7866225243), tensor(0.8581147790), tensor(1.2706509829), tensor(0.7063390017), tensor(1.3134136200), tensor(1.2522623539), tensor(0.8770581484), tensor(0.9989182949), tensor(1.3033387661), tensor(1.6723353863), tensor(0.9863188267), tensor(1.1834214926)]\n",
            "c:  [tensor(-0.0105223013), tensor(0.1079480723), tensor(0.1001819968), tensor(-0.0094683357), tensor(-0.0018446677), tensor(-0.0096741691), tensor(0.0021367329), tensor(0.0021770033), tensor(0.0870836601), tensor(0.0007243400), tensor(0.0598597936), tensor(-0.0047638132), tensor(-0.0001999049), tensor(0.0764964297), tensor(0.0058657094), tensor(0.0630564019), tensor(-0.0073484760), tensor(-0.0051936703)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.3246310949, 0.2626351118, 0.3131198287, 0.2160514593, 0.1559494287,\n",
            "        0.3594138622, 0.3661034107, 0.1949132681, 0.2716723680, 0.5018597245,\n",
            "        0.4531610608, 0.2914060056, 0.3697900772, 0.4997351170, 0.4070965648,\n",
            "        0.4898039103, 0.3706312776, 0.3247197270, 0.3546462655, 0.3954606056])\n",
            "btensor.grad: tensor([-0.0911033154,  0.0951913595,  0.2012922764,  0.1265730858,\n",
            "         0.2351384461,  0.0503572226,  0.1900489628,  0.3763811588,\n",
            "         0.5342342854, -0.0422030687,  0.0920454264, -0.0858644694,\n",
            "         0.2253716588, -0.0037512779,  0.0526020527,  0.3231075406,\n",
            "         0.3579088151,  0.3223912716,  0.1173231602,  0.2338703871])\n",
            "ctensor.grad: tensor([  0.1000808328, -18.0211219788, -17.1880550385,   1.0768585205,\n",
            "         -0.0229574181,   0.6000423431,  -0.3165944517,  -0.4484141767,\n",
            "        -12.8535575867,   0.2750489116,  -7.0973401070,   3.8174321651,\n",
            "          4.8723268509, -12.5933971405,   0.2421017736,  -7.6130714417,\n",
            "          3.0809617043,   3.2643780708])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1068.8128662109, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6268638372), tensor(0.9787744880), tensor(1.0848077536), tensor(1.1940548420), tensor(1.0949089527), tensor(1.4184734821), tensor(1.0518326759), tensor(1.1677941084), tensor(0.9168365002), tensor(1.2289558649), tensor(1.0816479921), tensor(1.4977223873), tensor(0.9750635624), tensor(1.2519141436), tensor(1.1677316427), tensor(0.9766463637), tensor(1.0752818584), tensor(0.8916919827), tensor(1.0918232203), tensor(0.9999671578)]\n",
            "b:  [tensor(0.6069708467), tensor(1.0944328308), tensor(1.3418231010), tensor(1.0116521120), tensor(1.4736198187), tensor(0.8149125576), tensor(1.2414243221), tensor(1.1629524231), tensor(1.7836424112), tensor(0.8584460020), tensor(1.2701963186), tensor(0.7068826556), tensor(1.3122035265), tensor(1.2523190975), tensor(0.8768330812), tensor(0.9971754551), tensor(1.3013806343), tensor(1.6705553532), tensor(0.9857368469), tensor(1.1821953058)]\n",
            "c:  [tensor(-0.0105110891), tensor(0.1174486801), tensor(0.1092805490), tensor(-0.0099967560), tensor(-0.0018331193), tensor(-0.0099572707), tensor(0.0023018364), tensor(0.0024111958), tensor(0.0937705487), tensor(0.0005731306), tensor(0.0632888526), tensor(-0.0069856988), tensor(-0.0031440398), tensor(0.0830720365), tensor(0.0057315817), tensor(0.0667833909), tensor(-0.0091793546), tensor(-0.0072895493)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.3601945639, 0.2833179832, 0.3429643512, 0.2360861301, 0.1638898551,\n",
            "        0.4041333199, 0.4085485339, 0.2148582935, 0.2910698652, 0.5549040437,\n",
            "        0.5011036396, 0.3253306448, 0.4052659273, 0.5614553690, 0.4478940964,\n",
            "        0.5424935818, 0.4116712213, 0.3621267080, 0.3882256150, 0.4287098646])\n",
            "btensor.grad: tensor([-0.1204980612,  0.0860528350,  0.2198576927,  0.1244082451,\n",
            "         0.2552334666,  0.0434527397,  0.2048188448,  0.4136320353,\n",
            "         0.5960183740, -0.0662410855,  0.0909405947, -0.1087322533,\n",
            "         0.2420262694, -0.0113558769,  0.0450097769,  0.3485636711,\n",
            "         0.3916334510,  0.3560184240,  0.1163989305,  0.2452466488])\n",
            "ctensor.grad: tensor([ -0.0224243775, -19.0012187958, -18.1971015930,   1.0568411350,\n",
            "         -0.0230968911,   0.5662024617,  -0.3302069008,  -0.4683849514,\n",
            "        -13.3737688065,   0.3024189770,  -6.8581147194,   4.4437704086,\n",
            "          5.8882694244, -13.1512107849,   0.2682549059,  -7.4539709091,\n",
            "          3.6617579460,   4.1917576790])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1068.1558837891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6249002218), tensor(0.9773015380), tensor(1.0829840899), tensor(1.1928093433), tensor(1.0940939188), tensor(1.4162409306), tensor(1.0496079922), tensor(1.1666507721), tensor(0.9153370261), tensor(1.2259451151), tensor(1.0789344311), tensor(1.4959429502), tensor(0.9729019403), tensor(1.2488145828), tensor(1.1653218269), tensor(0.9737043381), tensor(1.0730513334), tensor(0.8897279501), tensor(1.0897547007), tensor(0.9977000952)]\n",
            "b:  [tensor(0.6077640653), tensor(1.0941092968), tensor(1.3406665325), tensor(1.0110933781), tensor(1.4722880125), tensor(0.8147693872), tensor(1.2403653860), tensor(1.1607345343), tensor(1.7803838253), tensor(0.8589506745), tensor(1.2697962523), tensor(0.7075757384), tensor(1.3109561205), tensor(1.2524527311), tensor(0.8766893148), tensor(0.9953471422), tensor(1.2992948294), tensor(1.6686450243), tensor(0.9852041006), tensor(1.1809650660)]\n",
            "c:  [tensor(-0.0104524316), tensor(0.1274333596), tensor(0.1188679785), tensor(-0.0105127543), tensor(-0.0018215798), tensor(-0.0102230096), tensor(0.0024720759), tensor(0.0026517482), tensor(0.1007194221), tensor(0.0004080060), tensor(0.0665886551), tensor(-0.0095272213), tensor(-0.0065812743), tensor(0.0899139792), tensor(0.0055839079), tensor(0.0704001710), tensor(-0.0113155609), tensor(-0.0098620532)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.3927147388, 0.2945915461, 0.3647243977, 0.2491041422, 0.1630016267,\n",
            "        0.4465160370, 0.4449317455, 0.2286751270, 0.2998966575, 0.6021617651,\n",
            "        0.5427205563, 0.3558879197, 0.4323228002, 0.6199169159, 0.4819725752,\n",
            "        0.5883995295, 0.4460959435, 0.3928112388, 0.4136928916, 0.4534089565])\n",
            "btensor.grad: tensor([-0.1586443782,  0.0647178888,  0.2313038111,  0.1117579341,\n",
            "         0.2663578391,  0.0286291838,  0.2117954195,  0.4435687065,\n",
            "         0.6517063379, -0.1009292006,  0.0800231695, -0.1386126876,\n",
            "         0.2494743168, -0.0267292261,  0.0287507623,  0.3656614423,\n",
            "         0.4171697497,  0.3820564747,  0.1065548658,  0.2460507154])\n",
            "ctensor.grad: tensor([ -0.1173152030, -19.9693717957, -19.1748523712,   1.0319963694,\n",
            "         -0.0230790861,   0.5314787030,  -0.3404787779,  -0.4811051488,\n",
            "        -13.8977413177,   0.3302490711,  -6.5996088982,   5.0830445290,\n",
            "          6.8744688034, -13.6838884354,   0.2953474522,  -7.2335586548,\n",
            "          4.2724123001,   5.1450071335])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1067.4317626953, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6227896214), tensor(0.9758213758), tensor(1.0810947418), tensor(1.1915357113), tensor(1.0933293104), tensor(1.4138081074), tensor(1.0472335815), tensor(1.1654703617), tensor(0.9138491154), tensor(1.2227277756), tensor(1.0760451555), tensor(1.4940282106), tensor(0.9706496596), tensor(1.2454390526), tensor(1.1627763510), tensor(0.9705678821), tensor(1.0706837177), tensor(0.8876461983), tensor(1.0876019001), tensor(0.9953532219)]\n",
            "b:  [tensor(0.6087902784), tensor(1.0939558744), tensor(1.3394902945), tensor(1.0106520653), tensor(1.4709492922), tensor(0.8147394657), tensor(1.2393121719), tensor(1.1584049463), tensor(1.7768810987), tensor(0.8596827388), tensor(1.2695022821), tensor(0.7084514499), tensor(1.3097205162), tensor(1.2527025938), tensor(0.8766705394), tensor(0.9934757352), tensor(1.2971247435), tensor(1.6666464806), tensor(0.9847655892), tensor(1.1797858477)]\n",
            "c:  [tensor(-0.0103607448), tensor(0.1378987283), tensor(0.1289309412), tensor(-0.0110148061), tensor(-0.0018101133), tensor(-0.0104716094), tensor(0.0026458802), tensor(0.0028951073), tensor(0.1079397947), tensor(0.0002289573), tensor(0.0697632879), tensor(-0.0123868026), tensor(-0.0104708821), tensor(0.0970168114), tensor(0.0054224310), tensor(0.0738892928), tensor(-0.0137641709), tensor(-0.0128998002)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.4221297503, 0.2960280478, 0.3778690696, 0.2547363043, 0.1529258639,\n",
            "        0.4865601063, 0.4748848081, 0.2360851765, 0.2975821495, 0.6434670687,\n",
            "        0.5778666735, 0.3829436898, 0.4504610300, 0.6751041412, 0.5090971589,\n",
            "        0.6272885799, 0.4735298753, 0.4163553715, 0.4305552244, 0.4693772793])\n",
            "btensor.grad: tensor([-0.2052403688,  0.0306947231,  0.2352576852,  0.0882666707,\n",
            "         0.2677533627,  0.0059795380,  0.2106355727,  0.4659197330,\n",
            "         0.7005500197, -0.1464084387,  0.0588043928, -0.1751405895,\n",
            "         0.2471218705, -0.0499656200,  0.0037507415,  0.3742812872,\n",
            "         0.4340237975,  0.3997088671,  0.0876983404,  0.2358514071])\n",
            "ctensor.grad: tensor([ -0.1833740771, -20.9307365417, -20.1259326935,   1.0041035414,\n",
            "         -0.0229328610,   0.4971995354,  -0.3476087749,  -0.4867180884,\n",
            "        -14.4407377243,   0.3580974936,  -6.3492593765,   5.7191629410,\n",
            "          7.7792148590, -14.2056665421,   0.3229539096,  -6.9782419205,\n",
            "          4.8972201347,   6.0754942894])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1066.6365966797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6205441952), tensor(0.9743800759), tensor(1.0791804790), tensor(1.1902681589), tensor(1.0926582813), tensor(1.4111832380), tensor(1.0447382927), tensor(1.1642823219), tensor(0.9124256968), tensor(1.2193300724), tensor(1.0730085373), tensor(1.4919928312), tensor(0.9683485627), tensor(1.2417998314), tensor(1.1601265669), tensor(0.9672681689), tensor(1.0682106018), tensor(0.8854794502), tensor(1.0854052305), tensor(0.9929661751)]\n",
            "b:  [tensor(0.6100866199), tensor(1.0940327644), tensor(1.3383290768), tensor(1.0103791952), tensor(1.4696508646), tensor(0.8148573041), tensor(1.2383027077), tensor(1.1559978724), tensor(1.7731670141), tensor(0.8606922030), tensor(1.2693634033), tensor(0.7095381021), tensor(1.3085435629), tensor(1.2531042099), tensor(0.8768166304), tensor(0.9915992022), tensor(1.2949111462), tensor(1.6646007299), tensor(0.9844624400), tensor(1.1787093878)]\n",
            "c:  [tensor(-0.0102486415), tensor(0.1488408893), tensor(0.1394570768), tensor(-0.0115020890), tensor(-0.0017987677), tensor(-0.0107036186), tensor(0.0028219349), tensor(0.0031381319), tensor(0.1154452115), tensor(3.6173354601e-05), tensor(0.0728253052), tensor(-0.0155566158), tensor(-0.0147508783), tensor(0.1043807566), tensor(0.0052470965), tensor(0.0772438645), tensor(-0.0165254809), tensor(-0.0163690560)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.4490747452, 0.2882550657, 0.3828554153, 0.2535167933, 0.1342173964,\n",
            "        0.5249800682, 0.4990530610, 0.2376053333, 0.2846822143, 0.6795521975,\n",
            "        0.6073260903, 0.4070745409, 0.4602159262, 0.7278348207, 0.5299509764,\n",
            "        0.6599463224, 0.4946309924, 0.4333456755, 0.4393306971, 0.4774075747])\n",
            "btensor.grad: tensor([-0.2592695951, -0.0153784752,  0.2322346568,  0.0545825958,\n",
            "         0.2596943080, -0.0235629082,  0.2018903792,  0.4814087152,\n",
            "         0.7428274155, -0.2018941045,  0.0277744532, -0.2173330188,\n",
            "         0.2353836596, -0.0803290606, -0.0292149484,  0.3753029108,\n",
            "         0.4427111149,  0.4091554880,  0.0606243610,  0.2153030634])\n",
            "ctensor.grad: tensor([ -0.2242072225, -21.8843173981, -21.0522842407,   0.9745658040,\n",
            "         -0.0226911716,   0.4640192986,  -0.3521092236,  -0.4860491455,\n",
            "        -15.0108261108,   0.3855677843,  -6.1240358353,   6.3396258354,\n",
            "          8.5599918365, -14.7278900146,   0.3506693542,  -6.7091412544,\n",
            "          5.5226182938,   6.9385099411])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1065.7656250000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6181708574), tensor(0.9730164409), tensor(1.0772758722), tensor(1.1890347004), tensor(1.0921175480), tensor(1.4083683491), tensor(1.0421441793), tensor(1.1631104946), tensor(0.9111122489), tensor(1.2157713175), tensor(1.0698459148), tensor(1.4898459911), tensor(0.9660341740), tensor(1.2379025221), tensor(1.1573971510), tensor(0.9638289213), tensor(1.0656564236), tensor(0.8832539320), tensor(1.0831987858), tensor(0.9905713201)]\n",
            "b:  [tensor(0.6116827726), tensor(1.0943924189), tensor(1.3372119665), tensor(1.0103185177), tensor(1.4684344530), tensor(0.8151496053), tensor(1.2373685837), tensor(1.1535404921), tensor(1.7692691088), tensor(0.8620214462), tensor(1.2694221735), tensor(0.7108570337), tensor(1.3074660301), tensor(1.2536863089), tensor(0.8771602511), tensor(0.9897475839), tensor(1.2926886082), tensor(1.6625438929), tensor(0.9843283892), tensor(1.1777797937)]\n",
            "c:  [tensor(-0.0101251742), tensor(0.1602520645), tensor(0.1504326463), tensor(-0.0119742397), tensor(-0.0017875752), tensor(-0.0109196212), tensor(0.0029992550), tensor(0.0033782830), tensor(0.1232488900), tensor(-0.0001699959), tensor(0.0757895261), tensor(-0.0190247446), tensor(-0.0193444323), tensor(0.1120089963), tensor(0.0050580297), tensor(0.0804633349), tensor(-0.0195945352), tensor(-0.0202176794)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.4746681452, 0.2727253139, 0.3809126019, 0.2466816902, 0.1081442237,\n",
            "        0.5629820824, 0.5188145638, 0.2343697548, 0.2626912594, 0.7117415667,\n",
            "        0.6325291991, 0.4293566942, 0.4628761411, 0.7794722319, 0.5458801389,\n",
            "        0.6878553629, 0.5108252168, 0.4451012015, 0.4412897229, 0.4789677858])\n",
            "btensor.grad: tensor([-0.3192334175, -0.0719203949,  0.2234256864,  0.0121275187,\n",
            "         0.2432912290, -0.0584633350,  0.1868198812,  0.4914674759,\n",
            "         0.7795855999, -0.2658498287, -0.0117487907, -0.2637832165,\n",
            "         0.2154976428, -0.1164206266, -0.0687197298,  0.3703239560,\n",
            "         0.4445102513,  0.4113578796,  0.0268107653,  0.1859079599])\n",
            "ctensor.grad: tensor([-2.4693453312e-01, -2.2822364807e+01, -2.1951126099e+01,\n",
            "         9.4430166483e-01, -2.2384982556e-02,  4.3200600147e-01,\n",
            "        -3.5464018583e-01, -4.8030212522e-01, -1.5607355118e+01,\n",
            "         4.1233849525e-01, -5.9284410477e+00,  6.9362554550e+00,\n",
            "         9.1871080399e+00, -1.5256480217e+01,  3.7813404202e-01,\n",
            "        -6.4389371872e+00,  6.1381101608e+00,  7.6972446442e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1064.8200683594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6156694889), tensor(0.9717596769), tensor(1.0754072666), tensor(1.1878551245), tensor(1.0917354822), tensor(1.4053585529), tensor(1.0394643545), tensor(1.1619712114), tensor(0.9099438190), tensor(1.2120635509), tensor(1.0665700436), tensor(1.4875905514), tensor(0.9637334943), tensor(1.2337447405), tensor(1.1546043158), tensor(0.9602650404), tensor(1.0630365610), tensor(0.8809872866), tensor(1.0810081959), tensor(0.9881920218)]\n",
            "b:  [tensor(0.6135995388), tensor(1.0950758457), tensor(1.3361598253), tensor(1.0105044842), tensor(1.4673333168), tensor(0.8156340718), tensor(1.2365331650), tensor(1.1510511637), tensor(1.7652076483), tensor(0.8637026548), tensor(1.2697116137), tensor(0.7124215961), tensor(1.3065199852), tensor(1.2544684410), tensor(0.8777251244), tensor(0.9879411459), tensor(1.2904829979), tensor(1.6605051756), tensor(0.9843878746), tensor(1.1770313978)]\n",
            "c:  [tensor(-0.0099949194), tensor(0.1721178144), tensor(0.1618400365), tensor(-0.0124311056), tensor(-0.0017765560), tensor(-0.0111200288), tensor(0.0031771746), tensor(0.0036136569), tensor(0.1313595772), tensor(-0.0003890876), tensor(0.0786671266), tensor(-0.0227773283), tensor(-0.0241669677), tensor(0.1199045405), tensor(0.0048555057), tensor(0.0835490599), tensor(-0.0229627267), tensor(-0.0243800394)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.5002735853, 0.2513484061, 0.3737276196, 0.2359085083, 0.0764038265,\n",
            "        0.6019681692, 0.5359704494, 0.2278525829, 0.2336905599, 0.7415565848,\n",
            "        0.6551768184, 0.4510861039, 0.4601318836, 0.8315618038, 0.5585579276,\n",
            "        0.7127712965, 0.5239681005, 0.4533325434, 0.4381129742, 0.4758650064])\n",
            "btensor.grad: tensor([-0.3833560348, -0.1366845369,  0.2104378939, -0.0371834040,\n",
            "         0.2202191353, -0.0968948603,  0.1670738757,  0.4978650808,\n",
            "         0.8123036623, -0.3362447619, -0.0578871965, -0.3129095435,\n",
            "         0.1892125607, -0.1564198732, -0.1129755229,  0.3612895012,\n",
            "         0.4411262572,  0.4077409506, -0.0118963718,  0.1496820450])\n",
            "ctensor.grad: tensor([-2.6050940156e-01, -2.3731494904e+01, -2.2814785004e+01,\n",
            "         9.1373080015e-01, -2.2038439289e-02,  4.0081575513e-01,\n",
            "        -3.5583946109e-01, -4.7074767947e-01, -1.6221370697e+01,\n",
            "         4.3818333745e-01, -5.7552070618e+00,  7.5051665306e+00,\n",
            "         9.6450700760e+00, -1.5791083336e+01,  4.0504756570e-01,\n",
            "        -6.1714477539e+00,  6.7363843918e+00,  8.3247194290e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1063.8004150391, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6130334139), tensor(0.9706290960), tensor(1.0735915899), tensor(1.1867401600), tensor(1.0915313959), tensor(1.4021424055), tensor(1.0367027521), tensor(1.1608734131), tensor(0.9089440107), tensor(1.2082116604), tensor(1.0631856918), tensor(1.4852230549), tensor(0.9614650607), tensor(1.2293175459), tensor(1.1517561674), tensor(0.9565834403), tensor(1.0603566170), tensor(0.8786886930), tensor(1.0788505077), tensor(0.9858426452)]\n",
            "b:  [tensor(0.6158488989), tensor(1.0961115360), tensor(1.3351850510), tensor(1.0109609365), tensor(1.4663715363), tensor(0.8163190484), tensor(1.2358113527), tensor(1.1485395432), tensor(1.7609951496), tensor(0.8657569289), tensor(1.2702543736), tensor(0.7142375112), tensor(1.3057277203), tensor(1.2554603815), tensor(0.8785257339), tensor(0.9861904979), tensor(1.2883114815), tensor(1.6585057974), tensor(0.9846557975), tensor(1.1764873266)]\n",
            "c:  [tensor(-0.0098580141), tensor(0.1844153851), tensor(0.1736559272), tensor(-0.0128725311), tensor(-0.0017657230), tensor(-0.0113049820), tensor(0.0033552703), tensor(0.0038428910), tensor(0.1397784054), tensor(-0.0006205791), tensor(0.0814611837), tensor(-0.0268002748), tensor(-0.0291326679), tensor(0.1280674338), tensor(0.0046399166), tensor(0.0865004957), tensor(-0.0266191065), tensor(-0.0287819728)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.5272145271, 0.2261112034, 0.3631265461, 0.2229998112, 0.0408219695,\n",
            "        0.6432348490, 0.5523273945, 0.2195550203, 0.1999558210, 0.7703762650,\n",
            "        0.6768695712, 0.4735003114, 0.4536894560, 0.8854494095, 0.5696188807,\n",
            "        0.7363168001, 0.5359863043, 0.4597175121, 0.4315365553, 0.4698780775])\n",
            "btensor.grad: tensor([-0.4498730302, -0.2071448565,  0.1949450970, -0.0912894011,\n",
            "         0.1923601329, -0.1369918585,  0.1443635821,  0.5023143291,\n",
            "         0.8425003886, -0.4108590186, -0.1085449457, -0.3631844819,\n",
            "         0.1584485471, -0.1983926296, -0.1601199210,  0.3501303792,\n",
            "         0.4343085587,  0.3998700380, -0.0535799265,  0.1088037491])\n",
            "ctensor.grad: tensor([-2.7381128073e-01, -2.4595151901e+01, -2.3631772995e+01,\n",
            "         8.8285070658e-01, -2.1666089073e-02,  3.6990603805e-01,\n",
            "        -3.5619145632e-01, -4.5846837759e-01, -1.6837657928e+01,\n",
            "         4.6298298240e-01, -5.5881185532e+00,  8.0458936691e+00,\n",
            "         9.9314012527e+00, -1.6325794220e+01,  4.3117779493e-01,\n",
            "        -5.9028649330e+00,  7.3127593994e+00,  8.8038673401e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1062.7056884766, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6102507114), tensor(0.9696351886), tensor(1.0718377829), tensor(1.1856920719), tensor(1.0915158987), tensor(1.3987038136), tensor(1.0338557959), tensor(1.1598196030), tensor(0.9081258178), tensor(1.2042158842), tensor(1.0596916676), tensor(1.4827353954), tensor(0.9592402577), tensor(1.2246073484), tensor(1.1488542557), tensor(0.9527850747), tensor(1.0576139688), tensor(0.8763606548), tensor(1.0767352581), tensor(0.9835301638)]\n",
            "b:  [tensor(0.6184349060), tensor(1.0975155830), tensor(1.3342930079), tensor(1.0117024183), tensor(1.4655640125), tensor(0.8172047734), tensor(1.2352104187), tensor(1.1460087299), tensor(1.7566380501), tensor(0.8681946397), tensor(1.2710627317), tensor(0.7163040042), tensor(1.3051027060), tensor(1.2566632032), tensor(0.8795680404), tensor(0.9844983220), tensor(1.2861837149), tensor(1.6565601826), tensor(0.9851384163), tensor(1.1761609316)]\n",
            "c:  [tensor(-0.0097109098), tensor(0.1971134096), tensor(0.1858502924), tensor(-0.0132982116), tensor(-0.0017550868), tensor(-0.0114743514), tensor(0.0035332453), tensor(0.0040650014), tensor(0.1484973133), tensor(-0.0008639379), tensor(0.0841643140), tensor(-0.0310802925), tensor(-0.0341594145), tensor(0.1364928931), tensor(0.0044117328), tensor(0.0893127099), tensor(-0.0305512697), tensor(-0.0333451666)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.5565469265, 0.1987766922, 0.3507638872, 0.2096197605, 0.0030977726,\n",
            "        0.6877155304, 0.5693808794, 0.2107517719, 0.1636402011, 0.7991614938,\n",
            "        0.6988142133, 0.4975261986, 0.4449650049, 0.9420324564, 0.5803919435,\n",
            "        0.7596726418, 0.5485344529, 0.4656030536, 0.4230560660, 0.4624952078])\n",
            "btensor.grad: tensor([-0.5172007680, -0.2808037996,  0.1784158349, -0.1482889652,\n",
            "         0.1615041643, -0.1771432161,  0.1201903224,  0.5061564445,\n",
            "         0.8714179397, -0.4875460863, -0.1616803408, -0.4133038819,\n",
            "         0.1250014007, -0.2405526638, -0.2084645629,  0.3384376168,\n",
            "         0.4255612493,  0.3891252279, -0.0965199471,  0.0652776957])\n",
            "ctensor.grad: tensor([-2.9420933127e-01, -2.5396041870e+01, -2.4388719559e+01,\n",
            "         8.5136014223e-01, -2.1272374317e-02,  3.3873870969e-01,\n",
            "        -3.5594996810e-01, -4.4422116876e-01, -1.7437818527e+01,\n",
            "         4.8671755195e-01, -5.4062671661e+00,  8.5600347519e+00,\n",
            "         1.0053492546e+01, -1.6850906372e+01,  4.5636758208e-01,\n",
            "        -5.6244297028e+00,  7.8643250465e+00,  9.1263847351e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1061.5451660156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6073060036), tensor(0.9687817693), tensor(1.0701479912), tensor(1.1847064495), tensor(1.0916928053), tensor(1.3950245380), tensor(1.0309152603), tensor(1.1588078737), tensor(0.9074930549), tensor(1.2000741959), tensor(1.0560832024), tensor(1.4801170826), tensor(0.9570657015), tensor(1.2195992470), tensor(1.1458954811), tensor(0.9488679767), tensor(1.0547999144), tensor(0.8740015626), tensor(1.0746666193), tensor(0.9812564254)]\n",
            "b:  [tensor(0.6213551164), tensor(1.0992927551), tensor(1.3334830999), tensor(1.0127353668), tensor(1.4649183750), tensor(0.8182853460), tensor(1.2347320318), tensor(1.1434577703), tensor(1.7521388531), tensor(0.8710166812), tensor(1.2721403837), tensor(0.7186154723), tensor(1.3046510220), tensor(1.2580703497), tensor(0.8808512688), tensor(0.9828617573), tensor(1.2841038704), tensor(1.6546776295), tensor(0.9858351350), tensor(1.1760572195)]\n",
            "c:  [tensor(-0.0095476136), tensor(0.2101725340), tensor(0.1983864158), tensor(-0.0137076126), tensor(-0.0017446602), tensor(-0.0116278101), tensor(0.0037108092), tensor(0.0042792060), tensor(0.1574990451), tensor(-0.0011186627), tensor(0.0867585838), tensor(-0.0356051177), tensor(-0.0391716659), tensor(0.1451702416), tensor(0.0041714641), tensor(0.0919752121), tensor(-0.0347458571), tensor(-0.0379907489)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.5889502764,  0.1706895977,  0.3379524946,  0.1971158981,\n",
            "        -0.0353763700,  0.7358574867,  0.5881128907,  0.2023426294,\n",
            "         0.1265503764,  0.8283465505,  0.7216968536,  0.5236566067,\n",
            "         0.4349133968,  1.0016187429,  0.5917554498,  0.7834143639,\n",
            "         0.5628020167,  0.4718159437,  0.4137225747,  0.4547455311])\n",
            "btensor.grad: tensor([-0.5840452909, -0.3554289341,  0.1619721651, -0.2065948844,\n",
            "         0.1291327178, -0.2161110640,  0.0956699252,  0.5101993084,\n",
            "         0.8998287916, -0.5644094348, -0.2155330181, -0.4622887373,\n",
            "         0.0903318226, -0.2814241648, -0.2566491663,  0.3273164630,\n",
            "         0.4159694016,  0.3765180111, -0.1393405199,  0.0207519531])\n",
            "ctensor.grad: tensor([-3.2659232616e-01, -2.6118251801e+01, -2.5072256088e+01,\n",
            "         8.1880205870e-01, -2.0853130147e-02,  3.0691763759e-01,\n",
            "        -3.5512775183e-01, -4.2840892076e-01, -1.8003473282e+01,\n",
            "         5.0944948196e-01, -5.1885318756e+00,  9.0496511459e+00,\n",
            "         1.0024505615e+01, -1.7354705811e+01,  4.8053690791e-01,\n",
            "        -5.3250041008e+00,  8.3891744614e+00,  9.2911634445e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1060.3197021484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6041826010), tensor(0.9680681229), tensor(1.0685200691), tensor(1.1837742329), tensor(1.0920605659), tensor(1.3910865784), tensor(1.0278705359), tensor(1.1578339338), tensor(0.9070428014), tensor(1.1957851648), tensor(1.0523550510), tensor(1.4773575068), tensor(0.9549458623), tensor(1.2142796516), tensor(1.1428750753), tensor(0.9448305368), tensor(1.0519027710), tensor(0.8716084957), tensor(1.0726460218), tensor(0.9790204167)]\n",
            "b:  [tensor(0.6246021986), tensor(1.1014386415), tensor(1.3327517509), tensor(1.0140604973), tensor(1.4644367695), tensor(0.8195506930), tensor(1.2343746424), tensor(1.1408843994), tensor(1.7474989891), tensor(0.8742160797), tensor(1.2734838724), tensor(0.7211627364), tensor(1.3043735027), tensor(1.2596700191), tensor(0.8823696971), tensor(0.9812749028), tensor(1.2820731401), tensor(1.6528645754), tensor(0.9867404103), tensor(1.1761749983)]\n",
            "c:  [tensor(-0.0093610045), tensor(0.2235468030), tensor(0.2112217396), tensor(-0.0140999667), tensor(-0.0017344611), tensor(-0.0117649529), tensor(0.0038875754), tensor(0.0044847769), tensor(0.1667583883), tensor(-0.0013843109), tensor(0.0892171860), tensor(-0.0403631106), tensor(-0.0441013649), tensor(0.1540829241), tensor(0.0039196229), tensor(0.0944719985), tensor(-0.0391886719), tensor(-0.0426419973)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.6246821880,  0.1427276731,  0.3255804479,  0.1864454746,\n",
            "        -0.0735616684,  0.7875939608,  0.6089542508,  0.1947915554,\n",
            "         0.0900525451,  0.8578172922,  0.7456289530,  0.5519217253,\n",
            "         0.4239630103,  1.0639309883,  0.6040770411,  0.8074913025,\n",
            "         0.5794402957,  0.4786110520,  0.4041149616,  0.4471966028])\n",
            "btensor.grad: tensor([-0.6494202018, -0.4291843176,  0.1462765932, -0.2650346756,\n",
            "         0.0963135362, -0.2530748844,  0.0714763701,  0.5146809816,\n",
            "         0.9279748201, -0.6398836374, -0.2686982155, -0.5094534159,\n",
            "         0.0554997623, -0.3199249506, -0.3036882281,  0.3173694611,\n",
            "         0.4061501622,  0.3626132011, -0.1810591221, -0.0235626698])\n",
            "ctensor.grad: tensor([-3.7321898341e-01, -2.6748538971e+01, -2.5670640945e+01,\n",
            "         7.8470784426e-01, -2.0398298278e-02,  2.7428594232e-01,\n",
            "        -3.5353249311e-01, -4.1114160419e-01, -1.8518695831e+01,\n",
            "         5.3129649162e-01, -4.9172115326e+00,  9.5159854889e+00,\n",
            "         9.8593969345e+00, -1.7825353622e+01,  5.0368249416e-01,\n",
            "        -4.9935717583e+00,  8.8856267929e+00,  9.3024978638e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1059.0384521484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6008644104), tensor(0.9674913287), tensor(1.0669492483), tensor(1.1828831434), tensor(1.0926142931), tensor(1.3868745565), tensor(1.0247114897), tensor(1.1568930149), tensor(0.9067673087), tensor(1.1913498640), tensor(1.0485036373), tensor(1.4744478464), tensor(0.9528852105), tensor(1.2086385489), tensor(1.1397885084), tensor(0.9406737089), tensor(1.0489096642), tensor(0.8691796064), tensor(1.0706741810), tensor(0.9768202305)]\n",
            "b:  [tensor(0.6281650066), tensor(1.1039416790), tensor(1.3320935965), tensor(1.0156743526), tensor(1.4641181231), tensor(0.8209884763), tensor(1.2341351509), tensor(1.1382875443), tensor(1.7427208424), tensor(0.8777796030), tensor(1.2750844955), tensor(0.7239345908), tensor(1.3042676449), tensor(1.2614467144), tensor(0.8841142058), tensor(0.9797311425), tensor(1.2800915241), tensor(1.6511267424), tensor(0.9878455997), tensor(1.1765092611)]\n",
            "c:  [tensor(-0.0091439988), tensor(0.2371853292), tensor(0.2243090272), tensor(-0.0144743156), tensor(-0.0017245135), tensor(-0.0118854241), tensor(0.0040629921), tensor(0.0046809516), tensor(0.1762441844), tensor(-0.0016605165), tensor(0.0915072039), tensor(-0.0453424752), tensor(-0.0488876440), tensor(0.1632089913), tensor(0.0036566858), tensor(0.0967822745), tensor(-0.0438646674), tensor(-0.0472264253)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.6636289358,  0.1153616458,  0.3141673207,  0.1782201529,\n",
            "        -0.1107342541,  0.8424068689,  0.6318045855,  0.1881899834,\n",
            "         0.0550977588,  0.8870508671,  0.7702739835,  0.5819432139,\n",
            "         0.4121254683,  1.1282186508,  0.6173183918,  0.8313606977,\n",
            "         0.5986145735,  0.4857735038,  0.3943778276,  0.4400402308])\n",
            "btensor.grad: tensor([-0.7125603557, -0.5006059408,  0.1316260695, -0.3227829337,\n",
            "         0.0637403280, -0.2875579596,  0.0479048193,  0.5193625689,\n",
            "         0.9556179047, -0.7126991749, -0.3201341629, -0.5543720722,\n",
            "         0.0211755335, -0.3553283215, -0.3489066958,  0.3087535501,\n",
            "         0.3963243961,  0.3475567102, -0.2210338116, -0.0668420792])\n",
            "ctensor.grad: tensor([-4.3401062489e-01, -2.7277057648e+01, -2.6174589157e+01,\n",
            "         7.4869841337e-01, -1.9895039499e-02,  2.4094164371e-01,\n",
            "        -3.5083311796e-01, -3.9234924316e-01, -1.8971584320e+01,\n",
            "         5.5241119862e-01, -4.5800294876e+00,  9.9587297440e+00,\n",
            "         9.5725574493e+00, -1.8252138138e+01,  5.2587419748e-01,\n",
            "        -4.6205506325e+00,  9.3519906998e+00,  9.1688528061e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1057.7105712891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5973373652), tensor(0.9670473337), tensor(1.0654295683), tensor(1.1820192337), tensor(1.0933462381), tensor(1.3823773861), tensor(1.0214304924), tensor(1.1559811831), tensor(0.9066556692), tensor(1.1867736578), tensor(1.0445288420), tensor(1.4713826180), tensor(0.9508896470), tensor(1.2026715279), tensor(1.1366328001), tensor(0.9364029169), tensor(1.0458091497), tensor(0.8667157292), tensor(1.0687522888), tensor(0.9746540785)]\n",
            "b:  [tensor(0.6320293546), tensor(1.1067844629), tensor(1.3315035105), tensor(1.0175708532), tensor(1.4639590979), tensor(0.8225852251), tensor(1.2340103388), tensor(1.1356691122), tensor(1.7378100157), tensor(0.8816887736), tensor(1.2769299746), tensor(0.7269183397), tensor(1.3043289185), tensor(1.2633826733), tensor(0.8860734701), tensor(0.9782244563), tensor(1.2781593800), tensor(1.6494708061), tensor(0.9891399145), tensor(1.1770520210)]\n",
            "c:  [tensor(-0.0088903913), tensor(0.2510342300), tensor(0.2375979573), tensor(-0.0148295900), tensor(-0.0017148482), tensor(-0.0119890338), tensor(0.0042363098), tensor(0.0048669023), tensor(0.1859214306), tensor(-0.0019470002), tensor(0.0935924649), tensor(-0.0505304858), tensor(-0.0534760803), tensor(0.1725221574), tensor(0.0033830623), tensor(0.0988816619), tensor(-0.0487578586), tensor(-0.0516773015)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7054135799,  0.0887977332,  0.3039460480,  0.1727756262,\n",
            "        -0.1463778317,  0.8994457722,  0.6561954618,  0.1823625565,\n",
            "         0.0223230124,  0.9152426720,  0.7949622273,  0.6130518913,\n",
            "         0.3991087079,  1.1934074163,  0.6311504841,  0.8541539907,\n",
            "         0.6201143861,  0.4927747846,  0.3843749166,  0.4332305193])\n",
            "btensor.grad: tensor([-0.7728638649, -0.5685501099,  0.1180131435, -0.3792977333,\n",
            "         0.0317953378, -0.3193457127,  0.0249667764,  0.5236798525,\n",
            "         0.9821542501, -0.7818324566, -0.3690854311, -0.5967483521,\n",
            "        -0.0122576952, -0.3871991634, -0.3918575644,  0.3013336062,\n",
            "         0.3864403367,  0.3311954737, -0.2588597536, -0.1085548401])\n",
            "ctensor.grad: tensor([-5.0721508265e-01, -2.7697809219e+01, -2.6577856064e+01,\n",
            "         7.1054881811e-01, -1.9330456853e-02,  2.0721888542e-01,\n",
            "        -3.4663540125e-01, -3.7190124393e-01, -1.9354482651e+01,\n",
            "         5.7296729088e-01, -4.1705250740e+00,  1.0376022339e+01,\n",
            "         9.1768741608e+00, -1.8626333237e+01,  5.4724675417e-01,\n",
            "        -4.1987795830e+00,  9.7863826752e+00,  8.9017534256e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1056.3454589844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5935899019), tensor(0.9667318463), tensor(1.0639547110), tensor(1.1811679602), tensor(1.0942468643), tensor(1.3775891066), tensor(1.0180232525), tensor(1.1550962925), tensor(0.9066948295), tensor(1.1820660830), tensor(1.0404345989), tensor(1.4681605101), tensor(0.9489672184), tensor(1.1963801384), tensor(1.1334074736), tensor(0.9320286512), tensor(1.0425916910), tensor(0.8642210960), tensor(1.0668834448), tensor(0.9725210667)]\n",
            "b:  [tensor(0.6361784339), tensor(1.1099450588), tensor(1.3309772015), tensor(1.0197417736), tensor(1.4639558792), tensor(0.8243271112), tensor(1.2339978218), tensor(1.1330345869), tensor(1.7327760458), tensor(0.8859210014), tensor(1.2790049314), tensor(0.7301002145), tensor(1.3045520782), tensor(1.2654591799), tensor(0.8882346153), tensor(0.9767503738), tensor(1.2762778997), tensor(1.6479048729), tensor(0.9906113744), tensor(1.1777938604)]\n",
            "c:  [tensor(-0.0085953269), tensor(0.2650383115), tensor(0.2510366738), tensor(-0.0151646947), tensor(-0.0017055015), tensor(-0.0120758461), tensor(0.0044065844), tensor(0.0050417557), tensor(0.1957531869), tensor(-0.0022435763), tensor(0.0954360589), tensor(-0.0559128560), tensor(-0.0578180030), tensor(0.1819929034), tensor(0.0030990695), tensor(0.1007434875), tensor(-0.0538512208), tensor(-0.0559347309)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7494969368,  0.0631022304,  0.2949802279,  0.1702572107,\n",
            "        -0.1801285744,  0.9576605558,  0.6814510226,  0.1769698858,\n",
            "        -0.0078348517,  0.9415037036,  0.8188604116,  0.6444125175,\n",
            "         0.3844824433,  1.2582786083,  0.6450725794,  0.8748515844,\n",
            "         0.6434866786,  0.4989324808,  0.3737795353,  0.4266080856])\n",
            "btensor.grad: tensor([-8.2981240749e-01, -6.3211894035e-01,  1.0526561737e-01,\n",
            "        -4.3419271708e-01,  6.5238773823e-04, -3.4837830067e-01,\n",
            "         2.5033950806e-03,  5.2690982819e-01,  1.0067883730e+00,\n",
            "        -8.4643989801e-01, -4.1500258446e-01, -6.3637995720e-01,\n",
            "        -4.4624030590e-02, -4.1531014442e-01, -4.3223494291e-01,\n",
            "         2.9481589794e-01,  3.7628942728e-01,  3.1318998337e-01,\n",
            "        -2.9429256916e-01, -1.4837050438e-01])\n",
            "ctensor.grad: tensor([-5.9012794495e-01, -2.8008140564e+01, -2.6877454758e+01,\n",
            "         6.7020922899e-01, -1.8693597987e-02,  1.7362435162e-01,\n",
            "        -3.4054890275e-01, -3.4970691800e-01, -1.9663524628e+01,\n",
            "         5.9315228462e-01, -3.6871819496e+00,  1.0764737129e+01,\n",
            "         8.6838474274e+00, -1.8941484451e+01,  5.6798547506e-01,\n",
            "        -3.7236473560e+00,  1.0186723709e+01,  8.5148591995e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1054.9510498047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5896136761), tensor(0.9665402770), tensor(1.0625184774), tensor(1.1803144217), tensor(1.0953052044), tensor(1.3725095987), tensor(1.0144892931), tensor(1.1542382240), tensor(0.9068701267), tensor(1.1772414446), tensor(1.0362291336), tensor(1.4647849798), tensor(0.9471282363), tensor(1.1897721291), tensor(1.1301146746), tensor(0.9275664091), tensor(1.0392509699), tensor(0.8617033958), tensor(1.0650721788), tensor(0.9704208970)]\n",
            "b:  [tensor(0.6405931711), tensor(1.1133980751), tensor(1.3305116892), tensor(1.0221776962), tensor(1.4641039371), tensor(0.8262004852), tensor(1.2340964079), tensor(1.1303930283), tensor(1.7276328802), tensor(0.8904500008), tensor(1.2812924385), tensor(0.7334656715), tensor(1.3049312830), tensor(1.2676570415), tensor(0.8905836344), tensor(0.9753060937), tensor(1.2744497061), tensor(1.6464393139), tensor(0.9922471642), tensor(1.1787240505)]\n",
            "c:  [tensor(-0.0082555152), tensor(0.2791427672), tensor(0.2645734251), tensor(-0.0154785924), tensor(-0.0016965133), tensor(-0.0121462336), tensor(0.0045727035), tensor(0.0052046450), tensor(0.2057022154), tensor(-0.0025501586), tensor(0.0970022753), tensor(-0.0614733770), tensor(-0.0618701205), tensor(0.1915896237), tensor(0.0028049138), tensor(0.1023400873), tensor(-0.0591266043), tensor(-0.0599462353)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7952427864,  0.0383089483,  0.2872466445,  0.1707179546,\n",
            "        -0.2116653323,  1.0158967972,  0.7068006396,  0.1716245413,\n",
            "        -0.0350563526,  0.9649293423,  0.8410877585,  0.6751078367,\n",
            "         0.3677961826,  1.3215932846,  0.6585512161,  0.8924502134,\n",
            "         0.6681469679,  0.5035389662,  0.3622458577,  0.4200304747])\n",
            "btensor.grad: tensor([-0.8829495907, -0.6906050444,  0.0931135416, -0.4871950150,\n",
            "        -0.0296180695, -0.3746798038, -0.0197192430,  0.5283025503,\n",
            "         1.0286253691, -0.9058046341, -0.4574955702, -0.6730900407,\n",
            "        -0.0758468509, -0.4395638704, -0.4698075652,  0.2888547182,\n",
            "         0.3656340241,  0.2931149006, -0.3271595240, -0.1860457659])\n",
            "ctensor.grad: tensor([-6.7962259054e-01, -2.8208938599e+01, -2.7073528290e+01,\n",
            "         6.2779539824e-01, -1.7976500094e-02,  1.4077523351e-01,\n",
            "        -3.3223849535e-01, -3.2577869296e-01, -1.9898040771e+01,\n",
            "         6.1316466331e-01, -3.1324265003e+00,  1.1121045113e+01,\n",
            "         8.1042346954e+00, -1.9193431854e+01,  5.8831137419e-01,\n",
            "        -3.1931982040e+00,  1.0550765991e+01,  8.0230093002e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1053.5382080078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5854036808), tensor(0.9664679170), tensor(1.0611150265), tensor(1.1794437170), tensor(1.0965086222), tensor(1.3671445847), tensor(1.0108318329), tensor(1.1534085274), tensor(0.9071655273), tensor(1.1723177433), tensor(1.0319252014), tensor(1.4612636566), tensor(0.9453849196), tensor(1.1828612089), tensor(1.1267592907), tensor(0.9230361581), tensor(1.0357836485), tensor(0.8591735363), tensor(1.0633250475), tensor(0.9683538675)]\n",
            "b:  [tensor(0.6452524662), tensor(1.1171152592), tensor(1.3301053047), tensor(1.0248680115), tensor(1.4643990993), tensor(0.8281921148), tensor(1.2343062162), tensor(1.1277573109), tensor(1.7223989964), tensor(0.8952466249), tensor(1.2837737799), tensor(0.7369992733), tensor(1.3054609299), tensor(1.2699568272), tensor(0.8931055069), tensor(0.9738905430), tensor(1.2726783752), tensor(1.6450865269), tensor(0.9940338135), tensor(1.1798311472)]\n",
            "c:  [tensor(-0.0078692101), tensor(0.2932947576), tensor(0.2781579196), tensor(-0.0157703701), tensor(-0.0016879259), tensor(-0.0122009004), tensor(0.0047334312), tensor(0.0053547760), tensor(0.2157320380), tensor(-0.0028667639), tensor(0.0982580036), tensor(-0.0671938136), tensor(-0.0655944124), tensor(0.2012798786), tensor(0.0025006826), tensor(0.1036441028), tensor(-0.0645646751), tensor(-0.0636669248)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8419985771,  0.0144705176,  0.2806915343,  0.1741387844,\n",
            "        -0.2406886518,  1.0729938745,  0.7314844131,  0.1659512520,\n",
            "        -0.0590787530,  0.9847419858,  0.8607909679,  0.7042587996,\n",
            "         0.3486633301,  1.3821895123,  0.6710785627,  0.9060529470,\n",
            "         0.6934630871,  0.5059731007,  0.3494200706,  0.4134099483])\n",
            "btensor.grad: tensor([-0.9318552613, -0.7434374094,  0.0812659860, -0.5380709171,\n",
            "        -0.0590226352, -0.3983273506, -0.0419505835,  0.5271500349,\n",
            "         1.0467869043, -0.9593276978, -0.4962599277, -0.7067235708,\n",
            "        -0.1059176326, -0.4599660635, -0.5043789148,  0.2831140757,\n",
            "         0.3542578816,  0.2705500126, -0.3573347330, -0.2214089036])\n",
            "ctensor.grad: tensor([-7.7261114120e-01, -2.8303956985e+01, -2.7168972015e+01,\n",
            "         5.8355706930e-01, -1.7174659297e-02,  1.0933270305e-01,\n",
            "        -3.2145562768e-01, -3.0026224256e-01, -2.0059654236e+01,\n",
            "         6.3321042061e-01, -2.5114586353e+00,  1.1440871239e+01,\n",
            "         7.4485769272e+00, -1.9380517960e+01,  6.0846215487e-01,\n",
            "        -2.6080274582e+00,  1.0876148224e+01,  7.4413776398e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1052.1160888672, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5809580088), tensor(0.9665093422), tensor(1.0597386360), tensor(1.1785413027), tensor(1.0978430510), tensor(1.3615053892), tensor(1.0070576668), tensor(1.1526104212), tensor(0.9075638056), tensor(1.1673161983), tensor(1.0275390148), tensor(1.4576084614), tensor(0.9437508583), tensor(1.1756658554), tensor(1.1233481169), tensor(0.9184613824), tensor(1.0321894884), tensor(0.8566448092), tensor(1.0616497993), tensor(0.9663199782)]\n",
            "b:  [tensor(0.6501332521), tensor(1.1210659742), tensor(1.3297580481), tensor(1.0278010368), tensor(1.4648368359), tensor(0.8302891850), tensor(1.2346282005), tensor(1.1251429319), tensor(1.7170965672), tensor(0.9002792239), tensor(1.2864291668), tensor(0.7406850457), tensor(1.3061351776), tensor(1.2723398209), tensor(0.8957844377), tensor(0.9725039601), tensor(1.2709683180), tensor(1.6438609362), tensor(0.9959573746), tensor(1.1811025143)]\n",
            "c:  [tensor(-0.0074360436), tensor(0.3074444532), tensor(0.2917425036), tensor(-0.0160392914), tensor(-0.0016797825), tensor(-0.0122408690), tensor(0.0048874607), tensor(0.0054914947), tensor(0.2258078456), tensor(-0.0031935144), tensor(0.0991734937), tensor(-0.0730539784), tensor(-0.0689582303), tensor(0.2110314369), tensor(0.0021863421), tensor(0.1046295762), tensor(-0.0701448917), tensor(-0.0670593902)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8891379833, -0.0082808137,  0.2752894759,  0.1804816723,\n",
            "        -0.2668885291,  1.1278489828,  0.7548351884,  0.1596173048,\n",
            "        -0.0796521902,  1.0002996922,  0.8772338629,  0.7310308218,\n",
            "         0.3268092275,  1.4390599728,  0.6822372079,  0.9149532318,\n",
            "         0.7188223600,  0.5057486296,  0.3350574970,  0.4067735672])\n",
            "btensor.grad: tensor([-0.9761588573, -0.7901504040,  0.0694443583, -0.5866155028,\n",
            "        -0.0875544995, -0.4194158316, -0.0644080043,  0.5228815079,\n",
            "         1.0604758263, -1.0065157413, -0.5310798883, -0.7371486425,\n",
            "        -0.1348408759, -0.4765881300, -0.5357915163,  0.2773188949,\n",
            "         0.3420014977,  0.2451195717, -0.3847130537, -0.2542773485])\n",
            "ctensor.grad: tensor([-8.6633330584e-01, -2.8299417496e+01, -2.7169155121e+01,\n",
            "         5.3784185648e-01, -1.6286810860e-02,  7.9937152565e-02,\n",
            "        -3.0805945396e-01, -2.7343732119e-01, -2.0151611328e+01,\n",
            "         6.5350085497e-01, -1.8309754133e+00,  1.1720325470e+01,\n",
            "         6.7276406288e+00, -1.9503108978e+01,  6.2868106365e-01,\n",
            "        -1.9709402323e+00,  1.1160430908e+01,  6.7849321365e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1050.6932373047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5762776136), tensor(0.9666581154), tensor(1.0583834648), tensor(1.1775928736), tensor(1.0992928743), tensor(1.3556082249), tensor(1.0031759739), tensor(1.1518485546), tensor(0.9080465436), tensor(1.1622605324), tensor(1.0230900049), tensor(1.4538350105), tensor(0.9422403574), tensor(1.1682090759), tensor(1.1198894978), tensor(0.9138680696), tensor(1.0284711123), tensor(0.8541321754), tensor(1.0600547791), tensor(0.9643187523)]\n",
            "b:  [tensor(0.6552110314), tensor(1.1252179146), tensor(1.3294708729), tensor(1.0309642553), tensor(1.4654129744), tensor(0.8324795961), tensor(1.2350646257), tensor(1.1225676537), tensor(1.7117515802), tensor(0.9055141807), tensor(1.2892382145), tensor(0.7445063591), tensor(1.3069483042), tensor(1.2747875452), tensor(0.8986040354), tensor(0.9711477160), tensor(1.2693243027), tensor(1.6427781582), tensor(0.9980034828), tensor(1.1825251579)]\n",
            "c:  [tensor(-0.0069568204), tensor(0.3215462267), tensor(0.3052832484), tensor(-0.0162848197), tensor(-0.0016721253), tensor(-0.0122674555), tensor(0.0050334688), tensor(0.0056143440), tensor(0.2358970046), tensor(-0.0035306378), tensor(0.0997228771), tensor(-0.0790318921), tensor(-0.0719344541), tensor(0.2208130807), tensor(0.0018617404), tensor(0.1052728817), tensor(-0.0758454651), tensor(-0.0700933561)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9360718727, -0.0297517926,  0.2710394859,  0.1896891594,\n",
            "        -0.2899535596,  1.1794406176,  0.7763278484,  0.1523768902,\n",
            "        -0.0965483785,  1.0111422539,  0.8898016810,  0.7546997666,\n",
            "         0.3020992875,  1.4913610220,  0.6917290688,  0.9186590910,\n",
            "         0.7436786294,  0.5025295615,  0.3190149665,  0.4002435207])\n",
            "btensor.grad: tensor([-1.0155537128, -0.8303992748,  0.0574238896, -0.6326403618,\n",
            "        -0.1152242422, -0.4380770922, -0.0872964263,  0.5150613785,\n",
            "         1.0690039396, -1.0469889641, -0.5618060827, -0.7642644048,\n",
            "        -0.1626316011, -0.4895354509, -0.5639165640,  0.2712500095,\n",
            "         0.3287968040,  0.2165513039, -0.4092187881, -0.2845244408])\n",
            "ctensor.grad: tensor([-9.5844602585e-01, -2.8203536987e+01, -2.7081497192e+01,\n",
            "         4.9105605483e-01, -1.5314448625e-02,  5.3172193468e-02,\n",
            "        -2.9201647639e-01, -2.4569837749e-01, -2.0178302765e+01,\n",
            "         6.7424666882e-01, -1.0987614393e+00,  1.1955832481e+01,\n",
            "         5.9524412155e+00, -1.9563274384e+01,  6.4920359850e-01,\n",
            "        -1.2866038084e+00,  1.1401144028e+01,  6.0679259300e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1049.2766113281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5713661909), tensor(0.9669065475), tensor(1.0570435524), tensor(1.1765843630), tensor(1.1008406878), tensor(1.3494739532), tensor(0.9991980791), tensor(1.1511281729), tensor(0.9085942507), tensor(1.1571755409), tensor(1.0185998678), tensor(1.4499616623), tensor(0.9408676028), tensor(1.1605167389), tensor(1.1163926125), tensor(0.9092834592), tensor(1.0246332884), tensor(0.8516513705), tensor(1.0585485697), tensor(0.9623485208)]\n",
            "b:  [tensor(0.6604600549), tensor(1.1295375824), tensor(1.3292458057), tensor(1.0343441963), tensor(1.4661229849), tensor(0.8347518444), tensor(1.2356183529), tensor(1.1200506687), tensor(1.7063924074), tensor(0.9109165668), tensor(1.2921799421), tensor(0.7484464049), tensor(1.3078948259), tensor(1.2772823572), tensor(0.9015473127), tensor(0.9698239565), tensor(1.2677510977), tensor(1.6418548822), tensor(1.0001574755), tensor(1.1840852499)]\n",
            "c:  [tensor(-0.0064332355), tensor(0.3355592191), tensor(0.3187406957), tensor(-0.0165066328), tensor(-0.0016649948), tensor(-0.0122822179), tensor(0.0051701684), tensor(0.0057231053), tensor(0.2459693998), tensor(-0.0038784656), tensor(0.0998844355), tensor(-0.0851040706), tensor(-0.0745015591), tensor(0.2305953354), tensor(0.0015266140), tensor(0.1055534035), tensor(-0.0816434771), tensor(-0.0727453232)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9822750092, -0.0496910959,  0.2679715455,  0.2017042637,\n",
            "        -0.3095696867,  1.2268662453,  0.7955788970,  0.1440809965,\n",
            "        -0.1095433831,  1.0170040131,  0.8980269432,  0.7746670246,\n",
            "         0.2745505571,  1.5384603739,  0.6993839145,  0.9169259071,\n",
            "         0.7675679326,  0.4961613417,  0.3012488484,  0.3940508366])\n",
            "btensor.grad: tensor([-1.0497999191, -0.8639419079,  0.0450148582, -0.6759762764,\n",
            "        -0.1420022845, -0.4544506073, -0.1107484102,  0.5034072399,\n",
            "         1.0718452930, -1.0804786682, -0.5883439779, -0.7880114317,\n",
            "        -0.1892993301, -0.4989690781, -0.5886523724,  0.2647519708,\n",
            "         0.3146404326,  0.1846661568, -0.4308030605, -0.3120144606])\n",
            "ctensor.grad: tensor([-1.0471700430e+00, -2.8026004791e+01, -2.6914882660e+01,\n",
            "         4.4362708926e-01, -1.4261211269e-02,  2.9524333775e-02,\n",
            "        -2.7339914441e-01, -2.1752242744e-01, -2.0144784927e+01,\n",
            "         6.9565564394e-01, -3.2312190533e-01,  1.2144352913e+01,\n",
            "         5.1342077255e+00, -1.9564517975e+01,  6.7025285959e-01,\n",
            "        -5.6103658676e-01,  1.1596031189e+01,  5.3039336205e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1047.8721923828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5662298203), tensor(0.9672455788), tensor(1.0557128191), tensor(1.1755021811), tensor(1.1024678946), tensor(1.3431271315), tensor(0.9951361418), tensor(1.1504548788), tensor(0.9091864228), tensor(1.1520866156), tensor(1.0140918493), tensor(1.4460092783), tensor(0.9396460652), tensor(1.1526172161), tensor(1.1128668785), tensor(0.9047347903), tensor(1.0206826925), tensor(0.8492181897), tensor(1.0571393967), tensor(0.9604060054)]\n",
            "b:  [tensor(0.6658537984), tensor(1.1339906454), tensor(1.3290854692), tensor(1.0379266739), tensor(1.4669622183), tensor(0.8370953798), tensor(1.2362927198), tensor(1.1176117659), tensor(1.7010493279), tensor(0.9164507985), tensor(1.2952332497), tensor(0.7524883151), tensor(1.3089691401), tensor(1.2798076868), tensor(0.9045970440), tensor(0.9685353041), tensor(1.2662531137), tensor(1.6411079168), tensor(1.0024048090), tensor(1.1857684851)]\n",
            "c:  [tensor(-0.0058676223), tensor(0.3494479358), tensor(0.3320802748), tensor(-0.0167046208), tensor(-0.0016584287), tensor(-0.0122869033), tensor(0.0052963537), tensor(0.0058178185), tensor(0.2559976578), tensor(-0.0042374292), tensor(0.0996407047), tensor(-0.0912457556), tensor(-0.0766436905), tensor(0.2403509170), tensor(0.0011805960), tensor(0.1054540202), tensor(-0.0875149816), tensor(-0.0749981776)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0272855759, -0.0678056031,  0.2661495805,  0.2164466381,\n",
            "        -0.3254427612,  1.2693639994,  0.8123902678,  0.1346610785,\n",
            "        -0.1184304953,  1.0177791119,  0.9015934467,  0.7904669046,\n",
            "         0.2443123460,  1.5799032450,  0.7051526904,  0.9097347260,\n",
            "         0.7901303172,  0.4866367579,  0.2818360925,  0.3885054588])\n",
            "btensor.grad: tensor([-1.0787456036, -0.8906205893,  0.0320754051, -0.7164841890,\n",
            "        -0.1678571850, -0.4687124491, -0.1348763108,  0.4877890348,\n",
            "         1.0686055422, -1.1068434715, -0.6106609106, -0.8083814383,\n",
            "        -0.2148549855, -0.5050768852, -0.6099501252,  0.2577272654,\n",
            "         0.2996014655,  0.1493978500, -0.4494563341, -0.3366404772])\n",
            "ctensor.grad: tensor([-1.1312265396e+00, -2.7777460098e+01, -2.6679159164e+01,\n",
            "         3.9597776532e-01, -1.3132108375e-02,  9.3715861440e-03,\n",
            "        -2.5237041712e-01, -1.8942594528e-01, -2.0056514740e+01,\n",
            "         7.1792674065e-01,  4.8746883869e-01,  1.2283367157e+01,\n",
            "         4.2842607498e+00, -1.9511156082e+01,  6.9203609228e-01,\n",
            "         1.9876691699e-01,  1.1743000984e+01,  4.5057029724e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1046.4873046875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5608762503), tensor(0.9676644802), tensor(1.0543844700), tensor(1.1743330956), tensor(1.1041544676), tensor(1.3365956545), tensor(0.9910026193), tensor(1.1498341560), tensor(0.9098016024), tensor(1.1470189095), tensor(1.0095902681), tensor(1.4420005083), tensor(0.9385879040), tensor(1.1445401907), tensor(1.1093213558), tensor(0.9002484083), tensor(1.0166271925), tensor(0.8468477726), tensor(1.0558346510), tensor(0.9584862590)]\n",
            "b:  [tensor(0.6713653803), tensor(1.1385426521), tensor(1.3289929628), tensor(1.0416970253), tensor(1.4679259062), tensor(0.8395006657), tensor(1.2370915413), tensor(1.1152707338), tensor(1.6957541704), tensor(0.9220810533), tensor(1.2983771563), tensor(0.7566154003), tensor(1.3101656437), tensor(1.2823480368), tensor(0.9077361822), tensor(0.9672846794), tensor(1.2648341656), tensor(1.6405539513), tensor(1.0047308207), tensor(1.1875600815)]\n",
            "c:  [tensor(-0.0052627092), tensor(0.3631824553), tensor(0.3452727199), tensor(-0.0168788712), tensor(-0.0016524623), tensor(-0.0122833923), tensor(0.0054109381), tensor(0.0058987821), tensor(0.2659572959), tensor(-0.0046080532), tensor(0.0989784822), tensor(-0.0974311754), tensor(-0.0783505216), tensor(0.2500550449), tensor(0.0008232248), tensor(0.1049614027), tensor(-0.0934351087), tensor(-0.0768407956)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0707048178, -0.0837853849,  0.2656615674,  0.2338209152,\n",
            "        -0.3373067379,  1.3062909842,  0.8267023563,  0.1241382360,\n",
            "        -0.1230340004,  1.0135295391,  0.9003213644,  0.8017581701,\n",
            "         0.2116353512,  1.6154168844,  0.7091110349,  0.8972756863,\n",
            "         0.8111035228,  0.4740861654,  0.2609430552,  0.3839535713])\n",
            "btensor.grad: tensor([-1.1023156643, -0.9103964567,  0.0185065866, -0.7540593147,\n",
            "        -0.1927307695, -0.4810568094, -0.1597652435,  0.4681981802,\n",
            "         1.0590387583, -1.1260560751, -0.6287904978, -0.8254203796,\n",
            "        -0.2393124402, -0.5080792904, -0.6278238297,  0.2501295805,\n",
            "         0.2837788463,  0.1107859612, -0.4652100801, -0.3583310843])\n",
            "ctensor.grad: tensor([-1.2098264694e+00, -2.7469032288e+01, -2.6384891510e+01,\n",
            "         3.4850209951e-01, -1.1932818219e-02, -7.0229573175e-03,\n",
            "        -2.2916893661e-01, -1.6192692518e-01, -1.9919286728e+01,\n",
            "         7.4124777317e-01,  1.3244510889e+00,  1.2370832443e+01,\n",
            "         3.4136593342e+00, -1.9408252716e+01,  7.1474218369e-01,\n",
            "         9.8524057865e-01,  1.1840248108e+01,  3.6852350235e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1045.1230468750, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5553152561), tensor(0.9681511521), tensor(1.0530514717), tensor(1.1730645895), tensor(1.1058791876), tensor(1.3299098015), tensor(0.9868096709), tensor(1.1492711306), tensor(0.9104177356), tensor(1.1419966221), tensor(1.0051194429), tensor(1.4379588366), tensor(0.9377035499), tensor(1.1363157034), tensor(1.1057641506), tensor(0.8958487511), tensor(1.0124754906), tensor(0.8445540667), tensor(1.0546405315), tensor(0.9565824270)]\n",
            "b:  [tensor(0.6769679785), tensor(1.1431592703), tensor(1.3289718628), tensor(1.0456401110), tensor(1.4690086842), tensor(0.8419591784), tensor(1.2380188704), tensor(1.1130470037), tensor(1.6905390024), tensor(0.9277720451), tensor(1.3015912771), tensor(0.7608115673), tensor(1.3114790916), tensor(1.2848891020), tensor(0.9109477401), tensor(0.9660750628), tensor(1.2634975910), tensor(1.6402090788), tensor(1.0071215630), tensor(1.1894453764)]\n",
            "c:  [tensor(-0.0046214131), tensor(0.3767384291), tensor(0.3582940698), tensor(-0.0170296486), tensor(-0.0016471278), tensor(-0.0122736366), tensor(0.0055129835), tensor(0.0059665353), tensor(0.2758266628), tensor(-0.0049909498), tensor(0.0978888124), tensor(-0.1036338136), tensor(-0.0796171799), tensor(0.2596855760), tensor(0.0004539541), tensor(0.1040661559), tensor(-0.0993782803), tensor(-0.0782677531)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1121925116, -0.0973400921,  0.2666011751,  0.2537064552,\n",
            "        -0.3449378312,  1.3371661901,  0.8385952711,  0.1126062870,\n",
            "        -0.1232322454,  1.0044554472,  0.8941730261,  0.8083416820,\n",
            "         0.1768748760,  1.6448967457,  0.7114304900,  0.8799293041,\n",
            "         0.8303353786,  0.4587417841,  0.2388189435,  0.3807715178])\n",
            "btensor.grad: tensor([-1.1205140352, -0.9233120680,  0.0042301416, -0.7886247635,\n",
            "        -0.2165482044, -0.4916974306, -0.1854709387,  0.4447382689,\n",
            "         1.0430339575, -1.1381952763, -0.6428136826, -0.8392330408,\n",
            "        -0.2626960278, -0.5082188845, -0.6423168182,  0.2419208884,\n",
            "         0.2673065066,  0.0689674616, -0.4781502485, -0.3770558238])\n",
            "ctensor.grad: tensor([-1.2825921774e+00, -2.7111946106e+01, -2.6042675018e+01,\n",
            "         3.0155360699e-01, -1.0669093579e-02, -1.9510632381e-02,\n",
            "        -2.0409075916e-01, -1.3550639153e-01, -1.9738706589e+01,\n",
            "         7.6579308510e-01,  2.1793346405e+00,  1.2405271530e+01,\n",
            "         2.5333213806e+00, -1.9261074066e+01,  7.3854142427e-01,\n",
            "         1.7904871702e+00,  1.1886347771e+01,  2.8539183140e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1043.7843017578, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5495579243), tensor(0.9686921835), tensor(1.0517061949), tensor(1.1716848612), tensor(1.1076201200), tensor(1.3231016397), tensor(0.9825683236), tensor(1.1487700939), tensor(0.9110125899), tensor(1.1370422840), tensor(1.0007034540), tensor(1.4339082241), tensor(0.9370014071), tensor(1.1279739141), tensor(1.1022022963), tensor(0.8915576339), tensor(1.0082366467), tensor(0.8423494697), tensor(1.0535616875), tensor(0.9546858072)]\n",
            "b:  [tensor(0.6826351285), tensor(1.1478067636), tensor(1.3290258646), tensor(1.0497407913), tensor(1.4702049494), tensor(0.8444635272), tensor(1.2390791178), tensor(1.1109590530), tensor(1.6854360104), tensor(0.9334892631), tensor(1.3048557043), tensor(0.7650614381), tensor(1.3129042387), tensor(1.2874180079), tensor(0.9142155051), tensor(0.9649095535), tensor(1.2622460127), tensor(1.6400882006), tensor(1.0095635653), tensor(1.1914094687)]\n",
            "c:  [tensor(-0.0039466643), tensor(0.3900969326), tensor(0.3711255491), tensor(-0.0171573684), tensor(-0.0016424546), tensor(-0.0122596156), tensor(0.0056017186), tensor(0.0060218233), tensor(0.2855868936), tensor(-0.0053868112), tensor(0.0963669345), tensor(-0.1098266169), tensor(-0.0804440752), tensor(0.2692230940), tensor(7.2160444688e-05), tensor(0.1027629450), tensor(-0.1053183526), tensor(-0.0792789608)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1514682770, -0.1082094461,  0.2690665126,  0.2759532928,\n",
            "        -0.3481811583,  1.3616242409,  0.8482643366,  0.1002160311,\n",
            "        -0.1189656258,  0.9908632040,  0.8832094073,  0.8101270199,\n",
            "         0.1404291391,  1.6683558226,  0.7123603821,  0.8582249880,\n",
            "         0.8477582932,  0.4409222007,  0.2157737613,  0.3793247938])\n",
            "btensor.grad: tensor([-1.1334310770, -0.9295011759, -0.0108050108, -0.8201467395,\n",
            "        -0.2392440587, -0.5008749962, -0.2120473981,  0.4175997972,\n",
            "         1.0205868483, -1.1434488297, -0.6528824568, -0.8499715328,\n",
            "        -0.2850408852, -0.5057696104, -0.6535578966,  0.2330999970,\n",
            "         0.2503250241,  0.0241737366, -0.4884064198, -0.3928194046])\n",
            "ctensor.grad: tensor([-1.3494975567e+00, -2.6717004776e+01, -2.5662939072e+01,\n",
            "         2.5543874502e-01, -9.3462094665e-03, -2.8042057529e-02,\n",
            "        -1.7747022212e-01, -1.1057558656e-01, -1.9520469666e+01,\n",
            "         7.9172277451e-01,  3.0437572002e+00,  1.2385609627e+01,\n",
            "         1.6537958384e+00, -1.9075021744e+01,  7.6358729601e-01,\n",
            "         2.6064267159e+00,  1.1880137444e+01,  2.0224168301e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1042.4730224609, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5436164141), tensor(0.9692732096), tensor(1.0503405333), tensor(1.1701830626), tensor(1.1093548536), tensor(1.3162045479), tensor(0.9782885313), tensor(1.1483342648), tensor(0.9115638137), tensor(1.1321766376), tensor(0.9963655472), tensor(1.4298725128), tensor(0.9364877939), tensor(1.1195442677), tensor(1.0986412764), tensor(0.8873936534), tensor(1.0039197206), tensor(0.8402445912), tensor(1.0526009798), tensor(0.9527861476)]\n",
            "b:  [tensor(0.6883412600), tensor(1.1524528265), tensor(1.3291591406), tensor(1.0539839268), tensor(1.4715086222), tensor(0.8470078111), tensor(1.2402768135), tensor(1.1090238094), tensor(1.6804770231), tensor(0.9391998053), tensor(1.3081517220), tensor(0.7693506479), tensor(1.3144363165), tensor(1.2899231911), tensor(0.9175240993), tensor(0.9637913108), tensor(1.2610812187), tensor(1.6402046680), tensor(1.0120444298), tensor(1.1934379339)]\n",
            "c:  [tensor(-0.0032413162), tensor(0.4032441974), tensor(0.3837533295), tensor(-0.0172625743), tensor(-0.0016384703), tensor(-0.0122432876), tensor(0.0056765513), tensor(0.0060655493), tensor(0.2952219248), tensor(-0.0057964013), tensor(0.0944123417), tensor(-0.1159821749), tensor(-0.0808365270), tensor(0.2786508501), tensor(-0.0003228467), tensor(0.1010504887), tensor(-0.1112287790), tensor(-0.0798793361)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1882927418, -0.1162042469,  0.2731307447,  0.3003627062,\n",
            "        -0.3469521403,  1.3794283867,  0.8559560180,  0.0871541500,\n",
            "        -0.1102418900,  0.9731371403,  0.8675864339,  0.8071386814,\n",
            "         0.1027241945,  1.6859221458,  0.7122104764,  0.8327984810,\n",
            "         0.8633835912,  0.4209697843,  0.1921479702,  0.3799264431])\n",
            "btensor.grad: tensor([-1.1412223577, -0.9292052984, -0.0266495943, -0.8486263752,\n",
            "        -0.2607459426, -0.5088610649, -0.2395355403,  0.3870501518,\n",
            "         0.9917906523, -1.1421072483, -0.6592024565, -0.8578470945,\n",
            "        -0.3064072728, -0.5010271072, -0.6617220640,  0.2236514688,\n",
            "         0.2329704165, -0.0232967138, -0.4961639643, -0.4056956768])\n",
            "ctensor.grad: tensor([-1.4106960297e+00, -2.6294519424e+01, -2.5255535126e+01,\n",
            "         2.1041166782e-01, -7.9685719684e-03, -3.2655593008e-02,\n",
            "        -1.4966522157e-01, -8.7452530861e-02, -1.9270030975e+01,\n",
            "         8.1918001175e-01,  3.9091806412e+00,  1.2311118126e+01,\n",
            "         7.8491008282e-01, -1.8855487823e+01,  7.9001426697e-01,\n",
            "         3.4249186516e+00,  1.1820850372e+01,  1.2007535696e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1041.1899414062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5375039577), tensor(0.9698791504), tensor(1.0489462614), tensor(1.1685495377), tensor(1.1110609770), tensor(1.3092521429), tensor(0.9739785790), tensor(1.1479660273), tensor(0.9120495915), tensor(1.1274179220), tensor(0.9921277165), tensor(1.4258749485), tensor(0.9361667633), tensor(1.1110552549), tensor(1.0950845480), tensor(0.8833717704), tensor(0.9995331764), tensor(0.8382481933), tensor(1.0517593622), tensor(0.9508718848)]\n",
            "b:  [tensor(0.6940617561), tensor(1.1570663452), tensor(1.3293757439), tensor(1.0583543777), tensor(1.4729135036), tensor(0.8495873213), tensor(1.2416166067), tensor(1.1072568893), tensor(1.6756927967), tensor(0.9448724389), tensor(1.3114618063), tensor(0.7736661434), tensor(1.3160705566), tensor(1.2923946381), tensor(0.9208592176), tensor(0.9627233744), tensor(1.2600044012), tensor(1.6405699253), tensor(1.0145525932), tensor(1.1955168247)]\n",
            "c:  [tensor(-0.0025080047), tensor(0.4161711037), tensor(0.3961681128), tensor(-0.0173459128), tensor(-0.0016352006), tensor(-0.0122265546), tensor(0.0057370737), tensor(0.0060987244), tensor(0.3047181666), tensor(-0.0062205484), tensor(0.0920285210), tensor(-0.1220729724), tensor(-0.0808048323), tensor(0.2879545987), tensor(-0.0007318179), tensor(0.0989314243), tensor(-0.1170828342), tensor(-0.0800786838)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2224810123, -0.1211852580,  0.2788559496,  0.3267109394,\n",
            "        -0.3412328660,  1.3904795647,  0.8619919419,  0.0736470222,\n",
            "        -0.0971528888,  0.9517342448,  0.8475639224,  0.7995053530,\n",
            "         0.0642074347,  1.6978121996,  0.7113362551,  0.8043786287,\n",
            "         0.8773043156,  0.3992803693,  0.1683129668,  0.3828560114])\n",
            "btensor.grad: tensor([-1.1440997124, -0.9227113724, -0.0433206558, -0.8740897775,\n",
            "        -0.2809870839, -0.5159056187, -0.2679693103,  0.3533903360,\n",
            "         0.9568376541, -1.1345218420, -0.6620073318, -0.8630957007,\n",
            "        -0.3268529177, -0.4942920208, -0.6670210361,  0.2135931253,\n",
            "         0.2153654397, -0.0730577111, -0.5016335249, -0.4157796502])\n",
            "ctensor.grad: tensor([-1.4666231871e+00, -2.5853786469e+01, -2.4829536438e+01,\n",
            "         1.6667793691e-01, -6.5395301208e-03, -3.3466007560e-02,\n",
            "        -1.2104453892e-01, -6.6349990666e-02, -1.8992496490e+01,\n",
            "         8.4829425812e-01,  4.7676396370e+00,  1.2181598663e+01,\n",
            "        -6.3384503126e-02, -1.8607505798e+01,  8.1794238091e-01,\n",
            "         4.2381258011e+00,  1.1708112717e+01,  3.9869230986e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1039.9375000000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5312346220), tensor(0.9704948068), tensor(1.0475150347), tensor(1.1667760611), tensor(1.1127164364), tensor(1.3022783995), tensor(0.9696452618), tensor(1.1476664543), tensor(0.9124491215), tensor(1.1227823496), tensor(0.9880105853), tensor(1.4219378233), tensor(0.9360403419), tensor(1.1025338173), tensor(1.0915341377), tensor(0.8795032501), tensor(0.9950851202), tensor(0.8363671303), tensor(1.0510362387), tensor(0.9489305019)]\n",
            "b:  [tensor(0.6997734904), tensor(1.1616184711), tensor(1.3296800852), tensor(1.0628374815), tensor(1.4744130373), tensor(0.8521987796), tensor(1.2431036234), tensor(1.1056721210), tensor(1.6711130142), tensor(0.9504781961), tensor(1.3147698641), tensor(0.7779961228), tensor(1.3178029060), tensor(1.2948241234), tensor(0.9242078662), tensor(0.9617088437), tensor(1.2590163946), tensor(1.6411933899), tensor(1.0170780420), tensor(1.1976330280)]\n",
            "c:  [tensor(-0.0017491971), tensor(0.4288726151), tensor(0.4083645940), tensor(-0.0174081121), tensor(-0.0016326701), tensor(-0.0122112324), tensor(0.0057830606), tensor(0.0061224056), tensor(0.3140646219), tensor(-0.0066601355), tensor(0.0892232880), tensor(-0.1280713975), tensor(-0.0803635642), tensor(0.2971227169), tensor(-0.0011555522), tensor(0.0964126363), tensor(-0.1228536218), tensor(-0.0798910409)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2538690567, -0.1231330633,  0.2862462997,  0.3547010422,\n",
            "        -0.3310992420,  1.3947530985,  0.8666613102,  0.0599160194,\n",
            "        -0.0799018145,  0.9271148443,  0.8234219551,  0.7874182463,\n",
            "         0.0252788067,  1.7042875290,  0.7100768089,  0.7737014294,\n",
            "         0.8896132112,  0.3762066364,  0.1446171403,  0.3882765770])\n",
            "btensor.grad: tensor([-1.1423504353, -0.9104146957, -0.0608658791, -0.8966174722,\n",
            "        -0.2999140024, -0.5222970843, -0.2973964512,  0.3169561625,\n",
            "         0.9159625769, -1.1211529970, -0.6616015434, -0.8660007119,\n",
            "        -0.3464802504, -0.4858998060, -0.6697337627,  0.2029042244,\n",
            "         0.1975911111, -0.1247003078, -0.5050958395, -0.4232427478])\n",
            "ctensor.grad: tensor([-1.5176149607e+00, -2.5403013229e+01, -2.4392992020e+01,\n",
            "         1.2439733744e-01, -5.0611020997e-03, -3.0643567443e-02,\n",
            "        -9.1973394156e-02, -4.7361977398e-02, -1.8692916870e+01,\n",
            "         8.7917405367e-01,  5.6104693413e+00,  1.1996860504e+01,\n",
            "        -8.8253700733e-01, -1.8336208344e+01,  8.4746867418e-01,\n",
            "         5.0375685692e+00,  1.1541581154e+01, -3.7528795004e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1038.7110595703, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5248228312), tensor(0.9711050391), tensor(1.0460383892), tensor(1.1648558378), tensor(1.1142997742), tensor(1.2953164577), tensor(0.9652938247), tensor(1.1474353075), tensor(0.9127427340), tensor(1.1182832718), tensor(0.9840328097), tensor(1.4180818796), tensor(0.9361084700), tensor(1.0940054655), tensor(1.0879900455), tensor(0.8757954836), tensor(0.9905827045), tensor(0.8346064091), tensor(1.0504288673), tensor(0.9469489455)]\n",
            "b:  [tensor(0.7054547071), tensor(1.1660820246), tensor(1.3300764561), tensor(1.0674186945), tensor(1.4760001898), tensor(0.8548400402), tensor(1.2447426319), tensor(1.1042813063), tensor(1.6667654514), tensor(0.9559904933), tensor(1.3180612326), tensor(0.7823302150), tensor(1.3196296692), tensor(1.2972048521), tensor(0.9275584817), tensor(0.9607507586), tensor(1.2581176758), tensor(1.6420818567), tensor(1.0196119547), tensor(1.1997741461)]\n",
            "c:  [tensor(-0.0009670318), tensor(0.4413471520), tensor(0.4203408957), tensor(-0.0174499564), tensor(-0.0016309030), tensor(-0.0121990256), tensor(0.0058144676), tensor(0.0061376449), tensor(0.3232522905), tensor(-0.0071160952), tensor(0.0860080719), tensor(-0.1339502335), tensor(-0.0795321465), tensor(0.3061455786), tensor(-0.0015948938), tensor(0.0935045406), tensor(-0.1285145432), tensor(-0.0793351158)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2823681831, -0.1220417917,  0.2953284383,  0.3840535879,\n",
            "        -0.3166666627,  1.3923805952,  0.8702820539,  0.0462397337,\n",
            "        -0.0587183833,  0.8998080492,  0.7955579162,  0.7711958289,\n",
            "        -0.0136296749,  1.7056754827,  0.7088260651,  0.7415549755,\n",
            "         0.9004888535,  0.3521493077,  0.1214717627,  0.3963130713])\n",
            "btensor.grad: tensor([-1.1362476349, -0.8927059174, -0.0792731047, -0.9162491560,\n",
            "        -0.3174413443, -0.5282483697, -0.3277958632,  0.2781550884,\n",
            "         0.8695136309, -1.1024564505, -0.6582648754, -0.8668219447,\n",
            "        -0.3653518260, -0.4761350155, -0.6701208949,  0.1916218996,\n",
            "         0.1797536314, -0.1777049303, -0.5067921877, -0.4282249212])\n",
            "ctensor.grad: tensor([-1.5643305779e+00, -2.4949089050e+01, -2.3952579498e+01,\n",
            "         8.3687074482e-02, -3.5342012998e-03, -2.4412849918e-02,\n",
            "        -6.2813684344e-02, -3.0478559434e-02, -1.8375362396e+01,\n",
            "         9.1191887856e-01,  6.4304337502e+00,  1.1757673264e+01,\n",
            "        -1.6628396511e+00, -1.8045743942e+01,  8.7868309021e-01,\n",
            "         5.8161859512e+00,  1.1321853638e+01, -1.1118428707e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1037.5155029297, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5182836056), tensor(0.9716956019), tensor(1.0445085764), tensor(1.1627840996), tensor(1.1157907248), tensor(1.2883988619), tensor(0.9609287381), tensor(1.1472713947), tensor(0.9129130244), tensor(1.1139320135), tensor(0.9802112579), tensor(1.4143261909), tensor(0.9363696575), tensor(1.0854941607), tensor(1.0844507217), tensor(0.8722524643), tensor(0.9860326052), tensor(0.8329695463), tensor(1.0499330759), tensor(0.9449145198)]\n",
            "b:  [tensor(0.7110856771), tensor(1.1704328060), tensor(1.3305695057), tensor(1.0720843077), tensor(1.4776680470), tensor(0.8575103283), tensor(1.2465387583), tensor(1.1030946970), tensor(1.6626764536), tensor(0.9613856673), tensor(1.3213231564), tensor(0.7866597772), tensor(1.3215477467), tensor(1.2995315790), tensor(0.9309012294), tensor(0.9598522782), tensor(1.2573083639), tensor(1.6432398558), tensor(1.0221472979), tensor(1.2019292116)]\n",
            "c:  [tensor(-0.0001635809), tensor(0.4535959363), tensor(0.4320978820), tensor(-0.0174722709), tensor(-0.0016299238), tensor(-0.0121915126), tensor(0.0058314204), tensor(0.0061454317), tensor(0.3322746456), tensor(-0.0075893900), tensor(0.0823990479), tensor(-0.1396822184), tensor(-0.0783331096), tensor(0.3150160909), tensor(-0.0020507127), tensor(0.0902221650), tensor(-0.1340389103), tensor(-0.0784326568)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3078505993, -0.1181085110,  0.3059720397,  0.4143358469,\n",
            "        -0.2981994152,  1.3835120201,  0.8730229139,  0.0327939987,\n",
            "        -0.0340521932,  0.8702430129,  0.7643045187,  0.7511451244,\n",
            "        -0.0522434711,  1.7022526264,  0.7078646421,  0.7086056471,\n",
            "         0.9100242257,  0.3273686171,  0.0991600156,  0.4068807364])\n",
            "btensor.grad: tensor([-1.1261916161, -0.8701497316, -0.0986015797, -0.9331330061,\n",
            "        -0.3335784078, -0.5340548158, -0.3592160940,  0.2373238802,\n",
            "         0.8178038597, -1.0790336132, -0.6523864269, -0.8659093976,\n",
            "        -0.3836121857, -0.4653533697, -0.6685501337,  0.1796984076,\n",
            "         0.1618614346, -0.2315958142, -0.5070769787, -0.4310107231])\n",
            "ctensor.grad: tensor([-1.6069016457e+00, -2.4497541428e+01, -2.3513950348e+01,\n",
            "         4.4630371034e-02, -1.9583341200e-03, -1.5025977977e-02,\n",
            "        -3.3905941993e-02, -1.5573672950e-02, -1.8044698715e+01,\n",
            "         9.4658941031e-01,  7.2180442810e+00,  1.1463982582e+01,\n",
            "        -2.3980712891e+00, -1.7741039276e+01,  9.1163778305e-01,\n",
            "         6.5647573471e+00,  1.1048738480e+01, -1.8049153090e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1036.3475341797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5116319656), tensor(0.9722525477), tensor(1.0429174900), tensor(1.1605577469), tensor(1.1171700954), tensor(1.2815562487), tensor(0.9565528631), tensor(1.1471717358), tensor(0.9129440784), tensor(1.1097370386), tensor(0.9765602946), tensor(1.4106874466), tensor(0.9368201494), tensor(1.0770219564), tensor(1.0809127092), tensor(0.8688741922), tensor(0.9814403057), tensor(0.8314580917), tensor(1.0495423079), tensor(0.9428146482)]\n",
            "b:  [tensor(0.7166478634), tensor(1.1746485233), tensor(1.3311631680), tensor(1.0768204927), tensor(1.4794089794), tensor(0.8602093458), tensor(1.2484964132), tensor(1.1021196842), tensor(1.6588696241), tensor(0.9666423798), tensor(1.3245440722), tensor(0.7909770608), tensor(1.3235540390), tensor(1.3018003702), tensor(0.9342274070), tensor(0.9590159059), tensor(1.2565881014), tensor(1.6446683407), tensor(1.0246779919), tensor(1.2040877342)]\n",
            "c:  [tensor(0.0006595698), tensor(0.4656221271), tensor(0.4436384141), tensor(-0.0174759123), tensor(-0.0016297576), tensor(-0.0121901250), tensor(0.0058342125), tensor(0.0061466540), tensor(0.3411262035), tensor(-0.0080810189), tensor(0.0784149542), tensor(-0.1452413052), tensor(-0.0767944306), tensor(0.3237283230), tensor(-0.0025239112), tensor(0.0865830183), tensor(-0.1394010931), tensor(-0.0772105306)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3303394318, -0.1113891155,  0.3182126582,  0.4452648759,\n",
            "        -0.2758760154,  1.3685263395,  0.8751691580,  0.0199288130,\n",
            "        -0.0062062144,  0.8389980793,  0.7301973104,  0.7277607918,\n",
            "        -0.0900961161,  1.6944378614,  0.7075964212,  0.6756484509,\n",
            "         0.9184543490,  0.3022927046,  0.0781494975,  0.4199758768])\n",
            "btensor.grad: tensor([-1.1124405861, -0.8431375623, -0.1187437177, -0.9472329021,\n",
            "        -0.3481756449, -0.5397994518, -0.3915273547,  0.1950016022,\n",
            "         0.7613630295, -1.0513372421, -0.6441910267, -0.8634532094,\n",
            "        -0.4012471139, -0.4537596703, -0.6652340889,  0.1672728658,\n",
            "         0.1440554559, -0.2856968045, -0.5061358213, -0.4317073822])\n",
            "ctensor.grad: tensor([-1.6463013887e+00, -2.4052364349e+01, -2.3081058502e+01,\n",
            "         7.2829532437e-03, -3.3238640754e-04, -2.7759321965e-03,\n",
            "        -5.5838390253e-03, -2.4446784519e-03, -1.7703088760e+01,\n",
            "         9.8325771093e-01,  7.9681839943e+00,  1.1118179321e+01,\n",
            "        -3.0773537159e+00, -1.7424455643e+01,  9.4639670849e-01,\n",
            "         7.2782964706e+00,  1.0724361420e+01, -2.4442486763e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1035.2056884766, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5048836470), tensor(0.9727644920), tensor(1.0412591696), tensor(1.1581766605), tensor(1.1184211969), tensor(1.2748180628), tensor(0.9521700740), tensor(1.1471334696), tensor(0.9128240347), tensor(1.1057053804), tensor(0.9730929732), tensor(1.4071807861), tensor(0.9374560118), tensor(1.0686099529), tensor(1.0773723125), tensor(0.8656589985), tensor(0.9768119454), tensor(0.8300734758), tensor(1.0492497683), tensor(0.9406389594)]\n",
            "b:  [tensor(0.7221256495), tensor(1.1787108183), tensor(1.3318624496), tensor(1.0816146135), tensor(1.4812159538), tensor(0.8629385233), tensor(1.2506206036), tensor(1.1013625860), tensor(1.6553671360), tensor(0.9717432261), tensor(1.3277151585), tensor(0.7952765822), tensor(1.3256466389), tensor(1.3040093184), tensor(0.9375308156), tensor(0.9582450390), tensor(1.2559571266), tensor(1.6463663578), tensor(1.0272002220), tensor(1.2062416077)]\n",
            "c:  [tensor(0.0015004432), tensor(0.4774302244), tensor(0.4549667537), tensor(-0.0174617451), tensor(-0.0016304307), tensor(-0.0121961413), tensor(0.0058232867), tensor(0.0061420556), tensor(0.3498045206), tensor(-0.0085919714), tensor(0.0740808621), tensor(-0.1506008953), tensor(-0.0749442726), tensor(0.3322793245), tensor(-0.0030153811), tensor(0.0826107040), tensor(-0.1445748955), tensor(-0.0756957158)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3496657610, -0.1023854613,  0.3316677511,  0.4762078524,\n",
            "        -0.2502253652,  1.3476405144,  0.8765602112,  0.0076485872,\n",
            "         0.0240064263,  0.8063242435,  0.6934644580,  0.7013421059,\n",
            "        -0.1271752119,  1.6823942661,  0.7080829144,  0.6430379152,\n",
            "         0.9256733656,  0.2769236565,  0.0585047007,  0.4351338148])\n",
            "btensor.grad: tensor([-1.0955567360, -0.8124542236, -0.1398457289, -0.9588227272,\n",
            "        -0.3613917530, -0.5458353758, -0.4248264134,  0.1514286995,\n",
            "         0.7004956007, -1.0201716423, -0.6342133284, -0.8599058986,\n",
            "        -0.4185183048, -0.4417846203, -0.6606773138,  0.1541712284,\n",
            "         0.1261904538, -0.3395944834, -0.5044466257, -0.4307643771])\n",
            "ctensor.grad: tensor([-1.6817467213e+00, -2.3616199493e+01, -2.2656681061e+01,\n",
            "        -2.8332697228e-02,  1.3461507624e-03,  1.2033613399e-02,\n",
            "         2.1851986647e-02,  9.1965422034e-03, -1.7356620789e+01,\n",
            "         1.0219053030e+00,  8.6681785583e+00,  1.0719167709e+01,\n",
            "        -3.7003192902e+00, -1.7102029800e+01,  9.8293954134e-01,\n",
            "         7.9446272850e+00,  1.0347613335e+01, -3.0296235085e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1034.0928955078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4980529547), tensor(0.9732189178), tensor(1.0395259857), tensor(1.1556409597), tensor(1.1195273399), tensor(1.2682098150), tensor(0.9477818012), tensor(1.1471505165), tensor(0.9125413299), tensor(1.1018402576), tensor(0.9698179960), tensor(1.4038170576), tensor(0.9382694364), tensor(1.0602761507), tensor(1.0738227367), tensor(0.8626002669), tensor(0.9721512198), tensor(0.8288133144), tensor(1.0490446091), tensor(0.9383764863)]\n",
            "b:  [tensor(0.7275032997), tensor(1.1826019287), tensor(1.3326698542), tensor(1.0864521265), tensor(1.4830797911), tensor(0.8656978011), tensor(1.2529135942), tensor(1.1008250713), tensor(1.6521865129), tensor(0.9766716957), tensor(1.3308268785), tensor(0.7995524406), tensor(1.3278220892), tensor(1.3061559200), tensor(0.9408047199), tensor(0.9575405717), tensor(1.2554136515), tensor(1.6483277082), tensor(1.0297095776), tensor(1.2083814144)]\n",
            "c:  [tensor(0.0023582922), tensor(0.4890252352), tensor(0.4660877585), tensor(-0.0174306538), tensor(-0.0016319703), tensor(-0.0122106830), tensor(0.0057992316), tensor(0.0061322073), tensor(0.3583055735), tensor(-0.0091232788), tensor(0.0694200844), tensor(-0.1557379812), tensor(-0.0728204176), tensor(0.3406648636), tensor(-0.0035260522), tensor(0.0783272907), tensor(-0.1495374888), tensor(-0.0739241689)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3661364317, -0.0908904076,  0.3466425836,  0.5071491003,\n",
            "        -0.2212387323,  1.3216470480,  0.8776538372, -0.0034024715,\n",
            "         0.0565462112,  0.7730176449,  0.6549975872,  0.6727348566,\n",
            "        -0.1626791954,  1.6667549610,  0.7099213600,  0.6117504835,\n",
            "         0.9321452379,  0.2520350218,  0.0410321355,  0.4524958134])\n",
            "btensor.grad: tensor([-1.0755267143, -0.7782155871, -0.1614786386, -0.9674937725,\n",
            "        -0.3727725148, -0.5518527627, -0.4586032927,  0.1075078249,\n",
            "         0.6361252069, -0.9856922626, -0.6223518848, -0.8551704884,\n",
            "        -0.4350825548, -0.4293214083, -0.6547782421,  0.1408985257,\n",
            "         0.1086865291, -0.3922684193, -0.5018764734, -0.4279677868])\n",
            "ctensor.grad: tensor([-1.7156980038e+00, -2.3190004349e+01, -2.2241985321e+01,\n",
            "        -6.2182329595e-02,  3.0792281032e-03,  2.9084082693e-02,\n",
            "         4.8110149801e-02,  1.9696909934e-02, -1.7002132416e+01,\n",
            "         1.0626150370e+00,  9.3215475082e+00,  1.0274168015e+01,\n",
            "        -4.2477064133e+00, -1.6771093369e+01,  1.0213421583e+00,\n",
            "         8.5668315887e+00,  9.9251747131e+00, -3.5431010723e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1033.0053710938, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4911572933), tensor(0.9736104608), tensor(1.0377165079), tensor(1.1529574394), tensor(1.1204782724), tensor(1.2617579699), tensor(0.9433951378), tensor(1.1472202539), tensor(0.9120935798), tensor(1.0981471539), tensor(0.9667464495), tensor(1.4006083012), tensor(0.9392570853), tensor(1.0520405769), tensor(1.0702607632), tensor(0.8596944213), tensor(0.9674662352), tensor(0.8276801109), tensor(1.0489200354), tensor(0.9360236526)]\n",
            "b:  [tensor(0.7327710986), tensor(1.1863118410), tensor(1.3335913420), tensor(1.0913231373), tensor(1.4849957228), tensor(0.8684917092), tensor(1.2553808689), tensor(1.1005109549), tensor(1.6493462324), tensor(0.9814189076), tensor(1.3338756561), tensor(0.8038035035), tensor(1.3300809860), tensor(1.3082424402), tensor(0.9440479279), tensor(0.9569074512), tensor(1.2549597025), tensor(1.6505470276), tensor(1.0322070122), tensor(1.2105036974)]\n",
            "c:  [tensor(0.0032300421), tensor(0.5004122257), tensor(0.4770064652), tensor(-0.0173834879), tensor(-0.0016344057), tensor(-0.0122346813), tensor(0.0057627698), tensor(0.0061175269), tensor(0.3666328490), tensor(-0.0096758567), tensor(0.0644707978), tensor(-0.1606251001), tensor(-0.0704484954), tensor(0.3488878310), tensor(-0.0040567489), tensor(0.0737689584), tensor(-0.1542617381), tensor(-0.0719201863)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3791275024, -0.0783134401,  0.3618893027,  0.5367115140,\n",
            "        -0.1901881695,  1.2903665304,  0.8773377538, -0.0139410496,\n",
            "         0.0895453095,  0.7386119962,  0.6143143773,  0.6417528987,\n",
            "        -0.1975311041,  1.6471079588,  0.7124018669,  0.5811668634,\n",
            "         0.9369977713,  0.2266348004,  0.0249041319,  0.4705672264])\n",
            "btensor.grad: tensor([-1.0535545349, -0.7419937253, -0.1842958927, -0.9742082953,\n",
            "        -0.3831763268, -0.5587819815, -0.4934638441,  0.0628248453,\n",
            "         0.5680605769, -0.9494389296, -0.6097646952, -0.8502121568,\n",
            "        -0.4517829716, -0.4172986746, -0.6486363411,  0.1266239285,\n",
            "         0.0907982439, -0.4438682497, -0.4994945526, -0.4244660735])\n",
            "ctensor.grad: tensor([-1.7434993982e+00, -2.2773946762e+01, -2.1837390900e+01,\n",
            "        -9.4330787659e-02,  4.8708510585e-03,  4.7996032983e-02,\n",
            "         7.2923354805e-02,  2.9360607266e-02, -1.6654541016e+01,\n",
            "         1.1051560640e+00,  9.8985719681e+00,  9.7742424011e+00,\n",
            "        -4.7438406944e+00, -1.6445957184e+01,  1.0613930225e+00,\n",
            "         9.1166610718e+00,  9.4484872818e+00, -4.0079665184e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1031.9455566406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4842063189), tensor(0.9739230275), tensor(1.0358188152), tensor(1.1501245499), tensor(1.1212561131), tensor(1.2554777861), tensor(0.9390063286), tensor(1.1473286152), tensor(0.9114681482), tensor(1.0946201086), tensor(0.9638761878), tensor(1.3975551128), tensor(0.9404008389), tensor(1.0439137220), tensor(1.0666729212), tensor(0.8569247127), tensor(0.9627553821), tensor(0.8266617656), tensor(1.0488554239), tensor(0.9335671067)]\n",
            "b:  [tensor(0.7379116416), tensor(1.1898221970), tensor(1.3346234560), tensor(1.0962069035), tensor(1.4869481325), tensor(0.8713145256), tensor(1.2580173016), tensor(1.1004108191), tensor(1.6468520164), tensor(0.9859679937), tensor(1.3368493319), tensor(0.8080210090), tensor(1.3324142694), tensor(1.3102633953), tensor(0.9472507834), tensor(0.9563394785), tensor(1.2545874119), tensor(1.6530064344), tensor(1.0346847773), tensor(1.2125952244)]\n",
            "c:  [tensor(0.0041182181), tensor(0.5115953088), tensor(0.4877270460), tensor(-0.0173211861), tensor(-0.0016377665), tensor(-0.0122689893), tensor(0.0057147108), tensor(0.0060981177), tensor(0.3747758567), tensor(-0.0102508068), tensor(0.0592467599), tensor(-0.1652498543), tensor(-0.0678901002), tensor(0.3569380939), tensor(-0.0046084700), tensor(0.0689489320), tensor(-0.1587347388), tensor(-0.0697418973)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3901972771, -0.0625155419,  0.3795432448,  0.5665779114,\n",
            "        -0.1555659175,  1.2560431957,  0.8777568340, -0.0216649771,\n",
            "         0.1250898242,  0.7054052949,  0.5740513206,  0.6106475592,\n",
            "        -0.2287564278,  1.6253618002,  0.7175590992,  0.5539445877,\n",
            "         0.9421687722,  0.2036742568,  0.0129264593,  0.4913046360])\n",
            "btensor.grad: tensor([-1.0281105042, -0.7020642757, -0.2064188719, -0.9767490625,\n",
            "        -0.3904931545, -0.5645662546, -0.5272824764,  0.0200283527,\n",
            "         0.4988373518, -0.9098193645, -0.5947268009, -0.8435034752,\n",
            "        -0.4666598737, -0.4041970968, -0.6405684948,  0.1135888696,\n",
            "         0.0744544566, -0.4918752313, -0.4955611229, -0.4183071852])\n",
            "ctensor.grad: tensor([-1.7763522863e+00, -2.2366176605e+01, -2.1441164017e+01,\n",
            "        -1.2460239232e-01,  6.7215906456e-03,  6.8615421653e-02,\n",
            "         9.6118405461e-02,  3.8818567991e-02, -1.6285991669e+01,\n",
            "         1.1499007940e+00,  1.0448072433e+01,  9.2495012283e+00,\n",
            "        -5.1167945862e+00, -1.6100540161e+01,  1.1034420729e+00,\n",
            "         9.6400518417e+00,  8.9459886551e+00, -4.3565821648e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.9134521484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4772273302), tensor(0.9741723537), tensor(1.0338510275), tensor(1.1471688747), tensor(1.1218690872), tensor(1.2494009733), tensor(0.9346412420), tensor(1.1474863291), tensor(0.9106897116), tensor(1.0912754536), tensor(0.9612293243), tensor(1.3946754932), tensor(0.9417144656), tensor(1.0359246731), tensor(1.0630698204), tensor(0.8543022275), tensor(0.9580419660), tensor(0.8257789612), tensor(1.0488598347), tensor(0.9310265779)]\n",
            "b:  [tensor(0.7429322600), tensor(1.1931450367), tensor(1.3357825279), tensor(1.1011085510), tensor(1.4889465570), tensor(0.8741848469), tensor(1.2608381510), tensor(1.1005380154), tensor(1.6447285414), tensor(0.9903312325), tensor(1.3397604227), tensor(0.8122176528), tensor(1.3348358870), tensor(1.3122342825), tensor(0.9504277110), tensor(0.9558553696), tensor(1.2543109655), tensor(1.6557070017), tensor(1.0371592045), tensor(1.2146693468)]\n",
            "c:  [tensor(0.0050118593), tensor(0.5225781202), tensor(0.4982534349), tensor(-0.0172444098), tensor(-0.0016420853), tensor(-0.0123139732), tensor(0.0056560901), tensor(0.0060743247), tensor(0.3827610314), tensor(-0.0108486302), tensor(0.0538313352), tensor(-0.1695684940), tensor(-0.0651233941), tensor(0.3648406267), tensor(-0.0051816604), tensor(0.0639463738), tensor(-0.1629137695), tensor(-0.0673682019)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3957920074e+00, -4.9870714545e-02,  3.9356830716e-01,\n",
            "         5.9114116430e-01, -1.2260150909e-01,  1.2153742313e+00,\n",
            "         8.7301230431e-01, -3.1552672386e-02,  1.5568459034e-01,\n",
            "         6.6893553734e-01,  5.2937591076e-01,  5.7593154907e-01,\n",
            "        -2.6272785664e-01,  1.5978125334e+00,  7.2062397003e-01,\n",
            "         5.2450060844e-01,  9.4268620014e-01,  1.7656385899e-01,\n",
            "        -8.8709592819e-04,  5.0810551643e-01])\n",
            "btensor.grad: tensor([-1.0041234493, -0.6645668149, -0.2318134308, -0.9803181887,\n",
            "        -0.3996735513, -0.5740593076, -0.5641790628, -0.0254344940,\n",
            "         0.4246908426, -0.8726506233, -0.5822299719, -0.8393319845,\n",
            "        -0.4843319952, -0.3941727877, -0.6353827119,  0.0968213677,\n",
            "         0.0553006455, -0.5401053429, -0.4948866367, -0.4148283005])\n",
            "ctensor.grad: tensor([-1.7872819901e+00, -2.1965606689e+01, -2.1052799225e+01,\n",
            "        -1.5355391800e-01,  8.6376350373e-03,  8.9968472719e-02,\n",
            "         1.1724116653e-01,  4.7585606575e-02, -1.5970334053e+01,\n",
            "         1.1956465244e+00,  1.0830846786e+01,  8.6372919083e+00,\n",
            "        -5.5334053040e+00, -1.5805050850e+01,  1.1463811398e+00,\n",
            "         1.0005110741e+01,  8.3580465317e+00, -4.7473850250e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1029.9085693359, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4702005386), tensor(0.9742982388), tensor(1.0317580700), tensor(1.1440523863), tensor(1.1222666502), tensor(1.2435042858), tensor(0.9302539825), tensor(1.1476371288), tensor(0.9096990824), tensor(1.0880662203), tensor(0.9587571621), tensor(1.3919318914), tensor(0.9431245923), tensor(1.0280498266), tensor(1.0594003201), tensor(0.8517622948), tensor(0.9532874823), tensor(0.8249641657), tensor(1.0488612652), tensor(0.9283469319)]\n",
            "b:  [tensor(0.7477833629), tensor(1.1962234974), tensor(1.3370304108), tensor(1.1059651375), tensor(1.4909360409), tensor(0.8770576119), tensor(1.2637995481), tensor(1.1008373499), tensor(1.6429383755), tensor(0.9944561720), tensor(1.3425614834), tensor(0.8163535595), tensor(1.3372988701), tensor(1.3141189814), tensor(0.9535344839), tensor(0.9554073215), tensor(1.2540864944), tensor(1.6585892439), tensor(1.0395872593), tensor(1.2166737318)]\n",
            "c:  [tensor(0.0059349285), tensor(0.5333616138), tensor(0.5085867643), tensor(-0.0171546340), tensor(-0.0016473925), tensor(-0.0123709207), tensor(0.0055875541), tensor(0.0060448912), tensor(0.3905201554), tensor(-0.0114711672), tensor(0.0481330417), tensor(-0.1736273170), tensor(-0.0623483248), tensor(0.3725307286), tensor(-0.0057780202), tensor(0.0586752370), tensor(-0.1668414772), tensor(-0.0649880245)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.4053531885e+00, -2.5171488523e-02,  4.1858816147e-01,\n",
            "         6.2330746651e-01, -7.9510450363e-02,  1.1793422699e+00,\n",
            "         8.7745618820e-01, -3.0164003372e-02,  1.9812107086e-01,\n",
            "         6.4185154438e-01,  4.9443018436e-01,  5.4872393608e-01,\n",
            "        -2.8202044964e-01,  1.5749641657e+00,  7.3389935493e-01,\n",
            "         5.0798594952e-01,  9.5089858770e-01,  1.6296052933e-01,\n",
            "        -2.9098987579e-04,  5.3593420982e-01])\n",
            "btensor.grad: tensor([-0.9702177048, -0.6156951189, -0.2495745420, -0.9713280201,\n",
            "        -0.3979037404, -0.5745574236, -0.5922829509, -0.0598726273,\n",
            "         0.3580384851, -0.8249893785, -0.5602186918, -0.8271798491,\n",
            "        -0.4925954342, -0.3769471645, -0.6213579178,  0.0896145105,\n",
            "         0.0448858663, -0.5764481425, -0.4856047630, -0.4008710980])\n",
            "ctensor.grad: tensor([-1.8461381197e+00, -2.1566974640e+01, -2.0666603088e+01,\n",
            "        -1.7955103517e-01,  1.0614398867e-02,  1.1389467865e-01,\n",
            "         1.3707217574e-01,  5.8867346495e-02, -1.5518243790e+01,\n",
            "         1.2450745106e+00,  1.1396587372e+01,  8.1176452637e+00,\n",
            "        -5.5501413345e+00, -1.5380175591e+01,  1.1927194595e+00,\n",
            "         1.0542272568e+01,  7.8554205894e+00, -4.7603573799e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1028.9338378906, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4632321596), tensor(0.9744533300), tensor(1.0296893120), tensor(1.1409193277), tensor(1.1225675344), tensor(1.2379008532), tensor(0.9259994030), tensor(1.1479045153), tensor(0.9086789489), tensor(1.0851129293), tensor(0.9565957189), tensor(1.3894271851), tensor(0.9447897673), tensor(1.0204039812), tensor(1.0557826757), tensor(0.8494461775), tensor(0.9486248493), tensor(0.8243877888), tensor(1.0490049124), tensor(0.9256886244)]\n",
            "b:  [tensor(0.7525752783), tensor(1.1991976500), tensor(1.3384721279), tensor(1.1108957529), tensor(1.4930343628), tensor(0.8800569177), tensor(1.2670103312), tensor(1.1014283895), tensor(1.6415979862), tensor(0.9984752536), tensor(1.3453713655), tensor(0.8205329776), tensor(1.3399194479), tensor(1.3160216808), tensor(0.9566881061), tensor(0.9551236033), tensor(1.2540237904), tensor(1.6617456675), tensor(1.0420879126), tensor(1.2187364101)]\n",
            "c:  [tensor(0.0068162931), tensor(0.5439476371), tensor(0.5187299848), tensor(-0.0170507468), tensor(-0.0016537226), tensor(-0.0124373166), tensor(0.0055114916), tensor(0.0060131932), tensor(0.3982468545), tensor(-0.0121162925), tensor(0.0425495356), tensor(-0.1772324592), tensor(-0.0591509640), tensor(0.3801920712), tensor(-0.0063955612), tensor(0.0535123907), tensor(-0.1703323722), tensor(-0.0622082129)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3936853409, -0.0310219824,  0.4137533903,  0.6266077161,\n",
            "        -0.0601867437,  1.1206803322,  0.8509116769, -0.0534793139,\n",
            "         0.2040256858,  0.5906611681,  0.4322878718,  0.5009419322,\n",
            "        -0.3330292702,  1.5291594267,  0.7235177755,  0.4632179737,\n",
            "         0.9325218201,  0.1152746081, -0.0287383795,  0.5316644907])\n",
            "btensor.grad: tensor([-0.9583820701, -0.5948394537, -0.2883387804, -0.9861271381,\n",
            "        -0.4196579158, -0.5998576880, -0.6421517134, -0.1182159185,\n",
            "         0.2680795193, -0.8038171530, -0.5619810820, -0.8358777761,\n",
            "        -0.5241121054, -0.3805435896, -0.6307222843,  0.0567404628,\n",
            "         0.0125380605, -0.6312761903, -0.5001325607, -0.4125363827])\n",
            "ctensor.grad: tensor([-1.7627288103e+00, -2.1172027588e+01, -2.0286441803e+01,\n",
            "        -2.0777551830e-01,  1.2660153210e-02,  1.3279181719e-01,\n",
            "         1.5212459862e-01,  6.3395969570e-02, -1.5453418732e+01,\n",
            "         1.2902506590e+00,  1.1167011261e+01,  7.2102770805e+00,\n",
            "        -6.3947238922e+00, -1.5322656631e+01,  1.2350819111e+00,\n",
            "         1.0325691223e+01,  6.9817862511e+00, -5.5596218109e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1027.9893798828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4560655355), tensor(0.9741843939), tensor(1.0272128582), tensor(1.1373989582), tensor(1.1224131584), tensor(1.2322969437), tensor(0.9214596152), tensor(1.1478934288), tensor(0.9071298838), tensor(1.0820486546), tensor(0.9543408751), tensor(1.3868494034), tensor(0.9461959600), tensor(1.0126996040), tensor(1.0518498421), tensor(0.8468997478), tensor(0.9436983466), tensor(0.8235324025), tensor(1.0488101244), tensor(0.9226034284)]\n",
            "b:  [tensor(0.7569679022), tensor(1.2016540766), tensor(1.3397974968), tensor(1.1154991388), tensor(1.4948670864), tensor(0.8828223944), tensor(1.2701332569), tensor(1.1019209623), tensor(1.6403652430), tensor(1.0020016432), tensor(1.3478358984), tensor(0.8244494200), tensor(1.3423424959), tensor(1.3176470995), tensor(0.9595465064), tensor(0.9546264410), tensor(1.2537919283), tensor(1.6648250818), tensor(1.0443179607), tensor(1.2204712629)]\n",
            "c:  [tensor(0.0078583639), tensor(0.5543308854), tensor(0.5286764503), tensor(-0.0169400368), tensor(-0.0016611131), tensor(-0.0125230197), tensor(0.0054243566), tensor(0.0059661842), tensor(0.4053739309), tensor(-0.0127929496), tensor(0.0360544845), tensor(-0.1809034795), tensor(-0.0569053404), tensor(0.3872866929), tensor(-0.0070426282), tensor(0.0474871770), tensor(-0.1738787591), tensor(-0.0603306144)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.4333190918,  0.0537813157,  0.4952845573,  0.7040836811,\n",
            "         0.0308645368,  1.1207726002,  0.9079573750,  0.0022253990,\n",
            "         0.3098120093,  0.6128526330,  0.4509652257,  0.5155543089,\n",
            "        -0.2812339067,  1.5408710241,  0.7865685225,  0.5092833042,\n",
            "         0.9853038192,  0.1710755229,  0.0389679074,  0.6170370579])\n",
            "btensor.grad: tensor([-0.8785300255, -0.4912961721, -0.2650740147, -0.9206683636,\n",
            "        -0.3665351272, -0.5530938506, -0.6245863438, -0.0985109806,\n",
            "         0.2465420961, -0.7052689791, -0.4929047823, -0.7832927704,\n",
            "        -0.4846158624, -0.3250795603, -0.5716784596,  0.0994285345,\n",
            "         0.0463687927, -0.6158909798, -0.4460183382, -0.3469808102])\n",
            "ctensor.grad: tensor([-2.0841410160e+00, -2.0766513824e+01, -1.9892980576e+01,\n",
            "        -2.2142148018e-01,  1.4781023376e-02,  1.7140613496e-01,\n",
            "         1.7427012324e-01,  9.4017513096e-02, -1.4254142761e+01,\n",
            "         1.3533133268e+00,  1.2990104675e+01,  7.3420486450e+00,\n",
            "        -4.4912471771e+00, -1.4189238548e+01,  1.2941343784e+00,\n",
            "         1.2050424576e+01,  7.0927762985e+00, -3.7551972866e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1027.0798339844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4495350122), tensor(0.9748778343), tensor(1.0256614685), tensor(1.1346802711), tensor(1.1228990555), tensor(1.2276182175), tensor(0.9179603457), tensor(1.1487729549), tensor(0.9066303372), tensor(1.0799810886), tensor(0.9532194734), tensor(1.3851317167), tensor(0.9488590956), tensor(1.0058641434), tensor(1.0487133265), tensor(0.8454824090), tensor(0.9396383166), tensor(0.8239636421), tensor(1.0496847630), tensor(0.9205018878)]\n",
            "b:  [tensor(0.7619792223), tensor(1.2048532963), tensor(1.3419406414), tensor(1.1209313869), tensor(1.4975357056), tensor(0.8864411712), tensor(1.2741625309), tensor(1.1034467220), tensor(1.6402680874), tensor(1.0062186718), tensor(1.3510252237), tensor(0.8290310502), tensor(1.3456307650), tensor(1.3198999166), tensor(0.9631489515), tensor(0.9550537467), tensor(1.2543793917), tensor(1.6687953472), tensor(1.0473240614), tensor(1.2230408192)]\n",
            "c:  [tensor(0.0084345294), tensor(0.5645112395), tensor(0.5384330153), tensor(-0.0168011747), tensor(-0.0016695486), tensor(-0.0125933792), tensor(0.0053443392), tensor(0.0059481882), tensor(0.4136654735), tensor(-0.0134759899), tensor(0.0320012383), tensor(-0.1829765439), tensor(-0.0514828004), tensor(0.3954886794), tensor(-0.0076957652), tensor(0.0437756702), tensor(-0.1758970916), tensor(-0.0554397330)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3061082363, -0.1386909932,  0.3102671206,  0.5437456369,\n",
            "        -0.0971730351,  0.9357414246,  0.6998519301, -0.1759051085,\n",
            "         0.0999082327,  0.4135190547,  0.2242817879,  0.3435271382,\n",
            "        -0.5326281786,  1.3670860529,  0.6273044348,  0.2834702730,\n",
            "         0.8120061159, -0.0862451792, -0.1749188900,  0.4203110933])\n",
            "btensor.grad: tensor([-1.0022616386, -0.6398381591, -0.4286252558, -1.0864410400,\n",
            "        -0.5337295532, -0.7237535119, -0.8058602214, -0.3051508665,\n",
            "         0.0194401145, -0.8433963656, -0.6378680468, -0.9163318872,\n",
            "        -0.6576519608, -0.4505583048, -0.7204939127, -0.0854582787,\n",
            "        -0.1174901426, -0.7940453887, -0.6012115479, -0.5139028430])\n",
            "ctensor.grad: tensor([-1.1523307562e+00, -2.0360763550e+01, -1.9513175964e+01,\n",
            "        -2.7772262692e-01,  1.6870815307e-02,  1.4071992040e-01,\n",
            "         1.6003450751e-01,  3.5991542041e-02, -1.6583089828e+01,\n",
            "         1.3660800457e+00,  8.1064910889e+00,  4.1461315155e+00,\n",
            "        -1.0845081329e+01, -1.6403968811e+01,  1.3062741756e+00,\n",
            "         7.4230122566e+00,  4.0366625786e+00, -9.7817602158e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1026.2537841797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4410330057), tensor(0.9720917344), tensor(1.0207860470), tensor(1.1290314198), tensor(1.1205139160), tensor(1.2209063768), tensor(0.9113021493), tensor(1.1467542648), tensor(0.9021868110), tensor(1.0753350258), tensor(0.9492772818), tensor(1.3812445402), tensor(0.9478647709), tensor(0.9969735742), tensor(1.0427935123), tensor(0.8407832384), tensor(0.9328607321), tensor(0.8206396699), tensor(1.0470654964), tensor(0.9149073958)]\n",
            "b:  [tensor(0.7643298507), tensor(1.2047637701), tensor(1.3418889046), tensor(1.1234273911), tensor(1.4974848032), tensor(0.8874321580), tensor(1.2758702040), tensor(1.1023223400), tensor(1.6379716396), tensor(1.0073394775), tensor(1.3515026569), tensor(0.8312960267), tensor(1.3463453054), tensor(1.3198916912), tensor(0.9641594291), tensor(0.9527589083), tensor(1.2526096106), tensor(1.6704592705), tensor(1.0477538109), tensor(1.2227088213)]\n",
            "c:  [tensor(0.0105467634), tensor(0.5744436979), tensor(0.5479382277), tensor(-0.0167118087), tensor(-0.0016792914), tensor(-0.0127716092), tensor(0.0052109216), tensor(0.0057996050), tensor(0.4174379408), tensor(-0.0142480526), tensor(0.0196906514), tensor(-0.1888357401), tensor(-0.0564474165), tensor(0.3994038105), tensor(-0.0084322039), tensor(0.0322458483), tensor(-0.1815058887), tensor(-0.0603949204)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([1.7004073858, 0.5572165847, 0.9750798941, 1.1297638416, 0.4770316482,\n",
            "        1.3423769474, 1.3316354752, 0.4037352204, 0.8887006640, 0.9292110801,\n",
            "        0.7884406447, 0.7774242163, 0.1988639832, 1.7781143188, 1.1839746237,\n",
            "        0.9398384094, 1.3555126190, 0.6647991538, 0.5238421559, 1.1188960075])\n",
            "btensor.grad: tensor([-0.4701229930,  0.0179080367,  0.0103504062, -0.4991998672,\n",
            "         0.0101746321, -0.1981926560, -0.3415370882,  0.2248739004,\n",
            "         0.4592924118, -0.2241587341, -0.0954960585, -0.4529933035,\n",
            "        -0.1429156661,  0.0016449690, -0.2020997703,  0.4589717388,\n",
            "         0.3539475203, -0.3327802718, -0.0859558582,  0.0663999915])\n",
            "ctensor.grad: tensor([-4.2244682312e+00, -1.9864910126e+01, -1.9010404587e+01,\n",
            "        -1.7873068154e-01,  1.9485669211e-02,  3.5646083951e-01,\n",
            "         2.6683503389e-01,  2.9716670513e-01, -7.5449142456e+00,\n",
            "         1.5441243649e+00,  2.4621173859e+01,  1.1718403816e+01,\n",
            "         9.9292345047e+00, -7.8302578926e+00,  1.4728778601e+00,\n",
            "         2.3059642792e+01,  1.1217606544e+01,  9.9103736877e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1026.0518798828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4395527840), tensor(0.9810199738), tensor(1.0272026062), tensor(1.1332728863), tensor(1.1272041798), tensor(1.2223577499), tensor(0.9160728455), tensor(1.1546301842), tensor(0.9110532403), tensor(1.0804090500), tensor(0.9561346769), tensor(1.3854765892), tensor(0.9600208402), tensor(0.9962261915), tensor(1.0464588404), tensor(0.8480197787), tensor(0.9357432127), tensor(0.8309109807), tensor(1.0564872026), tensor(0.9212952256)]\n",
            "b:  [tensor(0.7748243213), tensor(1.2148329020), tensor(1.3497152328), tensor(1.1353954077), tensor(1.5065459013), tensor(0.8973478079), tensor(1.2860001326), tensor(1.1108525991), tensor(1.6447169781), tensor(1.0179238319), tensor(1.3607072830), tensor(0.8411249518), tensor(1.3559870720), tensor(1.3272811174), tensor(0.9736765027), tensor(0.9599514604), tensor(1.2591309547), tensor(1.6803432703), tensor(1.0567853451), tensor(1.2319666147)]\n",
            "c:  [tensor(0.0072917310), tensor(0.5836811066), tensor(0.5568383336), tensor(-0.0163654760), tensor(-0.0016890230), tensor(-0.0125472657), tensor(0.0052831783), tensor(0.0061708395), tensor(0.4366720319), tensor(-0.0148267737), tensor(0.0368583649), tensor(-0.1797706783), tensor(-0.0250046328), tensor(0.4180012047), tensor(-0.0089881448), tensor(0.0486670285), tensor(-0.1728832424), tensor(-0.0307934042)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.2960433364, -1.7856508493, -1.2833139896, -0.8482978344,\n",
            "        -1.3380640745, -0.2902710438, -0.9541352987, -1.5751721859,\n",
            "        -1.7732892036, -1.0148051977, -1.3714776039, -0.8464174867,\n",
            "        -2.4312112331,  0.1494746208, -0.7330602407, -1.4473074675,\n",
            "        -0.5765005350, -2.0542633533, -1.8843464851, -1.2775702477])\n",
            "btensor.grad: tensor([-2.0988936424, -2.0138158798, -1.5652697086, -2.3936018944,\n",
            "        -1.8122246265, -1.9831358194, -2.0259773731, -1.7060452700,\n",
            "        -1.3490620852, -2.1168661118, -1.8409368992, -1.9657833576,\n",
            "        -1.9283421040, -1.4778752327, -1.9034104347, -1.4385108948,\n",
            "        -1.3042749166, -1.9767942429, -1.8063113689, -1.8515551090])\n",
            "ctensor.grad: tensor([ 6.5100646019e+00, -1.8474802017e+01, -1.7800170898e+01,\n",
            "        -6.9266551733e-01,  1.9463296980e-02, -4.4868752360e-01,\n",
            "        -1.4451345801e-01, -7.4246853590e-01, -3.8468173981e+01,\n",
            "         1.1574413776e+00, -3.4335430145e+01, -1.8130134583e+01,\n",
            "        -6.2885566711e+01, -3.7194770813e+01,  1.1118819714e+00,\n",
            "        -3.2842353821e+01, -1.7245306015e+01, -5.9203029633e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1033.0974121094, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4161924124), tensor(0.9512362480), tensor(0.9974358082), tensor(1.1055490971), tensor(1.1024698019), tensor(1.1988780499), tensor(0.8854660392), tensor(1.1291024685), tensor(0.8772873282), tensor(1.0553447008), tensor(0.9296827912), tensor(1.3633679152), tensor(0.9308449030), tensor(0.9713234305), tensor(1.0196729898), tensor(0.8177515864), tensor(0.9075738788), tensor(0.7985833287), tensor(1.0272439718), tensor(0.8894068599)]\n",
            "b:  [tensor(0.7537853718), tensor(1.1876749992), tensor(1.3302620649), tensor(1.1126239300), tensor(1.4836068153), tensor(0.8755418658), tensor(1.2669163942), tensor(1.0871202946), tensor(1.6223955154), tensor(0.9925064445), tensor(1.3381427526), tensor(0.8228113055), tensor(1.3342995644), tensor(1.3082073927), tensor(0.9521822333), tensor(0.9348982573), tensor(1.2376409769), tensor(1.6615406275), tensor(1.0349470377), tensor(1.2073109150)]\n",
            "c:  [tensor(0.0210593417), tensor(0.5876980424), tensor(0.5606678128), tensor(-0.0166493021), tensor(-0.0017016034), tensor(-0.0133687863), tensor(0.0048207557), tensor(0.0051854639), tensor(0.4099117517), tensor(-0.0161073543), tensor(-0.0344963297), tensor(-0.2170285881), tensor(-0.1110190302), tensor(0.3927618265), tensor(-0.0102066211), tensor(-0.0189912543), tensor(-0.2083338499), tensor(-0.1127392575)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([4.6720647812, 5.9567494392, 5.9533658028, 5.5447516441, 4.9468865395,\n",
            "        4.6959409714, 6.1213598251, 5.1055479050, 6.7531852722, 5.0128684044,\n",
            "        5.2903804779, 4.4217290878, 5.8351821899, 4.9805498123, 5.3571748734,\n",
            "        6.0536437035, 5.6338620186, 6.4655337334, 5.8486423492, 6.3776750565])\n",
            "btensor.grad: tensor([4.2077851295, 5.4315795898, 3.8906435966, 4.5542898178, 4.5878105164,\n",
            "        4.3611869812, 3.8167400360, 4.7464618683, 4.4643011093, 5.0834765434,\n",
            "        4.5129003525, 3.6627349854, 4.3375105858, 3.8147454262, 4.2988538742,\n",
            "        5.0106363297, 4.2979907990, 3.7605311871, 4.3676567078, 4.9311351776])\n",
            "ctensor.grad: tensor([-2.7535221100e+01, -8.0338878632e+00, -7.6589407921e+00,\n",
            "         5.6765204668e-01,  2.5160798803e-02,  1.6430411339e+00,\n",
            "         9.2484515905e-01,  1.9707512856e+00,  5.3520561218e+01,\n",
            "         2.5611608028e+00,  1.4270938110e+02,  7.4515823364e+01,\n",
            "         1.7202879333e+02,  5.0478775024e+01,  2.4369528294e+00,\n",
            "         1.3531655884e+02,  7.0901206970e+01,  1.6389169312e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1112.9354248047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4905382395), tensor(1.0812655687), tensor(1.1264876127), tensor(1.2129675150), tensor(1.1951904297), tensor(1.2888407707), tensor(1.0272123814), tensor(1.2245455980), tensor(1.0374882221), tensor(1.1693835258), tensor(1.0614172220), tensor(1.4440234900), tensor(1.0825504065), tensor(1.0778851509), tensor(1.1357928514), tensor(0.9662041068), tensor(1.0190777779), tensor(0.9666074514), tensor(1.1577339172), tensor(1.0295608044)]\n",
            "b:  [tensor(0.8136643171), tensor(1.2658963203), tensor(1.3975658417), tensor(1.1862069368), tensor(1.5584460497), tensor(0.9470328093), tensor(1.3396667242), tensor(1.1698594093), tensor(1.7009421587), tensor(1.0627571344), tensor(1.4079357386), tensor(0.8834111094), tensor(1.4091155529), tensor(1.3721172810), tensor(1.0193984509), tensor(1.0154945850), tensor(1.3104799986), tensor(1.7319805622), tensor(1.1049957275), tensor(1.2866271734)]\n",
            "c:  [tensor(-0.0428432822), tensor(0.5272340775), tensor(0.5037521720), tensor(-0.0103029516), tensor(-0.0016694596), tensor(-0.0026406413), tensor(0.0099621881), tensor(0.0177733395), tensor(0.5823815465), tensor(-0.0151779484), tensor(0.2518830001), tensor(-0.0953494534), tensor(0.1853958219), tensor(0.5625154376), tensor(-0.0092958240), tensor(0.2642610967), tensor(-0.0870130882), tensor(0.1806477308)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-14.8691596985, -26.0058650970, -25.8103675842, -21.4836902618,\n",
            "        -18.5441188812, -17.9925460815, -28.3492717743, -19.0886211395,\n",
            "        -32.0401916504, -22.8077545166, -26.3468952179, -16.1311206818,\n",
            "        -30.3410911560, -21.3123378754, -23.2239627838, -29.6905059814,\n",
            "        -22.3007907867, -33.6048240662, -26.0979843140, -28.0307865143])\n",
            "btensor.grad: tensor([-11.9757947922, -15.6442680359, -13.4607610703, -14.7166061401,\n",
            "        -14.9678544998, -14.2981939316, -14.5500640869, -16.5478324890,\n",
            "        -15.7093248367, -14.0501279831, -13.9585971832, -12.1199579239,\n",
            "        -14.9631996155, -12.7819728851, -13.4432468414, -16.1192665100,\n",
            "        -14.5677967072, -14.0879802704, -14.0097274780, -15.8632526398])\n",
            "ctensor.grad: tensor([ 1.2780524445e+02,  1.2092788696e+02,  1.1383125305e+02,\n",
            "        -1.2692701340e+01, -6.4287573099e-02, -2.1456289291e+01,\n",
            "        -1.0282864571e+01, -2.5175750732e+01, -3.4493957520e+02,\n",
            "        -1.8588118553e+00, -5.7275866699e+02, -2.4335826111e+02,\n",
            "        -5.9282965088e+02, -3.3950723267e+02, -1.8215939999e+00,\n",
            "        -5.6650469971e+02, -2.4264151001e+02, -5.8677398682e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1195.2404785156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4322135448), tensor(0.9784405231), tensor(1.0398046970), tensor(1.1291855574), tensor(1.1006095409), tensor(1.2246774435), tensor(0.9414507151), tensor(1.1325864792), tensor(0.9350858331), tensor(1.0929189920), tensor(0.9780525565), tensor(1.3733943701), tensor(0.9854148030), tensor(1.0135999918), tensor(1.0573827028), tensor(0.8753933907), tensor(0.9368279576), tensor(0.8653873205), tensor(1.0658367872), tensor(0.9347462654)]\n",
            "b:  [tensor(0.6659201384), tensor(1.1398177147), tensor(1.2978893518), tensor(1.0611711740), tensor(1.4529898167), tensor(0.8222194910), tensor(1.2298774719), tensor(1.0728251934), tensor(1.6204136610), tensor(0.9224098921), tensor(1.2919007540), tensor(0.7457328439), tensor(1.3034622669), tensor(1.2561175823), tensor(0.8978978395), tensor(0.9181885719), tensor(1.2151751518), tensor(1.6308910847), tensor(0.9878883958), tensor(1.1721968651)]\n",
            "c:  [tensor(-0.0110065676), tensor(0.4439750016), tensor(0.4204669595), tensor(-0.0027663456), tensor(-0.0015160807), tensor(0.0136476057), tensor(0.0190266110), tensor(0.0384074822), tensor(0.5805925727), tensor(-0.0152676152), tensor(0.2465305328), tensor(-0.0991327316), tensor(0.1752417684), tensor(0.5608283281), tensor(-0.0093820691), tensor(0.2592713535), tensor(-0.0905300826), tensor(0.1710368693)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([11.6649274826, 20.5650119781, 17.3365879059, 16.7563915253,\n",
            "        18.9161891937, 12.8326606750, 17.1523399353, 18.3918285370,\n",
            "        20.4804744720, 15.2929067612, 16.6729297638, 14.1258172989,\n",
            "        19.4271163940, 12.8570423126, 15.6820373535, 18.1621379852,\n",
            "        16.4499626160, 20.2440299988, 18.3794288635, 18.9629039764])\n",
            "btensor.grad: tensor([29.5488414764, 25.2157154083, 19.9352989197, 25.0071525574,\n",
            "        21.0912475586, 24.9626617432, 21.9578609467, 19.4068393707,\n",
            "        16.1056900024, 28.0694465637, 23.2069950104, 27.5356578827,\n",
            "        21.1306476593, 23.1999340057, 24.3001251221, 19.4611968994,\n",
            "        19.0609703064, 20.2178897858, 23.4214706421, 22.8860530853])\n",
            "ctensor.grad: tensor([-63.6734237671, 166.5181579590, 166.5704498291, -15.0732107162,\n",
            "         -0.3067578971, -32.5764923096, -18.1288433075, -41.2682838440,\n",
            "          3.5779864788,   0.1793328673,  10.7049350739,   7.5665526390,\n",
            "         20.3081016541,   3.3741843700,   0.1724903733,   9.9794816971,\n",
            "          7.0339860916,  19.2217330933])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1107.2685546875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4087977409), tensor(0.9156310558), tensor(0.9899364710), tensor(1.0828896761), tensor(1.0436267853), tensor(1.1974836588), tensor(0.8954728842), tensor(1.0795501471), tensor(0.8703896403), tensor(1.0557460785), tensor(0.9338310361), tensor(1.3394346237), tensor(0.9257389903), tensor(0.9894306064), tensor(1.0163769722), tensor(0.8255097270), tensor(0.8944898844), tensor(0.8038726449), tensor(1.0121427774), tensor(0.8809436560)]\n",
            "b:  [tensor(0.5848808289), tensor(1.0679119825), tensor(1.2451871634), tensor(0.9899688363), tensor(1.3946294785), tensor(0.7573466301), tensor(1.1702382565), tensor(1.0289150476), tensor(1.5824553967), tensor(0.8419720531), tensor(1.2256309986), tensor(0.6726008058), tensor(1.2450326681), tensor(1.1916540861), tensor(0.8320126534), tensor(0.8757992983), tensor(1.1691561937), tensor(1.5750836134), tensor(0.9257781506), tensor(1.1108233929)]\n",
            "c:  [tensor(-0.0050946712), tensor(0.3990088701), tensor(0.3754604161), tensor(-0.0019202335), tensor(-0.0014857060), tensor(0.0159900784), tensor(0.0205334760), tensor(0.0417529345), tensor(0.5792983770), tensor(-0.0153353903), tensor(0.2424739450), tensor(-0.1019900516), tensor(0.1675361693), tensor(0.5596331358), tensor(-0.0094461255), tensor(0.2555668354), tensor(-0.0931325406), tensor(0.1638872325)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.6831650734, 12.5618953705,  9.9736433029,  9.2591695786,\n",
            "        11.3965606689,  5.4387631416,  9.1955604553, 10.6072778702,\n",
            "        12.9392414093,  7.4345912933,  8.8443040848,  6.7919492722,\n",
            "        11.9351606369,  4.8338723183,  8.2011547089,  9.9767303467,\n",
            "         8.4676113129, 12.3029394150, 10.7387914658, 10.7605199814])\n",
            "btensor.grad: tensor([16.2078647614, 14.3811416626, 10.5404348373, 14.2404699326,\n",
            "        11.6720724106, 12.9745740891, 11.9278345108,  8.7820310593,\n",
            "         7.5916576385, 16.0875682831, 13.2539634705, 14.6264114380,\n",
            "        11.6859245300, 12.8927049637, 13.1770362854,  8.4778585434,\n",
            "         9.2037982941, 11.1615018845, 12.4220466614, 12.2746849060])\n",
            "ctensor.grad: tensor([-1.1823792458e+01,  8.9932250977e+01,  9.0013069153e+01,\n",
            "        -1.6922243834e+00, -6.0749474913e-02, -4.6849446297e+00,\n",
            "        -3.0137290955e+00, -6.6909012794e+00,  2.5884199142e+00,\n",
            "         1.3555100560e-01,  8.1131744385e+00,  5.7146420479e+00,\n",
            "         1.5411188126e+01,  2.3904337883e+00,  1.2811221182e-01,\n",
            "         7.4090161324e+00,  5.2049164772e+00,  1.4299261093e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1081.9190673828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4037693739), tensor(0.8770412207), tensor(0.9614776373), tensor(1.0580137968), tensor(1.0089246035), tensor(1.1900891066), tensor(0.8724650741), tensor(1.0492668152), tensor(0.8288412690), tensor(1.0403114557), tensor(0.9119861126), tensor(1.3254809380), tensor(0.8886072040), tensor(0.9864390492), tensor(0.9965574741), tensor(0.7997490168), tensor(0.8749678135), tensor(0.7665770054), tensor(0.9808864594), tensor(0.8513960838)]\n",
            "b:  [tensor(0.5470757484), tensor(1.0275121927), tensor(1.2181172371), tensor(0.9503194094), tensor(1.3622583151), tensor(0.7274333835), tensor(1.1386001110), tensor(1.0125815868), tensor(1.5664879084), tensor(0.7982897758), tensor(1.1877272129), tensor(0.6388677359), tensor(1.2128922939), tensor(1.1564675570), tensor(0.7986629605), tensor(0.8616864681), tensor(1.1492002010), tensor(1.5439757109), tensor(0.8949168921), tensor(1.0791250467)]\n",
            "c:  [tensor(-0.0029042261), tensor(0.3760778010), tensor(0.3524760604), tensor(-0.0018967133), tensor(-0.0014752743), tensor(0.0164223015), tensor(0.0209258515), tensor(0.0425948575), tensor(0.5782644749), tensor(-0.0153926676), tensor(0.2390730530), tensor(-0.1043809950), tensor(0.1610634774), tensor(0.5586960912), tensor(-0.0094994046), tensor(0.2525170743), tensor(-0.0952708498), tensor(0.1579870880)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([1.0056658983, 7.7179641724, 5.6917695999, 4.9751691818, 6.9404392242,\n",
            "        1.4789046049, 4.6015620232, 6.0566644669, 8.3096733093, 3.0869314671,\n",
            "        4.3689908981, 2.7907261848, 7.4263601303, 0.5983141661, 3.9639034271,\n",
            "        5.1521425247, 3.9044103622, 7.4591274261, 6.2512588501, 5.9095096588])\n",
            "btensor.grad: tensor([7.5610203743, 8.0799694061, 5.4139943123, 7.9298825264, 6.4742264748,\n",
            "        5.9826526642, 6.3276271820, 3.2666971684, 3.1934924126, 8.7364540100,\n",
            "        7.5807647705, 6.7466197014, 6.4280676842, 7.0373077393, 6.6699395180,\n",
            "        2.8225679398, 3.9911937714, 6.2215886116, 6.1722564697, 6.3396692276])\n",
            "ctensor.grad: tensor([-4.3808898926e+00,  4.5862163544e+01,  4.5968692780e+01,\n",
            "        -4.7040387988e-02, -2.0863257349e-02, -8.6444640160e-01,\n",
            "        -7.8475093842e-01, -1.6838483810e+00,  2.0677471161e+00,\n",
            "         1.1455466598e-01,  6.8017845154e+00,  4.7818908691e+00,\n",
            "         1.2945385933e+01,  1.8741255999e+00,  1.0655871779e-01,\n",
            "         6.0995039940e+00,  4.2766146660e+00,  1.1800282478e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1074.8498535156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4075018167), tensor(0.8524861336), tensor(0.9447877407), tensor(1.0446271896), tensor(0.9868870378), tensor(1.1922276020), tensor(0.8616931438), tensor(1.0315960646), tensor(0.8010446429), tensor(1.0358468294), tensor(0.9019506574), tensor(1.3216116428), tensor(0.8645405769), tensor(0.9931157827), tensor(0.9878717661), tensor(0.7871307135), tensor(0.8673787713), tensor(0.7433291078), tensor(0.9621568322), tensor(0.8352724314)]\n",
            "b:  [tensor(0.5330740213), tensor(1.0048702955), tensor(1.2045432329), tensor(0.9284971356), tensor(1.3439536095), tensor(0.7157820463), tensor(1.1220682859), tensor(1.0092228651), tensor(1.5613665581), tensor(0.7757954001), tensor(1.1656950712), tensor(0.6261101961), tensor(1.1950461864), tensor(1.1374589205), tensor(0.7829995751), tensor(0.8603226542), tensor(1.1421784163), tensor(1.5261391401), tensor(0.8806539178), tensor(1.0633026361)]\n",
            "c:  [tensor(-0.0018465305), tensor(0.3650400043), tensor(0.3413709700), tensor(-0.0019966515), tensor(-0.0014704696), tensor(0.0164673645), tensor(0.0210546516), tensor(0.0428550281), tensor(0.5773432255), tensor(-0.0154463956), tensor(0.2359226942), tensor(-0.1065964252), tensor(0.1550547779), tensor(0.5578744411), tensor(-0.0095486799), tensor(0.2497350425), tensor(-0.0972214788), tensor(0.1525925547)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.7464791536,  4.9110217094,  3.3379797935,  2.6773226261,\n",
            "         4.4075093269, -0.4276888371,  2.1543810368,  3.5341515541,\n",
            "         5.5593309402,  0.8929249644,  2.0070965290,  0.7738558054,\n",
            "         4.8133316040, -1.3353472948,  1.7371358871,  2.5236644745,\n",
            "         1.5178081989,  4.6495742798,  3.7459254265,  3.2247340679])\n",
            "btensor.grad: tensor([2.8003435135, 4.5283803940, 2.7148096561, 4.3644590378, 3.6609406471,\n",
            "        2.3302621841, 3.3063673973, 0.6717362404, 1.0242732763, 4.4988780022,\n",
            "        4.4064207077, 2.5515084267, 3.5692150593, 3.8017334938, 3.1326735020,\n",
            "        0.2727676630, 1.4043548107, 3.5673117638, 2.8525915146, 3.1644873619])\n",
            "ctensor.grad: tensor([-2.1153910160e+00,  2.2075597763e+01,  2.2210205078e+01,\n",
            "         1.9987657666e-01, -9.6092876047e-03, -9.0127602220e-02,\n",
            "        -2.5759825110e-01, -5.2034223080e-01,  1.8425290585e+00,\n",
            "         1.0745596141e-01,  6.3007302284e+00,  4.4308662415e+00,\n",
            "         1.2017391205e+01,  1.6432938576e+00,  9.8550610244e-02,\n",
            "         5.5640578270e+00,  3.9012610912e+00,  1.0789072037e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7266845703, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4147894382), tensor(0.8358559012), tensor(0.9342796206), tensor(1.0370597839), tensor(0.9718866944), tensor(1.1982765198), tensor(0.8569172621), tensor(1.0206393003), tensor(0.7813093066), tensor(1.0363192558), tensor(0.8976433873), tensor(1.3223426342), tensor(0.8478975892), tensor(1.0032211542), tensor(0.9845842123), tensor(0.7811261415), tensor(0.8654398322), tensor(0.7279471159), tensor(0.9501625299), tensor(0.8261507154)]\n",
            "b:  [tensor(0.5297411680), tensor(0.9919264317), tensor(1.1977865696), tensor(0.9163703322), tensor(1.3331065178), tensor(0.7125663161), tensor(1.1133956909), tensor(1.0111553669), tensor(1.5611813068), tensor(0.7646722794), tensor(1.1524027586), tensor(0.6229043603), tensor(1.1847845316), tensor(1.1271624565), tensor(0.7762168646), tensor(0.8636877537), tensor(1.1410356760), tensor(1.5153121948), tensor(0.8745822310), tensor(1.0555642843)]\n",
            "c:  [tensor(-0.0012747943), tensor(0.3600635529), tensor(0.3363097310), tensor(-0.0020962672), tensor(-0.0014677903), tensor(0.0164318364), tensor(0.0211059414), tensor(0.0429494679), tensor(0.5764457583), tensor(-0.0155007150), tensor(0.2327762991), tensor(-0.1088137329), tensor(0.1490409672), tensor(0.5570854545), tensor(-0.0095978389), tensor(0.2469938695), tensor(-0.0991467759), tensor(0.1472665519)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.4575202465,  3.3260416985,  2.1016221046,  1.5134813786,\n",
            "         3.0000703335, -1.2097830772,  0.9551794529,  2.1913638115,\n",
            "         3.9470725060, -0.0944749787,  0.8614506721, -0.1462033987,\n",
            "         3.3285999298, -2.0210726261,  0.6575118303,  1.2009105682,\n",
            "         0.3877924085,  3.0763974190,  2.3988637924,  1.8243404627])\n",
            "btensor.grad: tensor([ 0.6665661931,  2.5887732506,  1.3513276577,  2.4253592491,\n",
            "         2.1694207191,  0.6431401372,  1.7345197201, -0.3865122795,\n",
            "         0.0370514393,  2.2246234417,  2.6584730148,  0.6411645412,\n",
            "         2.0523331165,  2.0592932701,  1.3565404415, -0.6730186343,\n",
            "         0.2285409868,  2.1653909683,  1.2143335342,  1.5476685762])\n",
            "ctensor.grad: tensor([-1.1434720755e+00,  9.9529027939e+00,  1.0122461319e+01,\n",
            "         1.9923134148e-01, -5.3585739806e-03,  7.1056351066e-02,\n",
            "        -1.0257837921e-01, -1.8887820840e-01,  1.7948838472e+00,\n",
            "         1.0863792151e-01,  6.2927808762e+00,  4.4346175194e+00,\n",
            "         1.2027621269e+01,  1.5779833794e+00,  9.8318710923e-02,\n",
            "         5.4823522568e+00,  3.8505992889e+00,  1.0651994705e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.8701171875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4230842590), tensor(0.8237997293), tensor(0.9270094633), tensor(1.0323741436), tensor(0.9609022141), tensor(1.2054806948), tensor(0.8548948169), tensor(1.0132615566), tensor(0.7664588690), tensor(1.0387076139), tensor(0.8959322572), tensor(1.3249186277), tensor(0.8355906606), tensor(1.0138109922), tensor(0.9837437868), tensor(0.7782728076), tensor(0.8659205437), tensor(0.7170175314), tensor(0.9418154955), tensor(0.8206046224)]\n",
            "b:  [tensor(0.5303623080), tensor(0.9842805266), tensor(1.1944178343), tensor(0.9094789028), tensor(1.3262617588), tensor(0.7128297091), tensor(1.1087831259), tensor(1.0147722960), tensor(1.5630168915), tensor(0.7593901753), tensor(1.1440064907), tensor(0.6234799027), tensor(1.1785789728), tensor(1.1215858459), tensor(0.7736581564), tensor(0.8681911826), tensor(1.1423013210), tensor(1.5082507133), tensor(0.8723495603), tensor(1.0518465042)]\n",
            "c:  [tensor(-0.0009339311), tensor(0.3581197858), tensor(0.3342583776), tensor(-0.0021769607), tensor(-0.0014660726), tensor(0.0163860209), tensor(0.0211309716), tensor(0.0429905355), tensor(0.5755184889), tensor(-0.0155581962), tensor(0.2294806540), tensor(-0.1111437902), tensor(0.1427285969), tensor(0.5562815070), tensor(-0.0096491566), tensor(0.2441601157), tensor(-0.1011426449), tensor(0.1417506635)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.6589725018,  2.4112367630,  1.4540333748,  0.9371213913,\n",
            "         2.1968929768, -1.4408400059,  0.4044926465,  1.4755506516,\n",
            "         2.9700901508, -0.4776825011,  0.3422237635, -0.5152004957,\n",
            "         2.4613857269, -2.1179735661,  0.1680907607,  0.5706620216,\n",
            "        -0.0961482525,  2.1859228611,  1.6694010496,  1.1092185974])\n",
            "btensor.grad: tensor([-0.1242251396,  1.5291850567,  0.6737414002,  1.3782804012,\n",
            "         1.3689548969, -0.0526733398,  0.9225056171, -0.7233892083,\n",
            "        -0.3671250343,  1.0564217567,  1.6792597771, -0.1151086390,\n",
            "         1.2411082983,  1.1153301001,  0.5117442608, -0.9006877542,\n",
            "        -0.2531273961,  1.4123057127,  0.4465374351,  0.7435481548])\n",
            "ctensor.grad: tensor([-6.8172651529e-01,  3.8875486851e+00,  4.1027355194e+00,\n",
            "         1.6138716042e-01, -3.4353376832e-03,  9.1629259288e-02,\n",
            "        -5.0060100853e-02, -8.2134842873e-02,  1.8545856476e+00,\n",
            "         1.1496172100e-01,  6.5912914276e+00,  4.6601214409e+00,\n",
            "         1.2624749184e+01,  1.6078429222e+00,  1.0263516009e-01,\n",
            "         5.6675186157e+00,  3.9917447567e+00,  1.1031774521e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.3511962891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4312328100), tensor(0.8145466447), tensor(0.9214986563), tensor(1.0291280746), tensor(0.9523714185), tensor(1.2126003504), tensor(0.8540504575), tensor(1.0078667402), tensor(0.7547664642), tensor(1.0416114330), tensor(0.8953109980), tensor(1.3280400038), tensor(0.8259840012), tensor(1.0236593485), tensor(0.9839237928), tensor(0.7768522501), tensor(0.8672778010), tensor(0.7087340951), tensor(0.9355379939), tensor(0.8169066310)]\n",
            "b:  [tensor(0.5321349502), tensor(0.9796035290), tensor(1.1927380562), tensor(0.9054648876), tensor(1.3216512203), tensor(0.7143234015), tensor(1.1063033342), tensor(1.0185353756), tensor(1.5655057430), tensor(0.7570487857), tensor(1.1384712458), tensor(0.6253169775), tensor(1.1746186018), tensor(1.1186492443), tensor(0.7730313540), tensor(0.8724235892), tensor(1.1443713903), tensor(1.5033081770), tensor(0.8718414903), tensor(1.0501174927)]\n",
            "c:  [tensor(-0.0007067662), tensor(0.3577300906), tensor(0.3337306976), tensor(-0.0022401214), tensor(-0.0014648645), tensor(0.0163451750), tensor(0.0211456046), tensor(0.0430119671), tensor(0.5745248199), tensor(-0.0156207243), tensor(0.2259262800), tensor(-0.1136666909), tensor(0.1359056383), tensor(0.5554327369), tensor(-0.0097041475), tensor(0.2411462665), tensor(-0.1032725871), tensor(0.1358737350)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.6296995878,  1.8506182432,  1.1021652222,  0.6492114067,\n",
            "         1.7061553001, -1.4239294529,  0.1688691676,  1.0789580345,\n",
            "         2.3384792805, -0.5807591677,  0.1242559552, -0.6242737174,\n",
            "         1.9213328362, -1.9696702957, -0.0359988809,  0.2841140628,\n",
            "        -0.2714501619,  1.6566822529,  1.2555036545,  0.7395961881])\n",
            "btensor.grad: tensor([-0.3545297980,  0.9354031086,  0.3359560370,  0.8028080463,\n",
            "         0.9221108556, -0.2987339497,  0.4959582686, -0.7526059151,\n",
            "        -0.4977793097,  0.4682763815,  1.1070420742, -0.3674128652,\n",
            "         0.7920650840,  0.5873254538,  0.1253566146, -0.8464795351,\n",
            "        -0.4140138328,  0.9885145426,  0.1016165614,  0.3458021879])\n",
            "ctensor.grad: tensor([-4.5432972908e-01,  7.7940952778e-01,  1.0553404093e+00,\n",
            "         1.2632146478e-01, -2.4164069910e-03,  8.1691846251e-02,\n",
            "        -2.9266985133e-02, -4.2860396206e-02,  1.9873484373e+00,\n",
            "         1.2505599856e-01,  7.1087508202e+00,  5.0458045006e+00,\n",
            "         1.3645916939e+01,  1.6975388527e+00,  1.0998262465e-01,\n",
            "         6.0276994705e+00,  4.2598915100e+00,  1.1753843307e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Score table after training\n",
            "                 Arsenal          Birmingham       Blackburn        Fulham           Leicester        Man United       Portsmouth       Charlton         Leeds            Liverpool        Bolton           Chelsea          Everton          Man City         Newcastle        Southampton      Tottenham        Wolves           Aston Villa      Middlesbrough    \n",
            "Arsenal          1.107            1.747            2.052            1.641            2.237            1.368            1.929            1.803            2.586            1.429            1.975            1.240            2.026            1.947            1.452            1.594            1.983            2.497            1.593            1.848            \n",
            "Birmingham       0.778            1.138            1.315            1.081            1.419            0.926            1.245            1.171            1.617            0.962            1.272            0.853            1.299            1.256            0.974            1.055            1.276            1.566            1.054            1.200            \n",
            "Blackburn        0.836            1.246            1.444            1.179            1.562            1.004            1.364            1.283            1.787            1.043            1.395            0.921            1.427            1.376            1.058            1.149            1.400            1.730            1.149            1.313            \n",
            "Fulham           0.893            1.351            1.572            1.276            1.704            1.080            1.483            1.392            1.955            1.124            1.517            0.989            1.553            1.497            1.141            1.243            1.523            1.891            1.242            1.426            \n",
            "Leicester        0.852            1.275            1.480            1.207            1.603            1.025            1.398            1.313            1.834            1.066            1.429            0.940            1.463            1.411            1.081            1.176            1.435            1.775            1.175            1.345            \n",
            "Man United       0.991            1.532            1.792            1.443            1.948            1.212            1.687            1.580            2.243            1.264            1.726            1.104            1.770            1.702            1.283            1.403            1.733            2.168            1.403            1.619            \n",
            "Portsmouth       0.800            1.180            1.364            1.118            1.473            0.955            1.290            1.214            1.681            0.992            1.318            0.879            1.348            1.301            1.005            1.090            1.322            1.628            1.090            1.242            \n",
            "Charlton         0.881            1.329            1.546            1.257            1.676            1.065            1.459            1.370            1.921            1.108            1.493            0.975            1.527            1.473            1.124            1.224            1.498            1.858            1.223            1.403            \n",
            "Leeds            0.747            1.081            1.245            1.027            1.341            0.884            1.179            1.112            1.525            0.917            1.204            0.816            1.230            1.189            0.928            1.003            1.208            1.478            1.003            1.137            \n",
            "Liverpool        0.900            1.365            1.588            1.289            1.722            1.090            1.498            1.406            1.976            1.134            1.531            0.997            1.569            1.511            1.151            1.254            1.537            1.911            1.254            1.439            \n",
            "Bolton           0.822            1.222            1.413            1.156            1.529            0.985            1.336            1.257            1.747            1.023            1.365            0.905            1.397            1.347            1.038            1.127            1.370            1.691            1.126            1.286            \n",
            "Chelsea          1.052            1.645            1.929            1.547            2.100            1.294            1.814            1.697            2.424            1.351            1.857            1.176            1.905            1.831            1.372            1.504            1.865            2.341            1.503            1.740            \n",
            "Everton          0.785            1.151            1.330            1.092            1.436            0.935            1.258            1.185            1.637            0.971            1.286            0.861            1.314            1.269            0.983            1.066            1.290            1.585            1.065            1.212            \n",
            "Man City         0.890            1.347            1.566            1.272            1.698            1.077            1.478            1.388            1.948            1.121            1.511            0.986            1.548            1.491            1.137            1.239            1.517            1.884            1.238            1.420            \n",
            "Newcastle        0.869            1.308            1.519            1.236            1.645            1.048            1.434            1.347            1.885            1.090            1.466            0.961            1.501            1.446            1.106            1.204            1.471            1.824            1.203            1.379            \n",
            "Southampton      0.759            1.105            1.272            1.048            1.372            0.900            1.205            1.136            1.561            0.934            1.230            0.831            1.257            1.215            0.946            1.023            1.234            1.513            1.023            1.161            \n",
            "Tottenham        0.807            1.194            1.380            1.130            1.491            0.965            1.305            1.228            1.702            1.002            1.333            0.888            1.364            1.316            1.016            1.102            1.338            1.648            1.101            1.256            \n",
            "Wolves           0.722            1.036            1.190            0.986            1.280            0.851            1.128            1.065            1.453            0.882            1.152            0.788            1.176            1.138            0.893            0.963            1.156            1.409            0.962            1.089            \n",
            "Aston Villa      0.843            1.260            1.461            1.192            1.581            1.014            1.380            1.297            1.809            1.054            1.411            0.930            1.444            1.392            1.068            1.161            1.416            1.751            1.161            1.328            \n",
            "Middlesbrough    0.780            1.145            1.320            1.085            1.425            0.929            1.249            1.177            1.624            0.964            1.276            0.856            1.305            1.259            0.977            1.058            1.280            1.573            1.058            1.203            \n",
            "\n",
            "\n",
            "\n",
            "=============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Historic Bookies Estimate:\n",
        "==========================\n",
        "\n",
        "The historic bookies estimate for goals landed by team i against team j is\n",
        "\n",
        "a_i b_j\n",
        "\n",
        "where\n",
        "\n",
        "a_i = A_i C^{-1/2}\n",
        "b_j = B_j C^{-1/2}\n",
        "\n",
        "A_i = goals landed per game by team i\n",
        "B_j = goals conceded per game by team j\n",
        "C   = average goals per game of all teams\n",
        "\n",
        "\n",
        "The historic 1980's max likelihood models\n",
        "=========================================\n",
        "\n",
        "Starting with Maher, the 1980's max likelihood model starts with this guess and\n",
        "perfects it by max likelihood, when team i lands k goals agaist team j\n",
        "loss was minus the log of the predicted probability by Poisson\n",
        "\n",
        "- log ( e^{-a_ib_j}(a_ib_j)^k /k!)\n",
        "\n",
        "\n",
        "A gitgub user said chi squared shows HST,AST,HR,AR have a significant effect\n",
        "\n",
        "HST = home shots on target\n",
        "AST = away shots on target\n",
        "HR  = home red cards\n",
        "AR  = away red cards\n",
        "HS  = home shots\n",
        "AS  = away shots\n",
        "HC  = home quarter kicks\n",
        "AC  = away corner kicks\n",
        "HF  = home fouls\n",
        "AF  = away fouls\n",
        "\n",
        "\n",
        "New Cross-entropy AI model with a neural layer\n",
        "==========================================\n",
        "\n",
        "Since gradient descent generalizes max likelihood we can replace a_ib_j by\n",
        "\n",
        "a_i b_j  +  (c_0 sigma ( c_3 HST + c_4 HR +c_5 HS + c_6 HC _c_7 HF)\n",
        "             + ...\n",
        "             +c_2 sigma(c_13 HST + c_14 HR + c_15 HS + c_16 HCC + c_17 HF) )b_j\n",
        "\n",
        "when i is the home team and\n",
        "\n",
        "a_i b_j  +  c_0 sigma ( c_1 AST + c_2 AR  ..... +AF  ) b_j\n",
        "\n",
        "when i is the away team, with sigma being the sigmoid function\n",
        "\n",
        "\n",
        "   sigma(x)=softmax(0,x) = e^x/(e^0+e^x) = 1/(1+e^{-x})\n",
        "\n",
        "\n",
        "\n",
        "This puts a *neural layer* behind the standard max likelihood model from the 1980s\n",
        "\n",
        "The weights are now the a_i,  b_i,   and c_i\n",
        "\n",
        "Since the c_i are shared by all teams the training rate for the c_i should be lower\n",
        "\n",
        "As in the model which this generalizes, the training is cross entropy versus the Poisson distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A comment about the way the databases are stored, variables like HST and AST refer to 'home'\n",
        "and 'away' team but we do not make a distinction between home versus away.\n",
        "\n",
        "This means that each row of a data table is interpreted as if it were two rows,\n",
        "one giving information about team i against team j, the other giving information\n",
        "about team j against team i.\n",
        "\n",
        "For instance to calculate the average goals scored by any team over all games\n",
        "each row gives goals scored by a home team and goals scored by an away team\n",
        "and we have to add 2 to total games.\n",
        "\n",
        "That is when we say total games it really means the sum over all teams\n",
        "of the number of games that team played in, which is twice the number\n",
        "of games.\n",
        "\n",
        "That explains the line  totalgames=totalgames+2 each time a row is read in.\n",
        "\n",
        "There is no need to change this architecture to include things causing a home\n",
        "team advantage. This starts with a constant taking the value 1 in the first line\n",
        "\n",
        "loss -=  ...\n",
        "\n",
        "and taking the value 0 in the second line\n",
        "\n",
        "loss -= ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import torch as t\n",
        "import torch.nn as n\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def sigma(x):\n",
        "  return t.exp(x)/(t.exp(t.tensor(1.0))+t.exp(x))-t.tensor(0.5)\n",
        "\n",
        "\n",
        "\n",
        "t.set_printoptions(precision=10)\n",
        "#print(\"beep boop\")\n",
        "#print(\"Aston Villa loses\")\n",
        "print(\"=============================================================\")\n",
        "\n",
        "#GITHUB LOCATION:\n",
        "#https://github.com/Pavlos01232/Match_Outcome_Prediction\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0304.csv?raw=true')\n",
        "\n",
        "\n",
        "#\"DEEP learning\" just means \"hidden\" layers\n",
        "\n",
        "\n",
        "#df.to_csv(r'C:\\Users\\Pavlos\\Desktop\\export_dataframe.csv', sep='\\t', encoding='utf-8')\n",
        "#print (df[2])\n",
        "#file_list = os.listdir('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/')\n",
        "#df = pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0405.csv?raw=true',sep='\\t', lineterminator='\\r')\n",
        "#print(df)\n",
        "\n",
        "#read function\n",
        "\n",
        "first = \"https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL\"\n",
        "last = \".csv?raw=true\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Starting with an array of data frames,\n",
        "and an array of column names, make a\n",
        "single array with the chosen columns\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine(dataFrames,columnNames):\n",
        " t=[]\n",
        " for i in range(0,len(dataFrames)):\n",
        "    theseColumns=dataFrames[i].columns.values[0].split(\",\")\n",
        "    for j in range(0 ,len(dataFrames[i])):\n",
        "      row=dataFrames[i].values[j][0].split(\",\")\n",
        "      newEntry=[]\n",
        "      for k in range(0, len(columnNames)):\n",
        "         for m in range(0, len(theseColumns)):\n",
        "             if(columnNames[k]==theseColumns[m] and m<=len(row)):\n",
        "                newEntry.append(row[m])\n",
        "      t.append(newEntry)\n",
        " return t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "can read years 3 to 23, 13*********************, there's something wrong with 14\n",
        "since the first two files in the training data are formatted incorrectly\n",
        "converts csv to dataframe\n",
        "'''\n",
        "\n",
        "df=[]\n",
        "\n",
        "for i in range(3, 4):\n",
        "  result = first + str('{:02.0f}'.format(i)) + str('{:02.0f}'.format(i+1)) + last\n",
        "  x = pd.read_csv(result, sep='\\t', encoding = 'unicode_escape', lineterminator='\\r')\n",
        "  df.append(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get an array with each (home) team listed once\n",
        "from an array of data frames with column \"HomeTeam\"\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getTeams(df):\n",
        " teams=[]\n",
        " homeTeams=combine(df,[\"HomeTeam\"])\n",
        " for i in range(len(homeTeams)):\n",
        "  if(len(homeTeams[i])>0):\n",
        "   found=False\n",
        "   for j in range(len(teams)):\n",
        "    if homeTeams[i][0] == teams[j]:\n",
        "      found=True\n",
        "      break\n",
        "   if found:\n",
        "    continue\n",
        "   teams.append(homeTeams[i][0])\n",
        " return teams\n",
        "\n",
        "\n",
        "'''\n",
        "Get the list of teams from the array of data frames called df\n",
        "and print it to the console\n",
        "'''\n",
        "\n",
        "\n",
        "teams=getTeams(df)\n",
        "print(\"\\nteams:\")\n",
        "print(teams)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Create an array called Data with just the team names and scores\n",
        "from the data framees in the array of frames df, and print it\n",
        "\n",
        "The list [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\"] can be\n",
        "made longer if other columnts may be useful to use\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Data=combine(df, [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\", \"HS\",\"AS\",\"HC\",\"AC\",\"HF\",\"AF\"])\n",
        "#print(\"\\n\\ndata: (team names respective goals scored, respective shots on target, respective red cards, respective shots, respective corner-kicks, respective fouls)\")\n",
        "#print(Data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get the numerical index of a team name x in the array teams\n",
        "otherwise just return x\n",
        "'''\n",
        "\n",
        "def getIndex(x,teams):\n",
        "  for i in range(len(teams)):\n",
        "   if(teams[i]==x):\n",
        "    return i\n",
        "  return x\n",
        "\n",
        "\n",
        "#print(\"\\n\\nIndex assigned to Everton:\")\n",
        "#print(getIndex(\"Everton\",teams))\n",
        "\n",
        "\n",
        "'''\n",
        "Replace any occurrence of names from the array teams\n",
        "which occur anywhere in A by their actual  numbers\n",
        "'''\n",
        "\n",
        "def teamsToNumbers(A,teams):\n",
        "  B=[]\n",
        "  for i in range(len(A)):\n",
        "    B.append([])\n",
        "    for j in range(len(A[i])):\n",
        "      B[i].append(getIndex(A[i][j],teams))\n",
        "  return B\n",
        "\n",
        "\n",
        "'''\n",
        "Create Data2 which is a copy of Data but with team names\n",
        "replaced by their index\n",
        "'''\n",
        "\n",
        "#print(\"\\n\\ndata2, using the team's index number instead of name\")\n",
        "Data2=teamsToNumbers(Data,teams)\n",
        "print(len(Data2))\n",
        "\n",
        "\n",
        "'''\n",
        "remove game 12 from Data2\n",
        "Caution: these indices must be high enough not to disrupt the indices of team names\n",
        "'''\n",
        "\n",
        "excluded=[137, 141, 144, 150, 153, 163, 191, 191, 201]\n",
        "if(len(excluded)>0):\n",
        "  print(\"excluded: \"+str(excluded))\n",
        "  print(\"\\nCAUTION: check that the teams list was not disrupted by exclusions:\\n\"+str(teams))\n",
        "validateGame=[]\n",
        "for i in range(len(excluded)):\n",
        " validateGame.append(Data2.pop(i))\n",
        "\n",
        "'''\n",
        "\n",
        "Assume Data2 has team indices in column 0 and 1 and scores\n",
        "in cols 2 and 3\n",
        "\n",
        "A[i] is array of average goals landed per game by tean i\n",
        "B[i] is array of average goals conceded per game by team i\n",
        "C is total games played by all teams (twide the number of games)\n",
        "games[i]=total games played by team i\n",
        "a[i]*b[j]=first approx of expected goals landed by i when playing\n",
        "    against j\n",
        "'''\n",
        "\n",
        "A=[0]*len(teams)\n",
        "B=[0]*len(teams)\n",
        "games=[0]*len(teams)\n",
        "a=[0]*len(teams)\n",
        "b=[0]*len(teams)\n",
        "C=0\n",
        "totalGames=0\n",
        "\n",
        "for i in range(len(Data2)):\n",
        "  if(len(Data2[i])<2):\n",
        "    continue\n",
        "  games[Data2[i][0]]+=1\n",
        "  games[Data2[i][1]]+=1\n",
        "  A[Data2[i][0]]+=int(Data2[i][2])\n",
        "  B[Data2[i][0]]+=int(Data2[i][3])\n",
        "  A[Data2[i][1]]+=int(Data2[i][3])\n",
        "  B[Data2[i][1]]+=int(Data2[i][2])\n",
        "  C+=int(Data2[i][2])+int(Data2[i][3])\n",
        "  totalGames+=2\n",
        "\n",
        "\n",
        "'''\n",
        "Initial estimates of a,b,c\n",
        "'''\n",
        "\n",
        "for i in range(len(A)):\n",
        "  a[i]=A[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "for i in range(len(B)):\n",
        "  b[i]=B[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "#c is another set of hidden weights for our weightrix. they are initally nonzero to avoid a stationary point.\n",
        "\n",
        "c=[0.001,0.002,0.004,-0.001,-0.002,-0.004,0.0005,0.0001,0.01,0.002,0.005,0.007,0.009,0.003,0.007,0.009,0.002,-0.001]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "AI training function\n",
        "\n",
        "The training is by gradient descent, the loss\n",
        "function will be cross entropy loss function against Poisson\n",
        "using loss.backward()\n",
        "\n",
        "The hidden weights at the moment are the entries of a,b,c\n",
        "the array c  is shared for all teams.\n",
        "These enter into the calculation of mu (which we\n",
        "call muHome or muAway during training) and are\n",
        "hidden as they have no direct meaning.\n",
        "\n",
        "Thus mu as a function of the entries of a,b,c is\n",
        "learned by training, the weights are the entries\n",
        "of the three arrays.\n",
        "\n",
        "\n",
        "\n",
        "tau is the training rate for c which should be small\n",
        "compared to eta\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def elementOf(i,A):\n",
        "  for j in range(len(A)):\n",
        "    if(A[j]==i):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def train(eta,tau):\n",
        "  loss=t.tensor(0.0)\n",
        "\n",
        "\n",
        "\n",
        "  atensor=t.tensor(a,requires_grad=True)\n",
        "  btensor=t.tensor(b,requires_grad=True)\n",
        "  ctensor=t.tensor(c,requires_grad=True)\n",
        "\n",
        "\n",
        "  for i in range(len(Data2)):\n",
        "    if(len(Data2[i])==0):\n",
        "      continue\n",
        "    homeTeam=int(Data2[i][0])\n",
        "    awayTeam=int(Data2[i][1])\n",
        "    homeGoals=int(Data2[i][2])\n",
        "    awayGoals=int(Data2[i][3])\n",
        "    HST=t.tensor(float(Data2[i][4]))\n",
        "    AST=t.tensor(float(Data2[i][5]))\n",
        "    HR=t.tensor(float(Data2[i][6]))\n",
        "    AR=t.tensor(float(Data2[i][7]))\n",
        "    HS=t.tensor(float(Data2[i][8]))\n",
        "    AS=t.tensor(float(Data2[i][9]))\n",
        "    HC=t.tensor(float(Data2[i][10]))\n",
        "    AC=t.tensor(float(Data2[i][11]))\n",
        "    HF=t.tensor(float(Data2[i][12]))\n",
        "    AF=t.tensor(float(Data2[i][13]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    The reason there are two lines of code for the loss is that\n",
        "    each game can be thought of as  two 'rows' of data where we label\n",
        "    the home team as team i  or team j.\n",
        "\n",
        "    Thus teams are interpreted symmetrically and there is not yet any\n",
        "    home team advantage but this can be put in\n",
        "    without modifying the architecture as a constant which is 1 in the\n",
        "    first line and 0 in the second\n",
        "    '''\n",
        "\n",
        "\n",
        "    muHome=atensor[homeTeam]*btensor[awayTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*HST+ctensor[4]*HR+ctensor[5]*HS+ctensor[6]*HC+ctensor[7]*HF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*HST+ctensor[9]*HR+ctensor[10]*HS+ctensor[11]*HC+ctensor[12]*HF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*HST+ctensor[14]*HR+ctensor[15]*HS+ctensor[16]*HC+ctensor[17]*HF)\n",
        "    muHome=muHome+neural*btensor[awayTeam]\n",
        "\n",
        "    muAway=atensor[awayTeam]*btensor[homeTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*AST+ctensor[4]*AR+ctensor[5]*AS+ctensor[6]*AC+ctensor[7]*AF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*AST+ctensor[9]*AR+ctensor[10]*AS+ctensor[11]*AC+ctensor[12]*AF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*AST+ctensor[14]*AR+ctensor[15]*AS+ctensor[16]*AC+ctensor[17]*AF)\n",
        "    muAway=muAway+neural*btensor[homeTeam]\n",
        "\n",
        "\n",
        "    loss-=t.log(t.exp(-muHome)*t.pow(muHome,homeGoals)/math.factorial(homeGoals))\n",
        "    loss-=t.log(t.exp(-muAway)*t.pow(muAway,awayGoals)/math.factorial(awayGoals))\n",
        "\n",
        "  loss.backward()\n",
        "  for i in range(len(c)):\n",
        "    c[i]=c[i]-tau*ctensor.grad[i]\n",
        "  for i in range(len(a)):\n",
        "    a[i]=a[i]-eta*atensor.grad[i]\n",
        "  for i in range(len(b)):\n",
        "    b[i]=b[i]-eta*btensor.grad[i]\n",
        "  print(\"\\n\\nCross-entropy loss vs Poisson: \"+str(loss))\n",
        "  print(\"\\n\\nWeights:\\n\\na:  \"+str(a))\n",
        "  print(\"b:  \"+str(b))\n",
        "  print(\"c:  \"+str(c))\n",
        "  print(\"\\n\\n\\n\")\n",
        "  print(\"Partial derivatives of loss w/r to hidden weights:\")\n",
        "  print(\"\\natensor.grad: \"+str(atensor.grad))\n",
        "  print(\"btensor.grad: \"+str(btensor.grad))\n",
        "  print(\"ctensor.grad: \"+str(ctensor.grad))\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Use weights to construct predicted expected goals scored by i\n",
        "against j and then find probability of k goals scored using\n",
        "Poisson when given values of R S ST  C F are provided\n",
        "'''\n",
        "\n",
        "# non-tensor version of sigma for using in the field\n",
        "\n",
        "def mathsigma(x):\n",
        "  return math.exp(x)/(math.exp(0)+math.exp(x))-0.5\n",
        "\n",
        "def goalProb(i,j,k,ST,R,S,C,F):\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return math.exp(-mu)*mu**k/math.factorial(k)\n",
        "\n",
        "\n",
        "def goalProb2(i,j,k):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  #print(\"\\nNaive prediction of ST, R, S, C, F: \"+str(ST)+\", \"+str(R)+\", \"+str(S)+\", \"+str(C)+\", \"+str(F))\n",
        "  return goalProb(i,j,k,ST,R,S,C,F)\n",
        "\n",
        "\n",
        "'''\n",
        "Pre-training estimates of ST, R, S, C, F\n",
        "'''\n",
        "\n",
        "def preTrain(i,j,u,v):\n",
        "  X=0\n",
        "  Y=0\n",
        "  Z=0\n",
        "  for s in range(len(Data2)):\n",
        "    if(len(Data2[s])<2):\n",
        "      continue\n",
        "    if(Data2[s][0]==i):\n",
        "      X+=int(Data2[s][u])\n",
        "    if(Data2[s][0]==j):\n",
        "      Y+=int(Data2[s][u])\n",
        "    if(Data2[s][1]==i):\n",
        "      X+=int(Data2[s][v])\n",
        "    if(Data2[s][1]==j):\n",
        "      Y+=int(Data2[s][v])\n",
        "    Z+=int(Data2[s][u])+int(Data2[s][v])\n",
        "  return (X/games[i])*(Y/games[j])/(Z/totalGames)\n",
        "\n",
        "def preTrainST(i,j):\n",
        "  return preTrain(i,j,4,5)\n",
        "\n",
        "def preTrainR(i,j):\n",
        "  return preTrain(i,j,6,7)\n",
        "\n",
        "def preTrainS(i,j):\n",
        "  return preTrain(i,j,8,9)\n",
        "\n",
        "def preTrainC(i,j):\n",
        "  return preTrain(i,j,10,11)\n",
        "\n",
        "def preTrainF(i,j):\n",
        "  return preTrain(i,j,12,13)\n",
        "\n",
        "\n",
        "def expectedGoals(i,j):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return mu\n",
        "\n",
        "'''\n",
        "Test training a bit\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "If training iterations is set to zero this is used\n",
        "'''\n",
        "\n",
        "\n",
        "import numpy\n",
        "\n",
        "def readWeights():\n",
        " aLocal = numpy.fromfile('a.bin', dtype=numpy.float32)\n",
        " bLocal = numpy.fromfile('b.bin', dtype=numpy.float32)\n",
        " cLocal = numpy.fromfile('c.bin', dtype=numpy.float32)\n",
        " for i in range(len(a)):\n",
        "  a[i]=aLocal[i]\n",
        " for i in range(len(b)):\n",
        "  b[i]=bLocal[i]\n",
        " for i in range(len(c)):\n",
        "  c[i]=cLocal[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n Pre-training score table\")\n",
        "st=\"{:<17}\".format(\"\")\n",
        "for i in range(len(teams)):\n",
        "  st+=\"{:<17}\".format(teams[i])\n",
        "print (st)\n",
        "for i in range(len(teams)):\n",
        "  st=\"{:<17}\".format(teams[i])\n",
        "  for j in range(len(teams)):\n",
        "    st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "  print(st)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def  validate():\n",
        "  MSE1=0.0\n",
        "  MSE2=0.0\n",
        "  loss1=0.0\n",
        "  loss2=0.0\n",
        "  for s in range(len(validateGame)):\n",
        "    ii=validateGame[s][0]\n",
        "    jj=validateGame[s][1]\n",
        "    print(\"\\nvalidating excluded game:\"+teams[ii]+\" vs \" +teams[jj])\n",
        "    print(\"pre-training prediction: \"+str(expectedGoals(ii,jj))+\" to \"+str(expectedGoals(jj,ii)))\n",
        "    MSE1+=(float(validateGame[s][2])-expectedGoals(ii,jj))**2\n",
        "    MSE1+=(float(validateGame[s][3])-expectedGoals(jj,ii))**2\n",
        "    loss1-=math.log(goalProb2(ii,jj,int(validateGame[s][2])))\n",
        "    print(\"*\"+str(loss1))\n",
        "    loss1-=math.log(goalProb2(jj,ii,int(validateGame[s][3])))\n",
        "    print(\"*\"+str(loss1))\n",
        "\n",
        "  readWeights()\n",
        "  print(\"\\n\\n\")\n",
        "  for s in range(len(validateGame)):\n",
        "    ii=validateGame[s][0]\n",
        "    jj=validateGame[s][1]\n",
        "    print(\"\\nblinded saved-weights prediction for \"+teams[ii]+ \" vs \"+teams[jj]+\": \"+str(expectedGoals(ii,jj))+\" to \"+str(expectedGoals(jj,ii)))\n",
        "    print(\"actual score of that game: \" +str(validateGame[s][2])+ \" to \"+str(validateGame[s][3]))\n",
        "    MSE2+=(float(validateGame[s][2])-expectedGoals(jj,ii))**2\n",
        "    MSE2+=(float(validateGame[s][3])-expectedGoals(jj,ii))**2\n",
        "    loss2-=math.log(goalProb2(ii,jj,int(validateGame[s][2])))\n",
        "    loss2-=math.log(goalProb2(jj,ii,int(validateGame[s][3])))\n",
        "  print(\"\\n\\npre-training MSE: \"+str(MSE1/len(validateGame))+\", post-training MSE: \"+str(MSE2/len(validateGame)))\n",
        "  print(\"\\n\\npre-training loss: \"+str(loss1)+\", post-training loss: \"+str(loss2))\n",
        "\n",
        "# TRAINING\n",
        "#NUMBER OF ITRATIONS,\n",
        "#TRAINING RATE FOR ETA, TAU.\n",
        "\n",
        "iterations=80\n",
        "\n",
        "\n",
        "for i in range(iterations):\n",
        "    train(0.005,0.0005)\n",
        "\n",
        "\n",
        "if(iterations>0):\n",
        "    print(\"\\n\\n Score table after training\")\n",
        "    st=\"{:<17}\".format(\"\")\n",
        "    for i in range(len(teams)):\n",
        "       st+=\"{:<17}\".format(teams[i])\n",
        "    print (st)\n",
        "    for i in range(len(teams)):\n",
        "      st=\"{:<17}\".format(teams[i])\n",
        "      for j in range(len(teams)):\n",
        "          st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "      print(st)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "'''\n",
        "save weights\n",
        "'''\n",
        "\n",
        "\n",
        "def writeWeights():\n",
        " float_array = numpy.array(a, dtype=numpy.float32)\n",
        " float_array.tofile('a.bin')\n",
        "\n",
        "\n",
        " float_array = numpy.array(b, dtype=numpy.float32)\n",
        " float_array.tofile('b.bin')\n",
        "\n",
        " float_array = numpy.array(c, dtype=numpy.float32)\n",
        " float_array.tofile('c.bin')\n",
        "\n",
        "\n",
        "if(iterations == 0):\n",
        "  validate()\n",
        "\n",
        "if(iterations>0):\n",
        "  writeWeights()\n",
        "\n",
        "print(\"=============================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting actual goals for each match in the dataset\n",
        "# Match[2] = home goals\n",
        "# Match[3] = away goals\n",
        "goals_true = []\n",
        "\n",
        "for match in Data2:\n",
        "  if len(match) >=4:\n",
        "    goals_true.append([int(match[2]),int(match[3])])\n",
        "\n",
        "# Extracting values from tensors a, b, and c\n",
        "a_values = []\n",
        "b_values = []\n",
        "c_values = []\n",
        "\n",
        "for value in a:\n",
        "    a_values.append(float(value))\n",
        "\n",
        "for value in b:\n",
        "    b_values.append(float(value))\n",
        "\n",
        "for value in c:\n",
        "    c_values.append(float(value))\n",
        "\n",
        "\n",
        "def predict_goals(home_team, away_team, a_values, b_values, c_values, teams):\n",
        "    # Indices of the home and away teams\n",
        "    home_team_index = getIndex(home_team, teams)\n",
        "    away_team_index = getIndex(away_team, teams)\n",
        "    predicted_home_goals=expectedGoals(home_team_index,away_team_index)\n",
        "    predicted_away_goals=expectedGoals(away_team_index,home_team_index)\n",
        "    return [predicted_home_goals, predicted_away_goals]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_goals = []\n",
        "actual_goals = []\n",
        "\n",
        "# Go over each match in Data2\n",
        "for match in Data2:\n",
        "    if len(match) >= 4:\n",
        "        home_team_index = match[0]\n",
        "        away_team_index = match[1]\n",
        "\n",
        "        # Call the predict_goals function to get the predicted goals\n",
        "        predicted_match_goals = predict_goals(teams[home_team_index], teams[away_team_index], a_values, b_values, c_values, teams)\n",
        "\n",
        "        predicted_goals.append(predicted_match_goals)\n",
        "        actual_goals.append([int(match[2]), int(match[3])])\n",
        "\n",
        "\n",
        "total_matches = len(actual_goals)\n",
        "correct_predictions = 0\n",
        "\n",
        "\n",
        "absolute_errors = []\n",
        "\n",
        "# Iterate over each match to calculate absolute errors\n",
        "for i in range(total_matches):\n",
        "    absolute_error_match = [] # Each match\n",
        "    print(\"Actual goals: \", actual_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][0] - predicted_goals[i][0]))\n",
        "\n",
        "    print(\"Predicted goals: \", predicted_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][1] - predicted_goals[i][1]))\n",
        "\n",
        "    absolute_errors.append(absolute_error_match)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = 0\n",
        "for i in range(total_matches):\n",
        "    if actual_goals[i] == predicted_goals[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_matches\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "\n",
        "# Iterate over each match to calculate sum of absolute errors\n",
        "sum_abs_errors = 0\n",
        "for error_p_match in absolute_errors:\n",
        "    sum_abs_errors += sum(error_p_match)\n",
        "\n",
        "# Calculate mean absolute error\n",
        "mean_absolute_error = sum_abs_errors / total_matches\n",
        "\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten\n",
        "true_goals_flat = []\n",
        "for match in actual_goals:\n",
        "    for goal in match:\n",
        "        true_goals_flat.append(goal)\n",
        "\n",
        "predicted_goals_flat = []\n",
        "for match in predicted_goals:\n",
        "    for goal in match:\n",
        "        predicted_goals_flat.append(goal)\n",
        "\n",
        "\n",
        "# Histogram of Predicted Goals vs. Actual Goals\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist([true_goals_flat, predicted_goals_flat], bins=10, label=['True Goals', 'Predicted Goals'])\n",
        "plt.xlabel('Goals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Predicted Goals vs. True Goals')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e3tJxBFgMOLT",
        "outputId": "ac1ab3a4-48b5-4f23-a848-13871e90c982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2761547565), tensor(1.1935992241)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.4258593321), tensor(1.0848342180)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.7260520458), tensor(0.9850830436)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.4725698233), tensor(1.3877537251)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9968372583), tensor(1.3508911133)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.0999336243), tensor(0.9402618408)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4778555632), tensor(1.3007606268)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1050238609), tensor(1.0546463728)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(1.0649856329), tensor(1.8582491875)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0537116528), tensor(1.2535837889)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.7801822424), tensor(1.8484401703)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.3762326241), tensor(1.5663564205)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.5274442434), tensor(1.1848790646)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0030674934), tensor(1.5607579947)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3451020718), tensor(1.4246658087)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.3177090883), tensor(1.3358826637)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5932044983), tensor(0.8432288766)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.5374643803), tensor(1.0020455122)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.1679275036), tensor(0.8512807488)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.5811593533), tensor(1.1750214100)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2570726871), tensor(1.4925942421)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.9291493893), tensor(0.9213743210)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(0.9706548452), tensor(1.5688408613)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.6236772537), tensor(1.1373987198)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3078624010), tensor(0.9736772776)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.1302863359), tensor(1.5227013826)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1283894777), tensor(1.6282156706)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.8902583122), tensor(1.9465810061)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9003906250), tensor(1.4033681154)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.9287210703), tensor(0.7998135090)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.0430647135), tensor(1.5878149271)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2856913805), tensor(1.2755342722)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.0649622679), tensor(1.5800957680)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.8650063276), tensor(0.8875589967)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(0.9834133983), tensor(1.5006371737)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5125583410), tensor(0.9630262852)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.0806465149), tensor(1.3512407541)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.2378959656), tensor(1.3919659853)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.8343411684), tensor(1.3409363031)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2974143028), tensor(1.2232358456)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.4965485334), tensor(1.2722028494)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.0811804533), tensor(1.6169861555)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.7219407558), tensor(1.0662853718)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4656543732), tensor(1.0375888348)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.3636587858), tensor(1.3644572496)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.1019238234), tensor(1.2342976332)]\n",
            "Actual goals:  [0, 5]\n",
            "Predicted goals:  [tensor(0.7877845764), tensor(2.3410546780)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.9907848835), tensor(1.3678809404)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3046236038), tensor(1.2124500275)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.4518371820), tensor(0.8690295815)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2447154522), tensor(1.1802105904)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.6910964251), tensor(1.1520414352)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.5030530691), tensor(0.9302204847)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(1.0254107714), tensor(1.9477444887)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1611907482), tensor(1.0580933094)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1792284250), tensor(1.5723239183)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.1082469225), tensor(1.4061607122)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.6365160942), tensor(1.2299962044)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.5168843269), tensor(1.3156175613)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.7044210434), tensor(1.2066115141)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2445178032), tensor(1.7868905067)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.8998203278), tensor(1.4290554523)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.5323517323), tensor(0.9263339639)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2037500143), tensor(0.9458827972)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2142032385), tensor(1.4593288898)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.3635464907), tensor(1.2900582552)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1379426718), tensor(1.8839932680)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4105212688), tensor(1.1260074377)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.8561948538), tensor(1.7399672270)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.8531560302), tensor(1.6447589397)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2404055595), tensor(1.0521279573)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.8910796642), tensor(0.9857168198)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8840568066), tensor(2.2432422638)]\n",
            "Actual goals:  [6, 2]\n",
            "Predicted goals:  [tensor(1.5109586716), tensor(1.3470909595)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9768881798), tensor(1.3786292076)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9919903278), tensor(1.4977706671)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0540177822), tensor(1.2603185177)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0655468702), tensor(1.2574441433)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.4346576929), tensor(1.4910298586)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2829320431), tensor(1.5464740992)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.1406050920), tensor(1.2359652519)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4436603785), tensor(1.0648871660)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2218091488), tensor(1.2720972300)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.8310815096), tensor(0.9855821729)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.9758143425), tensor(0.9165606499)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.4432195425), tensor(1.0803828239)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.4336876869), tensor(1.0053772926)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2717887163), tensor(1.1491445303)]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [tensor(1.2801522017), tensor(1.7751606703)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8813756704), tensor(1.8028157949)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2560880184), tensor(1.2801889181)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8613458276), tensor(1.9047845602)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.7466211319), tensor(2.5855867863)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.6868331432), tensor(0.9553976059)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5729324818), tensor(1.0891240835)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2030752897), tensor(1.0684499741)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.2145196199), tensor(1.2385590076)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3328193426), tensor(1.3700168133)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1244924068), tensor(1.2885347605)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4804750681), tensor(1.5624452829)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1714068651), tensor(1.3290721178)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.9832710028), tensor(0.8069180250)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.3277572393), tensor(1.0575455427)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1265875101), tensor(1.2299211025)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.2565951347), tensor(1.3922104836)]\n",
            "Actual goals:  [6, 1]\n",
            "Predicted goals:  [tensor(1.6813051701), tensor(1.1792848110)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0360383987), tensor(1.5662746429)]\n",
            "Actual goals:  [5, 0]\n",
            "Predicted goals:  [tensor(1.3719179630), tensor(0.9605655074)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0895994902), tensor(1.2635496855)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.6981148720), tensor(1.4105638266)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.4270298481), tensor(1.3298028708)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(0.7780574560), tensor(1.7466495037)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5853372812), tensor(1.1761209965)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.2043663263), tensor(1.7467008829)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3134880066), tensor(1.6755123138)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.7916682959), tensor(1.0036017895)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.9639647007), tensor(1.4393408298)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.4461476803), tensor(1.1368027925)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8311012387), tensor(1.5039358139)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1013443470), tensor(1.4158184528)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4833346605), tensor(1.1181194782)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1614518166), tensor(1.0225584507)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3996808529), tensor(1.3795794249)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3969382048), tensor(1.2856483459)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.9208164215), tensor(1.1117516756)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.4732949734), tensor(1.3981497288)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8925578594), tensor(1.8238247633)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6412017345), tensor(0.8929055333)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2940877676), tensor(1.1036973000)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.3652499914), tensor(0.9615353346)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4204697609), tensor(1.2593390942)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(1.3151085377), tensor(1.2462708950)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5169765949), tensor(1.1560164690)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8164256215), tensor(2.4235115051)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8519382477), tensor(2.2367386818)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.4025850296), tensor(1.0136665106)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2490036488), tensor(1.2421104908)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0903840065), tensor(1.1507040262)]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [tensor(1.6483848095), tensor(1.1556539536)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2692474127), tensor(1.5476702452)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.1359660625), tensor(1.2240012884)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.8574117422), tensor(0.9053295851)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.2752977610), tensor(1.4188979864)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2542433739), tensor(0.9336320162)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.7020182610), tensor(1.0767674446)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1769286394), tensor(1.4032588005)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.4712551832), tensor(1.0157138109)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.3477770090), tensor(1.2583823204)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.0524477959), tensor(0.8357234001)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.7509498596), tensor(0.9624516368)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.0273723602), tensor(1.9550638199)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1485055685), tensor(1.4609565735)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8219579458), tensor(1.9749491215)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1237994432), tensor(1.3468480110)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.4355142117), tensor(1.4625246525)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9885149002), tensor(1.5474772453)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.2046141624), tensor(1.0902737379)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9649195075), tensor(1.7330677509)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.9475654364), tensor(1.1893398762)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(2.4966297150), tensor(0.7221879959)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2558456659), tensor(1.3474339247)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.3129653931), tensor(1.3196389675)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(0.9748243690), tensor(1.6972439289)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2429003716), tensor(1.0484771729)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0025146008), tensor(1.8090677261)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0810488462), tensor(1.6452399492)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.5313988924), tensor(1.0233484507)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.7695273161), tensor(0.9352041483)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3224807978), tensor(1.3046022654)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.1920616627), tensor(1.2422013283)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.5285209417), tensor(1.4294794798)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.8143289089), tensor(0.8791660070)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1514925957), tensor(1.2991373539)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1205196381), tensor(1.5107575655)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9290197492), tensor(1.6188571453)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.5187522173), tensor(1.0575361252)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2279658318), tensor(1.4979865551)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.4525417089), tensor(1.4776610136)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.7588501573), tensor(1.5941013098)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3800835609), tensor(1.0896914005)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9850830436), tensor(1.7260520458)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3508911133), tensor(0.9968372583)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.7847340107), tensor(2.0263516903)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3877537251), tensor(1.4725698233)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.0848342180), tensor(1.4258593321)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.8849160671), tensor(0.9280357361)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3715819120), tensor(1.1757254601)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.1935992241), tensor(1.2761547565)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1896992922), tensor(1.7296617031)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.8484401703), tensor(0.7801822424)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.0546463728), tensor(1.1050238609)]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [tensor(1.3945131302), tensor(1.4132848978)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.8582491875), tensor(1.0649856329)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5531885624), tensor(1.0922551155)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2083079815), tensor(1.7023030519)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2535837889), tensor(1.0537116528)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.3007606268), tensor(1.4778555632)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.9402618408), tensor(2.0999336243)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2828314304), tensor(1.0482931137)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3358826637), tensor(1.3177090883)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1848790646), tensor(1.5274442434)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.5663564205), tensor(1.3762326241)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.4246658087), tensor(1.3451020718)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5607579947), tensor(1.0030674934)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.0020455122), tensor(1.5374643803)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.8512807488), tensor(2.1679275036)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8432288766), tensor(1.5932044983)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6447589397), tensor(0.8531560302)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.2359652519), tensor(1.1406050920)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8817926645), tensor(1.9111094475)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.9736772776), tensor(1.3078624010)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.4925942421), tensor(1.2570726871)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5227013826), tensor(1.1302863359)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.1373987198), tensor(1.6236772537)]\n",
            "Actual goals:  [0, 5]\n",
            "Predicted goals:  [tensor(1.1750214100), tensor(1.5811593533)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.5688408613), tensor(0.9706548452)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.4033681154), tensor(0.9003906250)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6282156706), tensor(1.1283894777)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.9465810061), tensor(0.8902583122)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(0.9213743210), tensor(1.9291493893)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.8090677261), tensor(1.0025146008)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.0233484507), tensor(1.5313988924)]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [tensor(0.9352041483), tensor(1.7695273161)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3196389675), tensor(1.3129653931)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.6452399492), tensor(1.0810488462)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0484771729), tensor(1.2429003716)]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [tensor(1.3046022654), tensor(1.3224807978)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(0.7221879959), tensor(2.4966297150)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.6972439289), tensor(0.9748243690)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3474339247), tensor(1.2558456659)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5941013098), tensor(0.7588501573)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.4776610136), tensor(1.4525417089)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4294794798), tensor(1.5285209417)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.2991373539), tensor(1.1514925957)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0575361252), tensor(1.5187522173)]\n",
            "Actual goals:  [2, 4]\n",
            "Predicted goals:  [tensor(1.4979865551), tensor(1.2279658318)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2422013283), tensor(1.1920616627)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5107575655), tensor(1.1205196381)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.6188571453), tensor(0.9290197492)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8791660070), tensor(1.8143289089)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.3470909595), tensor(1.5109586716)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.5464740992), tensor(1.2829320431)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0521279573), tensor(1.2404055595)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(2.2432422638), tensor(0.8840568066)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3786292076), tensor(0.9768881798)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.2574441433), tensor(1.0655468702)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9857168198), tensor(1.8910796642)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.2603185177), tensor(1.0540177822)]\n",
            "Actual goals:  [4, 4]\n",
            "Predicted goals:  [tensor(1.4910298586), tensor(1.4346576929)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.8028157949), tensor(0.8813756704)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1491445303), tensor(1.2717887163)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.0648871660), tensor(1.4436603785)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0803828239), tensor(1.4432195425)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.7751606703), tensor(1.2801522017)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9855821729), tensor(1.8310815096)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(0.9165606499), tensor(1.9758143425)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0053772926), tensor(1.4336876869)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.1997011900), tensor(1.1445760727)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2720972300), tensor(1.2218091488)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2801889181), tensor(1.2560880184)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4188979864), tensor(1.2752977610)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8357234001), tensor(2.0524477959)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9053295851), tensor(1.8574117422)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.4032588005), tensor(1.1769286394)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2583823204), tensor(1.3477770090)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.9550638199), tensor(1.0273723602)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.0767674446), tensor(1.7020182610)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.9336320162), tensor(1.2542433739)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0157138109), tensor(1.4712551832)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.9624516368), tensor(1.7509498596)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.4977706671), tensor(0.9919903278)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.9749491215), tensor(0.8219579458)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.4609565735), tensor(1.1485055685)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5474772453), tensor(0.9885149002)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4625246525), tensor(1.4355142117)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.9111094475), tensor(0.8817926645)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.7330677509), tensor(0.9649195075)]\n",
            "Actual goals:  [5, 3]\n",
            "Predicted goals:  [tensor(1.1445760727), tensor(1.1997011900)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.3468480110), tensor(1.1237994432)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0902737379), tensor(1.2046141624)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1893398762), tensor(1.9475654364)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.6169861555), tensor(1.0811804533)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.3644572496), tensor(1.3636587858)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2232358456), tensor(1.2974143028)]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [tensor(2.3410546780), tensor(0.7877845764)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2124500275), tensor(1.3046236038)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2722028494), tensor(1.4965485334)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2342976332), tensor(1.1019238234)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3678809404), tensor(0.9907848835)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0375888348), tensor(1.4656543732)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0662853718), tensor(1.7219407558)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3512407541), tensor(1.0806465149)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2755342722), tensor(1.2856913805)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.5006371737), tensor(0.9834133983)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8875589967), tensor(1.8650063276)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.9630262852), tensor(1.5125583410)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3919659853), tensor(1.2378959656)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.5878149271), tensor(1.0430647135)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.3409363031), tensor(1.8343411684)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.4290554523), tensor(0.8998203278)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.2900582552), tensor(1.3635464907)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9263339639), tensor(1.5323517323)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.7868905067), tensor(1.2445178032)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1260074377), tensor(1.4105212688)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4593288898), tensor(1.2142032385)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.7399672270), tensor(0.8561948538)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.2066115141), tensor(1.7044210434)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.8839932680), tensor(1.1379426718)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.8690295815), tensor(1.4518371820)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(0.9302204847), tensor(1.5030530691)]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [tensor(1.5723239183), tensor(1.1792284250)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4061607122), tensor(1.1082469225)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.0580933094), tensor(1.1611907482)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.1802105904), tensor(1.2447154522)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3156175613), tensor(1.5168843269)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1520414352), tensor(1.6910964251)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2299962044), tensor(1.6365160942)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.9477444887), tensor(1.0254107714)]\n",
            "Actual goals:  [5, 0]\n",
            "Predicted goals:  [tensor(2.5855867863), tensor(0.7466211319)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.5624452829), tensor(1.4804750681)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3700168133), tensor(1.3328193426)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3290721178), tensor(1.1714068651)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.9047845602), tensor(0.8613458276)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2885347605), tensor(1.1244924068)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.2385590076), tensor(1.2145196199)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9553976059), tensor(1.6868331432)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.0891240835), tensor(1.5729324818)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0684499741), tensor(1.2030752897)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5800957680), tensor(1.0649622679)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3298028708), tensor(1.4270298481)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3922104836), tensor(1.2565951347)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4105638266), tensor(1.6981148720)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2635496855), tensor(1.0895994902)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0575455427), tensor(1.3277572393)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2299211025), tensor(1.1265875101)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.5662746429), tensor(1.0360383987)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1792848110), tensor(1.6813051701)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9605655074), tensor(1.3719179630)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(0.8069180250), tensor(1.9832710028)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.7466495037), tensor(0.7780574560)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0036017895), tensor(1.7916682959)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.6755123138), tensor(1.3134880066)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.5039358139), tensor(0.8311012387)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1368027925), tensor(1.4461476803)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1181194782), tensor(1.4833346605)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1761209965), tensor(1.5853372812)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.4158184528), tensor(1.1013443470)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.7467008829), tensor(1.2043663263)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4393408298), tensor(0.9639647007)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.7998135090), tensor(1.9287210703)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(0.9615353346), tensor(1.3652499914)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2856483459), tensor(1.3969382048)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.1117516756), tensor(1.9208164215)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.3981497288), tensor(1.4732949734)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1036973000), tensor(1.2940877676)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2593390942), tensor(1.4204697609)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0225584507), tensor(1.1614518166)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3795794249), tensor(1.3996808529)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8929055333), tensor(1.6412017345)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.8238247633), tensor(0.8925578594)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(0.9458827972), tensor(1.2037500143)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.2367386818), tensor(0.8519382477)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.0136665106), tensor(1.4025850296)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2462708950), tensor(1.3151085377)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1560164690), tensor(1.5169765949)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2240012884), tensor(1.1359660625)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.4235115051), tensor(0.8164256215)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1507040262), tensor(1.0903840065)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(1.5476702452), tensor(1.2692474127)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(1.2421104908), tensor(1.2490036488)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1556539536), tensor(1.6483848095)]\n",
            "Accuracy:  0.0\n",
            "Mean Absolute Error:  tensor(1.8534085751)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}