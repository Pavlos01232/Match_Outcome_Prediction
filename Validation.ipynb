{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavlos01232/Match_Outcome_Prediction/blob/main/Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsKoaXp4LRvc",
        "outputId": "e451cd62-8b53-48fd-ad0a-cea8d459b6ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================================\n",
            "\n",
            "teams:\n",
            "['Arsenal', 'Birmingham', 'Blackburn', 'Fulham', 'Leicester', 'Man United', 'Portsmouth', 'Charlton', 'Leeds', 'Liverpool', 'Bolton', 'Chelsea', 'Everton', 'Man City', 'Newcastle', 'Southampton', 'Tottenham', 'Wolves', 'Aston Villa', 'Middlesbrough']\n",
            "381\n",
            "excluded: [137, 141, 144, 150, 153, 163, 191, 191, 201]\n",
            "\n",
            "CAUTION: check that the teams list was not disrupted by exclusions:\n",
            "['Arsenal', 'Birmingham', 'Blackburn', 'Fulham', 'Leicester', 'Man United', 'Portsmouth', 'Charlton', 'Leeds', 'Liverpool', 'Bolton', 'Chelsea', 'Everton', 'Man City', 'Newcastle', 'Southampton', 'Tottenham', 'Wolves', 'Aston Villa', 'Middlesbrough']\n",
            "\n",
            "\n",
            " Pre-training score table\n",
            "                 Arsenal          Birmingham       Blackburn        Fulham           Leicester        Man United       Portsmouth       Charlton         Leeds            Liverpool        Bolton           Chelsea          Everton          Man City         Newcastle        Southampton      Tottenham        Wolves           Aston Villa      Middlesbrough    \n",
            "Arsenal          0.984            1.839            2.265            1.692            2.479            1.338            2.086            1.954            3.033            1.418            2.125            1.150            2.184            2.069            1.456            1.692            2.204            2.833            1.653            1.993            \n",
            "Birmingham       0.580            1.085            1.336            0.998            1.462            0.789            1.230            1.152            1.789            0.836            1.253            0.678            1.288            1.220            0.859            0.998            1.300            1.671            0.975            1.175            \n",
            "Blackburn        0.627            1.172            1.443            1.078            1.579            0.852            1.329            1.245            1.932            0.903            1.354            0.732            1.391            1.318            0.928            1.078            1.404            1.805            1.053            1.269            \n",
            "Fulham           0.707            1.321            1.627            1.216            1.781            0.961            1.498            1.404            2.179            1.019            1.527            0.826            1.569            1.486            1.046            1.216            1.583            2.035            1.187            1.431            \n",
            "Leicester        0.638            1.192            1.467            1.096            1.606            0.867            1.351            1.266            1.965            0.919            1.377            0.745            1.415            1.341            0.943            1.096            1.428            1.836            1.071            1.291            \n",
            "Man United       0.859            1.606            1.978            1.478            2.165            1.169            1.821            1.706            2.649            1.238            1.856            1.004            1.907            1.807            1.272            1.478            1.924            2.474            1.443            1.740            \n",
            "Portsmouth       0.624            1.166            1.436            1.073            1.571            0.848            1.322            1.239            1.923            0.899            1.347            0.729            1.384            1.312            0.923            1.073            1.397            1.796            1.048            1.263            \n",
            "Charlton         0.688            1.286            1.584            1.184            1.734            0.936            1.459            1.367            2.122            0.992            1.486            0.804            1.528            1.447            1.018            1.184            1.541            1.982            1.156            1.394            \n",
            "Leeds            0.527            0.985            1.213            0.906            1.328            0.717            1.117            1.047            1.625            0.760            1.138            0.616            1.170            1.108            0.780            0.906            1.180            1.518            0.885            1.067            \n",
            "Liverpool        0.742            1.387            1.708            1.276            1.870            1.009            1.573            1.474            2.288            1.070            1.603            0.867            1.647            1.561            1.098            1.276            1.662            2.137            1.247            1.503            \n",
            "Bolton           0.638            1.192            1.468            1.096            1.606            0.867            1.351            1.266            1.965            0.919            1.377            0.745            1.415            1.341            0.944            1.096            1.428            1.836            1.071            1.291            \n",
            "Chelsea          0.904            1.690            2.081            1.555            2.278            1.230            1.916            1.796            2.787            1.303            1.953            1.056            2.007            1.901            1.338            1.555            2.025            2.603            1.519            1.831            \n",
            "Everton          0.584            1.092            1.344            1.004            1.471            0.794            1.238            1.160            1.800            0.842            1.261            0.682            1.296            1.228            0.864            1.004            1.308            1.682            0.981            1.183            \n",
            "Man City         0.742            1.387            1.708            1.276            1.870            1.009            1.573            1.474            2.288            1.070            1.603            0.867            1.647            1.561            1.098            1.276            1.662            2.137            1.247            1.503            \n",
            "Newcastle        0.698            1.305            1.607            1.200            1.759            0.949            1.479            1.386            2.152            1.006            1.507            0.816            1.549            1.468            1.033            1.200            1.563            2.010            1.172            1.413            \n",
            "Southampton      0.582            1.088            1.340            1.001            1.467            0.792            1.234            1.156            1.794            0.839            1.257            0.680            1.292            1.224            0.861            1.001            1.304            1.676            0.978            1.179            \n",
            "Tottenham        0.624            1.166            1.436            1.073            1.571            0.848            1.322            1.239            1.923            0.899            1.347            0.729            1.384            1.312            0.923            1.073            1.397            1.796            1.048            1.263            \n",
            "Wolves           0.513            0.959            1.180            0.882            1.292            0.697            1.087            1.018            1.581            0.739            1.108            0.599            1.138            1.078            0.759            0.882            1.149            1.477            0.861            1.038            \n",
            "Aston Villa      0.651            1.218            1.499            1.120            1.641            0.886            1.381            1.294            2.008            0.939            1.407            0.761            1.446            1.370            0.964            1.120            1.459            1.876            1.094            1.319            \n",
            "Middlesbrough    0.594            1.110            1.367            1.021            1.496            0.808            1.259            1.179            1.830            0.856            1.282            0.694            1.318            1.249            0.879            1.021            1.330            1.710            0.997            1.202            \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.1118164062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6664271355), tensor(0.9850926995), tensor(1.0699005127), tensor(1.1999126673), tensor(1.0854712725), tensor(1.4545227289), tensor(1.0594611168), tensor(1.1691074371), tensor(0.9013012648), tensor(1.2575557232), tensor(1.0850569010), tensor(1.5309759378), tensor(0.9900228381), tensor(1.2614544630), tensor(1.1842513084), tensor(0.9889701009), tensor(1.0642963648), tensor(0.8774982691), tensor(1.1052327156), tensor(1.0089190006)]\n",
            "b:  [tensor(0.5934676528), tensor(1.0991032124), tensor(1.3528553247), tensor(1.0124939680), tensor(1.4821271896), tensor(0.8041849136), tensor(1.2470684052), tensor(1.1696966887), tensor(1.8123178482), tensor(0.8497199416), tensor(1.2706383467), tensor(0.6920222044), tensor(1.3083268404), tensor(1.2393925190), tensor(0.8721671700), tensor(1.0108259916), tensor(1.3166310787), tensor(1.6922575235), tensor(0.9883502126), tensor(1.1910165548)]\n",
            "c:  [tensor(0.0006175477), tensor(0.0027526792), tensor(0.0045260028), tensor(-0.0009359902), tensor(-0.0020010551), tensor(-0.0039534150), tensor(0.0004909860), tensor(9.2386362667e-05), tensor(0.0101514030), tensor(0.0019977042), tensor(0.0051151915), tensor(0.0069832145), tensor(0.0089936294), tensor(0.0032852963), tensor(0.0069956831), tensor(0.0092156576), tensor(0.0019680320), tensor(-0.0010110710)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0013159513,  0.1090390980, -1.0619471073,  0.1389383078,\n",
            "        -0.5141189694,  1.0078197718, -0.0203450322, -0.0190247297,\n",
            "        -1.2154382467,  0.6287783980, -0.4312559962,  0.9570941925,\n",
            "         0.3964468837, -0.1509771347,  0.2631532550, -0.0469359159,\n",
            "        -0.9873890877, -1.2938888073,  0.2418425083, -0.0718551278])\n",
            "btensor.grad: tensor([-0.9869332314,  0.2287415862,  0.4156872630, -0.0434427857,\n",
            "         0.1952003390, -0.7560182810,  0.1243171990, -0.1368646622,\n",
            "         0.4651151896, -0.3225972652,  0.1185947657, -0.8735825419,\n",
            "        -0.3567301631, -0.3229476213, -0.2276645601,  0.2901619673,\n",
            "         0.3365422487,  0.5435038805,  0.0770481825,  0.1835277081])\n",
            "ctensor.grad: tensor([ 0.7649046183, -1.5053580999, -1.0520049334, -0.1280196607,\n",
            "         0.0021100282, -0.0931706205,  0.0180280395,  0.0152272647,\n",
            "        -0.3028057218,  0.0045918403, -0.2303827852,  0.0335717201,\n",
            "         0.0127398307, -0.5705927014,  0.0086340727, -0.4313150644,\n",
            "         0.0639361590,  0.0221418776])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0505371094, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6620829105), tensor(0.9846364260), tensor(1.0741184950), tensor(1.1993253231), tensor(1.0874966383), tensor(1.4502321482), tensor(1.0595101118), tensor(1.1691665649), tensor(0.9059343338), tensor(1.2549444437), tensor(1.0867471695), tensor(1.5268847942), tensor(0.9884753227), tensor(1.2620650530), tensor(1.1831549406), tensor(0.9891112447), tensor(1.0681921244), tensor(0.8823415637), tensor(1.1042230129), tensor(1.0091680288)]\n",
            "b:  [tensor(0.5965512395), tensor(1.0981541872), tensor(1.3511629105), tensor(1.0126210451), tensor(1.4812902212), tensor(0.8068836331), tensor(1.2465205193), tensor(1.1702260971), tensor(1.8103061914), tensor(0.8508624434), tensor(1.2701659203), tensor(0.6949420571), tensor(1.3097518682), tensor(1.2407063246), tensor(0.8730008602), tensor(1.0096663237), tensor(1.3152707815), tensor(1.6899441481), tensor(0.9880091548), tensor(1.1902430058)]\n",
            "c:  [tensor(0.0003115111), tensor(0.0035784757), tensor(0.0051491284), tensor(-0.0008965571), tensor(-0.0020017093), tensor(-0.0039248480), tensor(0.0004853229), tensor(8.7180575065e-05), tensor(0.0103594121), tensor(0.0019945297), tensor(0.0052728560), tensor(0.0069596577), tensor(0.0089824172), tensor(0.0036080221), tensor(0.0069907759), tensor(0.0094588725), tensor(0.0019312880), tensor(-0.0010269645)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8688437939,  0.0912604332, -0.8436011076,  0.1174608469,\n",
            "        -0.4050633907,  0.8581132889, -0.0097893178, -0.0118221045,\n",
            "        -0.9266175032,  0.5222594738, -0.3380604386,  0.8182313442,\n",
            "         0.3095042109, -0.1221137047,  0.2192682624, -0.0282275677,\n",
            "        -0.7791621685, -0.9686642885,  0.2019480467, -0.0498149395])\n",
            "btensor.grad: tensor([-0.6167185307,  0.1898061633,  0.3384777904, -0.0254043341,\n",
            "         0.1673960984, -0.5397441387,  0.1095810533, -0.1058856249,\n",
            "         0.4023422003, -0.2285051346,  0.0944784880, -0.5839745402,\n",
            "        -0.2850140631, -0.2627710104, -0.1667385995,  0.2319360375,\n",
            "         0.2720490992,  0.4626779556,  0.0682075024,  0.1547214985])\n",
            "ctensor.grad: tensor([ 6.1207324266e-01, -1.6515927315e+00, -1.2462507486e+00,\n",
            "        -7.8866168857e-02,  1.3086247491e-03, -5.7134114206e-02,\n",
            "         1.1326209642e-02,  1.0411570780e-02, -4.1601753235e-01,\n",
            "         6.3487128355e-03, -3.1532916427e-01,  4.7113399953e-02,\n",
            "         2.2425143048e-02, -6.4545184374e-01,  9.8146377131e-03,\n",
            "        -4.8642903566e-01,  7.3487915099e-02,  3.1786855310e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1073.0117187500, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6583418846), tensor(0.9842984080), tensor(1.0775148869), tensor(1.1988687515), tensor(1.0891337395), tensor(1.4466158152), tensor(1.0595637560), tensor(1.1692402363), tensor(0.9095178246), tensor(1.2528150082), tensor(1.0881124735), tensor(1.5234211683), tensor(0.9873058200), tensor(1.2625976801), tensor(1.1822776794), tensor(0.9892289639), tensor(1.0713117123), tensor(0.8860217929), tensor(1.1034209728), tensor(1.0093787909)]\n",
            "b:  [tensor(0.5985087752), tensor(1.0974012613), tensor(1.3498152494), tensor(1.0127185583), tensor(1.4806051254), tensor(0.8088396192), tensor(1.2460728884), tensor(1.1706677675), tensor(1.8085925579), tensor(0.8517006636), tensor(1.2698225975), tensor(0.6969261765), tensor(1.3109219074), tensor(1.2418104410), tensor(0.8736397624), tensor(1.0087718964), tensor(1.3142021894), tensor(1.6880017519), tensor(0.9877431989), tensor(1.1896247864)]\n",
            "c:  [tensor(3.7377292756e-05), tensor(0.0044528190), tensor(0.0058365273), tensor(-0.0008766638), tensor(-0.0020020402), tensor(-0.0039104433), tensor(0.0004824631), tensor(8.4437713667e-05), tensor(0.0106301792), tensor(0.0019903935), tensor(0.0054781395), tensor(0.0069290847), tensor(0.0089666005), tensor(0.0039761853), tensor(0.0069851801), tensor(0.0097366059), tensor(0.0018896955), tensor(-0.0010462826)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7482151985,  0.0676011443, -0.6792782545,  0.0913127661,\n",
            "        -0.3274271488,  0.7232652903, -0.0107269883, -0.0147366524,\n",
            "        -0.7167031765,  0.4258777201, -0.2730640769,  0.6927300692,\n",
            "         0.2338984609, -0.1065243483,  0.1754459143, -0.0235415697,\n",
            "        -0.6239112616, -0.7360494137,  0.1603975892, -0.0421459675])\n",
            "btensor.grad: tensor([-0.3915081024,  0.1505839229,  0.2695238590, -0.0194988847,\n",
            "         0.1370308995, -0.3911962509,  0.0895336270, -0.0883390903,\n",
            "         0.3427343965, -0.1676390767,  0.0686662197, -0.3968208432,\n",
            "        -0.2340193838, -0.2208305597, -0.1277759820,  0.1788946986,\n",
            "         0.2137207389,  0.3884884119,  0.0531902313,  0.1236412525])\n",
            "ctensor.grad: tensor([ 5.4826754332e-01, -1.7486867905e+00, -1.3747975826e+00,\n",
            "        -3.9786539972e-02,  6.6148955375e-04, -2.8809331357e-02,\n",
            "         5.7195983827e-03,  5.4857246578e-03, -5.4153484106e-01,\n",
            "         8.2722995430e-03, -4.1056671739e-01,  6.1145763844e-02,\n",
            "         3.1632959843e-02, -7.3632615805e-01,  1.1192102917e-02,\n",
            "        -5.5546718836e-01,  8.3185054362e-02,  3.8636069745e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9858398438, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6551346779), tensor(0.9840752482), tensor(1.0802773237), tensor(1.1985378265), tensor(1.0904823542), tensor(1.4435878992), tensor(1.0596425533), tensor(1.1693447828), tensor(0.9123213291), tensor(1.2511010170), tensor(1.0892395973), tensor(1.5205069780), tensor(0.9864476919), tensor(1.2630828619), tensor(1.1815966368), tensor(0.9893519282), tensor(1.0738370419), tensor(0.8888521194), tensor(1.1028079987), tensor(1.0095813274)]\n",
            "b:  [tensor(0.5997686982), tensor(1.0968238115), tensor(1.3487609625), tensor(1.0128126144), tensor(1.4800627232), tensor(0.8102751374), tensor(1.2457262278), tensor(1.1710548401), tensor(1.8071461916), tensor(0.8523348570), tensor(1.2695962191), tensor(0.6982942820), tensor(1.3119015694), tensor(1.2427583933), tensor(0.8741471767), tensor(1.0081014633), tensor(1.3133817911), tensor(1.6863850355), tensor(0.9875550866), tensor(1.1891499758)]\n",
            "c:  [tensor(-0.0002269825), tensor(0.0053680609), tensor(0.0065754992), tensor(-0.0008742744), tensor(-0.0020020800), tensor(-0.0039087110), tensor(0.0004821218), tensor(8.4102633991e-05), tensor(0.0109680789), tensor(0.0019852405), tensor(0.0057348507), tensor(0.0068914588), tensor(0.0089466404), tensor(0.0043952689), tensor(0.0069788289), tensor(0.0100536421), tensor(0.0018432208), tensor(-0.0010680435)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.6414321661,  0.0446346998, -0.5524797440,  0.0661758184,\n",
            "        -0.2697290778,  0.6055836678, -0.0157670081, -0.0209152699,\n",
            "        -0.5606993437,  0.3427889943, -0.2254343629,  0.5828495026,\n",
            "         0.1716217399, -0.0970429182,  0.1362179518, -0.0245879889,\n",
            "        -0.5050697327, -0.5660712719,  0.1225908995, -0.0405011773])\n",
            "btensor.grad: tensor([-0.2519799471,  0.1155009866,  0.2108570337, -0.0188150406,\n",
            "         0.1084774211, -0.2871026993,  0.0693337619, -0.0774135590,\n",
            "         0.2892783880, -0.1268333793,  0.0452769995, -0.2736228108,\n",
            "        -0.1959214807, -0.1896016598, -0.1014881879,  0.1340959072,\n",
            "         0.1640759706,  0.3233529329,  0.0376279354,  0.0949704647])\n",
            "ctensor.grad: tensor([ 5.2871960402e-01, -1.8304836750e+00, -1.4779436588e+00,\n",
            "        -4.7789597884e-03,  7.9449273471e-05, -3.4647635184e-03,\n",
            "         6.8257120438e-04,  6.7015696550e-04, -6.7579919100e-01,\n",
            "         1.0305974633e-02, -5.1342272758e-01,  7.5251743197e-02,\n",
            "         3.9920352399e-02, -8.3816719055e-01,  1.2701979838e-02,\n",
            "        -6.3407248259e-01,  9.2949561775e-02,  4.3521948159e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9664306641, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6523928642), tensor(0.9839510918), tensor(1.0825415850), tensor(1.1983166933), tensor(1.0916093588), tensor(1.4410643578), tensor(1.0597504377), tensor(1.1694813967), tensor(0.9145355821), tensor(1.2497354746), tensor(1.0901851654), tensor(1.5180653334), tensor(0.9858379364), tensor(1.2635340691), tensor(1.1810817719), tensor(0.9894893169), tensor(1.0758987665), tensor(0.8910515904), tensor(1.1023563147), tensor(1.0097862482)]\n",
            "b:  [tensor(0.6005911827), tensor(1.0963944197), tensor(1.3479496241), tensor(1.0129131079), tensor(1.4796459675), tensor(0.8113406301), tensor(1.2454707623), tensor(1.1714037657), tensor(1.8059326410), tensor(0.8528282046), tensor(1.2694680691), tensor(0.6992515326), tensor(1.3127334118), tensor(1.2435841560), tensor(0.8745617270), tensor(1.0076124668), tensor(1.3127657175), tensor(1.6850477457), tensor(0.9874363542), tensor(1.1887983084)]\n",
            "c:  [tensor(-0.0004924968), tensor(0.0063253399), tensor(0.0073637250), tensor(-0.0008888036), tensor(-0.0020018385), tensor(-0.0039192638), tensor(0.0004841791), tensor(8.6151012511e-05), tensor(0.0113769500), tensor(0.0019790239), tensor(0.0060463618), tensor(0.0068467711), tensor(0.0089230603), tensor(0.0048697898), tensor(0.0069716685), tensor(0.0104138916), tensor(0.0017918089), tensor(-0.0010914771)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.5483577251,  0.0248332918, -0.4528605938,  0.0442361832,\n",
            "        -0.2254034877,  0.5047174692, -0.0215684772, -0.0273212194,\n",
            "        -0.4428555965,  0.2731123567, -0.1891153455,  0.4883369207,\n",
            "         0.1219455600, -0.0902421474,  0.1029730439, -0.0274778605,\n",
            "        -0.4123501778, -0.4398957491,  0.0903398395, -0.0409920812])\n",
            "btensor.grad: tensor([-0.1644966602,  0.0858705044,  0.1622599363, -0.0200934410,\n",
            "         0.0833401754, -0.2130999565,  0.0510841012, -0.0697914362,\n",
            "         0.2426982522, -0.0986727476,  0.0256305933, -0.1914558411,\n",
            "        -0.1663685143, -0.1651411057, -0.0829089358,  0.0977969766,\n",
            "         0.1232037395,  0.2674685717,  0.0237470865,  0.0703246593])\n",
            "ctensor.grad: tensor([ 5.3102856874e-01, -1.9145584106e+00, -1.5764518976e+00,\n",
            "         2.9058411717e-02, -4.8269989202e-04,  2.1105218679e-02,\n",
            "        -4.1145179421e-03, -4.0967622772e-03, -8.1774294376e-01,\n",
            "         1.2433157302e-02, -6.2302219868e-01,  8.9374952018e-02,\n",
            "         4.7159649432e-02, -9.4904148579e-01,  1.4320677146e-02,\n",
            "        -7.2049891949e-01,  1.0282377154e-01,  4.6867154539e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9525146484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6500531435), tensor(0.9839073420), tensor(1.0844095945), tensor(1.1981865168), tensor(1.0925620794), tensor(1.4389684200), tensor(1.0598841906), tensor(1.1696453094), tensor(0.9162995815), tensor(1.2486572266), tensor(1.0909880400), tensor(1.5160260201), tensor(0.9854223728), tensor(1.2639567852), tensor(1.1807035208), tensor(0.9896418452), tensor(1.0775939226), tensor(0.8927774429), tensor(1.1020373106), tensor(1.0099958181)]\n",
            "b:  [tensor(0.6011375785), tensor(1.0960865021), tensor(1.3473365307), tensor(1.0130225420), tensor(1.4793362617), tensor(0.8121406436), tensor(1.2452937365), tensor(1.1717232466), tensor(1.8049191236), tensor(0.8532223105), tensor(1.2694190741), tensor(0.6999325156), tensor(1.3134475946), tensor(1.2443108559), tensor(0.8749083281), tensor(1.0072669983), tensor(1.3123146296), tensor(1.6839473248), tensor(0.9873754382), tensor(1.1885488033)]\n",
            "c:  [tensor(-0.0007646017), tensor(0.0073304507), tensor(0.0082043232), tensor(-0.0009203641), tensor(-0.0020013147), tensor(-0.0039422270), tensor(0.0004886105), tensor(9.0604400611e-05), tensor(0.0118607748), tensor(0.0019716949), tensor(0.0064161192), tensor(0.0067949668), tensor(0.0088963928), tensor(0.0054041194), tensor(0.0069636456), tensor(0.0108210845), tensor(0.0017353592), tensor(-0.0011159166)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.4679369926,  0.0087521970, -0.3736093640,  0.0260329247,\n",
            "        -0.1905372143,  0.4191800356, -0.0267463923, -0.0327773094,\n",
            "        -0.3528056145,  0.2156602144, -0.1605769992,  0.4078617394,\n",
            "         0.0831183195, -0.0845466852,  0.0756602287, -0.0305031538,\n",
            "        -0.3390341997, -0.3451761007,  0.0638093352, -0.0419086218])\n",
            "btensor.grad: tensor([-0.1092758179,  0.0615916252,  0.1226149201, -0.0218809247,\n",
            "         0.0619450584, -0.1600003242,  0.0354157388, -0.0639020205,\n",
            "         0.2027062774, -0.0788248777,  0.0097904205, -0.1361954063,\n",
            "        -0.1428434402, -0.1453281641, -0.0693204179,  0.0691001415,\n",
            "         0.0902182609,  0.2200852633,  0.0121806860,  0.0499037504])\n",
            "ctensor.grad: tensor([ 5.4420971870e-01, -2.0102214813e+00, -1.6811956167e+00,\n",
            "         6.3120931387e-02, -1.0475722374e-03,  4.5926462859e-02,\n",
            "        -8.8627859950e-03, -8.9067723602e-03, -9.6765011549e-01,\n",
            "         1.4658017084e-02, -7.3951482773e-01,  1.0360831022e-01,\n",
            "         5.3335811943e-02, -1.0686588287e+00,  1.6045838594e-02,\n",
            "        -8.1438595057e-01,  1.1289943010e-01,  4.8879072070e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9403076172, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6480592489), tensor(0.9839266539), tensor(1.0859596729), tensor(1.1981296539), tensor(1.0933755636), tensor(1.4372328520), tensor(1.0600386858), tensor(1.1698302031), tensor(0.9177168012), tensor(1.2478132248), tensor(1.0916764736), tensor(1.5143272877), tensor(0.9851564169), tensor(1.2643532753), tensor(1.1804352999), tensor(0.9898068905), tensor(1.0789965391), tensor(0.8941450715), tensor(1.1018249989), tensor(1.0102087259)]\n",
            "b:  [tensor(0.6015094519), tensor(1.0958763361), tensor(1.3468837738), tensor(1.0131405592), tensor(1.4791160822), tensor(0.8127492070), tensor(1.2451821566), tensor(1.1720182896), tensor(1.8040760756), tensor(0.8535456657), tensor(1.2694325447), tensor(0.7004270554), tensor(1.3140666485), tensor(1.2449557781), tensor(0.8752042055), tensor(1.0070331097), tensor(1.3119950294), tensor(1.6830466986), tensor(0.9873610735), tensor(1.1883821487)]\n",
            "c:  [tensor(-0.0010461423), tensor(0.0083917752), tensor(0.0091033662), tensor(-0.0009694026), tensor(-0.0020005014), tensor(-0.0039779614), tensor(0.0004954475), tensor(9.7522839496e-05), tensor(0.0124240639), tensor(0.0019631970), tensor(0.0068479571), tensor(0.0067359214), tensor(0.0088671781), tensor(0.0060029342), tensor(0.0069547025), tensor(0.0112791620), tensor(0.0016737219), tensor(-0.0011407230)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.3987807035, -0.0038594306, -0.3100122213,  0.0113661289,\n",
            "        -0.1626850814,  0.3471040726, -0.0308910310, -0.0369833708,\n",
            "        -0.2834403515,  0.1687996984, -0.1376749873,  0.3397442102,\n",
            "         0.0531919003, -0.0792921782,  0.0536494255, -0.0330108404,\n",
            "        -0.2805294991, -0.2735227346,  0.0424549580, -0.0425879955])\n",
            "btensor.grad: tensor([-0.0743712187,  0.0420252085,  0.0905436277, -0.0236144662,\n",
            "         0.0440410823, -0.1217104197,  0.0223104656, -0.0590167046,\n",
            "         0.1686193347, -0.0646722317, -0.0026898384, -0.0989027694,\n",
            "        -0.1238161623, -0.1289955378, -0.0591756329,  0.0467665792,\n",
            "         0.0639096349,  0.1801230907,  0.0028680563,  0.0333213806])\n",
            "ctensor.grad: tensor([ 5.6308126450e-01, -2.1226480007e+00, -1.7980849743e+00,\n",
            "         9.8076947033e-02, -1.6265419545e-03,  7.1468785405e-02,\n",
            "        -1.3674032874e-02, -1.3836873695e-02, -1.1265777349e+00,\n",
            "         1.6995524988e-02, -8.6367565393e-01,  1.1809073389e-01,\n",
            "         5.8430105448e-02, -1.1976296902e+00,  1.7886221409e-02,\n",
            "        -9.1615450382e-01,  1.2327446789e-01,  4.9612704664e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9291992188, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6463619471), tensor(0.9839944243), tensor(1.0872530937), tensor(1.1981308460), tensor(1.0940767527), tensor(1.4357998371), tensor(1.0602085590), tensor(1.1700303555), tensor(0.9188655019), tensor(1.2471588850), tensor(1.0922715664), tensor(1.5129159689), tensor(0.9850047231), tensor(1.2647244930), tensor(1.1802546978), tensor(0.9899810553), tensor(1.0801643133), tensor(0.8952403069), tensor(1.1016974449), tensor(1.0104229450)]\n",
            "b:  [tensor(0.6017715931), tensor(1.0957444906), tensor(1.3465602398), tensor(1.0132660866), tensor(1.4789701700), tensor(0.8132196069), tensor(1.2451248169), tensor(1.1722922325), tensor(1.8033777475), tensor(0.8538185358), tensor(1.2694945335), tensor(0.7007959485), tensor(1.3146080971), tensor(1.2455329895), tensor(0.8754619360), tensor(1.0068854094), tensor(1.3117796183), tensor(1.6823142767), tensor(0.9873836637), tensor(1.1882822514)]\n",
            "c:  [tensor(-0.0013388376), tensor(0.0095193041), tensor(0.0100686718), tensor(-0.0010365301), tensor(-0.0019993884), tensor(-0.0040269350), tensor(0.0005047568), tensor(0.0001069973), tensor(0.0130720949), tensor(0.0019534638), tensor(0.0073463037), tensor(0.0066694384), tensor(0.0088359909), tensor(0.0066714850), tensor(0.0069447742), tensor(0.0117925154), tensor(0.0016067037), tensor(-0.0011652248)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.3946979046e-01, -1.3556659222e-02, -2.5868338346e-01,\n",
            "        -2.3913383484e-04, -1.4022995532e-01,  2.8660309315e-01,\n",
            "        -3.3982157707e-02, -4.0019869804e-02, -2.2973442078e-01,\n",
            "         1.3086038828e-01, -1.1902248859e-01,  2.8226944804e-01,\n",
            "         3.0343711376e-02, -7.4235200882e-02,  3.6131978035e-02,\n",
            "        -3.4827709198e-02, -2.3355805874e-01, -2.1904516220e-01,\n",
            "         2.5506436825e-02, -4.2844116688e-02])\n",
            "btensor.grad: tensor([-0.0524269342,  0.0263620615,  0.0647019744, -0.0251159072,\n",
            "         0.0291772038, -0.0940847397,  0.0114687085, -0.0547983646,\n",
            "         0.1396576166, -0.0545682907, -0.0124046803, -0.0737826005,\n",
            "        -0.1082958132, -0.1154426336, -0.0515471399,  0.0295503139,\n",
            "         0.0430806130,  0.1464734077, -0.0045167208,  0.0199900866])\n",
            "ctensor.grad: tensor([ 5.8539044857e-01, -2.2550580502e+00, -1.9306101799e+00,\n",
            "         1.3425526023e-01, -2.2256320808e-03,  9.7947306931e-02,\n",
            "        -1.8618565053e-02, -1.8948914483e-02, -1.2960612774e+00,\n",
            "         1.9466714934e-02, -9.9669259787e-01,  1.3296613097e-01,\n",
            "         6.2374569476e-02, -1.3371014595e+00,  1.9856650382e-02,\n",
            "        -1.0267072916e+00,  1.3403643668e-01,  4.9003586173e-02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9185791016, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6449185610), tensor(0.9840990305), tensor(1.0883386135), tensor(1.1981774569), tensor(1.0946869850), tensor(1.4346201420), tensor(1.0603893995), tensor(1.1702407598), tensor(0.9198055863), tensor(1.2466572523), tensor(1.0927898884), tensor(1.5117466450), tensor(0.9849395752), tensor(1.2650710344), tensor(1.1801431179), tensor(0.9901609421), tensor(1.0811427832), tensor(0.8961278200), tensor(1.1016365290), tensor(1.0106363297)]\n",
            "b:  [tensor(0.6019657850), tensor(1.0956752300), tensor(1.3463406563), tensor(1.0133979321), tensor(1.4788858891), tensor(0.8135907054), tensor(1.2451121807), tensor(1.1725476980), tensor(1.8028023243), tensor(0.8540555835), tensor(1.2695941925), tensor(0.7010810375), tensor(1.3150860071), tensor(1.2460540533), tensor(0.8756911159), tensor(1.0068035126), tensor(1.3116462231), tensor(1.6817235947), tensor(0.9874354005), tensor(1.1882356405)]\n",
            "c:  [tensor(-0.0016439559), tensor(0.0107242353), tensor(0.0111092981), tensor(-0.0011224488), tensor(-0.0019979642), tensor(-0.0040896689), tensor(0.0005166302), tensor(0.0001191460), tensor(0.0138110779), tensor(0.0019424155), tensor(0.0079163276), tensor(0.0065952521), tensor(0.0088034682), tensor(0.0074157682), tensor(0.0069337864), tensor(0.0123661458), tensor(0.0015340738), tensor(-0.0011886724)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.2886794806, -0.0209207535, -0.2170950174, -0.0093159676,\n",
            "        -0.1220381707,  0.2359453440, -0.0361674726, -0.0420860052,\n",
            "        -0.1880160570,  0.1003244519, -0.1036715508,  0.2338733077,\n",
            "         0.0130348206, -0.0692996979,  0.0223207474, -0.0359784365,\n",
            "        -0.1956853867, -0.1775064468,  0.0121893883, -0.0426844358])\n",
            "btensor.grad: tensor([-0.0388332605,  0.0138583183,  0.0439056158, -0.0263773799,\n",
            "         0.0168626010, -0.0742192268,  0.0025384724, -0.0510950089,\n",
            "         0.1150788665, -0.0474148393, -0.0199433565, -0.0570132434,\n",
            "        -0.0955903232, -0.1042085886, -0.0458322391,  0.0163807869,\n",
            "         0.0266714692,  0.1181246042, -0.0103431940,  0.0093291998])\n",
            "ctensor.grad: tensor([ 0.6102368236, -2.4098632336, -2.0812525749,  0.1718374044,\n",
            "        -0.0028485248,  0.1254680753, -0.0237470064, -0.0242974423,\n",
            "        -1.4779651165,  0.0220964868, -1.1400482655,  0.1483726799,\n",
            "         0.0650462061, -1.4885663986,  0.0219757278, -1.1472615004,\n",
            "         0.1452597082,  0.0468952134])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.9074707031, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6436923742), tensor(0.9842314124), tensor(1.0892550945), tensor(1.1982592344), tensor(1.0952234268), tensor(1.4336521626), tensor(1.0605775118), tensor(1.1704578400), tensor(0.9205833673), tensor(1.2462778091), tensor(1.0932445526), tensor(1.5107808113), tensor(0.9849395752), tensor(1.2653934956), tensor(1.1800854206), tensor(0.9903436899), tensor(1.0819680691), tensor(0.8968567252), tensor(1.1016274691), tensor(1.0108472109)]\n",
            "b:  [tensor(0.6021190882), tensor(1.0956560373), tensor(1.3462048769), tensor(1.0135351419), tensor(1.4788526297), tensor(0.8138909936), tensor(1.2451362610), tensor(1.1727867126), tensor(1.8023310900), tensor(0.8542679548), tensor(1.2697232962), tensor(0.7013111711), tensor(1.3155119419), tensor(1.2465288639), tensor(0.8758991957), tensor(1.0067716837), tensor(1.3115773201), tensor(1.6812525988), tensor(0.9875102043), tensor(1.1882315874)]\n",
            "c:  [tensor(-0.0019626722), tensor(0.0120188724), tensor(0.0122353695), tensor(-0.0012279213), tensor(-0.0019962152), tensor(-0.0041667153), tensor(0.0005311809), tensor(0.0001341135), tensor(0.0146482904), tensor(0.0019299590), tensor(0.0085640578), tensor(0.0065130298), tensor(0.0087703355), tensor(0.0082426658), tensor(0.0069216541), tensor(0.0130057903), tensor(0.0014555687), tensor(-0.0012101987)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.4522745609e-01, -2.6475727558e-02, -1.8329668045e-01,\n",
            "        -1.6364455223e-02, -1.0727865994e-01,  1.9360327721e-01,\n",
            "        -3.7618011236e-02, -4.3420314789e-02, -1.5555942059e-01,\n",
            "         7.5890280306e-02, -9.0926229954e-02,  1.9316524267e-01,\n",
            "         5.9604644775e-06, -6.4485907555e-02,  1.1533975601e-02,\n",
            "        -3.6549806595e-02, -1.6504836082e-01, -1.4578449726e-01,\n",
            "         1.8172264099e-03, -4.2175292969e-02])\n",
            "btensor.grad: tensor([-0.0306664705,  0.0038459301,  0.0271549821, -0.0274426341,\n",
            "         0.0066554397, -0.0600519180, -0.0048274398, -0.0478147268,\n",
            "         0.0942392945, -0.0424737930, -0.0258132219, -0.0460242331,\n",
            "        -0.0851789415, -0.0949623585, -0.0416169688,  0.0063669086,\n",
            "         0.0137920678,  0.0942046642, -0.0149614811,  0.0008113384])\n",
            "ctensor.grad: tensor([ 0.6374323964, -2.5892751217, -2.2521429062,  0.2109449655,\n",
            "        -0.0034979635,  0.1540931016, -0.0291013662, -0.0299349073,\n",
            "        -1.6744241714,  0.0249128416, -1.2954601049,  0.1644448191,\n",
            "         0.0662663504, -1.6537950039,  0.0242648944, -1.2792882919,\n",
            "         0.1570103616,  0.0430527814])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8948974609, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6426519156), tensor(0.9843846560), tensor(1.0900338888), tensor(1.1983681917), tensor(1.0956999063), tensor(1.4328607321), tensor(1.0607699156), tensor(1.1706789732), tensor(0.9212347269), tensor(1.2459954023), tensor(1.0936456919), tensor(1.5099860430), tensor(0.9849880934), tensor(1.2656923532), tensor(1.1800693274), tensor(0.9905267954), tensor(1.0826690197), tensor(0.8974643946), tensor(1.1016583443), tensor(1.0110540390)]\n",
            "b:  [tensor(0.6022493839), tensor(1.0956771374), tensor(1.3461366892), tensor(1.0136770010), tensor(1.4788616896), tensor(0.8141413331), tensor(1.2451908588), tensor(1.1730110645), tensor(1.8019480705), tensor(0.8544640541), tensor(1.2698754072), tensor(0.7015064955), tensor(1.3158951998), tensor(1.2469661236), tensor(0.8760921359), tensor(1.0067776442), tensor(1.3115586042), tensor(1.6808825731), tensor(0.9876034856), tensor(1.1882615089)]\n",
            "c:  [tensor(-0.0022961514), tensor(0.0134166870), tensor(0.0134581113), tensor(-0.0013537637), tensor(-0.0019941269), tensor(-0.0042586518), tensor(0.0005485420), tensor(0.0001520721), tensor(0.0155921998), tensor(0.0019159856), tensor(0.0092964871), tensor(0.0064223669), tensor(0.0087374244), tensor(0.0091600670), tensor(0.0069082798), tensor(0.0137180267), tensor(0.0013708909), tensor(-0.0012287893)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.2080981731, -0.0306461155, -0.1557598114, -0.0218021870,\n",
            "        -0.0953007042,  0.1582759619, -0.0384866893, -0.0442159176,\n",
            "        -0.1302745342,  0.0564866811, -0.0802316070,  0.1589589417,\n",
            "        -0.0097073317, -0.0597805977,  0.0032078028, -0.0366256237,\n",
            "        -0.1401920319, -0.1215372086, -0.0061821938, -0.0413666964])\n",
            "btensor.grad: tensor([-0.0260622501, -0.0042110682,  0.0136465430, -0.0283613205,\n",
            "        -0.0018237233, -0.0500705242, -0.0109237731, -0.0448757410,\n",
            "         0.0766040683, -0.0392160416, -0.0304164886, -0.0390689820,\n",
            "        -0.0766490400, -0.0874474049, -0.0385936499, -0.0011831522,\n",
            "         0.0037451237,  0.0739969015, -0.0186505318, -0.0059818029])\n",
            "ctensor.grad: tensor([ 0.6669582725, -2.7956290245, -2.4454834461,  0.2516847253,\n",
            "        -0.0041764989,  0.1838725656, -0.0347221717, -0.0359172523,\n",
            "        -1.8878190517,  0.0279469043, -1.4648593664,  0.1813252866,\n",
            "         0.0658216327, -1.8348025084,  0.0267483220, -1.4244731665,\n",
            "         0.1693555564,  0.0371812806])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8801269531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6417697668), tensor(0.9845533967), tensor(1.0907001495), tensor(1.1984980106), tensor(1.0961278677), tensor(1.4322164059), tensor(1.0609643459), tensor(1.1709021330), tensor(0.9217874408), tensor(1.2457891703), tensor(1.0940014124), tensor(1.5093346834), tensor(0.9850723147), tensor(1.2659680843), tensor(1.1800848246), tensor(0.9907079935), tensor(1.0832686424), tensor(0.8979793191), tensor(1.1017196178), tensor(1.0112555027)]\n",
            "b:  [tensor(0.6023684144), tensor(1.0957307816), tensor(1.3461229801), tensor(1.0138229132), tensor(1.4789060354), tensor(0.8143572807), tensor(1.2452707291), tensor(1.1732220650), tensor(1.8016393185), tensor(0.8546502590), tensor(1.2700457573), tensor(0.7016812563), tensor(1.3162435293), tensor(1.2473733425), tensor(0.8762747645), tensor(1.0068116188), tensor(1.3115787506), tensor(1.6805980206), tensor(0.9877116084), tensor(1.1883183718)]\n",
            "c:  [tensor(-0.0026455936), tensor(0.0149324723), tensor(0.0147899678), tensor(-0.0015008418), tensor(-0.0019916836), tensor(-0.0043660766), tensor(0.0005688675), tensor(0.0001732252), tensor(0.0166525934), tensor(0.0019003690), tensor(0.0101216715), tensor(0.0063227792), tensor(0.0087056886), tensor(0.0101769995), tensor(0.0068935533), tensor(0.0145103838), tensor(0.0012797061), tensor(-0.0012432566)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1764341593, -0.0337509513, -0.1332464218, -0.0259608030,\n",
            "        -0.0855901390,  0.1288722754, -0.0388854742, -0.0446228981,\n",
            "        -0.1105464697,  0.0412458330, -0.0711371899,  0.1302633882,\n",
            "        -0.0168499947, -0.0551578999, -0.0030994415, -0.0362379551,\n",
            "        -0.1199357510, -0.1029815674, -0.0122604370, -0.0402892232])\n",
            "btensor.grad: tensor([-0.0238062143, -0.0107327700,  0.0027366877, -0.0291715860,\n",
            "        -0.0088707358, -0.0431859493, -0.0159808099, -0.0421955585,\n",
            "         0.0617552996, -0.0372397900, -0.0340778828, -0.0349565446,\n",
            "        -0.0696556568, -0.0814374685, -0.0365199298, -0.0067917705,\n",
            "        -0.0040220022,  0.0569128990, -0.0216234922, -0.0113828182])\n",
            "ctensor.grad: tensor([ 0.6988844275, -3.0315694809, -2.6637134552,  0.2941560745,\n",
            "        -0.0048866714,  0.2148495913, -0.0406509750, -0.0423062220,\n",
            "        -2.1207885742,  0.0312333144, -1.6503689289,  0.1991750896,\n",
            "         0.0634723231, -2.0338642597,  0.0294531677, -1.5847138166,\n",
            "         0.1823693663,  0.0289345980])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8631591797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6410222054), tensor(0.9847334027), tensor(1.0912737846), tensor(1.1986434460), tensor(1.0965163708), tensor(1.4316939116), tensor(1.0611586571), tensor(1.1711258888), tensor(0.9222629070), tensor(1.2456417084), tensor(1.0943175554), tensor(1.5088033676), tensor(0.9851820469), tensor(1.2662208080), tensor(1.1801233292), tensor(0.9908849597), tensor(1.0837852955), tensor(0.8984229565), tensor(1.1018033028), tensor(1.0114500523)]\n",
            "b:  [tensor(0.6024839282), tensor(1.0958110094), tensor(1.3461533785), tensor(1.0139722824), tensor(1.4789795876), tensor(0.8145501018), tensor(1.2453715801), tensor(1.1734203100), tensor(1.8013924360), tensor(0.8548315167), tensor(1.2702308893), tensor(0.7018455267), tensor(1.3165630102), tensor(1.2477569580), tensor(0.8764507174), tensor(1.0068657398), tensor(1.3116282225), tensor(1.6803854704), tensor(0.9878317714), tensor(1.1883964539)]\n",
            "c:  [tensor(-0.0030121882), tensor(0.0165825579), tensor(0.0162448101), tensor(-0.0016700694), tensor(-0.0019888680), tensor(-0.0044896053), tensor(0.0005923343), tensor(0.0001978121), tensor(0.0178407077), tensor(0.0018829635), tensor(0.0110488245), tensor(0.0062136804), tensor(0.0086761890), tensor(0.0113037648), tensor(0.0068773483), tensor(0.0153914448), tensor(0.0011816313), tensor(-0.0012522234)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1495145559, -0.0360058844, -0.1147361398, -0.0290913582,\n",
            "        -0.0777021497,  0.1045080423, -0.0388669670, -0.0447424650,\n",
            "        -0.0950988531,  0.0295034125, -0.0632369518,  0.1062594950,\n",
            "        -0.0219466090, -0.0505452156, -0.0076953173, -0.0353906155,\n",
            "        -0.1033208370, -0.0887290239, -0.0167385936, -0.0389155746])\n",
            "btensor.grad: tensor([-0.0231075287, -0.0160461664, -0.0060682297, -0.0298799872,\n",
            "        -0.0147024095, -0.0385620594, -0.0201613307, -0.0396598577,\n",
            "         0.0493810773, -0.0362503529, -0.0370271206, -0.0328522623,\n",
            "        -0.0638890862, -0.0767350197, -0.0351871401, -0.0108160973,\n",
            "        -0.0098994672,  0.0425050259, -0.0240277052, -0.0156228542])\n",
            "ctensor.grad: tensor([ 0.7331890464, -3.3001718521, -2.9096837044,  0.3384552002,\n",
            "        -0.0056311218,  0.2470576465, -0.0469336659, -0.0491738021,\n",
            "        -2.3762295246,  0.0348109603, -1.8543058634,  0.2181975693,\n",
            "         0.0589986891, -2.2535300255,  0.0324101970, -1.7621219158,\n",
            "         0.1961496025,  0.0179334413])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8430175781, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6403884888), tensor(0.9849210978), tensor(1.0917706490), tensor(1.1988002062), tensor(1.0968726873), tensor(1.4312715530), tensor(1.0613508224), tensor(1.1713489294), tensor(0.9226773977), tensor(1.2455378771), tensor(1.0945984125), tensor(1.5083719492), tensor(0.9853088856), tensor(1.2664499283), tensor(1.1801773310), tensor(0.9910550714), tensor(1.0842329264), tensor(0.8988113403), tensor(1.1019024849), tensor(1.0116360188)]\n",
            "b:  [tensor(0.6026012301), tensor(1.0959128141), tensor(1.3462190628), tensor(1.0141246319), tensor(1.4790769815), tensor(0.8147280216), tensor(1.2454894781), tensor(1.1736060381), tensor(1.8011960983), tensor(0.8550115824), tensor(1.2704279423), tensor(0.7020063996), tensor(1.3168582916), tensor(1.2481226921), tensor(0.8766227961), tensor(1.0069333315), tensor(1.3116990328), tensor(1.6802332401), tensor(0.9879615307), tensor(1.1884907484)]\n",
            "c:  [tensor(-0.0033970510), tensor(0.0183850452), tensor(0.0178381633), tensor(-0.0018624011), tensor(-0.0019856617), tensor(-0.0046298630), tensor(0.0006191448), tensor(0.0002261139), tensor(0.0191693678), tensor(0.0018636015), tensor(0.0120883975), tensor(0.0060943500), tensor(0.0086500766), tensor(0.0125520909), tensor(0.0068595209), tensor(0.0163709614), tensor(0.0010762142), tensor(-0.0012541184)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1267478466, -0.0375436842, -0.0993823409, -0.0313611031,\n",
            "        -0.0712598860,  0.0844633579, -0.0384302735, -0.0446183681,\n",
            "        -0.0828992128,  0.0207717642, -0.0561671853,  0.0862874389,\n",
            "        -0.0253632665, -0.0458273888, -0.0107930899, -0.0340249538,\n",
            "        -0.0895186663, -0.0776814222, -0.0198400617, -0.0371826291])\n",
            "btensor.grad: tensor([-0.0234558582, -0.0203680992, -0.0131250620, -0.0304729939,\n",
            "        -0.0194692165, -0.0355830193, -0.0235772729, -0.0371413231,\n",
            "         0.0392736793, -0.0360190868, -0.0394166708, -0.0321755558,\n",
            "        -0.0590659976, -0.0731556416, -0.0344192386, -0.0135135055,\n",
            "        -0.0141671300,  0.0304361582, -0.0259491205, -0.0188579559])\n",
            "ctensor.grad: tensor([ 0.7697256804, -3.6049747467, -3.1867046356,  0.3846635520,\n",
            "        -0.0064124884,  0.2805148661, -0.0536209717, -0.0566034690,\n",
            "        -2.6573212147,  0.0387239121, -2.0791466236,  0.2386610359,\n",
            "         0.0522241555, -2.4966518879,  0.0356545076, -1.9590318203,\n",
            "         0.2108342350,  0.0037900079])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.8188476562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6398501396), tensor(0.9851131439), tensor(1.0922029018), tensor(1.1989644766), tensor(1.0972023010), tensor(1.4309306145), tensor(1.0615384579), tensor(1.1715701818), tensor(0.9230428934), tensor(1.2454643250), tensor(1.0948461294), tensor(1.5080227852), tensor(0.9854454994), tensor(1.2666541338), tensor(1.1802397966), tensor(0.9912151694), tensor(1.0846220255), tensor(0.8991560936), tensor(1.1020108461), tensor(1.0118107796)]\n",
            "b:  [tensor(0.6027237177), tensor(1.0960320234), tensor(1.3463125229), tensor(1.0142791271), tensor(1.4791932106), tensor(0.8148968220), tensor(1.2456208467), tensor(1.1737782955), tensor(1.8010394573), tensor(0.8551933169), tensor(1.2706346512), tensor(0.7021690011), tensor(1.3171328306), tensor(1.2484753132), tensor(0.8767930865), tensor(1.0070083141), tensor(1.3117839098), tensor(1.6801308393), tensor(0.9880986214), tensor(1.1885966063)]\n",
            "c:  [tensor(-0.0038011260), tensor(0.0203600731), tensor(0.0195874758), tensor(-0.0020788205), tensor(-0.0019820451), tensor(-0.0047874670), tensor(0.0006495301), tensor(0.0002584616), tensor(0.0206531230), tensor(0.0018420903), tensor(0.0132521437), tensor(0.0059638801), tensor(0.0086285258), tensor(0.0139352903), tensor(0.0068399077), tensor(0.0174599458), tensor(0.0009629001), tensor(-0.0012471952)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.1076815128, -0.0384093821, -0.0864389539, -0.0328627825,\n",
            "        -0.0659188032,  0.0681805611, -0.0375207663, -0.0442483425,\n",
            "        -0.0731039047,  0.0147153139, -0.0495514274,  0.0698300302,\n",
            "        -0.0273191929, -0.0408527851, -0.0125000477, -0.0320230722,\n",
            "        -0.0778276920, -0.0689474344, -0.0216810107, -0.0349535942])\n",
            "btensor.grad: tensor([-0.0244972706, -0.0238415003, -0.0186901689, -0.0308972597,\n",
            "        -0.0232539773, -0.0337634087, -0.0262807012, -0.0344626904,\n",
            "         0.0313231945, -0.0363507867, -0.0413382053, -0.0325168818,\n",
            "        -0.0549128354, -0.0705201626, -0.0340526700, -0.0150074363,\n",
            "        -0.0169813037,  0.0204731226, -0.0274231434, -0.0211622715])\n",
            "ctensor.grad: tensor([ 0.8081498742, -3.9500551224, -3.4986245632,  0.4328386486,\n",
            "        -0.0072333119,  0.3152076602, -0.0607706122, -0.0646955371,\n",
            "        -2.9675097466,  0.0430225544, -2.3274922371,  0.2609397769,\n",
            "         0.0431021526, -2.7663986683,  0.0392265581, -2.1779682636,\n",
            "         0.2266282588, -0.0138464812])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7884521484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6393903494), tensor(0.9853060246), tensor(1.0925791264), tensor(1.1991325617), tensor(1.0975090265), tensor(1.4306544065), tensor(1.0617185831), tensor(1.1717880964), tensor(0.9233677983), tensor(1.2454086542), tensor(1.0950611830), tensor(1.5077403784), tensor(0.9855850339), tensor(1.2668311596), tensor(1.1803039312), tensor(0.9913611412), tensor(1.0849601030), tensor(0.8994649053), tensor(1.1021223068), tensor(1.0119709969)]\n",
            "b:  [tensor(0.6028537154), tensor(1.0961647034), tensor(1.3464270830), tensor(1.0144344568), tensor(1.4793236256), tensor(0.8150604367), tensor(1.2457622290), tensor(1.1739354134), tensor(1.8009119034), tensor(0.8553787470), tensor(1.2708487511), tensor(0.7023369074), tensor(1.3173885345), tensor(1.2488185167), tensor(0.8769627213), tensor(1.0070850849), tensor(1.3118759394), tensor(1.6800683737), tensor(0.9882407784), tensor(1.1887092590)]\n",
            "c:  [tensor(-0.0042250841), tensor(0.0225300789), tensor(0.0215124078), tensor(-0.0023203176), tensor(-0.0019779971), tensor(-0.0049630050), tensor(0.0006837543), tensor(0.0002952460), tensor(0.0223083626), tensor(0.0018182078), tensor(0.0145531269), tensor(0.0058210972), tensor(0.0086126197), tensor(0.0154684233), tensor(0.0068183211), tensor(0.0186707694), tensor(0.0008409800), tensor(-0.0012295750)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0919607878, -0.0385729969, -0.0752431154, -0.0336226225,\n",
            "        -0.0613531470,  0.0552519560, -0.0360175073, -0.0435714722,\n",
            "        -0.0649771690,  0.0111448616, -0.0430215597,  0.0564895570,\n",
            "        -0.0279014707, -0.0353978872, -0.0128333569, -0.0291976929,\n",
            "        -0.0676137209, -0.0617671013, -0.0222927928, -0.0320527554])\n",
            "btensor.grad: tensor([-0.0260040760, -0.0265471935, -0.0229206681, -0.0310699940,\n",
            "        -0.0260724276, -0.0327175856, -0.0282660425, -0.0314277411,\n",
            "         0.0255209804, -0.0370818377, -0.0428187847, -0.0335812569,\n",
            "        -0.0511494577, -0.0686503649, -0.0339242667, -0.0153443813,\n",
            "        -0.0184002668,  0.0124861002, -0.0284324884, -0.0225391388])\n",
            "ctensor.grad: tensor([ 0.8479157686, -4.3400135040, -3.8498649597,  0.4829942286,\n",
            "        -0.0080958297,  0.3510759473, -0.0684483349, -0.0735687464,\n",
            "        -3.3104791641,  0.0477650650, -2.6019668579,  0.2855658829,\n",
            "         0.0318118781, -3.0662651062,  0.0431733355, -2.4216482639,\n",
            "         0.2438402027, -0.0352402963])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7526855469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6389936209), tensor(0.9854956269), tensor(1.0929050446), tensor(1.1993005276), tensor(1.0977952480), tensor(1.4304274321), tensor(1.0618872643), tensor(1.1720005274), tensor(0.9236571193), tensor(1.2453585863), tensor(1.0952420235), tensor(1.5075103045), tensor(0.9857203960), tensor(1.2669770718), tensor(1.1803624630), tensor(0.9914875627), tensor(1.0852514505), tensor(0.8997423053), tensor(1.1022303104), tensor(1.0121121407)]\n",
            "b:  [tensor(0.6029927135), tensor(1.0963071585), tensor(1.3465564251), tensor(1.0145888329), tensor(1.4794628620), tensor(0.8152210116), tensor(1.2459095716), tensor(1.1740742922), tensor(1.8008021116), tensor(0.8555690646), tensor(1.2710678577), tensor(0.7025126219), tensor(1.3176259995), tensor(1.2491554022), tensor(0.8771320581), tensor(1.0071573257), tensor(1.3119678497), tensor(1.6800361872), tensor(0.9883853793), tensor(1.1888238192)]\n",
            "c:  [tensor(-0.0046691610), tensor(0.0249200556), tensor(0.0236351322), tensor(-0.0025878588), tensor(-0.0019734963), tensor(-0.0051570018), tensor(0.0007221194), tensor(0.0003369295), tensor(0.0241533965), tensor(0.0017916983), tensor(0.0160056669), tensor(0.0056644417), tensor(0.0086031575), tensor(0.0171684492), tensor(0.0067945463), tensor(0.0200171880), tensor(0.0007095090), tensor(-0.0011993536)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0793430805, -0.0379225612, -0.0651893020, -0.0335910320,\n",
            "        -0.0572439879,  0.0454043150, -0.0337394476, -0.0424822569,\n",
            "        -0.0578665733,  0.0100160018, -0.0361717343,  0.0460054874,\n",
            "        -0.0270754099, -0.0291928053, -0.0116995573, -0.0252883434,\n",
            "        -0.0582690239, -0.0554759502, -0.0216049552, -0.0282220840])\n",
            "btensor.grad: tensor([-0.0278052092, -0.0284875631, -0.0258738399, -0.0308708549,\n",
            "        -0.0278588980, -0.0321170092, -0.0294650197, -0.0277674198,\n",
            "         0.0219682455, -0.0380623937, -0.0438327789, -0.0351466238,\n",
            "        -0.0474826097, -0.0673730373, -0.0338726044, -0.0144506693,\n",
            "        -0.0183735788,  0.0064429045, -0.0289144516, -0.0229232311])\n",
            "ctensor.grad: tensor([ 0.8881536126, -4.7799525261, -4.2454471588,  0.5350823402,\n",
            "        -0.0090017822,  0.3879938722, -0.0767301992, -0.0833668709,\n",
            "        -3.6900684834,  0.0530190505, -2.9050800800,  0.3133109510,\n",
            "         0.0189246666, -3.4000518322,  0.0475499481, -2.6928360462,\n",
            "         0.2629420757, -0.0604427196])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7082519531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6386451721), tensor(0.9856770039), tensor(1.0931835175), tensor(1.1994638443), tensor(1.0980615616), tensor(1.4302350283), tensor(1.0620394945), tensor(1.1722046137), tensor(0.9239129424), tensor(1.2453014851), tensor(1.0953847170), tensor(1.5073192120), tensor(0.9858438969), tensor(1.2670865059), tensor(1.1804070473), tensor(0.9915873408), tensor(1.0854974985), tensor(0.8999895453), tensor(1.1023275852), tensor(1.0122277737)]\n",
            "b:  [tensor(0.6031415462), tensor(1.0964552164), tensor(1.3466939926), tensor(1.0147395134), tensor(1.4796053171), tensor(0.8153793812), tensor(1.2460583448), tensor(1.1741901636), tensor(1.8006978035), tensor(0.8557648063), tensor(1.2712893486), tensor(0.7026978731), tensor(1.3178439140), tensor(1.2494878769), tensor(0.8773006797), tensor(1.0072182417), tensor(1.3120516539), tensor(1.6800241470), tensor(0.9885290861), tensor(1.1889346838)]\n",
            "c:  [tensor(-0.0051329983), tensor(0.0275577288), tensor(0.0259806067), tensor(-0.0028823419), tensor(-0.0019685202), tensor(-0.0053698737), tensor(0.0007649714), tensor(0.0003840606), tensor(0.0262084641), tensor(0.0017622665), tensor(0.0176251438), tensor(0.0054917959), tensor(0.0086003412), tensor(0.0190543476), tensor(0.0067683356), tensor(0.0215143189), tensor(0.0005671850), tensor(-0.0011547764)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0696942806, -0.0362806320, -0.0556845665, -0.0326528549,\n",
            "        -0.0532539636,  0.0384918451, -0.0304371417, -0.0408192873,\n",
            "        -0.0511629581,  0.0114151537, -0.0285467505,  0.0382125080,\n",
            "        -0.0247024298, -0.0218819380, -0.0089175701, -0.0199544430,\n",
            "        -0.0492069721, -0.0494509935, -0.0194547176, -0.0231351256])\n",
            "btensor.grad: tensor([-0.0297679901, -0.0296074152, -0.0275199413, -0.0301474333,\n",
            "        -0.0284979939, -0.0316796303, -0.0297556520, -0.0231670141,\n",
            "         0.0208666921, -0.0391498804, -0.0442925692, -0.0370465964,\n",
            "        -0.0435885191, -0.0664916039, -0.0337248445, -0.0121927857,\n",
            "        -0.0167599171,  0.0024100542, -0.0287437439, -0.0221685171])\n",
            "ctensor.grad: tensor([ 0.9276749492, -5.2753467560, -4.6909499168,  0.5889659524,\n",
            "        -0.0099521335,  0.4257441461, -0.0857038945, -0.0942623615,\n",
            "        -4.1101365089,  0.0588635989, -3.2389528751,  0.3452912569,\n",
            "         0.0056329295, -3.7717967033,  0.0524213687, -2.9942629337,\n",
            "         0.2846478820, -0.0891544074])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.6544189453, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6383302212), tensor(0.9858438373), tensor(1.0934140682), tensor(1.1996169090), tensor(1.0983066559), tensor(1.4300625324), tensor(1.0621682405), tensor(1.1723964214), tensor(0.9241341949), tensor(1.2452236414), tensor(1.0954828262), tensor(1.5071538687), tensor(0.9859464765), tensor(1.2671515942), tensor(1.1804280281), tensor(0.9916511178), tensor(1.0856965780), tensor(0.9002048969), tensor(1.1024054289), tensor(1.0123096704)]\n",
            "b:  [tensor(0.6033004522), tensor(1.0966041088), tensor(1.3468326330), tensor(1.0148829222), tensor(1.4797441959), tensor(0.8155350089), tensor(1.2462030649), tensor(1.1742763519), tensor(1.8005850315), tensor(0.8559657335), tensor(1.2715095282), tensor(0.7028934360), tensor(1.3180394173), tensor(1.2498168945), tensor(0.8774670362), tensor(1.0072597265), tensor(1.3121181726), tensor(1.6800212860), tensor(0.9886679053), tensor(1.1890349388)]\n",
            "c:  [tensor(-0.0056154151), tensor(0.0304736774), tensor(0.0285768267), tensor(-0.0032045376), tensor(-0.0019630468), tensor(-0.0056018680), tensor(0.0008127072), tensor(0.0004372929), tensor(0.0284955930), tensor(0.0017295708), tensor(0.0194276143), tensor(0.0053002327), tensor(0.0086032832), tensor(0.0211471636), tensor(0.0067394031), tensor(0.0231784768), tensor(0.0004121683), tensor(-0.0010945490)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0629903078, -0.0333714783, -0.0461210608, -0.0306181908,\n",
            "        -0.0490235239,  0.0344965458, -0.0257594585, -0.0383538008,\n",
            "        -0.0442544222,  0.0155671313, -0.0196317434,  0.0330774486,\n",
            "        -0.0205121040, -0.0130230188, -0.0042071342, -0.0127512217,\n",
            "        -0.0398153067, -0.0430685282, -0.0155747533, -0.0163818598])\n",
            "btensor.grad: tensor([-0.0317827463, -0.0297757387, -0.0277369022, -0.0286870599,\n",
            "        -0.0277654678, -0.0311262608, -0.0289431810, -0.0172394514,\n",
            "         0.0225484371, -0.0401895642, -0.0440428257, -0.0391185135,\n",
            "        -0.0391053557, -0.0657954216, -0.0332740396, -0.0083057880,\n",
            "        -0.0133087337,  0.0005648136, -0.0277584791, -0.0200508833])\n",
            "ctensor.grad: tensor([ 0.9648335576, -5.8318972588, -5.1924395561,  0.6443913579,\n",
            "        -0.0109467153,  0.4639890492, -0.0954717174, -0.1064645499,\n",
            "        -4.5742588043,  0.0653914884, -3.6049418449,  0.3831261098,\n",
            "        -0.0058848448, -4.1856303215,  0.0578645915, -3.3283174038,\n",
            "         0.3100334406, -0.1204549223])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.5870361328, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6380336285), tensor(0.9859879613), tensor(1.0935934782), tensor(1.1997530460), tensor(1.0985274315), tensor(1.4298948050), tensor(1.0622646809), tensor(1.1725703478), tensor(0.9243168235), tensor(1.2451094389), tensor(1.0955269337), tensor(1.5070004463), tensor(0.9860171080), tensor(1.2671619654), tensor(1.1804138422), tensor(0.9916667938), tensor(1.0858438015), tensor(0.9003833532), tensor(1.1024534702), tensor(1.0123469830)]\n",
            "b:  [tensor(0.6034691930), tensor(1.0967481136), tensor(1.3469641209), tensor(1.0150140524), tensor(1.4798710346), tensor(0.8156859279), tensor(1.2463368177), tensor(1.1743239164), tensor(1.8004475832), tensor(0.8561708331), tensor(1.2717239857), tensor(0.7030996680), tensor(1.3182075024), tensor(1.2501420975), tensor(0.8776285648), tensor(1.0072720051), tensor(1.3121564388), tensor(1.6800153255), tensor(0.9887965322), tensor(1.1891162395)]\n",
            "c:  [tensor(-0.0061141565), tensor(0.0337012485), tensor(0.0314549506), tensor(-0.0035550147), tensor(-0.0019570549), tensor(-0.0058529875), tensor(0.0008657828), tensor(0.0004974039), tensor(0.0310383029), tensor(0.0016932149), tensor(0.0214291476), tensor(0.0050856671), tensor(0.0086092921), tensor(0.0234699268), tensor(0.0067074173), tensor(0.0250268169), tensor(0.0002418205), tensor(-0.0010183145)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0593154430, -0.0288293958, -0.0358803272, -0.0272194147,\n",
            "        -0.0441586375,  0.0335352421, -0.0192836225, -0.0347867012,\n",
            "        -0.0365202427,  0.0228473991, -0.0088283420,  0.0306764543,\n",
            "        -0.0141218305, -0.0020827055,  0.0028306246, -0.0031337738,\n",
            "        -0.0294435024, -0.0356925726, -0.0095989704, -0.0074613094])\n",
            "btensor.grad: tensor([-0.0337508917, -0.0288025141, -0.0263078809, -0.0262264609,\n",
            "        -0.0253680944, -0.0301790237, -0.0267585814, -0.0095083714,\n",
            "         0.0274822712, -0.0410149693, -0.0428799391, -0.0412469953,\n",
            "        -0.0336130857, -0.0650460720, -0.0323011279, -0.0024590492,\n",
            "        -0.0076607615,  0.0011882782, -0.0257238150, -0.0162668228])\n",
            "ctensor.grad: tensor([ 0.9974830747, -6.4551386833, -5.7562484741,  0.7009543777,\n",
            "        -0.0119838258,  0.5022390485, -0.1061511934, -0.1202219576,\n",
            "        -5.0854182243,  0.0727117434, -4.0030670166,  0.4291313589,\n",
            "        -0.0120175518, -4.6455254555,  0.0639712214, -3.6966793537,\n",
            "         0.3406955898, -0.1524688900])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.5063476562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6377393007), tensor(0.9860989451), tensor(1.0937148333), tensor(1.1998634338), tensor(1.0987185240), tensor(1.4297155142), tensor(1.0623170137), tensor(1.1727191210), tensor(0.9244531989), tensor(1.2449406385), tensor(1.0955041647), tensor(1.5068445206), tensor(0.9860422611), tensor(1.2671040297), tensor(1.1803503036), tensor(0.9916188717), tensor(1.0859307051), tensor(0.9005165100), tensor(1.1024587154), tensor(1.0123258829)]\n",
            "b:  [tensor(0.6036470532), tensor(1.0968801975), tensor(1.3470788002), tensor(1.0151263475), tensor(1.4799755812), tensor(0.8158286810), tensor(1.2464511395), tensor(1.1743209362), tensor(1.8002661467), tensor(0.8563780189), tensor(1.2719265223), tensor(0.7033160925), tensor(1.3183406591), tensor(1.2504619360), tensor(0.8777812123), tensor(1.0072430372), tensor(1.3121531010), tensor(1.6799918413), tensor(0.9889083505), tensor(1.1891683340)]\n",
            "c:  [tensor(-0.0066256030), tensor(0.0372762159), tensor(0.0346492603), tensor(-0.0039340430), tensor(-0.0019505251), tensor(-0.0061228964), tensor(0.0009247212), tensor(0.0005653183), tensor(0.0338609703), tensor(0.0016527389), tensor(0.0236447677), tensor(0.0048423754), tensor(0.0086128311), tensor(0.0260473657), tensor(0.0066719921), tensor(0.0270766709), tensor(5.2330506151e-05), tensor(-0.0009273979)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0588649511, -0.0222002566, -0.0242730379, -0.0220887661,\n",
            "        -0.0382165611,  0.0358507633, -0.0104554296, -0.0297571421,\n",
            "        -0.0272800922,  0.0337642580,  0.0045547485,  0.0311964154,\n",
            "        -0.0050270557,  0.0115957260,  0.0127077103,  0.0095835924,\n",
            "        -0.0173795223, -0.0266257524, -0.0010447502,  0.0042292476])\n",
            "btensor.grad: tensor([-0.0355706215, -0.0264261961, -0.0229295492, -0.0224502683,\n",
            "        -0.0209186822, -0.0285474062, -0.0228545964,  0.0005990267,\n",
            "         0.0362768769, -0.0414332747, -0.0405170918, -0.0432871580,\n",
            "        -0.0266262591, -0.0639747381, -0.0305343568,  0.0058025718,\n",
            "         0.0006629080,  0.0046871901, -0.0223608017, -0.0104153156])\n",
            "ctensor.grad: tensor([ 1.0228925943e+00, -7.1499304771e+00, -6.3886198997e+00,\n",
            "         7.5805616379e-01, -1.3059720397e-02,  5.3981810808e-01,\n",
            "        -1.1787662655e-01, -1.3582886755e-01, -5.6453375816e+00,\n",
            "         8.0951996148e-02, -4.4312396049e+00,  4.8658385873e-01,\n",
            "        -7.0783495903e-03, -5.1548771858e+00,  7.0850342512e-02,\n",
            "        -4.0997085571e+00,  3.7897989154e-01, -1.8183329701e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.4066162109, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6374295950), tensor(0.9861635566), tensor(1.0937676430), tensor(1.1999374628), tensor(1.0988720655), tensor(1.4295064211), tensor(1.0623102188), tensor(1.1728332043), tensor(0.9245322943), tensor(1.2446957827), tensor(1.0953977108), tensor(1.5066697598), tensor(0.9860053062), tensor(1.2669601440), tensor(1.1802201271), tensor(0.9914879799), tensor(1.0859448910), tensor(0.9005920291), tensor(1.1024053097), tensor(1.0122290850)]\n",
            "b:  [tensor(0.6038327813), tensor(1.0969917774), tensor(1.3471647501), tensor(1.0152112246), tensor(1.4800451994), tensor(0.8159582615), tensor(1.2465351820), tensor(1.1742522717), tensor(1.8000177145), tensor(0.8565842509), tensor(1.2721096277), tensor(0.7035416961), tensor(1.3184286356), tensor(1.2507733107), tensor(0.8779196739), tensor(1.0071579218), tensor(1.3120918274), tensor(1.6799339056), tensor(0.9889950156), tensor(1.1891784668)]\n",
            "c:  [tensor(-0.0071444507), tensor(0.0412360393), tensor(0.0381968096), tensor(-0.0043414813), tensor(-0.0019434411), tensor(-0.0064108139), tensor(0.0009901186), tensor(0.0006421273), tensor(0.0369878709), tensor(0.0016076087), tensor(0.0260869097), tensor(0.0045623709), tensor(0.0086041111), tensor(0.0289053358), tensor(0.0066326763), tensor(0.0293444842), tensor(-0.0001617899), tensor(-0.0008258614)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0619438887, -0.0129274726, -0.0105727911, -0.0148003101,\n",
            "        -0.0307017267,  0.0418179035,  0.0013593435, -0.0228171349,\n",
            "        -0.0158244371,  0.0489647239,  0.0212978125,  0.0349446535,\n",
            "         0.0073905587,  0.0287756920,  0.0260436535,  0.0261803865,\n",
            "        -0.0028419495, -0.0151077509,  0.0106796622,  0.0193648934])\n",
            "btensor.grad: tensor([-0.0371409655, -0.0223244429, -0.0171968937, -0.0169792771,\n",
            "        -0.0139345080, -0.0259207487, -0.0168021023,  0.0137366056,\n",
            "         0.0496822000, -0.0412510633, -0.0366218090, -0.0451198816,\n",
            "        -0.0175939500, -0.0622748137, -0.0276899785,  0.0170122385,\n",
            "         0.0122457743,  0.0115790367, -0.0173311234, -0.0020310879])\n",
            "ctensor.grad: tensor([ 1.0376957655, -7.9196486473, -7.0950951576,  0.8148766160,\n",
            "        -0.0141680064,  0.5758343935, -0.1307948679, -0.1536179483,\n",
            "        -6.2537970543,  0.0902603418, -4.8842844963,  0.5600086451,\n",
            "         0.0174401850, -5.7159409523,  0.0786312670, -4.5356254578,\n",
            "         0.4282408655, -0.2030730397])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.2847900391, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6370848417), tensor(0.9861655831), tensor(1.0937377214), tensor(1.1999616623), tensor(1.0989775658), tensor(1.4292469025), tensor(1.0622256994), tensor(1.1729005575), tensor(0.9245393276), tensor(1.2443498373), tensor(1.0951864719), tensor(1.5064581633), tensor(0.9858861566), tensor(1.2667086124), tensor(1.1800024509), tensor(0.9912503362), tensor(1.0858699083), tensor(0.9005936980), tensor(1.1022741795), tensor(1.0120357275)]\n",
            "b:  [tensor(0.6040247083), tensor(1.0970724821), tensor(1.3472080231), tensor(1.0152583122), tensor(1.4800645113), tensor(0.8160681725), tensor(1.2465758324), tensor(1.1740990877), tensor(1.7996748686), tensor(0.8567855954), tensor(1.2722636461), tensor(0.7037748098), tensor(1.3184581995), tensor(1.2510714531), tensor(0.8780369163), tensor(1.0069991350), tensor(1.3119530678), tensor(1.6798214912), tensor(0.9890463948), tensor(1.1891313791)]\n",
            "c:  [tensor(-0.0076634684), tensor(0.0456186198), tensor(0.0421366282), tensor(-0.0047766482), tensor(-0.0019357916), tensor(-0.0067153946), tensor(0.0010626467), tensor(0.0007290998), tensor(0.0404417180), tensor(0.0015572059), tensor(0.0287633389), tensor(0.0042346437), tensor(0.0085673099), tensor(0.0320698395), tensor(0.0065889433), tensor(0.0318442434), tensor(-0.0004083705), tensor(-0.0007219713)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0689407587, -0.0004030764,  0.0059874654, -0.0048439503,\n",
            "        -0.0210924000,  0.0519053936,  0.0168996751, -0.0134691000,\n",
            "        -0.0014027357,  0.0691851526,  0.0422466993,  0.0423095822,\n",
            "         0.0238329768,  0.0503103733,  0.0435304642,  0.0475337505,\n",
            "         0.0150060654, -0.0003297329,  0.0262328386,  0.0386636257])\n",
            "btensor.grad: tensor([-0.0383870602, -0.0161460638, -0.0086528659, -0.0094247460,\n",
            "        -0.0038624257, -0.0219800472, -0.0081309378,  0.0306383371,\n",
            "         0.0685591698, -0.0402688980, -0.0308033228, -0.0466245115,\n",
            "        -0.0059241354, -0.0596207380, -0.0234513134,  0.0317565799,\n",
            "         0.0277440101,  0.0224756002, -0.0102770329,  0.0094155073])\n",
            "ctensor.grad: tensor([ 1.0380352736, -8.7651586533, -7.8796367645,  0.8703342676,\n",
            "        -0.0152989598,  0.6091616750, -0.1450563818, -0.1739450395,\n",
            "        -6.9076957703,  0.1008055732, -5.3528594971,  0.6554548144,\n",
            "         0.0736031830, -6.3290052414,  0.0874658376, -4.9995160103,\n",
            "         0.4931611121, -0.2077800035])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.1354980469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6366833448), tensor(0.9860855341), tensor(1.0936068296), tensor(1.1999201775), tensor(1.0990220308), tensor(1.4289135933), tensor(1.0620409250), tensor(1.1729065180), tensor(0.9244557023), tensor(1.2438738346), tensor(1.0948451757), tensor(1.5061894655), tensor(0.9856609702), tensor(1.2663232088), tensor(1.1796730757), tensor(0.9908775687), tensor(1.0856847763), tensor(0.9005009532), tensor(1.1020425558), tensor(1.0117216110)]\n",
            "b:  [tensor(0.6042210460), tensor(1.0971101522), tensor(1.3471920490), tensor(1.0152552128), tensor(1.4800151587), tensor(0.8161503077), tensor(1.2465575933), tensor(1.1738388538), tensor(1.7992056608), tensor(0.8569771647), tensor(1.2723770142), tensor(0.7040134072), tensor(1.3184132576), tensor(1.2513499260), tensor(0.8781245351), tensor(1.0067458153), tensor(1.3117138147), tensor(1.6796313524), tensor(0.9890506864), tensor(1.1890091896)]\n",
            "c:  [tensor(-0.0081732981), tensor(0.0504604429), tensor(0.0465083644), tensor(-0.0052381908), tensor(-0.0019275722), tensor(-0.0070346221), tensor(0.0011430448), tensor(0.0008276766), tensor(0.0442418009), tensor(0.0015008188), tensor(0.0316745751), tensor(0.0038442989), tensor(0.0084784739), tensor(0.0355655588), tensor(0.0065401788), tensor(0.0345852859), tensor(-0.0006983916), tensor(-0.0006300722)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.0803016424,  0.0160105824,  0.0261893868,  0.0083053112,\n",
            "        -0.0088889003,  0.0666638613,  0.0369435549, -0.0011897087,\n",
            "         0.0167295933,  0.0952090696,  0.0682564378,  0.0537480116,\n",
            "         0.0450397134,  0.0770924091,  0.0658856630,  0.0745511055,\n",
            "         0.0370222330,  0.0185514688,  0.0463138819,  0.0628209710])\n",
            "btensor.grad: tensor([-0.0392713547, -0.0075370073,  0.0032066703,  0.0006103516,\n",
            "         0.0098795146, -0.0164291859,  0.0036568642,  0.0520534515,\n",
            "         0.0938315988, -0.0383116603, -0.0226789713, -0.0477177799,\n",
            "         0.0089828372, -0.0556901693, -0.0175231248,  0.0506560206,\n",
            "         0.0478509665,  0.0380324125, -0.0008600950,  0.0244420767])\n",
            "ctensor.grad: tensor([ 1.0196595192, -9.6836481094, -8.7434759140,  0.9230846167,\n",
            "        -0.0164388698,  0.6384549141, -0.1607961804, -0.1971535534,\n",
            "        -7.6001634598,  0.1127743423, -5.8224725723,  0.7806894183,\n",
            "         0.1776725054, -6.9914383888,  0.0975290686, -5.4820876122,\n",
            "         0.5800421238, -0.1837982684])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.9547119141, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6362010241), tensor(0.9859012365), tensor(1.0933532715), tensor(1.1997945309), tensor(1.0989903212), tensor(1.4284805059), tensor(1.0617297888), tensor(1.1728340387), tensor(0.9242597222), tensor(1.2432352304), tensor(1.0943447351), tensor(1.5058408976), tensor(0.9853026867), tensor(1.2657734156), tensor(1.1792043447), tensor(0.9903373718), tensor(1.0853648186), tensor(0.9002894759), tensor(1.1016849279), tensor(1.0112596750)]\n",
            "b:  [tensor(0.6044202447), tensor(1.0970914364), tensor(1.3470978737), tensor(1.0151882172), tensor(1.4798760414), tensor(0.8161955476), tensor(1.2464625835), tensor(1.1734455824), tensor(1.7985738516), tensor(0.8571537137), tensor(1.2724366188), tensor(0.7042554021), tensor(1.3182749748), tensor(1.2516011000), tensor(0.8781729937), tensor(1.0063747168), tensor(1.3113479614), tensor(1.6793370247), tensor(0.9889948368), tensor(1.1887918711)]\n",
            "c:  [tensor(-0.0086625367), tensor(0.0557941906), tensor(0.0513502508), tensor(-0.0057239663), tensor(-0.0019187875), tensor(-0.0073657301), tensor(0.0012320942), tensor(0.0009394264), tensor(0.0484017842), tensor(0.0014376375), tensor(0.0348111168), tensor(0.0033717514), tensor(0.0083034346), tensor(0.0394138992), tensor(0.0064856708), tensor(0.0375696681), tensor(-0.0010468703), tensor(-0.0005727539)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.0964561701, 0.0368554890, 0.0507053733, 0.0251226425, 0.0063325614,\n",
            "        0.0866268873, 0.0622230470, 0.0144886971, 0.0391937494, 0.1277323216,\n",
            "        0.1000806689, 0.0697143376, 0.0716543198, 0.1099700928, 0.0937503576,\n",
            "        0.1080400944, 0.0639895201, 0.0422980785, 0.0715355873, 0.0923920274])\n",
            "btensor.grad: tensor([-0.0398448706,  0.0037434101,  0.0188233852,  0.0134109259,\n",
            "         0.0278196931, -0.0090497732,  0.0189998150,  0.0786505938,\n",
            "         0.1263666749, -0.0353049636, -0.0119316578, -0.0483944267,\n",
            "         0.0276518464, -0.0502244234, -0.0096956789,  0.0742151141,\n",
            "         0.0731771141,  0.0588641167,  0.0111669302,  0.0434751511])\n",
            "ctensor.grad: tensor([  0.9784778357, -10.6674947739,  -9.6837692261,   0.9715511799,\n",
            "         -0.0175693911,   0.6622161865,  -0.1780989468,  -0.2234996259,\n",
            "         -8.3199672699,   0.1263626069,  -6.2730855942,   0.9450950623,\n",
            "          0.3500778079,  -7.6966767311,   0.1090161353,  -5.9687614441,\n",
            "          0.6969572306,  -0.1146366149])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.7353515625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6356124878), tensor(0.9855889678), tensor(1.0929533243), tensor(1.1995649338), tensor(1.0988664627), tensor(1.4279195070), tensor(1.0612633228), tensor(1.1726646423), tensor(0.9239280224), tensor(1.2423992157), tensor(1.0936536789), tensor(1.5053882599), tensor(0.9847822189), tensor(1.2650254965), tensor(1.1785666943), tensor(0.9895946383), tensor(1.0848824978), tensor(0.8999320865), tensor(1.1011734009), tensor(1.0106215477)]\n",
            "b:  [tensor(0.6046218872), tensor(1.0970028639), tensor(1.3469054699), tensor(1.0150429010), tensor(1.4796246290), tensor(0.8161945939), tensor(1.2462718487), tensor(1.1728912592), tensor(1.7977398634), tensor(0.8573104739), tensor(1.2724287510), tensor(0.7044993043), tensor(1.3180229664), tensor(1.2518166304), tensor(0.8781726360), tensor(1.0058611631), tensor(1.3108273745), tensor(1.6789100170), tensor(0.9888656139), tensor(1.1884583235)]\n",
            "c:  [tensor(-0.0091181332), tensor(0.0616460294), tensor(0.0566964410), tensor(-0.0062309722), tensor(-0.0019094539), tensor(-0.0077051939), tensor(0.0013305683), tensor(0.0010659499), tensor(0.0529275984), tensor(0.0013667574), tensor(0.0381510183), tensor(0.0027921947), tensor(0.0079962956), tensor(0.0436306819), tensor(0.0064246040), tensor(0.0407892540), tensor(-0.0014736657), tensor(-0.0005829979)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1177111864, 0.0624504089, 0.0799868703, 0.0459159613, 0.0247783959,\n",
            "        0.1122090816, 0.0932925940, 0.0338897705, 0.0663439035, 0.1672042608,\n",
            "        0.1382119060, 0.0905373394, 0.1040894389, 0.1495902538, 0.1275192499,\n",
            "        0.1485453844, 0.0964732170, 0.0714826584, 0.1023045778, 0.1276156306])\n",
            "btensor.grad: tensor([-4.0330529213e-02,  1.7719268799e-02,  3.8470864296e-02,\n",
            "         2.9068470001e-02,  5.0290763378e-02,  1.9598007202e-04,\n",
            "         3.8145661354e-02,  1.1086630821e-01,  1.6680473089e-01,\n",
            "        -3.1357765198e-02,  1.5760660172e-03, -4.8785388470e-02,\n",
            "         5.0399720669e-02, -4.3112874031e-02,  7.0303678513e-05,\n",
            "         1.0269904137e-01,  1.0411087424e-01,  8.5392951965e-02,\n",
            "         2.5840520859e-02,  6.6699743271e-02])\n",
            "ctensor.grad: tensor([  0.9111928344, -11.7036743164, -10.6923789978,   1.0140117407,\n",
            "         -0.0186673123,   0.6789273024,  -0.1969481558,  -0.2530470788,\n",
            "         -9.0516300201,   0.1417601556,  -6.6798028946,   1.1591134071,\n",
            "          0.6142776012,  -8.4335622787,   0.1221336424,  -6.4391694069,\n",
            "          0.8535907865,   0.0204879493])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.4761962891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6348918676), tensor(0.9851255417), tensor(1.0923829079), tensor(1.1992115974), tensor(1.0986349583), tensor(1.4272017479), tensor(1.0606117249), tensor(1.1723794937), tensor(0.9234375954), tensor(1.2413311005), tensor(1.0927402973), tensor(1.5048067570), tensor(0.9840706587), tensor(1.2640444040), tensor(1.1777307987), tensor(0.9886140823), tensor(1.0842092037), tensor(0.8994007111), tensor(1.1004804373), tensor(1.0097804070)]\n",
            "b:  [tensor(0.6048278809), tensor(1.0968327522), tensor(1.3465949297), tensor(1.0148065090), tensor(1.4792382717), tensor(0.8161393404), tensor(1.2459667921), tensor(1.1721476316), tensor(1.7966631651), tensor(0.8574448228), tensor(1.2723405361), tensor(0.7047455907), tensor(1.3176372051), tensor(1.2519891262), tensor(0.8781150579), tensor(1.0051814318), tensor(1.3101243973), tensor(1.6783216000), tensor(0.9886513352), tensor(1.1879891157)]\n",
            "c:  [tensor(-0.0095263161), tensor(0.0680331662), tensor(0.0625740737), tensor(-0.0067553530), tensor(-0.0018996014), tensor(-0.0080488250), tensor(0.0014391488), tensor(0.0012087132), tensor(0.0578159653), tensor(0.0012871947), tensor(0.0416587070), tensor(0.0020756721), tensor(0.0074992715), tensor(0.0482238829), tensor(0.0063560628), tensor(0.0442232899), tensor(-0.0020039408), tensor(-0.0007056522)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1441168785, 0.0926862657, 0.1140853167, 0.0706707239, 0.0463045985,\n",
            "        0.1435534954, 0.1303223073, 0.0570284128, 0.0980850458, 0.2136177123,\n",
            "        0.1826872230, 0.1163052022, 0.1423117518, 0.1962127686, 0.1671679020,\n",
            "        0.1961171627, 0.1346507072, 0.1062734127, 0.1385831237, 0.1682237387])\n",
            "btensor.grad: tensor([-0.0411964655,  0.0340236425,  0.0621085763,  0.0472779274,\n",
            "         0.0772647262,  0.0110560656,  0.0610138476,  0.1487197876,\n",
            "         0.2153411508, -0.0268751979,  0.0176548958, -0.0492521971,\n",
            "         0.0771538317, -0.0345057249,  0.0115099400,  0.1359518170,\n",
            "         0.1406019628,  0.1176754236,  0.0428591967,  0.0938444138])\n",
            "ctensor.grad: tensor([  0.8163664937, -12.7742748260, -11.7552700043,   1.0487616062,\n",
            "         -0.0197048243,   0.6872622371,  -0.2171609998,  -0.2855265141,\n",
            "         -9.7767353058,   0.1591255963,  -7.0153803825,   1.4330447912,\n",
            "          0.9940480590,  -9.1864051819,   0.1370820552,  -6.8680701256,\n",
            "          1.0605499744,   0.2453086674])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.1689453125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6340153217), tensor(0.9844914079), tensor(1.0916206837), tensor(1.1987171173), tensor(1.0982837677), tensor(1.4262998104), tensor(1.0597472191), tensor(1.1719621420), tensor(0.9227694273), tensor(1.2399996519), tensor(1.0915759802), tensor(1.5040732622), tensor(0.9831425548), tensor(1.2627968788), tensor(1.1766705513), tensor(0.9873635769), tensor(1.0833188295), tensor(0.8986694217), tensor(1.0995818377), tensor(1.0087139606)]\n",
            "b:  [tensor(0.6050441265), tensor(1.0965741873), tensor(1.3461489677), tensor(1.0144704580), tensor(1.4786975384), tensor(0.8160250783), tensor(1.2455316782), tensor(1.1711896658), tensor(1.7953057289), tensor(0.8575583100), tensor(1.2721624374), tensor(0.7049978375), tensor(1.3171008825), tensor(1.2521138191), tensor(0.8779954314), tensor(1.0043154955), tensor(1.3092144728), tensor(1.6775456667), tensor(0.9883441925), tensor(1.1873688698)]\n",
            "c:  [tensor(-0.0098740207), tensor(0.0749624446), tensor(0.0690006763), tensor(-0.0072925230), tensor(-0.0018892760), tensor(-0.0083920034), tensor(0.0015583120), tensor(0.0013688090), tensor(0.0630543604), tensor(0.0011979177), tensor(0.0452860147), tensor(0.0011880809), tensor(0.0067448202), tensor(0.0531920642), tensor(0.0062790485), tensor(0.0478372462), tensor(-0.0026679300), tensor(-0.0009974070)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.1753202677, 0.1268221140, 0.1524453163, 0.0988883972, 0.0702263415,\n",
            "        0.1803839207, 0.1729048491, 0.0834681988, 0.1336365938, 0.2663015425,\n",
            "        0.2328668833, 0.1467011273, 0.1856251955, 0.2494981289, 0.2120412588,\n",
            "        0.2501043081, 0.1780746579, 0.1462568045, 0.1797119379, 0.2132823467])\n",
            "btensor.grad: tensor([-0.0432548523,  0.0517205000,  0.0891969204,  0.0672082305,\n",
            "         0.1081474125,  0.0228582621,  0.0870119333,  0.1915948391,\n",
            "         0.2714820504, -0.0227009654,  0.0356091261, -0.0504451692,\n",
            "         0.1072736084, -0.0249438286,  0.0239230394,  0.1731985211,\n",
            "         0.1819855571,  0.1551804543,  0.0614266396,  0.1240564585])\n",
            "ctensor.grad: tensor([  0.6954086423, -13.8585529327, -12.8532114029,   1.0743396282,\n",
            "         -0.0206508227,   0.6863561273,  -0.2383264154,  -0.3201918304,\n",
            "        -10.4767847061,   0.1785538048,  -7.2546148300,   1.7751824856,\n",
            "          1.5089024305,  -9.9363613129,   0.1540282369,  -7.2279148102,\n",
            "          1.3279783726,   0.5835096836])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1070.8103027344, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6329630613), tensor(0.9836746454), tensor(1.0906518698), tensor(1.1980699301), tensor(1.0978075266), tensor(1.4251904488), tensor(1.0586477518), tensor(1.1714011431), tensor(0.9219123721), tensor(1.2383805513), tensor(1.0901393890), tensor(1.5031687021), tensor(0.9819799066), tensor(1.2612547874), tensor(1.1753667593), tensor(0.9858186245), tensor(1.0821909904), tensor(0.8977181315), tensor(1.0984604359), tensor(1.0074086189)]\n",
            "b:  [tensor(0.6052826047), tensor(1.0962282419), tensor(1.3455560207), tensor(1.0140335560), tensor(1.4779893160), tensor(0.8158528805), tensor(1.2449570894), tensor(1.1699991226), tensor(1.7936363220), tensor(0.8576592207), tensor(1.2718917131), tensor(0.7052646279), tensor(1.3164037466), tensor(1.2521910667), tensor(0.8778150082), tensor(1.0032509565), tensor(1.3080804348), tensor(1.6765624285), tensor(0.9879432917), tensor(1.1865903139)]\n",
            "c:  [tensor(-0.0101507185), tensor(0.0824306831), tensor(0.0759826973), tensor(-0.0078374166), tensor(-0.0018785392), tensor(-0.0087300381), tensor(0.0016881995), tensor(0.0015466767), tensor(0.0686228275), tensor(0.0010978961), tensor(0.0489759408), tensor(9.3243550509e-05), tensor(0.0056606075), tensor(0.0585241541), tensor(0.0061925133), tensor(0.0515840054), tensor(-0.0034997247), tensor(-0.0015243541)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.2104636431, 0.1633517146, 0.1937686801, 0.1294457912, 0.0952369869,\n",
            "        0.2218812704, 0.2198937237, 0.1122096777, 0.1714054346, 0.3238152862,\n",
            "        0.2873291969, 0.1809031665, 0.2325319052, 0.3084107637, 0.2607517242,\n",
            "        0.3089927435, 0.2255590558, 0.1902573705, 0.2242763638, 0.2610700130])\n",
            "btensor.grad: tensor([-0.0477014780,  0.0691817999,  0.1185938120,  0.0873804688,\n",
            "         0.1416463852,  0.0344430208,  0.1149286628,  0.2381186485,\n",
            "         0.3338875175, -0.0201788545,  0.0541384220, -0.0533635467,\n",
            "         0.1394225955, -0.0154496431,  0.0360885561,  0.2129191160,\n",
            "         0.2268157303,  0.1966565847,  0.0801767111,  0.1557042599])\n",
            "ctensor.grad: tensor([  0.5533960462, -14.9364805222, -13.9640417099,   1.0897877216,\n",
            "         -0.0214734375,   0.6760685444,  -0.2597749829,  -0.3557354212,\n",
            "        -11.1369323730,   0.2000432760,  -7.3798542023,   2.1896746159,\n",
            "          2.1684255600, -10.6641788483,   0.1730707884,  -7.4935150146,\n",
            "          1.6635894775,   1.0538940430])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1070.3986816406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6317220926), tensor(0.9826745391), tensor(1.0894716978), tensor(1.1972668171), tensor(1.0972104073), tensor(1.4238569736), tensor(1.0573008060), tensor(1.1706926823), tensor(0.9208674431), tensor(1.2364606857), tensor(1.0884196758), tensor(1.5020809174), tensor(0.9805761576), tensor(1.2593986988), tensor(1.1738107204), tensor(0.9839662910), tensor(1.0808151960), tensor(0.8965364099), tensor(1.0971099138), tensor(1.0058625937)]\n",
            "b:  [tensor(0.6055628657), tensor(1.0958076715), tensor(1.3448132277), tensor(1.0135051012), tensor(1.4771106243), tensor(0.8156320453), tensor(1.2442424297), tensor(1.1685682535), tensor(1.7916344404), tensor(0.8577649593), tensor(1.2715351582), tensor(0.7055611610), tensor(1.3155460358), tensor(1.2522286177), tensor(0.8775835633), tensor(1.0019863844), tensor(1.3067159653), tensor(1.6753619909), tensor(0.9874571562), tensor(1.1856580973)]\n",
            "c:  [tensor(-0.0103502702), tensor(0.0904271454), tensor(0.0835159272), tensor(-0.0083848462), tensor(-0.0018674672), tensor(-0.0090586031), tensor(0.0018285074), tensor(0.0017418462), tensor(0.0744977817), tensor(0.0009861602), tensor(0.0526690707), tensor(-0.0012441161), tensor(0.0041770637), tensor(0.0642011762), tensor(0.0060954108), tensor(0.0554076955), tensor(-0.0045349970), tensor(-0.0023567560)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.2481977940, 0.2000249624, 0.2360408902, 0.1606258154, 0.1194225699,\n",
            "        0.2667007446, 0.2693936825, 0.1416932344, 0.2089884281, 0.3839809597,\n",
            "        0.3439317942, 0.2175657153, 0.2807509303, 0.3712171316, 0.3111968040,\n",
            "        0.3704633713, 0.2751543522, 0.2363479137, 0.2700961232, 0.3092061281])\n",
            "btensor.grad: tensor([-0.0560556650,  0.0841061473,  0.1485503912,  0.1056920290,\n",
            "         0.1757303476,  0.0441640615,  0.1429348290,  0.2861791849,\n",
            "         0.4003784060, -0.0211461782,  0.0713194609, -0.0593060926,\n",
            "         0.1715481281, -0.0075199604,  0.0462891459,  0.2529075742,\n",
            "         0.2729043961,  0.2400982380,  0.0972230434,  0.1864460707])\n",
            "ctensor.grad: tensor([  0.3991034627, -15.9929180145, -15.0664653778,   1.0948599577,\n",
            "         -0.0221439954,   0.6571301818,  -0.2806157768,  -0.3903388977,\n",
            "        -11.7499008179,   0.2234717309,  -7.3862571716,   2.6747190952,\n",
            "          2.9670875072, -11.3540420532,   0.1942048520,  -7.6473817825,\n",
            "          2.0705447197,   1.6648037434])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1069.9299316406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6302880049), tensor(0.9815042019), tensor(1.0880881548), tensor(1.1963155270), tensor(1.0965081453), tensor(1.4222912788), tensor(1.0557059050), tensor(1.1698429585), tensor(0.9196502566), tensor(1.2342396975), tensor(1.0864193439), tensor(1.5008059740), tensor(0.9789388180), tensor(1.2572200298), tensor(1.1720066071), tensor(0.9818078876), tensor(1.0791933537), tensor(0.8951261640), tensor(1.0955375433), tensor(1.0040880442)]\n",
            "b:  [tensor(0.6059128642), tensor(1.0953390598), tensor(1.3439288139), tensor(1.0129070282), tensor(1.4760715961), tensor(0.8153815866), tensor(1.2433986664), tensor(1.1669025421), tensor(1.7892936468), tensor(0.8579037786), tensor(1.2711112499), tensor(0.7059096694), tensor(1.3145405054), tensor(1.2522436380), tensor(0.8773211837), tensor(1.0005338192), tensor(1.3051283360), tensor(1.6739473343), tensor(0.9869054556), tensor(1.1845908165)]\n",
            "c:  [tensor(-0.0104722613), tensor(0.0989375114), tensor(0.0915879682), tensor(-0.0089299064), tensor(-0.0018561464), tensor(-0.0093741482), tensor(0.0019784358), tensor(0.0019527961), tensor(0.0806568787), tensor(0.0008618645), tensor(0.0563111231), tensor(-0.0028550969), tensor(0.0022362587), tensor(0.0701997280), tensor(0.0059867604), tensor(0.0592498630), tensor(-0.0058079949), tensor(-0.0035614811)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.2868261337, 0.2340675294, 0.2767121196, 0.1902500391, 0.1404560059,\n",
            "        0.3131276369, 0.3189868331, 0.1699420214, 0.2434356213, 0.4441930950,\n",
            "        0.4000738263, 0.2549933195, 0.3274671435, 0.4357297421, 0.3608332276,\n",
            "        0.4316854477, 0.3243801594, 0.2820438743, 0.3144794106, 0.3549062610])\n",
            "btensor.grad: tensor([-0.0699998140,  0.0937275290,  0.1768724322,  0.1196129322,\n",
            "         0.2078136355,  0.0500974655,  0.1687560081,  0.3331537247,\n",
            "         0.4681512713, -0.0277668834,  0.0847867727, -0.0696984828,\n",
            "         0.2010956109, -0.0029962063,  0.0524733812,  0.2905110717,\n",
            "         0.3175347447,  0.2829297781,  0.1103378534,  0.2134625912])\n",
            "ctensor.grad: tensor([  0.2439824194, -17.0207309723, -16.1440792084,   1.0901206732,\n",
            "         -0.0226416048,   0.6310896277,  -0.2998566329,  -0.4218997657,\n",
            "        -12.3181905746,   0.2485915422,  -7.2841043472,   3.2219617367,\n",
            "          3.8816101551, -11.9971017838,   0.2173003852,  -7.6843323708,\n",
            "          2.5459949970,   2.4094500542])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1069.4017333984, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6286648512), tensor(0.9801910520), tensor(1.0865225792), tensor(1.1952352524), tensor(1.0957283974), tensor(1.4204941988), tensor(1.0538754463), tensor(1.1688684225), tensor(0.9182918668), tensor(1.2317303419), tensor(1.0841535330), tensor(1.4993489981), tensor(0.9770898819), tensor(1.2547214031), tensor(1.1699711084), tensor(0.9793588519), tensor(1.0773402452), tensor(0.8935025930), tensor(1.0937643051), tensor(1.0021107197)]\n",
            "b:  [tensor(0.6063683629), tensor(1.0948630571), tensor(1.3429223299), tensor(1.0122741461), tensor(1.4748959541), tensor(0.8151298165), tensor(1.2424484491), tensor(1.1650205851), tensor(1.7866225243), tensor(0.8581147790), tensor(1.2706509829), tensor(0.7063390017), tensor(1.3134136200), tensor(1.2522623539), tensor(0.8770581484), tensor(0.9989182949), tensor(1.3033387661), tensor(1.6723353863), tensor(0.9863188267), tensor(1.1834214926)]\n",
            "c:  [tensor(-0.0105223013), tensor(0.1079480723), tensor(0.1001819968), tensor(-0.0094683357), tensor(-0.0018446677), tensor(-0.0096741691), tensor(0.0021367329), tensor(0.0021770033), tensor(0.0870836601), tensor(0.0007243400), tensor(0.0598597936), tensor(-0.0047638132), tensor(-0.0001999049), tensor(0.0764964297), tensor(0.0058657094), tensor(0.0630564019), tensor(-0.0073484760), tensor(-0.0051936703)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.3246310949, 0.2626351118, 0.3131198287, 0.2160514593, 0.1559494287,\n",
            "        0.3594138622, 0.3661034107, 0.1949132681, 0.2716723680, 0.5018597245,\n",
            "        0.4531610608, 0.2914060056, 0.3697900772, 0.4997351170, 0.4070965648,\n",
            "        0.4898039103, 0.3706312776, 0.3247197270, 0.3546462655, 0.3954606056])\n",
            "btensor.grad: tensor([-0.0911033154,  0.0951913595,  0.2012922764,  0.1265730858,\n",
            "         0.2351384461,  0.0503572226,  0.1900489628,  0.3763811588,\n",
            "         0.5342342854, -0.0422030687,  0.0920454264, -0.0858644694,\n",
            "         0.2253716588, -0.0037512779,  0.0526020527,  0.3231075406,\n",
            "         0.3579088151,  0.3223912716,  0.1173231602,  0.2338703871])\n",
            "ctensor.grad: tensor([  0.1000808328, -18.0211219788, -17.1880550385,   1.0768585205,\n",
            "         -0.0229574181,   0.6000423431,  -0.3165944517,  -0.4484141767,\n",
            "        -12.8535575867,   0.2750489116,  -7.0973401070,   3.8174321651,\n",
            "          4.8723268509, -12.5933971405,   0.2421017736,  -7.6130714417,\n",
            "          3.0809617043,   3.2643780708])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1068.8128662109, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6268638372), tensor(0.9787744880), tensor(1.0848077536), tensor(1.1940548420), tensor(1.0949089527), tensor(1.4184734821), tensor(1.0518326759), tensor(1.1677941084), tensor(0.9168365002), tensor(1.2289558649), tensor(1.0816479921), tensor(1.4977223873), tensor(0.9750635624), tensor(1.2519141436), tensor(1.1677316427), tensor(0.9766463637), tensor(1.0752818584), tensor(0.8916919827), tensor(1.0918232203), tensor(0.9999671578)]\n",
            "b:  [tensor(0.6069708467), tensor(1.0944328308), tensor(1.3418231010), tensor(1.0116521120), tensor(1.4736198187), tensor(0.8149125576), tensor(1.2414243221), tensor(1.1629524231), tensor(1.7836424112), tensor(0.8584460020), tensor(1.2701963186), tensor(0.7068826556), tensor(1.3122035265), tensor(1.2523190975), tensor(0.8768330812), tensor(0.9971754551), tensor(1.3013806343), tensor(1.6705553532), tensor(0.9857368469), tensor(1.1821953058)]\n",
            "c:  [tensor(-0.0105110891), tensor(0.1174486801), tensor(0.1092805490), tensor(-0.0099967560), tensor(-0.0018331193), tensor(-0.0099572707), tensor(0.0023018364), tensor(0.0024111958), tensor(0.0937705487), tensor(0.0005731306), tensor(0.0632888526), tensor(-0.0069856988), tensor(-0.0031440398), tensor(0.0830720365), tensor(0.0057315817), tensor(0.0667833909), tensor(-0.0091793546), tensor(-0.0072895493)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.3601945639, 0.2833179832, 0.3429643512, 0.2360861301, 0.1638898551,\n",
            "        0.4041333199, 0.4085485339, 0.2148582935, 0.2910698652, 0.5549040437,\n",
            "        0.5011036396, 0.3253306448, 0.4052659273, 0.5614553690, 0.4478940964,\n",
            "        0.5424935818, 0.4116712213, 0.3621267080, 0.3882256150, 0.4287098646])\n",
            "btensor.grad: tensor([-0.1204980612,  0.0860528350,  0.2198576927,  0.1244082451,\n",
            "         0.2552334666,  0.0434527397,  0.2048188448,  0.4136320353,\n",
            "         0.5960183740, -0.0662410855,  0.0909405947, -0.1087322533,\n",
            "         0.2420262694, -0.0113558769,  0.0450097769,  0.3485636711,\n",
            "         0.3916334510,  0.3560184240,  0.1163989305,  0.2452466488])\n",
            "ctensor.grad: tensor([ -0.0224243775, -19.0012187958, -18.1971015930,   1.0568411350,\n",
            "         -0.0230968911,   0.5662024617,  -0.3302069008,  -0.4683849514,\n",
            "        -13.3737688065,   0.3024189770,  -6.8581147194,   4.4437704086,\n",
            "          5.8882694244, -13.1512107849,   0.2682549059,  -7.4539709091,\n",
            "          3.6617579460,   4.1917576790])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1068.1558837891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6249002218), tensor(0.9773015380), tensor(1.0829840899), tensor(1.1928093433), tensor(1.0940939188), tensor(1.4162409306), tensor(1.0496079922), tensor(1.1666507721), tensor(0.9153370261), tensor(1.2259451151), tensor(1.0789344311), tensor(1.4959429502), tensor(0.9729019403), tensor(1.2488145828), tensor(1.1653218269), tensor(0.9737043381), tensor(1.0730513334), tensor(0.8897279501), tensor(1.0897547007), tensor(0.9977000952)]\n",
            "b:  [tensor(0.6077640653), tensor(1.0941092968), tensor(1.3406665325), tensor(1.0110933781), tensor(1.4722880125), tensor(0.8147693872), tensor(1.2403653860), tensor(1.1607345343), tensor(1.7803838253), tensor(0.8589506745), tensor(1.2697962523), tensor(0.7075757384), tensor(1.3109561205), tensor(1.2524527311), tensor(0.8766893148), tensor(0.9953471422), tensor(1.2992948294), tensor(1.6686450243), tensor(0.9852041006), tensor(1.1809650660)]\n",
            "c:  [tensor(-0.0104524316), tensor(0.1274333596), tensor(0.1188679785), tensor(-0.0105127543), tensor(-0.0018215798), tensor(-0.0102230096), tensor(0.0024720759), tensor(0.0026517482), tensor(0.1007194221), tensor(0.0004080060), tensor(0.0665886551), tensor(-0.0095272213), tensor(-0.0065812743), tensor(0.0899139792), tensor(0.0055839079), tensor(0.0704001710), tensor(-0.0113155609), tensor(-0.0098620532)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.3927147388, 0.2945915461, 0.3647243977, 0.2491041422, 0.1630016267,\n",
            "        0.4465160370, 0.4449317455, 0.2286751270, 0.2998966575, 0.6021617651,\n",
            "        0.5427205563, 0.3558879197, 0.4323228002, 0.6199169159, 0.4819725752,\n",
            "        0.5883995295, 0.4460959435, 0.3928112388, 0.4136928916, 0.4534089565])\n",
            "btensor.grad: tensor([-0.1586443782,  0.0647178888,  0.2313038111,  0.1117579341,\n",
            "         0.2663578391,  0.0286291838,  0.2117954195,  0.4435687065,\n",
            "         0.6517063379, -0.1009292006,  0.0800231695, -0.1386126876,\n",
            "         0.2494743168, -0.0267292261,  0.0287507623,  0.3656614423,\n",
            "         0.4171697497,  0.3820564747,  0.1065548658,  0.2460507154])\n",
            "ctensor.grad: tensor([ -0.1173152030, -19.9693717957, -19.1748523712,   1.0319963694,\n",
            "         -0.0230790861,   0.5314787030,  -0.3404787779,  -0.4811051488,\n",
            "        -13.8977413177,   0.3302490711,  -6.5996088982,   5.0830445290,\n",
            "          6.8744688034, -13.6838884354,   0.2953474522,  -7.2335586548,\n",
            "          4.2724123001,   5.1450071335])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1067.4317626953, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6227896214), tensor(0.9758213758), tensor(1.0810947418), tensor(1.1915357113), tensor(1.0933293104), tensor(1.4138081074), tensor(1.0472335815), tensor(1.1654703617), tensor(0.9138491154), tensor(1.2227277756), tensor(1.0760451555), tensor(1.4940282106), tensor(0.9706496596), tensor(1.2454390526), tensor(1.1627763510), tensor(0.9705678821), tensor(1.0706837177), tensor(0.8876461983), tensor(1.0876019001), tensor(0.9953532219)]\n",
            "b:  [tensor(0.6087902784), tensor(1.0939558744), tensor(1.3394902945), tensor(1.0106520653), tensor(1.4709492922), tensor(0.8147394657), tensor(1.2393121719), tensor(1.1584049463), tensor(1.7768810987), tensor(0.8596827388), tensor(1.2695022821), tensor(0.7084514499), tensor(1.3097205162), tensor(1.2527025938), tensor(0.8766705394), tensor(0.9934757352), tensor(1.2971247435), tensor(1.6666464806), tensor(0.9847655892), tensor(1.1797858477)]\n",
            "c:  [tensor(-0.0103607448), tensor(0.1378987283), tensor(0.1289309412), tensor(-0.0110148061), tensor(-0.0018101133), tensor(-0.0104716094), tensor(0.0026458802), tensor(0.0028951073), tensor(0.1079397947), tensor(0.0002289573), tensor(0.0697632879), tensor(-0.0123868026), tensor(-0.0104708821), tensor(0.0970168114), tensor(0.0054224310), tensor(0.0738892928), tensor(-0.0137641709), tensor(-0.0128998002)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.4221297503, 0.2960280478, 0.3778690696, 0.2547363043, 0.1529258639,\n",
            "        0.4865601063, 0.4748848081, 0.2360851765, 0.2975821495, 0.6434670687,\n",
            "        0.5778666735, 0.3829436898, 0.4504610300, 0.6751041412, 0.5090971589,\n",
            "        0.6272885799, 0.4735298753, 0.4163553715, 0.4305552244, 0.4693772793])\n",
            "btensor.grad: tensor([-0.2052403688,  0.0306947231,  0.2352576852,  0.0882666707,\n",
            "         0.2677533627,  0.0059795380,  0.2106355727,  0.4659197330,\n",
            "         0.7005500197, -0.1464084387,  0.0588043928, -0.1751405895,\n",
            "         0.2471218705, -0.0499656200,  0.0037507415,  0.3742812872,\n",
            "         0.4340237975,  0.3997088671,  0.0876983404,  0.2358514071])\n",
            "ctensor.grad: tensor([ -0.1833740771, -20.9307365417, -20.1259326935,   1.0041035414,\n",
            "         -0.0229328610,   0.4971995354,  -0.3476087749,  -0.4867180884,\n",
            "        -14.4407377243,   0.3580974936,  -6.3492593765,   5.7191629410,\n",
            "          7.7792148590, -14.2056665421,   0.3229539096,  -6.9782419205,\n",
            "          4.8972201347,   6.0754942894])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1066.6365966797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6205441952), tensor(0.9743800759), tensor(1.0791804790), tensor(1.1902681589), tensor(1.0926582813), tensor(1.4111832380), tensor(1.0447382927), tensor(1.1642823219), tensor(0.9124256968), tensor(1.2193300724), tensor(1.0730085373), tensor(1.4919928312), tensor(0.9683485627), tensor(1.2417998314), tensor(1.1601265669), tensor(0.9672681689), tensor(1.0682106018), tensor(0.8854794502), tensor(1.0854052305), tensor(0.9929661751)]\n",
            "b:  [tensor(0.6100866199), tensor(1.0940327644), tensor(1.3383290768), tensor(1.0103791952), tensor(1.4696508646), tensor(0.8148573041), tensor(1.2383027077), tensor(1.1559978724), tensor(1.7731670141), tensor(0.8606922030), tensor(1.2693634033), tensor(0.7095381021), tensor(1.3085435629), tensor(1.2531042099), tensor(0.8768166304), tensor(0.9915992022), tensor(1.2949111462), tensor(1.6646007299), tensor(0.9844624400), tensor(1.1787093878)]\n",
            "c:  [tensor(-0.0102486415), tensor(0.1488408893), tensor(0.1394570768), tensor(-0.0115020890), tensor(-0.0017987677), tensor(-0.0107036186), tensor(0.0028219349), tensor(0.0031381319), tensor(0.1154452115), tensor(3.6173354601e-05), tensor(0.0728253052), tensor(-0.0155566158), tensor(-0.0147508783), tensor(0.1043807566), tensor(0.0052470965), tensor(0.0772438645), tensor(-0.0165254809), tensor(-0.0163690560)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.4490747452, 0.2882550657, 0.3828554153, 0.2535167933, 0.1342173964,\n",
            "        0.5249800682, 0.4990530610, 0.2376053333, 0.2846822143, 0.6795521975,\n",
            "        0.6073260903, 0.4070745409, 0.4602159262, 0.7278348207, 0.5299509764,\n",
            "        0.6599463224, 0.4946309924, 0.4333456755, 0.4393306971, 0.4774075747])\n",
            "btensor.grad: tensor([-0.2592695951, -0.0153784752,  0.2322346568,  0.0545825958,\n",
            "         0.2596943080, -0.0235629082,  0.2018903792,  0.4814087152,\n",
            "         0.7428274155, -0.2018941045,  0.0277744532, -0.2173330188,\n",
            "         0.2353836596, -0.0803290606, -0.0292149484,  0.3753029108,\n",
            "         0.4427111149,  0.4091554880,  0.0606243610,  0.2153030634])\n",
            "ctensor.grad: tensor([ -0.2242072225, -21.8843173981, -21.0522842407,   0.9745658040,\n",
            "         -0.0226911716,   0.4640192986,  -0.3521092236,  -0.4860491455,\n",
            "        -15.0108261108,   0.3855677843,  -6.1240358353,   6.3396258354,\n",
            "          8.5599918365, -14.7278900146,   0.3506693542,  -6.7091412544,\n",
            "          5.5226182938,   6.9385099411])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1065.7656250000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6181708574), tensor(0.9730164409), tensor(1.0772758722), tensor(1.1890347004), tensor(1.0921175480), tensor(1.4083683491), tensor(1.0421441793), tensor(1.1631104946), tensor(0.9111122489), tensor(1.2157713175), tensor(1.0698459148), tensor(1.4898459911), tensor(0.9660341740), tensor(1.2379025221), tensor(1.1573971510), tensor(0.9638289213), tensor(1.0656564236), tensor(0.8832539320), tensor(1.0831987858), tensor(0.9905713201)]\n",
            "b:  [tensor(0.6116827726), tensor(1.0943924189), tensor(1.3372119665), tensor(1.0103185177), tensor(1.4684344530), tensor(0.8151496053), tensor(1.2373685837), tensor(1.1535404921), tensor(1.7692691088), tensor(0.8620214462), tensor(1.2694221735), tensor(0.7108570337), tensor(1.3074660301), tensor(1.2536863089), tensor(0.8771602511), tensor(0.9897475839), tensor(1.2926886082), tensor(1.6625438929), tensor(0.9843283892), tensor(1.1777797937)]\n",
            "c:  [tensor(-0.0101251742), tensor(0.1602520645), tensor(0.1504326463), tensor(-0.0119742397), tensor(-0.0017875752), tensor(-0.0109196212), tensor(0.0029992550), tensor(0.0033782830), tensor(0.1232488900), tensor(-0.0001699959), tensor(0.0757895261), tensor(-0.0190247446), tensor(-0.0193444323), tensor(0.1120089963), tensor(0.0050580297), tensor(0.0804633349), tensor(-0.0195945352), tensor(-0.0202176794)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.4746681452, 0.2727253139, 0.3809126019, 0.2466816902, 0.1081442237,\n",
            "        0.5629820824, 0.5188145638, 0.2343697548, 0.2626912594, 0.7117415667,\n",
            "        0.6325291991, 0.4293566942, 0.4628761411, 0.7794722319, 0.5458801389,\n",
            "        0.6878553629, 0.5108252168, 0.4451012015, 0.4412897229, 0.4789677858])\n",
            "btensor.grad: tensor([-0.3192334175, -0.0719203949,  0.2234256864,  0.0121275187,\n",
            "         0.2432912290, -0.0584633350,  0.1868198812,  0.4914674759,\n",
            "         0.7795855999, -0.2658498287, -0.0117487907, -0.2637832165,\n",
            "         0.2154976428, -0.1164206266, -0.0687197298,  0.3703239560,\n",
            "         0.4445102513,  0.4113578796,  0.0268107653,  0.1859079599])\n",
            "ctensor.grad: tensor([-2.4693453312e-01, -2.2822364807e+01, -2.1951126099e+01,\n",
            "         9.4430166483e-01, -2.2384982556e-02,  4.3200600147e-01,\n",
            "        -3.5464018583e-01, -4.8030212522e-01, -1.5607355118e+01,\n",
            "         4.1233849525e-01, -5.9284410477e+00,  6.9362554550e+00,\n",
            "         9.1871080399e+00, -1.5256480217e+01,  3.7813404202e-01,\n",
            "        -6.4389371872e+00,  6.1381101608e+00,  7.6972446442e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1064.8200683594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6156694889), tensor(0.9717596769), tensor(1.0754072666), tensor(1.1878551245), tensor(1.0917354822), tensor(1.4053585529), tensor(1.0394643545), tensor(1.1619712114), tensor(0.9099438190), tensor(1.2120635509), tensor(1.0665700436), tensor(1.4875905514), tensor(0.9637334943), tensor(1.2337447405), tensor(1.1546043158), tensor(0.9602650404), tensor(1.0630365610), tensor(0.8809872866), tensor(1.0810081959), tensor(0.9881920218)]\n",
            "b:  [tensor(0.6135995388), tensor(1.0950758457), tensor(1.3361598253), tensor(1.0105044842), tensor(1.4673333168), tensor(0.8156340718), tensor(1.2365331650), tensor(1.1510511637), tensor(1.7652076483), tensor(0.8637026548), tensor(1.2697116137), tensor(0.7124215961), tensor(1.3065199852), tensor(1.2544684410), tensor(0.8777251244), tensor(0.9879411459), tensor(1.2904829979), tensor(1.6605051756), tensor(0.9843878746), tensor(1.1770313978)]\n",
            "c:  [tensor(-0.0099949194), tensor(0.1721178144), tensor(0.1618400365), tensor(-0.0124311056), tensor(-0.0017765560), tensor(-0.0111200288), tensor(0.0031771746), tensor(0.0036136569), tensor(0.1313595772), tensor(-0.0003890876), tensor(0.0786671266), tensor(-0.0227773283), tensor(-0.0241669677), tensor(0.1199045405), tensor(0.0048555057), tensor(0.0835490599), tensor(-0.0229627267), tensor(-0.0243800394)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.5002735853, 0.2513484061, 0.3737276196, 0.2359085083, 0.0764038265,\n",
            "        0.6019681692, 0.5359704494, 0.2278525829, 0.2336905599, 0.7415565848,\n",
            "        0.6551768184, 0.4510861039, 0.4601318836, 0.8315618038, 0.5585579276,\n",
            "        0.7127712965, 0.5239681005, 0.4533325434, 0.4381129742, 0.4758650064])\n",
            "btensor.grad: tensor([-0.3833560348, -0.1366845369,  0.2104378939, -0.0371834040,\n",
            "         0.2202191353, -0.0968948603,  0.1670738757,  0.4978650808,\n",
            "         0.8123036623, -0.3362447619, -0.0578871965, -0.3129095435,\n",
            "         0.1892125607, -0.1564198732, -0.1129755229,  0.3612895012,\n",
            "         0.4411262572,  0.4077409506, -0.0118963718,  0.1496820450])\n",
            "ctensor.grad: tensor([-2.6050940156e-01, -2.3731494904e+01, -2.2814785004e+01,\n",
            "         9.1373080015e-01, -2.2038439289e-02,  4.0081575513e-01,\n",
            "        -3.5583946109e-01, -4.7074767947e-01, -1.6221370697e+01,\n",
            "         4.3818333745e-01, -5.7552070618e+00,  7.5051665306e+00,\n",
            "         9.6450700760e+00, -1.5791083336e+01,  4.0504756570e-01,\n",
            "        -6.1714477539e+00,  6.7363843918e+00,  8.3247194290e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1063.8004150391, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6130334139), tensor(0.9706290960), tensor(1.0735915899), tensor(1.1867401600), tensor(1.0915313959), tensor(1.4021424055), tensor(1.0367027521), tensor(1.1608734131), tensor(0.9089440107), tensor(1.2082116604), tensor(1.0631856918), tensor(1.4852230549), tensor(0.9614650607), tensor(1.2293175459), tensor(1.1517561674), tensor(0.9565834403), tensor(1.0603566170), tensor(0.8786886930), tensor(1.0788505077), tensor(0.9858426452)]\n",
            "b:  [tensor(0.6158488989), tensor(1.0961115360), tensor(1.3351850510), tensor(1.0109609365), tensor(1.4663715363), tensor(0.8163190484), tensor(1.2358113527), tensor(1.1485395432), tensor(1.7609951496), tensor(0.8657569289), tensor(1.2702543736), tensor(0.7142375112), tensor(1.3057277203), tensor(1.2554603815), tensor(0.8785257339), tensor(0.9861904979), tensor(1.2883114815), tensor(1.6585057974), tensor(0.9846557975), tensor(1.1764873266)]\n",
            "c:  [tensor(-0.0098580141), tensor(0.1844153851), tensor(0.1736559272), tensor(-0.0128725311), tensor(-0.0017657230), tensor(-0.0113049820), tensor(0.0033552703), tensor(0.0038428910), tensor(0.1397784054), tensor(-0.0006205791), tensor(0.0814611837), tensor(-0.0268002748), tensor(-0.0291326679), tensor(0.1280674338), tensor(0.0046399166), tensor(0.0865004957), tensor(-0.0266191065), tensor(-0.0287819728)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.5272145271, 0.2261112034, 0.3631265461, 0.2229998112, 0.0408219695,\n",
            "        0.6432348490, 0.5523273945, 0.2195550203, 0.1999558210, 0.7703762650,\n",
            "        0.6768695712, 0.4735003114, 0.4536894560, 0.8854494095, 0.5696188807,\n",
            "        0.7363168001, 0.5359863043, 0.4597175121, 0.4315365553, 0.4698780775])\n",
            "btensor.grad: tensor([-0.4498730302, -0.2071448565,  0.1949450970, -0.0912894011,\n",
            "         0.1923601329, -0.1369918585,  0.1443635821,  0.5023143291,\n",
            "         0.8425003886, -0.4108590186, -0.1085449457, -0.3631844819,\n",
            "         0.1584485471, -0.1983926296, -0.1601199210,  0.3501303792,\n",
            "         0.4343085587,  0.3998700380, -0.0535799265,  0.1088037491])\n",
            "ctensor.grad: tensor([-2.7381128073e-01, -2.4595151901e+01, -2.3631772995e+01,\n",
            "         8.8285070658e-01, -2.1666089073e-02,  3.6990603805e-01,\n",
            "        -3.5619145632e-01, -4.5846837759e-01, -1.6837657928e+01,\n",
            "         4.6298298240e-01, -5.5881185532e+00,  8.0458936691e+00,\n",
            "         9.9314012527e+00, -1.6325794220e+01,  4.3117779493e-01,\n",
            "        -5.9028649330e+00,  7.3127593994e+00,  8.8038673401e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1062.7056884766, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6102507114), tensor(0.9696351886), tensor(1.0718377829), tensor(1.1856920719), tensor(1.0915158987), tensor(1.3987038136), tensor(1.0338557959), tensor(1.1598196030), tensor(0.9081258178), tensor(1.2042158842), tensor(1.0596916676), tensor(1.4827353954), tensor(0.9592402577), tensor(1.2246073484), tensor(1.1488542557), tensor(0.9527850747), tensor(1.0576139688), tensor(0.8763606548), tensor(1.0767352581), tensor(0.9835301638)]\n",
            "b:  [tensor(0.6184349060), tensor(1.0975155830), tensor(1.3342930079), tensor(1.0117024183), tensor(1.4655640125), tensor(0.8172047734), tensor(1.2352104187), tensor(1.1460087299), tensor(1.7566380501), tensor(0.8681946397), tensor(1.2710627317), tensor(0.7163040042), tensor(1.3051027060), tensor(1.2566632032), tensor(0.8795680404), tensor(0.9844983220), tensor(1.2861837149), tensor(1.6565601826), tensor(0.9851384163), tensor(1.1761609316)]\n",
            "c:  [tensor(-0.0097109098), tensor(0.1971134096), tensor(0.1858502924), tensor(-0.0132982116), tensor(-0.0017550868), tensor(-0.0114743514), tensor(0.0035332453), tensor(0.0040650014), tensor(0.1484973133), tensor(-0.0008639379), tensor(0.0841643140), tensor(-0.0310802925), tensor(-0.0341594145), tensor(0.1364928931), tensor(0.0044117328), tensor(0.0893127099), tensor(-0.0305512697), tensor(-0.0333451666)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([0.5565469265, 0.1987766922, 0.3507638872, 0.2096197605, 0.0030977726,\n",
            "        0.6877155304, 0.5693808794, 0.2107517719, 0.1636402011, 0.7991614938,\n",
            "        0.6988142133, 0.4975261986, 0.4449650049, 0.9420324564, 0.5803919435,\n",
            "        0.7596726418, 0.5485344529, 0.4656030536, 0.4230560660, 0.4624952078])\n",
            "btensor.grad: tensor([-0.5172007680, -0.2808037996,  0.1784158349, -0.1482889652,\n",
            "         0.1615041643, -0.1771432161,  0.1201903224,  0.5061564445,\n",
            "         0.8714179397, -0.4875460863, -0.1616803408, -0.4133038819,\n",
            "         0.1250014007, -0.2405526638, -0.2084645629,  0.3384376168,\n",
            "         0.4255612493,  0.3891252279, -0.0965199471,  0.0652776957])\n",
            "ctensor.grad: tensor([-2.9420933127e-01, -2.5396041870e+01, -2.4388719559e+01,\n",
            "         8.5136014223e-01, -2.1272374317e-02,  3.3873870969e-01,\n",
            "        -3.5594996810e-01, -4.4422116876e-01, -1.7437818527e+01,\n",
            "         4.8671755195e-01, -5.4062671661e+00,  8.5600347519e+00,\n",
            "         1.0053492546e+01, -1.6850906372e+01,  4.5636758208e-01,\n",
            "        -5.6244297028e+00,  7.8643250465e+00,  9.1263847351e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1061.5451660156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6073060036), tensor(0.9687817693), tensor(1.0701479912), tensor(1.1847064495), tensor(1.0916928053), tensor(1.3950245380), tensor(1.0309152603), tensor(1.1588078737), tensor(0.9074930549), tensor(1.2000741959), tensor(1.0560832024), tensor(1.4801170826), tensor(0.9570657015), tensor(1.2195992470), tensor(1.1458954811), tensor(0.9488679767), tensor(1.0547999144), tensor(0.8740015626), tensor(1.0746666193), tensor(0.9812564254)]\n",
            "b:  [tensor(0.6213551164), tensor(1.0992927551), tensor(1.3334830999), tensor(1.0127353668), tensor(1.4649183750), tensor(0.8182853460), tensor(1.2347320318), tensor(1.1434577703), tensor(1.7521388531), tensor(0.8710166812), tensor(1.2721403837), tensor(0.7186154723), tensor(1.3046510220), tensor(1.2580703497), tensor(0.8808512688), tensor(0.9828617573), tensor(1.2841038704), tensor(1.6546776295), tensor(0.9858351350), tensor(1.1760572195)]\n",
            "c:  [tensor(-0.0095476136), tensor(0.2101725340), tensor(0.1983864158), tensor(-0.0137076126), tensor(-0.0017446602), tensor(-0.0116278101), tensor(0.0037108092), tensor(0.0042792060), tensor(0.1574990451), tensor(-0.0011186627), tensor(0.0867585838), tensor(-0.0356051177), tensor(-0.0391716659), tensor(0.1451702416), tensor(0.0041714641), tensor(0.0919752121), tensor(-0.0347458571), tensor(-0.0379907489)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.5889502764,  0.1706895977,  0.3379524946,  0.1971158981,\n",
            "        -0.0353763700,  0.7358574867,  0.5881128907,  0.2023426294,\n",
            "         0.1265503764,  0.8283465505,  0.7216968536,  0.5236566067,\n",
            "         0.4349133968,  1.0016187429,  0.5917554498,  0.7834143639,\n",
            "         0.5628020167,  0.4718159437,  0.4137225747,  0.4547455311])\n",
            "btensor.grad: tensor([-0.5840452909, -0.3554289341,  0.1619721651, -0.2065948844,\n",
            "         0.1291327178, -0.2161110640,  0.0956699252,  0.5101993084,\n",
            "         0.8998287916, -0.5644094348, -0.2155330181, -0.4622887373,\n",
            "         0.0903318226, -0.2814241648, -0.2566491663,  0.3273164630,\n",
            "         0.4159694016,  0.3765180111, -0.1393405199,  0.0207519531])\n",
            "ctensor.grad: tensor([-3.2659232616e-01, -2.6118251801e+01, -2.5072256088e+01,\n",
            "         8.1880205870e-01, -2.0853130147e-02,  3.0691763759e-01,\n",
            "        -3.5512775183e-01, -4.2840892076e-01, -1.8003473282e+01,\n",
            "         5.0944948196e-01, -5.1885318756e+00,  9.0496511459e+00,\n",
            "         1.0024505615e+01, -1.7354705811e+01,  4.8053690791e-01,\n",
            "        -5.3250041008e+00,  8.3891744614e+00,  9.2911634445e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1060.3197021484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6041826010), tensor(0.9680681229), tensor(1.0685200691), tensor(1.1837742329), tensor(1.0920605659), tensor(1.3910865784), tensor(1.0278705359), tensor(1.1578339338), tensor(0.9070428014), tensor(1.1957851648), tensor(1.0523550510), tensor(1.4773575068), tensor(0.9549458623), tensor(1.2142796516), tensor(1.1428750753), tensor(0.9448305368), tensor(1.0519027710), tensor(0.8716084957), tensor(1.0726460218), tensor(0.9790204167)]\n",
            "b:  [tensor(0.6246021986), tensor(1.1014386415), tensor(1.3327517509), tensor(1.0140604973), tensor(1.4644367695), tensor(0.8195506930), tensor(1.2343746424), tensor(1.1408843994), tensor(1.7474989891), tensor(0.8742160797), tensor(1.2734838724), tensor(0.7211627364), tensor(1.3043735027), tensor(1.2596700191), tensor(0.8823696971), tensor(0.9812749028), tensor(1.2820731401), tensor(1.6528645754), tensor(0.9867404103), tensor(1.1761749983)]\n",
            "c:  [tensor(-0.0093610045), tensor(0.2235468030), tensor(0.2112217396), tensor(-0.0140999667), tensor(-0.0017344611), tensor(-0.0117649529), tensor(0.0038875754), tensor(0.0044847769), tensor(0.1667583883), tensor(-0.0013843109), tensor(0.0892171860), tensor(-0.0403631106), tensor(-0.0441013649), tensor(0.1540829241), tensor(0.0039196229), tensor(0.0944719985), tensor(-0.0391886719), tensor(-0.0426419973)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.6246821880,  0.1427276731,  0.3255804479,  0.1864454746,\n",
            "        -0.0735616684,  0.7875939608,  0.6089542508,  0.1947915554,\n",
            "         0.0900525451,  0.8578172922,  0.7456289530,  0.5519217253,\n",
            "         0.4239630103,  1.0639309883,  0.6040770411,  0.8074913025,\n",
            "         0.5794402957,  0.4786110520,  0.4041149616,  0.4471966028])\n",
            "btensor.grad: tensor([-0.6494202018, -0.4291843176,  0.1462765932, -0.2650346756,\n",
            "         0.0963135362, -0.2530748844,  0.0714763701,  0.5146809816,\n",
            "         0.9279748201, -0.6398836374, -0.2686982155, -0.5094534159,\n",
            "         0.0554997623, -0.3199249506, -0.3036882281,  0.3173694611,\n",
            "         0.4061501622,  0.3626132011, -0.1810591221, -0.0235626698])\n",
            "ctensor.grad: tensor([-3.7321898341e-01, -2.6748538971e+01, -2.5670640945e+01,\n",
            "         7.8470784426e-01, -2.0398298278e-02,  2.7428594232e-01,\n",
            "        -3.5353249311e-01, -4.1114160419e-01, -1.8518695831e+01,\n",
            "         5.3129649162e-01, -4.9172115326e+00,  9.5159854889e+00,\n",
            "         9.8593969345e+00, -1.7825353622e+01,  5.0368249416e-01,\n",
            "        -4.9935717583e+00,  8.8856267929e+00,  9.3024978638e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1059.0384521484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.6008644104), tensor(0.9674913287), tensor(1.0669492483), tensor(1.1828831434), tensor(1.0926142931), tensor(1.3868745565), tensor(1.0247114897), tensor(1.1568930149), tensor(0.9067673087), tensor(1.1913498640), tensor(1.0485036373), tensor(1.4744478464), tensor(0.9528852105), tensor(1.2086385489), tensor(1.1397885084), tensor(0.9406737089), tensor(1.0489096642), tensor(0.8691796064), tensor(1.0706741810), tensor(0.9768202305)]\n",
            "b:  [tensor(0.6281650066), tensor(1.1039416790), tensor(1.3320935965), tensor(1.0156743526), tensor(1.4641181231), tensor(0.8209884763), tensor(1.2341351509), tensor(1.1382875443), tensor(1.7427208424), tensor(0.8777796030), tensor(1.2750844955), tensor(0.7239345908), tensor(1.3042676449), tensor(1.2614467144), tensor(0.8841142058), tensor(0.9797311425), tensor(1.2800915241), tensor(1.6511267424), tensor(0.9878455997), tensor(1.1765092611)]\n",
            "c:  [tensor(-0.0091439988), tensor(0.2371853292), tensor(0.2243090272), tensor(-0.0144743156), tensor(-0.0017245135), tensor(-0.0118854241), tensor(0.0040629921), tensor(0.0046809516), tensor(0.1762441844), tensor(-0.0016605165), tensor(0.0915072039), tensor(-0.0453424752), tensor(-0.0488876440), tensor(0.1632089913), tensor(0.0036566858), tensor(0.0967822745), tensor(-0.0438646674), tensor(-0.0472264253)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.6636289358,  0.1153616458,  0.3141673207,  0.1782201529,\n",
            "        -0.1107342541,  0.8424068689,  0.6318045855,  0.1881899834,\n",
            "         0.0550977588,  0.8870508671,  0.7702739835,  0.5819432139,\n",
            "         0.4121254683,  1.1282186508,  0.6173183918,  0.8313606977,\n",
            "         0.5986145735,  0.4857735038,  0.3943778276,  0.4400402308])\n",
            "btensor.grad: tensor([-0.7125603557, -0.5006059408,  0.1316260695, -0.3227829337,\n",
            "         0.0637403280, -0.2875579596,  0.0479048193,  0.5193625689,\n",
            "         0.9556179047, -0.7126991749, -0.3201341629, -0.5543720722,\n",
            "         0.0211755335, -0.3553283215, -0.3489066958,  0.3087535501,\n",
            "         0.3963243961,  0.3475567102, -0.2210338116, -0.0668420792])\n",
            "ctensor.grad: tensor([-4.3401062489e-01, -2.7277057648e+01, -2.6174589157e+01,\n",
            "         7.4869841337e-01, -1.9895039499e-02,  2.4094164371e-01,\n",
            "        -3.5083311796e-01, -3.9234924316e-01, -1.8971584320e+01,\n",
            "         5.5241119862e-01, -4.5800294876e+00,  9.9587297440e+00,\n",
            "         9.5725574493e+00, -1.8252138138e+01,  5.2587419748e-01,\n",
            "        -4.6205506325e+00,  9.3519906998e+00,  9.1688528061e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1057.7105712891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5973373652), tensor(0.9670473337), tensor(1.0654295683), tensor(1.1820192337), tensor(1.0933462381), tensor(1.3823773861), tensor(1.0214304924), tensor(1.1559811831), tensor(0.9066556692), tensor(1.1867736578), tensor(1.0445288420), tensor(1.4713826180), tensor(0.9508896470), tensor(1.2026715279), tensor(1.1366328001), tensor(0.9364029169), tensor(1.0458091497), tensor(0.8667157292), tensor(1.0687522888), tensor(0.9746540785)]\n",
            "b:  [tensor(0.6320293546), tensor(1.1067844629), tensor(1.3315035105), tensor(1.0175708532), tensor(1.4639590979), tensor(0.8225852251), tensor(1.2340103388), tensor(1.1356691122), tensor(1.7378100157), tensor(0.8816887736), tensor(1.2769299746), tensor(0.7269183397), tensor(1.3043289185), tensor(1.2633826733), tensor(0.8860734701), tensor(0.9782244563), tensor(1.2781593800), tensor(1.6494708061), tensor(0.9891399145), tensor(1.1770520210)]\n",
            "c:  [tensor(-0.0088903913), tensor(0.2510342300), tensor(0.2375979573), tensor(-0.0148295900), tensor(-0.0017148482), tensor(-0.0119890338), tensor(0.0042363098), tensor(0.0048669023), tensor(0.1859214306), tensor(-0.0019470002), tensor(0.0935924649), tensor(-0.0505304858), tensor(-0.0534760803), tensor(0.1725221574), tensor(0.0033830623), tensor(0.0988816619), tensor(-0.0487578586), tensor(-0.0516773015)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7054135799,  0.0887977332,  0.3039460480,  0.1727756262,\n",
            "        -0.1463778317,  0.8994457722,  0.6561954618,  0.1823625565,\n",
            "         0.0223230124,  0.9152426720,  0.7949622273,  0.6130518913,\n",
            "         0.3991087079,  1.1934074163,  0.6311504841,  0.8541539907,\n",
            "         0.6201143861,  0.4927747846,  0.3843749166,  0.4332305193])\n",
            "btensor.grad: tensor([-0.7728638649, -0.5685501099,  0.1180131435, -0.3792977333,\n",
            "         0.0317953378, -0.3193457127,  0.0249667764,  0.5236798525,\n",
            "         0.9821542501, -0.7818324566, -0.3690854311, -0.5967483521,\n",
            "        -0.0122576952, -0.3871991634, -0.3918575644,  0.3013336062,\n",
            "         0.3864403367,  0.3311954737, -0.2588597536, -0.1085548401])\n",
            "ctensor.grad: tensor([-5.0721508265e-01, -2.7697809219e+01, -2.6577856064e+01,\n",
            "         7.1054881811e-01, -1.9330456853e-02,  2.0721888542e-01,\n",
            "        -3.4663540125e-01, -3.7190124393e-01, -1.9354482651e+01,\n",
            "         5.7296729088e-01, -4.1705250740e+00,  1.0376022339e+01,\n",
            "         9.1768741608e+00, -1.8626333237e+01,  5.4724675417e-01,\n",
            "        -4.1987795830e+00,  9.7863826752e+00,  8.9017534256e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1056.3454589844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5935899019), tensor(0.9667318463), tensor(1.0639547110), tensor(1.1811679602), tensor(1.0942468643), tensor(1.3775891066), tensor(1.0180232525), tensor(1.1550962925), tensor(0.9066948295), tensor(1.1820660830), tensor(1.0404345989), tensor(1.4681605101), tensor(0.9489672184), tensor(1.1963801384), tensor(1.1334074736), tensor(0.9320286512), tensor(1.0425916910), tensor(0.8642210960), tensor(1.0668834448), tensor(0.9725210667)]\n",
            "b:  [tensor(0.6361784339), tensor(1.1099450588), tensor(1.3309772015), tensor(1.0197417736), tensor(1.4639558792), tensor(0.8243271112), tensor(1.2339978218), tensor(1.1330345869), tensor(1.7327760458), tensor(0.8859210014), tensor(1.2790049314), tensor(0.7301002145), tensor(1.3045520782), tensor(1.2654591799), tensor(0.8882346153), tensor(0.9767503738), tensor(1.2762778997), tensor(1.6479048729), tensor(0.9906113744), tensor(1.1777938604)]\n",
            "c:  [tensor(-0.0085953269), tensor(0.2650383115), tensor(0.2510366738), tensor(-0.0151646947), tensor(-0.0017055015), tensor(-0.0120758461), tensor(0.0044065844), tensor(0.0050417557), tensor(0.1957531869), tensor(-0.0022435763), tensor(0.0954360589), tensor(-0.0559128560), tensor(-0.0578180030), tensor(0.1819929034), tensor(0.0030990695), tensor(0.1007434875), tensor(-0.0538512208), tensor(-0.0559347309)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7494969368,  0.0631022304,  0.2949802279,  0.1702572107,\n",
            "        -0.1801285744,  0.9576605558,  0.6814510226,  0.1769698858,\n",
            "        -0.0078348517,  0.9415037036,  0.8188604116,  0.6444125175,\n",
            "         0.3844824433,  1.2582786083,  0.6450725794,  0.8748515844,\n",
            "         0.6434866786,  0.4989324808,  0.3737795353,  0.4266080856])\n",
            "btensor.grad: tensor([-8.2981240749e-01, -6.3211894035e-01,  1.0526561737e-01,\n",
            "        -4.3419271708e-01,  6.5238773823e-04, -3.4837830067e-01,\n",
            "         2.5033950806e-03,  5.2690982819e-01,  1.0067883730e+00,\n",
            "        -8.4643989801e-01, -4.1500258446e-01, -6.3637995720e-01,\n",
            "        -4.4624030590e-02, -4.1531014442e-01, -4.3223494291e-01,\n",
            "         2.9481589794e-01,  3.7628942728e-01,  3.1318998337e-01,\n",
            "        -2.9429256916e-01, -1.4837050438e-01])\n",
            "ctensor.grad: tensor([-5.9012794495e-01, -2.8008140564e+01, -2.6877454758e+01,\n",
            "         6.7020922899e-01, -1.8693597987e-02,  1.7362435162e-01,\n",
            "        -3.4054890275e-01, -3.4970691800e-01, -1.9663524628e+01,\n",
            "         5.9315228462e-01, -3.6871819496e+00,  1.0764737129e+01,\n",
            "         8.6838474274e+00, -1.8941484451e+01,  5.6798547506e-01,\n",
            "        -3.7236473560e+00,  1.0186723709e+01,  8.5148591995e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1054.9510498047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5896136761), tensor(0.9665402770), tensor(1.0625184774), tensor(1.1803144217), tensor(1.0953052044), tensor(1.3725095987), tensor(1.0144892931), tensor(1.1542382240), tensor(0.9068701267), tensor(1.1772414446), tensor(1.0362291336), tensor(1.4647849798), tensor(0.9471282363), tensor(1.1897721291), tensor(1.1301146746), tensor(0.9275664091), tensor(1.0392509699), tensor(0.8617033958), tensor(1.0650721788), tensor(0.9704208970)]\n",
            "b:  [tensor(0.6405931711), tensor(1.1133980751), tensor(1.3305116892), tensor(1.0221776962), tensor(1.4641039371), tensor(0.8262004852), tensor(1.2340964079), tensor(1.1303930283), tensor(1.7276328802), tensor(0.8904500008), tensor(1.2812924385), tensor(0.7334656715), tensor(1.3049312830), tensor(1.2676570415), tensor(0.8905836344), tensor(0.9753060937), tensor(1.2744497061), tensor(1.6464393139), tensor(0.9922471642), tensor(1.1787240505)]\n",
            "c:  [tensor(-0.0082555152), tensor(0.2791427672), tensor(0.2645734251), tensor(-0.0154785924), tensor(-0.0016965133), tensor(-0.0121462336), tensor(0.0045727035), tensor(0.0052046450), tensor(0.2057022154), tensor(-0.0025501586), tensor(0.0970022753), tensor(-0.0614733770), tensor(-0.0618701205), tensor(0.1915896237), tensor(0.0028049138), tensor(0.1023400873), tensor(-0.0591266043), tensor(-0.0599462353)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.7952427864,  0.0383089483,  0.2872466445,  0.1707179546,\n",
            "        -0.2116653323,  1.0158967972,  0.7068006396,  0.1716245413,\n",
            "        -0.0350563526,  0.9649293423,  0.8410877585,  0.6751078367,\n",
            "         0.3677961826,  1.3215932846,  0.6585512161,  0.8924502134,\n",
            "         0.6681469679,  0.5035389662,  0.3622458577,  0.4200304747])\n",
            "btensor.grad: tensor([-0.8829495907, -0.6906050444,  0.0931135416, -0.4871950150,\n",
            "        -0.0296180695, -0.3746798038, -0.0197192430,  0.5283025503,\n",
            "         1.0286253691, -0.9058046341, -0.4574955702, -0.6730900407,\n",
            "        -0.0758468509, -0.4395638704, -0.4698075652,  0.2888547182,\n",
            "         0.3656340241,  0.2931149006, -0.3271595240, -0.1860457659])\n",
            "ctensor.grad: tensor([-6.7962259054e-01, -2.8208938599e+01, -2.7073528290e+01,\n",
            "         6.2779539824e-01, -1.7976500094e-02,  1.4077523351e-01,\n",
            "        -3.3223849535e-01, -3.2577869296e-01, -1.9898040771e+01,\n",
            "         6.1316466331e-01, -3.1324265003e+00,  1.1121045113e+01,\n",
            "         8.1042346954e+00, -1.9193431854e+01,  5.8831137419e-01,\n",
            "        -3.1931982040e+00,  1.0550765991e+01,  8.0230093002e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1053.5382080078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5854036808), tensor(0.9664679170), tensor(1.0611150265), tensor(1.1794437170), tensor(1.0965086222), tensor(1.3671445847), tensor(1.0108318329), tensor(1.1534085274), tensor(0.9071655273), tensor(1.1723177433), tensor(1.0319252014), tensor(1.4612636566), tensor(0.9453849196), tensor(1.1828612089), tensor(1.1267592907), tensor(0.9230361581), tensor(1.0357836485), tensor(0.8591735363), tensor(1.0633250475), tensor(0.9683538675)]\n",
            "b:  [tensor(0.6452524662), tensor(1.1171152592), tensor(1.3301053047), tensor(1.0248680115), tensor(1.4643990993), tensor(0.8281921148), tensor(1.2343062162), tensor(1.1277573109), tensor(1.7223989964), tensor(0.8952466249), tensor(1.2837737799), tensor(0.7369992733), tensor(1.3054609299), tensor(1.2699568272), tensor(0.8931055069), tensor(0.9738905430), tensor(1.2726783752), tensor(1.6450865269), tensor(0.9940338135), tensor(1.1798311472)]\n",
            "c:  [tensor(-0.0078692101), tensor(0.2932947576), tensor(0.2781579196), tensor(-0.0157703701), tensor(-0.0016879259), tensor(-0.0122009004), tensor(0.0047334312), tensor(0.0053547760), tensor(0.2157320380), tensor(-0.0028667639), tensor(0.0982580036), tensor(-0.0671938136), tensor(-0.0655944124), tensor(0.2012798786), tensor(0.0025006826), tensor(0.1036441028), tensor(-0.0645646751), tensor(-0.0636669248)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8419985771,  0.0144705176,  0.2806915343,  0.1741387844,\n",
            "        -0.2406886518,  1.0729938745,  0.7314844131,  0.1659512520,\n",
            "        -0.0590787530,  0.9847419858,  0.8607909679,  0.7042587996,\n",
            "         0.3486633301,  1.3821895123,  0.6710785627,  0.9060529470,\n",
            "         0.6934630871,  0.5059731007,  0.3494200706,  0.4134099483])\n",
            "btensor.grad: tensor([-0.9318552613, -0.7434374094,  0.0812659860, -0.5380709171,\n",
            "        -0.0590226352, -0.3983273506, -0.0419505835,  0.5271500349,\n",
            "         1.0467869043, -0.9593276978, -0.4962599277, -0.7067235708,\n",
            "        -0.1059176326, -0.4599660635, -0.5043789148,  0.2831140757,\n",
            "         0.3542578816,  0.2705500126, -0.3573347330, -0.2214089036])\n",
            "ctensor.grad: tensor([-7.7261114120e-01, -2.8303956985e+01, -2.7168972015e+01,\n",
            "         5.8355706930e-01, -1.7174659297e-02,  1.0933270305e-01,\n",
            "        -3.2145562768e-01, -3.0026224256e-01, -2.0059654236e+01,\n",
            "         6.3321042061e-01, -2.5114586353e+00,  1.1440871239e+01,\n",
            "         7.4485769272e+00, -1.9380517960e+01,  6.0846215487e-01,\n",
            "        -2.6080274582e+00,  1.0876148224e+01,  7.4413776398e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1052.1160888672, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5809580088), tensor(0.9665093422), tensor(1.0597386360), tensor(1.1785413027), tensor(1.0978430510), tensor(1.3615053892), tensor(1.0070576668), tensor(1.1526104212), tensor(0.9075638056), tensor(1.1673161983), tensor(1.0275390148), tensor(1.4576084614), tensor(0.9437508583), tensor(1.1756658554), tensor(1.1233481169), tensor(0.9184613824), tensor(1.0321894884), tensor(0.8566448092), tensor(1.0616497993), tensor(0.9663199782)]\n",
            "b:  [tensor(0.6501332521), tensor(1.1210659742), tensor(1.3297580481), tensor(1.0278010368), tensor(1.4648368359), tensor(0.8302891850), tensor(1.2346282005), tensor(1.1251429319), tensor(1.7170965672), tensor(0.9002792239), tensor(1.2864291668), tensor(0.7406850457), tensor(1.3061351776), tensor(1.2723398209), tensor(0.8957844377), tensor(0.9725039601), tensor(1.2709683180), tensor(1.6438609362), tensor(0.9959573746), tensor(1.1811025143)]\n",
            "c:  [tensor(-0.0074360436), tensor(0.3074444532), tensor(0.2917425036), tensor(-0.0160392914), tensor(-0.0016797825), tensor(-0.0122408690), tensor(0.0048874607), tensor(0.0054914947), tensor(0.2258078456), tensor(-0.0031935144), tensor(0.0991734937), tensor(-0.0730539784), tensor(-0.0689582303), tensor(0.2110314369), tensor(0.0021863421), tensor(0.1046295762), tensor(-0.0701448917), tensor(-0.0670593902)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.8891379833, -0.0082808137,  0.2752894759,  0.1804816723,\n",
            "        -0.2668885291,  1.1278489828,  0.7548351884,  0.1596173048,\n",
            "        -0.0796521902,  1.0002996922,  0.8772338629,  0.7310308218,\n",
            "         0.3268092275,  1.4390599728,  0.6822372079,  0.9149532318,\n",
            "         0.7188223600,  0.5057486296,  0.3350574970,  0.4067735672])\n",
            "btensor.grad: tensor([-0.9761588573, -0.7901504040,  0.0694443583, -0.5866155028,\n",
            "        -0.0875544995, -0.4194158316, -0.0644080043,  0.5228815079,\n",
            "         1.0604758263, -1.0065157413, -0.5310798883, -0.7371486425,\n",
            "        -0.1348408759, -0.4765881300, -0.5357915163,  0.2773188949,\n",
            "         0.3420014977,  0.2451195717, -0.3847130537, -0.2542773485])\n",
            "ctensor.grad: tensor([-8.6633330584e-01, -2.8299417496e+01, -2.7169155121e+01,\n",
            "         5.3784185648e-01, -1.6286810860e-02,  7.9937152565e-02,\n",
            "        -3.0805945396e-01, -2.7343732119e-01, -2.0151611328e+01,\n",
            "         6.5350085497e-01, -1.8309754133e+00,  1.1720325470e+01,\n",
            "         6.7276406288e+00, -1.9503108978e+01,  6.2868106365e-01,\n",
            "        -1.9709402323e+00,  1.1160430908e+01,  6.7849321365e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1050.6932373047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5762776136), tensor(0.9666581154), tensor(1.0583834648), tensor(1.1775928736), tensor(1.0992928743), tensor(1.3556082249), tensor(1.0031759739), tensor(1.1518485546), tensor(0.9080465436), tensor(1.1622605324), tensor(1.0230900049), tensor(1.4538350105), tensor(0.9422403574), tensor(1.1682090759), tensor(1.1198894978), tensor(0.9138680696), tensor(1.0284711123), tensor(0.8541321754), tensor(1.0600547791), tensor(0.9643187523)]\n",
            "b:  [tensor(0.6552110314), tensor(1.1252179146), tensor(1.3294708729), tensor(1.0309642553), tensor(1.4654129744), tensor(0.8324795961), tensor(1.2350646257), tensor(1.1225676537), tensor(1.7117515802), tensor(0.9055141807), tensor(1.2892382145), tensor(0.7445063591), tensor(1.3069483042), tensor(1.2747875452), tensor(0.8986040354), tensor(0.9711477160), tensor(1.2693243027), tensor(1.6427781582), tensor(0.9980034828), tensor(1.1825251579)]\n",
            "c:  [tensor(-0.0069568204), tensor(0.3215462267), tensor(0.3052832484), tensor(-0.0162848197), tensor(-0.0016721253), tensor(-0.0122674555), tensor(0.0050334688), tensor(0.0056143440), tensor(0.2358970046), tensor(-0.0035306378), tensor(0.0997228771), tensor(-0.0790318921), tensor(-0.0719344541), tensor(0.2208130807), tensor(0.0018617404), tensor(0.1052728817), tensor(-0.0758454651), tensor(-0.0700933561)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9360718727, -0.0297517926,  0.2710394859,  0.1896891594,\n",
            "        -0.2899535596,  1.1794406176,  0.7763278484,  0.1523768902,\n",
            "        -0.0965483785,  1.0111422539,  0.8898016810,  0.7546997666,\n",
            "         0.3020992875,  1.4913610220,  0.6917290688,  0.9186590910,\n",
            "         0.7436786294,  0.5025295615,  0.3190149665,  0.4002435207])\n",
            "btensor.grad: tensor([-1.0155537128, -0.8303992748,  0.0574238896, -0.6326403618,\n",
            "        -0.1152242422, -0.4380770922, -0.0872964263,  0.5150613785,\n",
            "         1.0690039396, -1.0469889641, -0.5618060827, -0.7642644048,\n",
            "        -0.1626316011, -0.4895354509, -0.5639165640,  0.2712500095,\n",
            "         0.3287968040,  0.2165513039, -0.4092187881, -0.2845244408])\n",
            "ctensor.grad: tensor([-9.5844602585e-01, -2.8203536987e+01, -2.7081497192e+01,\n",
            "         4.9105605483e-01, -1.5314448625e-02,  5.3172193468e-02,\n",
            "        -2.9201647639e-01, -2.4569837749e-01, -2.0178302765e+01,\n",
            "         6.7424666882e-01, -1.0987614393e+00,  1.1955832481e+01,\n",
            "         5.9524412155e+00, -1.9563274384e+01,  6.4920359850e-01,\n",
            "        -1.2866038084e+00,  1.1401144028e+01,  6.0679259300e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1049.2766113281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5713661909), tensor(0.9669065475), tensor(1.0570435524), tensor(1.1765843630), tensor(1.1008406878), tensor(1.3494739532), tensor(0.9991980791), tensor(1.1511281729), tensor(0.9085942507), tensor(1.1571755409), tensor(1.0185998678), tensor(1.4499616623), tensor(0.9408676028), tensor(1.1605167389), tensor(1.1163926125), tensor(0.9092834592), tensor(1.0246332884), tensor(0.8516513705), tensor(1.0585485697), tensor(0.9623485208)]\n",
            "b:  [tensor(0.6604600549), tensor(1.1295375824), tensor(1.3292458057), tensor(1.0343441963), tensor(1.4661229849), tensor(0.8347518444), tensor(1.2356183529), tensor(1.1200506687), tensor(1.7063924074), tensor(0.9109165668), tensor(1.2921799421), tensor(0.7484464049), tensor(1.3078948259), tensor(1.2772823572), tensor(0.9015473127), tensor(0.9698239565), tensor(1.2677510977), tensor(1.6418548822), tensor(1.0001574755), tensor(1.1840852499)]\n",
            "c:  [tensor(-0.0064332355), tensor(0.3355592191), tensor(0.3187406957), tensor(-0.0165066328), tensor(-0.0016649948), tensor(-0.0122822179), tensor(0.0051701684), tensor(0.0057231053), tensor(0.2459693998), tensor(-0.0038784656), tensor(0.0998844355), tensor(-0.0851040706), tensor(-0.0745015591), tensor(0.2305953354), tensor(0.0015266140), tensor(0.1055534035), tensor(-0.0816434771), tensor(-0.0727453232)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.9822750092, -0.0496910959,  0.2679715455,  0.2017042637,\n",
            "        -0.3095696867,  1.2268662453,  0.7955788970,  0.1440809965,\n",
            "        -0.1095433831,  1.0170040131,  0.8980269432,  0.7746670246,\n",
            "         0.2745505571,  1.5384603739,  0.6993839145,  0.9169259071,\n",
            "         0.7675679326,  0.4961613417,  0.3012488484,  0.3940508366])\n",
            "btensor.grad: tensor([-1.0497999191, -0.8639419079,  0.0450148582, -0.6759762764,\n",
            "        -0.1420022845, -0.4544506073, -0.1107484102,  0.5034072399,\n",
            "         1.0718452930, -1.0804786682, -0.5883439779, -0.7880114317,\n",
            "        -0.1892993301, -0.4989690781, -0.5886523724,  0.2647519708,\n",
            "         0.3146404326,  0.1846661568, -0.4308030605, -0.3120144606])\n",
            "ctensor.grad: tensor([-1.0471700430e+00, -2.8026004791e+01, -2.6914882660e+01,\n",
            "         4.4362708926e-01, -1.4261211269e-02,  2.9524333775e-02,\n",
            "        -2.7339914441e-01, -2.1752242744e-01, -2.0144784927e+01,\n",
            "         6.9565564394e-01, -3.2312190533e-01,  1.2144352913e+01,\n",
            "         5.1342077255e+00, -1.9564517975e+01,  6.7025285959e-01,\n",
            "        -5.6103658676e-01,  1.1596031189e+01,  5.3039336205e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1047.8721923828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5662298203), tensor(0.9672455788), tensor(1.0557128191), tensor(1.1755021811), tensor(1.1024678946), tensor(1.3431271315), tensor(0.9951361418), tensor(1.1504548788), tensor(0.9091864228), tensor(1.1520866156), tensor(1.0140918493), tensor(1.4460092783), tensor(0.9396460652), tensor(1.1526172161), tensor(1.1128668785), tensor(0.9047347903), tensor(1.0206826925), tensor(0.8492181897), tensor(1.0571393967), tensor(0.9604060054)]\n",
            "b:  [tensor(0.6658537984), tensor(1.1339906454), tensor(1.3290854692), tensor(1.0379266739), tensor(1.4669622183), tensor(0.8370953798), tensor(1.2362927198), tensor(1.1176117659), tensor(1.7010493279), tensor(0.9164507985), tensor(1.2952332497), tensor(0.7524883151), tensor(1.3089691401), tensor(1.2798076868), tensor(0.9045970440), tensor(0.9685353041), tensor(1.2662531137), tensor(1.6411079168), tensor(1.0024048090), tensor(1.1857684851)]\n",
            "c:  [tensor(-0.0058676223), tensor(0.3494479358), tensor(0.3320802748), tensor(-0.0167046208), tensor(-0.0016584287), tensor(-0.0122869033), tensor(0.0052963537), tensor(0.0058178185), tensor(0.2559976578), tensor(-0.0042374292), tensor(0.0996407047), tensor(-0.0912457556), tensor(-0.0766436905), tensor(0.2403509170), tensor(0.0011805960), tensor(0.1054540202), tensor(-0.0875149816), tensor(-0.0749981776)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0272855759, -0.0678056031,  0.2661495805,  0.2164466381,\n",
            "        -0.3254427612,  1.2693639994,  0.8123902678,  0.1346610785,\n",
            "        -0.1184304953,  1.0177791119,  0.9015934467,  0.7904669046,\n",
            "         0.2443123460,  1.5799032450,  0.7051526904,  0.9097347260,\n",
            "         0.7901303172,  0.4866367579,  0.2818360925,  0.3885054588])\n",
            "btensor.grad: tensor([-1.0787456036, -0.8906205893,  0.0320754051, -0.7164841890,\n",
            "        -0.1678571850, -0.4687124491, -0.1348763108,  0.4877890348,\n",
            "         1.0686055422, -1.1068434715, -0.6106609106, -0.8083814383,\n",
            "        -0.2148549855, -0.5050768852, -0.6099501252,  0.2577272654,\n",
            "         0.2996014655,  0.1493978500, -0.4494563341, -0.3366404772])\n",
            "ctensor.grad: tensor([-1.1312265396e+00, -2.7777460098e+01, -2.6679159164e+01,\n",
            "         3.9597776532e-01, -1.3132108375e-02,  9.3715861440e-03,\n",
            "        -2.5237041712e-01, -1.8942594528e-01, -2.0056514740e+01,\n",
            "         7.1792674065e-01,  4.8746883869e-01,  1.2283367157e+01,\n",
            "         4.2842607498e+00, -1.9511156082e+01,  6.9203609228e-01,\n",
            "         1.9876691699e-01,  1.1743000984e+01,  4.5057029724e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1046.4873046875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5608762503), tensor(0.9676644802), tensor(1.0543844700), tensor(1.1743330956), tensor(1.1041544676), tensor(1.3365956545), tensor(0.9910026193), tensor(1.1498341560), tensor(0.9098016024), tensor(1.1470189095), tensor(1.0095902681), tensor(1.4420005083), tensor(0.9385879040), tensor(1.1445401907), tensor(1.1093213558), tensor(0.9002484083), tensor(1.0166271925), tensor(0.8468477726), tensor(1.0558346510), tensor(0.9584862590)]\n",
            "b:  [tensor(0.6713653803), tensor(1.1385426521), tensor(1.3289929628), tensor(1.0416970253), tensor(1.4679259062), tensor(0.8395006657), tensor(1.2370915413), tensor(1.1152707338), tensor(1.6957541704), tensor(0.9220810533), tensor(1.2983771563), tensor(0.7566154003), tensor(1.3101656437), tensor(1.2823480368), tensor(0.9077361822), tensor(0.9672846794), tensor(1.2648341656), tensor(1.6405539513), tensor(1.0047308207), tensor(1.1875600815)]\n",
            "c:  [tensor(-0.0052627092), tensor(0.3631824553), tensor(0.3452727199), tensor(-0.0168788712), tensor(-0.0016524623), tensor(-0.0122833923), tensor(0.0054109381), tensor(0.0058987821), tensor(0.2659572959), tensor(-0.0046080532), tensor(0.0989784822), tensor(-0.0974311754), tensor(-0.0783505216), tensor(0.2500550449), tensor(0.0008232248), tensor(0.1049614027), tensor(-0.0934351087), tensor(-0.0768407956)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.0707048178, -0.0837853849,  0.2656615674,  0.2338209152,\n",
            "        -0.3373067379,  1.3062909842,  0.8267023563,  0.1241382360,\n",
            "        -0.1230340004,  1.0135295391,  0.9003213644,  0.8017581701,\n",
            "         0.2116353512,  1.6154168844,  0.7091110349,  0.8972756863,\n",
            "         0.8111035228,  0.4740861654,  0.2609430552,  0.3839535713])\n",
            "btensor.grad: tensor([-1.1023156643, -0.9103964567,  0.0185065866, -0.7540593147,\n",
            "        -0.1927307695, -0.4810568094, -0.1597652435,  0.4681981802,\n",
            "         1.0590387583, -1.1260560751, -0.6287904978, -0.8254203796,\n",
            "        -0.2393124402, -0.5080792904, -0.6278238297,  0.2501295805,\n",
            "         0.2837788463,  0.1107859612, -0.4652100801, -0.3583310843])\n",
            "ctensor.grad: tensor([-1.2098264694e+00, -2.7469032288e+01, -2.6384891510e+01,\n",
            "         3.4850209951e-01, -1.1932818219e-02, -7.0229573175e-03,\n",
            "        -2.2916893661e-01, -1.6192692518e-01, -1.9919286728e+01,\n",
            "         7.4124777317e-01,  1.3244510889e+00,  1.2370832443e+01,\n",
            "         3.4136593342e+00, -1.9408252716e+01,  7.1474218369e-01,\n",
            "         9.8524057865e-01,  1.1840248108e+01,  3.6852350235e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1045.1230468750, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5553152561), tensor(0.9681511521), tensor(1.0530514717), tensor(1.1730645895), tensor(1.1058791876), tensor(1.3299098015), tensor(0.9868096709), tensor(1.1492711306), tensor(0.9104177356), tensor(1.1419966221), tensor(1.0051194429), tensor(1.4379588366), tensor(0.9377035499), tensor(1.1363157034), tensor(1.1057641506), tensor(0.8958487511), tensor(1.0124754906), tensor(0.8445540667), tensor(1.0546405315), tensor(0.9565824270)]\n",
            "b:  [tensor(0.6769679785), tensor(1.1431592703), tensor(1.3289718628), tensor(1.0456401110), tensor(1.4690086842), tensor(0.8419591784), tensor(1.2380188704), tensor(1.1130470037), tensor(1.6905390024), tensor(0.9277720451), tensor(1.3015912771), tensor(0.7608115673), tensor(1.3114790916), tensor(1.2848891020), tensor(0.9109477401), tensor(0.9660750628), tensor(1.2634975910), tensor(1.6402090788), tensor(1.0071215630), tensor(1.1894453764)]\n",
            "c:  [tensor(-0.0046214131), tensor(0.3767384291), tensor(0.3582940698), tensor(-0.0170296486), tensor(-0.0016471278), tensor(-0.0122736366), tensor(0.0055129835), tensor(0.0059665353), tensor(0.2758266628), tensor(-0.0049909498), tensor(0.0978888124), tensor(-0.1036338136), tensor(-0.0796171799), tensor(0.2596855760), tensor(0.0004539541), tensor(0.1040661559), tensor(-0.0993782803), tensor(-0.0782677531)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1121925116, -0.0973400921,  0.2666011751,  0.2537064552,\n",
            "        -0.3449378312,  1.3371661901,  0.8385952711,  0.1126062870,\n",
            "        -0.1232322454,  1.0044554472,  0.8941730261,  0.8083416820,\n",
            "         0.1768748760,  1.6448967457,  0.7114304900,  0.8799293041,\n",
            "         0.8303353786,  0.4587417841,  0.2388189435,  0.3807715178])\n",
            "btensor.grad: tensor([-1.1205140352, -0.9233120680,  0.0042301416, -0.7886247635,\n",
            "        -0.2165482044, -0.4916974306, -0.1854709387,  0.4447382689,\n",
            "         1.0430339575, -1.1381952763, -0.6428136826, -0.8392330408,\n",
            "        -0.2626960278, -0.5082188845, -0.6423168182,  0.2419208884,\n",
            "         0.2673065066,  0.0689674616, -0.4781502485, -0.3770558238])\n",
            "ctensor.grad: tensor([-1.2825921774e+00, -2.7111946106e+01, -2.6042675018e+01,\n",
            "         3.0155360699e-01, -1.0669093579e-02, -1.9510632381e-02,\n",
            "        -2.0409075916e-01, -1.3550639153e-01, -1.9738706589e+01,\n",
            "         7.6579308510e-01,  2.1793346405e+00,  1.2405271530e+01,\n",
            "         2.5333213806e+00, -1.9261074066e+01,  7.3854142427e-01,\n",
            "         1.7904871702e+00,  1.1886347771e+01,  2.8539183140e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1043.7843017578, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5495579243), tensor(0.9686921835), tensor(1.0517061949), tensor(1.1716848612), tensor(1.1076201200), tensor(1.3231016397), tensor(0.9825683236), tensor(1.1487700939), tensor(0.9110125899), tensor(1.1370422840), tensor(1.0007034540), tensor(1.4339082241), tensor(0.9370014071), tensor(1.1279739141), tensor(1.1022022963), tensor(0.8915576339), tensor(1.0082366467), tensor(0.8423494697), tensor(1.0535616875), tensor(0.9546858072)]\n",
            "b:  [tensor(0.6826351285), tensor(1.1478067636), tensor(1.3290258646), tensor(1.0497407913), tensor(1.4702049494), tensor(0.8444635272), tensor(1.2390791178), tensor(1.1109590530), tensor(1.6854360104), tensor(0.9334892631), tensor(1.3048557043), tensor(0.7650614381), tensor(1.3129042387), tensor(1.2874180079), tensor(0.9142155051), tensor(0.9649095535), tensor(1.2622460127), tensor(1.6400882006), tensor(1.0095635653), tensor(1.1914094687)]\n",
            "c:  [tensor(-0.0039466643), tensor(0.3900969326), tensor(0.3711255491), tensor(-0.0171573684), tensor(-0.0016424546), tensor(-0.0122596156), tensor(0.0056017186), tensor(0.0060218233), tensor(0.2855868936), tensor(-0.0053868112), tensor(0.0963669345), tensor(-0.1098266169), tensor(-0.0804440752), tensor(0.2692230940), tensor(7.2160444688e-05), tensor(0.1027629450), tensor(-0.1053183526), tensor(-0.0792789608)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1514682770, -0.1082094461,  0.2690665126,  0.2759532928,\n",
            "        -0.3481811583,  1.3616242409,  0.8482643366,  0.1002160311,\n",
            "        -0.1189656258,  0.9908632040,  0.8832094073,  0.8101270199,\n",
            "         0.1404291391,  1.6683558226,  0.7123603821,  0.8582249880,\n",
            "         0.8477582932,  0.4409222007,  0.2157737613,  0.3793247938])\n",
            "btensor.grad: tensor([-1.1334310770, -0.9295011759, -0.0108050108, -0.8201467395,\n",
            "        -0.2392440587, -0.5008749962, -0.2120473981,  0.4175997972,\n",
            "         1.0205868483, -1.1434488297, -0.6528824568, -0.8499715328,\n",
            "        -0.2850408852, -0.5057696104, -0.6535578966,  0.2330999970,\n",
            "         0.2503250241,  0.0241737366, -0.4884064198, -0.3928194046])\n",
            "ctensor.grad: tensor([-1.3494975567e+00, -2.6717004776e+01, -2.5662939072e+01,\n",
            "         2.5543874502e-01, -9.3462094665e-03, -2.8042057529e-02,\n",
            "        -1.7747022212e-01, -1.1057558656e-01, -1.9520469666e+01,\n",
            "         7.9172277451e-01,  3.0437572002e+00,  1.2385609627e+01,\n",
            "         1.6537958384e+00, -1.9075021744e+01,  7.6358729601e-01,\n",
            "         2.6064267159e+00,  1.1880137444e+01,  2.0224168301e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1042.4730224609, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5436164141), tensor(0.9692732096), tensor(1.0503405333), tensor(1.1701830626), tensor(1.1093548536), tensor(1.3162045479), tensor(0.9782885313), tensor(1.1483342648), tensor(0.9115638137), tensor(1.1321766376), tensor(0.9963655472), tensor(1.4298725128), tensor(0.9364877939), tensor(1.1195442677), tensor(1.0986412764), tensor(0.8873936534), tensor(1.0039197206), tensor(0.8402445912), tensor(1.0526009798), tensor(0.9527861476)]\n",
            "b:  [tensor(0.6883412600), tensor(1.1524528265), tensor(1.3291591406), tensor(1.0539839268), tensor(1.4715086222), tensor(0.8470078111), tensor(1.2402768135), tensor(1.1090238094), tensor(1.6804770231), tensor(0.9391998053), tensor(1.3081517220), tensor(0.7693506479), tensor(1.3144363165), tensor(1.2899231911), tensor(0.9175240993), tensor(0.9637913108), tensor(1.2610812187), tensor(1.6402046680), tensor(1.0120444298), tensor(1.1934379339)]\n",
            "c:  [tensor(-0.0032413162), tensor(0.4032441974), tensor(0.3837533295), tensor(-0.0172625743), tensor(-0.0016384703), tensor(-0.0122432876), tensor(0.0056765513), tensor(0.0060655493), tensor(0.2952219248), tensor(-0.0057964013), tensor(0.0944123417), tensor(-0.1159821749), tensor(-0.0808365270), tensor(0.2786508501), tensor(-0.0003228467), tensor(0.1010504887), tensor(-0.1112287790), tensor(-0.0798793361)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.1882927418, -0.1162042469,  0.2731307447,  0.3003627062,\n",
            "        -0.3469521403,  1.3794283867,  0.8559560180,  0.0871541500,\n",
            "        -0.1102418900,  0.9731371403,  0.8675864339,  0.8071386814,\n",
            "         0.1027241945,  1.6859221458,  0.7122104764,  0.8327984810,\n",
            "         0.8633835912,  0.4209697843,  0.1921479702,  0.3799264431])\n",
            "btensor.grad: tensor([-1.1412223577, -0.9292052984, -0.0266495943, -0.8486263752,\n",
            "        -0.2607459426, -0.5088610649, -0.2395355403,  0.3870501518,\n",
            "         0.9917906523, -1.1421072483, -0.6592024565, -0.8578470945,\n",
            "        -0.3064072728, -0.5010271072, -0.6617220640,  0.2236514688,\n",
            "         0.2329704165, -0.0232967138, -0.4961639643, -0.4056956768])\n",
            "ctensor.grad: tensor([-1.4106960297e+00, -2.6294519424e+01, -2.5255535126e+01,\n",
            "         2.1041166782e-01, -7.9685719684e-03, -3.2655593008e-02,\n",
            "        -1.4966522157e-01, -8.7452530861e-02, -1.9270030975e+01,\n",
            "         8.1918001175e-01,  3.9091806412e+00,  1.2311118126e+01,\n",
            "         7.8491008282e-01, -1.8855487823e+01,  7.9001426697e-01,\n",
            "         3.4249186516e+00,  1.1820850372e+01,  1.2007535696e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1041.1899414062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5375039577), tensor(0.9698791504), tensor(1.0489462614), tensor(1.1685495377), tensor(1.1110609770), tensor(1.3092521429), tensor(0.9739785790), tensor(1.1479660273), tensor(0.9120495915), tensor(1.1274179220), tensor(0.9921277165), tensor(1.4258749485), tensor(0.9361667633), tensor(1.1110552549), tensor(1.0950845480), tensor(0.8833717704), tensor(0.9995331764), tensor(0.8382481933), tensor(1.0517593622), tensor(0.9508718848)]\n",
            "b:  [tensor(0.6940617561), tensor(1.1570663452), tensor(1.3293757439), tensor(1.0583543777), tensor(1.4729135036), tensor(0.8495873213), tensor(1.2416166067), tensor(1.1072568893), tensor(1.6756927967), tensor(0.9448724389), tensor(1.3114618063), tensor(0.7736661434), tensor(1.3160705566), tensor(1.2923946381), tensor(0.9208592176), tensor(0.9627233744), tensor(1.2600044012), tensor(1.6405699253), tensor(1.0145525932), tensor(1.1955168247)]\n",
            "c:  [tensor(-0.0025080047), tensor(0.4161711037), tensor(0.3961681128), tensor(-0.0173459128), tensor(-0.0016352006), tensor(-0.0122265546), tensor(0.0057370737), tensor(0.0060987244), tensor(0.3047181666), tensor(-0.0062205484), tensor(0.0920285210), tensor(-0.1220729724), tensor(-0.0808048323), tensor(0.2879545987), tensor(-0.0007318179), tensor(0.0989314243), tensor(-0.1170828342), tensor(-0.0800786838)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2224810123, -0.1211852580,  0.2788559496,  0.3267109394,\n",
            "        -0.3412328660,  1.3904795647,  0.8619919419,  0.0736470222,\n",
            "        -0.0971528888,  0.9517342448,  0.8475639224,  0.7995053530,\n",
            "         0.0642074347,  1.6978121996,  0.7113362551,  0.8043786287,\n",
            "         0.8773043156,  0.3992803693,  0.1683129668,  0.3828560114])\n",
            "btensor.grad: tensor([-1.1440997124, -0.9227113724, -0.0433206558, -0.8740897775,\n",
            "        -0.2809870839, -0.5159056187, -0.2679693103,  0.3533903360,\n",
            "         0.9568376541, -1.1345218420, -0.6620073318, -0.8630957007,\n",
            "        -0.3268529177, -0.4942920208, -0.6670210361,  0.2135931253,\n",
            "         0.2153654397, -0.0730577111, -0.5016335249, -0.4157796502])\n",
            "ctensor.grad: tensor([-1.4666231871e+00, -2.5853786469e+01, -2.4829536438e+01,\n",
            "         1.6667793691e-01, -6.5395301208e-03, -3.3466007560e-02,\n",
            "        -1.2104453892e-01, -6.6349990666e-02, -1.8992496490e+01,\n",
            "         8.4829425812e-01,  4.7676396370e+00,  1.2181598663e+01,\n",
            "        -6.3384503126e-02, -1.8607505798e+01,  8.1794238091e-01,\n",
            "         4.2381258011e+00,  1.1708112717e+01,  3.9869230986e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1039.9375000000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5312346220), tensor(0.9704948068), tensor(1.0475150347), tensor(1.1667760611), tensor(1.1127164364), tensor(1.3022783995), tensor(0.9696452618), tensor(1.1476664543), tensor(0.9124491215), tensor(1.1227823496), tensor(0.9880105853), tensor(1.4219378233), tensor(0.9360403419), tensor(1.1025338173), tensor(1.0915341377), tensor(0.8795032501), tensor(0.9950851202), tensor(0.8363671303), tensor(1.0510362387), tensor(0.9489305019)]\n",
            "b:  [tensor(0.6997734904), tensor(1.1616184711), tensor(1.3296800852), tensor(1.0628374815), tensor(1.4744130373), tensor(0.8521987796), tensor(1.2431036234), tensor(1.1056721210), tensor(1.6711130142), tensor(0.9504781961), tensor(1.3147698641), tensor(0.7779961228), tensor(1.3178029060), tensor(1.2948241234), tensor(0.9242078662), tensor(0.9617088437), tensor(1.2590163946), tensor(1.6411933899), tensor(1.0170780420), tensor(1.1976330280)]\n",
            "c:  [tensor(-0.0017491971), tensor(0.4288726151), tensor(0.4083645940), tensor(-0.0174081121), tensor(-0.0016326701), tensor(-0.0122112324), tensor(0.0057830606), tensor(0.0061224056), tensor(0.3140646219), tensor(-0.0066601355), tensor(0.0892232880), tensor(-0.1280713975), tensor(-0.0803635642), tensor(0.2971227169), tensor(-0.0011555522), tensor(0.0964126363), tensor(-0.1228536218), tensor(-0.0798910409)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2538690567, -0.1231330633,  0.2862462997,  0.3547010422,\n",
            "        -0.3310992420,  1.3947530985,  0.8666613102,  0.0599160194,\n",
            "        -0.0799018145,  0.9271148443,  0.8234219551,  0.7874182463,\n",
            "         0.0252788067,  1.7042875290,  0.7100768089,  0.7737014294,\n",
            "         0.8896132112,  0.3762066364,  0.1446171403,  0.3882765770])\n",
            "btensor.grad: tensor([-1.1423504353, -0.9104146957, -0.0608658791, -0.8966174722,\n",
            "        -0.2999140024, -0.5222970843, -0.2973964512,  0.3169561625,\n",
            "         0.9159625769, -1.1211529970, -0.6616015434, -0.8660007119,\n",
            "        -0.3464802504, -0.4858998060, -0.6697337627,  0.2029042244,\n",
            "         0.1975911111, -0.1247003078, -0.5050958395, -0.4232427478])\n",
            "ctensor.grad: tensor([-1.5176149607e+00, -2.5403013229e+01, -2.4392992020e+01,\n",
            "         1.2439733744e-01, -5.0611020997e-03, -3.0643567443e-02,\n",
            "        -9.1973394156e-02, -4.7361977398e-02, -1.8692916870e+01,\n",
            "         8.7917405367e-01,  5.6104693413e+00,  1.1996860504e+01,\n",
            "        -8.8253700733e-01, -1.8336208344e+01,  8.4746867418e-01,\n",
            "         5.0375685692e+00,  1.1541581154e+01, -3.7528795004e-01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1038.7110595703, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5248228312), tensor(0.9711050391), tensor(1.0460383892), tensor(1.1648558378), tensor(1.1142997742), tensor(1.2953164577), tensor(0.9652938247), tensor(1.1474353075), tensor(0.9127427340), tensor(1.1182832718), tensor(0.9840328097), tensor(1.4180818796), tensor(0.9361084700), tensor(1.0940054655), tensor(1.0879900455), tensor(0.8757954836), tensor(0.9905827045), tensor(0.8346064091), tensor(1.0504288673), tensor(0.9469489455)]\n",
            "b:  [tensor(0.7054547071), tensor(1.1660820246), tensor(1.3300764561), tensor(1.0674186945), tensor(1.4760001898), tensor(0.8548400402), tensor(1.2447426319), tensor(1.1042813063), tensor(1.6667654514), tensor(0.9559904933), tensor(1.3180612326), tensor(0.7823302150), tensor(1.3196296692), tensor(1.2972048521), tensor(0.9275584817), tensor(0.9607507586), tensor(1.2581176758), tensor(1.6420818567), tensor(1.0196119547), tensor(1.1997741461)]\n",
            "c:  [tensor(-0.0009670318), tensor(0.4413471520), tensor(0.4203408957), tensor(-0.0174499564), tensor(-0.0016309030), tensor(-0.0121990256), tensor(0.0058144676), tensor(0.0061376449), tensor(0.3232522905), tensor(-0.0071160952), tensor(0.0860080719), tensor(-0.1339502335), tensor(-0.0795321465), tensor(0.3061455786), tensor(-0.0015948938), tensor(0.0935045406), tensor(-0.1285145432), tensor(-0.0793351158)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.2823681831, -0.1220417917,  0.2953284383,  0.3840535879,\n",
            "        -0.3166666627,  1.3923805952,  0.8702820539,  0.0462397337,\n",
            "        -0.0587183833,  0.8998080492,  0.7955579162,  0.7711958289,\n",
            "        -0.0136296749,  1.7056754827,  0.7088260651,  0.7415549755,\n",
            "         0.9004888535,  0.3521493077,  0.1214717627,  0.3963130713])\n",
            "btensor.grad: tensor([-1.1362476349, -0.8927059174, -0.0792731047, -0.9162491560,\n",
            "        -0.3174413443, -0.5282483697, -0.3277958632,  0.2781550884,\n",
            "         0.8695136309, -1.1024564505, -0.6582648754, -0.8668219447,\n",
            "        -0.3653518260, -0.4761350155, -0.6701208949,  0.1916218996,\n",
            "         0.1797536314, -0.1777049303, -0.5067921877, -0.4282249212])\n",
            "ctensor.grad: tensor([-1.5643305779e+00, -2.4949089050e+01, -2.3952579498e+01,\n",
            "         8.3687074482e-02, -3.5342012998e-03, -2.4412849918e-02,\n",
            "        -6.2813684344e-02, -3.0478559434e-02, -1.8375362396e+01,\n",
            "         9.1191887856e-01,  6.4304337502e+00,  1.1757673264e+01,\n",
            "        -1.6628396511e+00, -1.8045743942e+01,  8.7868309021e-01,\n",
            "         5.8161859512e+00,  1.1321853638e+01, -1.1118428707e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1037.5155029297, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5182836056), tensor(0.9716956019), tensor(1.0445085764), tensor(1.1627840996), tensor(1.1157907248), tensor(1.2883988619), tensor(0.9609287381), tensor(1.1472713947), tensor(0.9129130244), tensor(1.1139320135), tensor(0.9802112579), tensor(1.4143261909), tensor(0.9363696575), tensor(1.0854941607), tensor(1.0844507217), tensor(0.8722524643), tensor(0.9860326052), tensor(0.8329695463), tensor(1.0499330759), tensor(0.9449145198)]\n",
            "b:  [tensor(0.7110856771), tensor(1.1704328060), tensor(1.3305695057), tensor(1.0720843077), tensor(1.4776680470), tensor(0.8575103283), tensor(1.2465387583), tensor(1.1030946970), tensor(1.6626764536), tensor(0.9613856673), tensor(1.3213231564), tensor(0.7866597772), tensor(1.3215477467), tensor(1.2995315790), tensor(0.9309012294), tensor(0.9598522782), tensor(1.2573083639), tensor(1.6432398558), tensor(1.0221472979), tensor(1.2019292116)]\n",
            "c:  [tensor(-0.0001635809), tensor(0.4535959363), tensor(0.4320978820), tensor(-0.0174722709), tensor(-0.0016299238), tensor(-0.0121915126), tensor(0.0058314204), tensor(0.0061454317), tensor(0.3322746456), tensor(-0.0075893900), tensor(0.0823990479), tensor(-0.1396822184), tensor(-0.0783331096), tensor(0.3150160909), tensor(-0.0020507127), tensor(0.0902221650), tensor(-0.1340389103), tensor(-0.0784326568)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3078505993, -0.1181085110,  0.3059720397,  0.4143358469,\n",
            "        -0.2981994152,  1.3835120201,  0.8730229139,  0.0327939987,\n",
            "        -0.0340521932,  0.8702430129,  0.7643045187,  0.7511451244,\n",
            "        -0.0522434711,  1.7022526264,  0.7078646421,  0.7086056471,\n",
            "         0.9100242257,  0.3273686171,  0.0991600156,  0.4068807364])\n",
            "btensor.grad: tensor([-1.1261916161, -0.8701497316, -0.0986015797, -0.9331330061,\n",
            "        -0.3335784078, -0.5340548158, -0.3592160940,  0.2373238802,\n",
            "         0.8178038597, -1.0790336132, -0.6523864269, -0.8659093976,\n",
            "        -0.3836121857, -0.4653533697, -0.6685501337,  0.1796984076,\n",
            "         0.1618614346, -0.2315958142, -0.5070769787, -0.4310107231])\n",
            "ctensor.grad: tensor([-1.6069016457e+00, -2.4497541428e+01, -2.3513950348e+01,\n",
            "         4.4630371034e-02, -1.9583341200e-03, -1.5025977977e-02,\n",
            "        -3.3905941993e-02, -1.5573672950e-02, -1.8044698715e+01,\n",
            "         9.4658941031e-01,  7.2180442810e+00,  1.1463982582e+01,\n",
            "        -2.3980712891e+00, -1.7741039276e+01,  9.1163778305e-01,\n",
            "         6.5647573471e+00,  1.1048738480e+01, -1.8049153090e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1036.3475341797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5116319656), tensor(0.9722525477), tensor(1.0429174900), tensor(1.1605577469), tensor(1.1171700954), tensor(1.2815562487), tensor(0.9565528631), tensor(1.1471717358), tensor(0.9129440784), tensor(1.1097370386), tensor(0.9765602946), tensor(1.4106874466), tensor(0.9368201494), tensor(1.0770219564), tensor(1.0809127092), tensor(0.8688741922), tensor(0.9814403057), tensor(0.8314580917), tensor(1.0495423079), tensor(0.9428146482)]\n",
            "b:  [tensor(0.7166478634), tensor(1.1746485233), tensor(1.3311631680), tensor(1.0768204927), tensor(1.4794089794), tensor(0.8602093458), tensor(1.2484964132), tensor(1.1021196842), tensor(1.6588696241), tensor(0.9666423798), tensor(1.3245440722), tensor(0.7909770608), tensor(1.3235540390), tensor(1.3018003702), tensor(0.9342274070), tensor(0.9590159059), tensor(1.2565881014), tensor(1.6446683407), tensor(1.0246779919), tensor(1.2040877342)]\n",
            "c:  [tensor(0.0006595698), tensor(0.4656221271), tensor(0.4436384141), tensor(-0.0174759123), tensor(-0.0016297576), tensor(-0.0121901250), tensor(0.0058342125), tensor(0.0061466540), tensor(0.3411262035), tensor(-0.0080810189), tensor(0.0784149542), tensor(-0.1452413052), tensor(-0.0767944306), tensor(0.3237283230), tensor(-0.0025239112), tensor(0.0865830183), tensor(-0.1394010931), tensor(-0.0772105306)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3303394318, -0.1113891155,  0.3182126582,  0.4452648759,\n",
            "        -0.2758760154,  1.3685263395,  0.8751691580,  0.0199288130,\n",
            "        -0.0062062144,  0.8389980793,  0.7301973104,  0.7277607918,\n",
            "        -0.0900961161,  1.6944378614,  0.7075964212,  0.6756484509,\n",
            "         0.9184543490,  0.3022927046,  0.0781494975,  0.4199758768])\n",
            "btensor.grad: tensor([-1.1124405861, -0.8431375623, -0.1187437177, -0.9472329021,\n",
            "        -0.3481756449, -0.5397994518, -0.3915273547,  0.1950016022,\n",
            "         0.7613630295, -1.0513372421, -0.6441910267, -0.8634532094,\n",
            "        -0.4012471139, -0.4537596703, -0.6652340889,  0.1672728658,\n",
            "         0.1440554559, -0.2856968045, -0.5061358213, -0.4317073822])\n",
            "ctensor.grad: tensor([-1.6463013887e+00, -2.4052364349e+01, -2.3081058502e+01,\n",
            "         7.2829532437e-03, -3.3238640754e-04, -2.7759321965e-03,\n",
            "        -5.5838390253e-03, -2.4446784519e-03, -1.7703088760e+01,\n",
            "         9.8325771093e-01,  7.9681839943e+00,  1.1118179321e+01,\n",
            "        -3.0773537159e+00, -1.7424455643e+01,  9.4639670849e-01,\n",
            "         7.2782964706e+00,  1.0724361420e+01, -2.4442486763e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1035.2056884766, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.5048836470), tensor(0.9727644920), tensor(1.0412591696), tensor(1.1581766605), tensor(1.1184211969), tensor(1.2748180628), tensor(0.9521700740), tensor(1.1471334696), tensor(0.9128240347), tensor(1.1057053804), tensor(0.9730929732), tensor(1.4071807861), tensor(0.9374560118), tensor(1.0686099529), tensor(1.0773723125), tensor(0.8656589985), tensor(0.9768119454), tensor(0.8300734758), tensor(1.0492497683), tensor(0.9406389594)]\n",
            "b:  [tensor(0.7221256495), tensor(1.1787108183), tensor(1.3318624496), tensor(1.0816146135), tensor(1.4812159538), tensor(0.8629385233), tensor(1.2506206036), tensor(1.1013625860), tensor(1.6553671360), tensor(0.9717432261), tensor(1.3277151585), tensor(0.7952765822), tensor(1.3256466389), tensor(1.3040093184), tensor(0.9375308156), tensor(0.9582450390), tensor(1.2559571266), tensor(1.6463663578), tensor(1.0272002220), tensor(1.2062416077)]\n",
            "c:  [tensor(0.0015004432), tensor(0.4774302244), tensor(0.4549667537), tensor(-0.0174617451), tensor(-0.0016304307), tensor(-0.0121961413), tensor(0.0058232867), tensor(0.0061420556), tensor(0.3498045206), tensor(-0.0085919714), tensor(0.0740808621), tensor(-0.1506008953), tensor(-0.0749442726), tensor(0.3322793245), tensor(-0.0030153811), tensor(0.0826107040), tensor(-0.1445748955), tensor(-0.0756957158)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3496657610, -0.1023854613,  0.3316677511,  0.4762078524,\n",
            "        -0.2502253652,  1.3476405144,  0.8765602112,  0.0076485872,\n",
            "         0.0240064263,  0.8063242435,  0.6934644580,  0.7013421059,\n",
            "        -0.1271752119,  1.6823942661,  0.7080829144,  0.6430379152,\n",
            "         0.9256733656,  0.2769236565,  0.0585047007,  0.4351338148])\n",
            "btensor.grad: tensor([-1.0955567360, -0.8124542236, -0.1398457289, -0.9588227272,\n",
            "        -0.3613917530, -0.5458353758, -0.4248264134,  0.1514286995,\n",
            "         0.7004956007, -1.0201716423, -0.6342133284, -0.8599058986,\n",
            "        -0.4185183048, -0.4417846203, -0.6606773138,  0.1541712284,\n",
            "         0.1261904538, -0.3395944834, -0.5044466257, -0.4307643771])\n",
            "ctensor.grad: tensor([-1.6817467213e+00, -2.3616199493e+01, -2.2656681061e+01,\n",
            "        -2.8332697228e-02,  1.3461507624e-03,  1.2033613399e-02,\n",
            "         2.1851986647e-02,  9.1965422034e-03, -1.7356620789e+01,\n",
            "         1.0219053030e+00,  8.6681785583e+00,  1.0719167709e+01,\n",
            "        -3.7003192902e+00, -1.7102029800e+01,  9.8293954134e-01,\n",
            "         7.9446272850e+00,  1.0347613335e+01, -3.0296235085e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1034.0928955078, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4980529547), tensor(0.9732189178), tensor(1.0395259857), tensor(1.1556409597), tensor(1.1195273399), tensor(1.2682098150), tensor(0.9477818012), tensor(1.1471505165), tensor(0.9125413299), tensor(1.1018402576), tensor(0.9698179960), tensor(1.4038170576), tensor(0.9382694364), tensor(1.0602761507), tensor(1.0738227367), tensor(0.8626002669), tensor(0.9721512198), tensor(0.8288133144), tensor(1.0490446091), tensor(0.9383764863)]\n",
            "b:  [tensor(0.7275032997), tensor(1.1826019287), tensor(1.3326698542), tensor(1.0864521265), tensor(1.4830797911), tensor(0.8656978011), tensor(1.2529135942), tensor(1.1008250713), tensor(1.6521865129), tensor(0.9766716957), tensor(1.3308268785), tensor(0.7995524406), tensor(1.3278220892), tensor(1.3061559200), tensor(0.9408047199), tensor(0.9575405717), tensor(1.2554136515), tensor(1.6483277082), tensor(1.0297095776), tensor(1.2083814144)]\n",
            "c:  [tensor(0.0023582922), tensor(0.4890252352), tensor(0.4660877585), tensor(-0.0174306538), tensor(-0.0016319703), tensor(-0.0122106830), tensor(0.0057992316), tensor(0.0061322073), tensor(0.3583055735), tensor(-0.0091232788), tensor(0.0694200844), tensor(-0.1557379812), tensor(-0.0728204176), tensor(0.3406648636), tensor(-0.0035260522), tensor(0.0783272907), tensor(-0.1495374888), tensor(-0.0739241689)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3661364317, -0.0908904076,  0.3466425836,  0.5071491003,\n",
            "        -0.2212387323,  1.3216470480,  0.8776538372, -0.0034024715,\n",
            "         0.0565462112,  0.7730176449,  0.6549975872,  0.6727348566,\n",
            "        -0.1626791954,  1.6667549610,  0.7099213600,  0.6117504835,\n",
            "         0.9321452379,  0.2520350218,  0.0410321355,  0.4524958134])\n",
            "btensor.grad: tensor([-1.0755267143, -0.7782155871, -0.1614786386, -0.9674937725,\n",
            "        -0.3727725148, -0.5518527627, -0.4586032927,  0.1075078249,\n",
            "         0.6361252069, -0.9856922626, -0.6223518848, -0.8551704884,\n",
            "        -0.4350825548, -0.4293214083, -0.6547782421,  0.1408985257,\n",
            "         0.1086865291, -0.3922684193, -0.5018764734, -0.4279677868])\n",
            "ctensor.grad: tensor([-1.7156980038e+00, -2.3190004349e+01, -2.2241985321e+01,\n",
            "        -6.2182329595e-02,  3.0792281032e-03,  2.9084082693e-02,\n",
            "         4.8110149801e-02,  1.9696909934e-02, -1.7002132416e+01,\n",
            "         1.0626150370e+00,  9.3215475082e+00,  1.0274168015e+01,\n",
            "        -4.2477064133e+00, -1.6771093369e+01,  1.0213421583e+00,\n",
            "         8.5668315887e+00,  9.9251747131e+00, -3.5431010723e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1033.0053710938, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4911572933), tensor(0.9736104608), tensor(1.0377165079), tensor(1.1529574394), tensor(1.1204782724), tensor(1.2617579699), tensor(0.9433951378), tensor(1.1472202539), tensor(0.9120935798), tensor(1.0981471539), tensor(0.9667464495), tensor(1.4006083012), tensor(0.9392570853), tensor(1.0520405769), tensor(1.0702607632), tensor(0.8596944213), tensor(0.9674662352), tensor(0.8276801109), tensor(1.0489200354), tensor(0.9360236526)]\n",
            "b:  [tensor(0.7327710986), tensor(1.1863118410), tensor(1.3335913420), tensor(1.0913231373), tensor(1.4849957228), tensor(0.8684917092), tensor(1.2553808689), tensor(1.1005109549), tensor(1.6493462324), tensor(0.9814189076), tensor(1.3338756561), tensor(0.8038035035), tensor(1.3300809860), tensor(1.3082424402), tensor(0.9440479279), tensor(0.9569074512), tensor(1.2549597025), tensor(1.6505470276), tensor(1.0322070122), tensor(1.2105036974)]\n",
            "c:  [tensor(0.0032300421), tensor(0.5004122257), tensor(0.4770064652), tensor(-0.0173834879), tensor(-0.0016344057), tensor(-0.0122346813), tensor(0.0057627698), tensor(0.0061175269), tensor(0.3666328490), tensor(-0.0096758567), tensor(0.0644707978), tensor(-0.1606251001), tensor(-0.0704484954), tensor(0.3488878310), tensor(-0.0040567489), tensor(0.0737689584), tensor(-0.1542617381), tensor(-0.0719201863)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3791275024, -0.0783134401,  0.3618893027,  0.5367115140,\n",
            "        -0.1901881695,  1.2903665304,  0.8773377538, -0.0139410496,\n",
            "         0.0895453095,  0.7386119962,  0.6143143773,  0.6417528987,\n",
            "        -0.1975311041,  1.6471079588,  0.7124018669,  0.5811668634,\n",
            "         0.9369977713,  0.2266348004,  0.0249041319,  0.4705672264])\n",
            "btensor.grad: tensor([-1.0535545349, -0.7419937253, -0.1842958927, -0.9742082953,\n",
            "        -0.3831763268, -0.5587819815, -0.4934638441,  0.0628248453,\n",
            "         0.5680605769, -0.9494389296, -0.6097646952, -0.8502121568,\n",
            "        -0.4517829716, -0.4172986746, -0.6486363411,  0.1266239285,\n",
            "         0.0907982439, -0.4438682497, -0.4994945526, -0.4244660735])\n",
            "ctensor.grad: tensor([-1.7434993982e+00, -2.2773946762e+01, -2.1837390900e+01,\n",
            "        -9.4330787659e-02,  4.8708510585e-03,  4.7996032983e-02,\n",
            "         7.2923354805e-02,  2.9360607266e-02, -1.6654541016e+01,\n",
            "         1.1051560640e+00,  9.8985719681e+00,  9.7742424011e+00,\n",
            "        -4.7438406944e+00, -1.6445957184e+01,  1.0613930225e+00,\n",
            "         9.1166610718e+00,  9.4484872818e+00, -4.0079665184e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1031.9455566406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4842063189), tensor(0.9739230275), tensor(1.0358188152), tensor(1.1501245499), tensor(1.1212561131), tensor(1.2554777861), tensor(0.9390063286), tensor(1.1473286152), tensor(0.9114681482), tensor(1.0946201086), tensor(0.9638761878), tensor(1.3975551128), tensor(0.9404008389), tensor(1.0439137220), tensor(1.0666729212), tensor(0.8569247127), tensor(0.9627553821), tensor(0.8266617656), tensor(1.0488554239), tensor(0.9335671067)]\n",
            "b:  [tensor(0.7379116416), tensor(1.1898221970), tensor(1.3346234560), tensor(1.0962069035), tensor(1.4869481325), tensor(0.8713145256), tensor(1.2580173016), tensor(1.1004108191), tensor(1.6468520164), tensor(0.9859679937), tensor(1.3368493319), tensor(0.8080210090), tensor(1.3324142694), tensor(1.3102633953), tensor(0.9472507834), tensor(0.9563394785), tensor(1.2545874119), tensor(1.6530064344), tensor(1.0346847773), tensor(1.2125952244)]\n",
            "c:  [tensor(0.0041182181), tensor(0.5115953088), tensor(0.4877270460), tensor(-0.0173211861), tensor(-0.0016377665), tensor(-0.0122689893), tensor(0.0057147108), tensor(0.0060981177), tensor(0.3747758567), tensor(-0.0102508068), tensor(0.0592467599), tensor(-0.1652498543), tensor(-0.0678901002), tensor(0.3569380939), tensor(-0.0046084700), tensor(0.0689489320), tensor(-0.1587347388), tensor(-0.0697418973)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3901972771, -0.0625155419,  0.3795432448,  0.5665779114,\n",
            "        -0.1555659175,  1.2560431957,  0.8777568340, -0.0216649771,\n",
            "         0.1250898242,  0.7054052949,  0.5740513206,  0.6106475592,\n",
            "        -0.2287564278,  1.6253618002,  0.7175590992,  0.5539445877,\n",
            "         0.9421687722,  0.2036742568,  0.0129264593,  0.4913046360])\n",
            "btensor.grad: tensor([-1.0281105042, -0.7020642757, -0.2064188719, -0.9767490625,\n",
            "        -0.3904931545, -0.5645662546, -0.5272824764,  0.0200283527,\n",
            "         0.4988373518, -0.9098193645, -0.5947268009, -0.8435034752,\n",
            "        -0.4666598737, -0.4041970968, -0.6405684948,  0.1135888696,\n",
            "         0.0744544566, -0.4918752313, -0.4955611229, -0.4183071852])\n",
            "ctensor.grad: tensor([-1.7763522863e+00, -2.2366176605e+01, -2.1441164017e+01,\n",
            "        -1.2460239232e-01,  6.7215906456e-03,  6.8615421653e-02,\n",
            "         9.6118405461e-02,  3.8818567991e-02, -1.6285991669e+01,\n",
            "         1.1499007940e+00,  1.0448072433e+01,  9.2495012283e+00,\n",
            "        -5.1167945862e+00, -1.6100540161e+01,  1.1034420729e+00,\n",
            "         9.6400518417e+00,  8.9459886551e+00, -4.3565821648e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1030.9134521484, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4772273302), tensor(0.9741723537), tensor(1.0338510275), tensor(1.1471688747), tensor(1.1218690872), tensor(1.2494009733), tensor(0.9346412420), tensor(1.1474863291), tensor(0.9106897116), tensor(1.0912754536), tensor(0.9612293243), tensor(1.3946754932), tensor(0.9417144656), tensor(1.0359246731), tensor(1.0630698204), tensor(0.8543022275), tensor(0.9580419660), tensor(0.8257789612), tensor(1.0488598347), tensor(0.9310265779)]\n",
            "b:  [tensor(0.7429322600), tensor(1.1931450367), tensor(1.3357825279), tensor(1.1011085510), tensor(1.4889465570), tensor(0.8741848469), tensor(1.2608381510), tensor(1.1005380154), tensor(1.6447285414), tensor(0.9903312325), tensor(1.3397604227), tensor(0.8122176528), tensor(1.3348358870), tensor(1.3122342825), tensor(0.9504277110), tensor(0.9558553696), tensor(1.2543109655), tensor(1.6557070017), tensor(1.0371592045), tensor(1.2146693468)]\n",
            "c:  [tensor(0.0050118593), tensor(0.5225781202), tensor(0.4982534349), tensor(-0.0172444098), tensor(-0.0016420853), tensor(-0.0123139732), tensor(0.0056560901), tensor(0.0060743247), tensor(0.3827610314), tensor(-0.0108486302), tensor(0.0538313352), tensor(-0.1695684940), tensor(-0.0651233941), tensor(0.3648406267), tensor(-0.0051816604), tensor(0.0639463738), tensor(-0.1629137695), tensor(-0.0673682019)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3957920074e+00, -4.9870714545e-02,  3.9356830716e-01,\n",
            "         5.9114116430e-01, -1.2260150909e-01,  1.2153742313e+00,\n",
            "         8.7301230431e-01, -3.1552672386e-02,  1.5568459034e-01,\n",
            "         6.6893553734e-01,  5.2937591076e-01,  5.7593154907e-01,\n",
            "        -2.6272785664e-01,  1.5978125334e+00,  7.2062397003e-01,\n",
            "         5.2450060844e-01,  9.4268620014e-01,  1.7656385899e-01,\n",
            "        -8.8709592819e-04,  5.0810551643e-01])\n",
            "btensor.grad: tensor([-1.0041234493, -0.6645668149, -0.2318134308, -0.9803181887,\n",
            "        -0.3996735513, -0.5740593076, -0.5641790628, -0.0254344940,\n",
            "         0.4246908426, -0.8726506233, -0.5822299719, -0.8393319845,\n",
            "        -0.4843319952, -0.3941727877, -0.6353827119,  0.0968213677,\n",
            "         0.0553006455, -0.5401053429, -0.4948866367, -0.4148283005])\n",
            "ctensor.grad: tensor([-1.7872819901e+00, -2.1965606689e+01, -2.1052799225e+01,\n",
            "        -1.5355391800e-01,  8.6376350373e-03,  8.9968472719e-02,\n",
            "         1.1724116653e-01,  4.7585606575e-02, -1.5970334053e+01,\n",
            "         1.1956465244e+00,  1.0830846786e+01,  8.6372919083e+00,\n",
            "        -5.5334053040e+00, -1.5805050850e+01,  1.1463811398e+00,\n",
            "         1.0005110741e+01,  8.3580465317e+00, -4.7473850250e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1029.9085693359, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4702005386), tensor(0.9742982388), tensor(1.0317580700), tensor(1.1440523863), tensor(1.1222666502), tensor(1.2435042858), tensor(0.9302539825), tensor(1.1476371288), tensor(0.9096990824), tensor(1.0880662203), tensor(0.9587571621), tensor(1.3919318914), tensor(0.9431245923), tensor(1.0280498266), tensor(1.0594003201), tensor(0.8517622948), tensor(0.9532874823), tensor(0.8249641657), tensor(1.0488612652), tensor(0.9283469319)]\n",
            "b:  [tensor(0.7477833629), tensor(1.1962234974), tensor(1.3370304108), tensor(1.1059651375), tensor(1.4909360409), tensor(0.8770576119), tensor(1.2637995481), tensor(1.1008373499), tensor(1.6429383755), tensor(0.9944561720), tensor(1.3425614834), tensor(0.8163535595), tensor(1.3372988701), tensor(1.3141189814), tensor(0.9535344839), tensor(0.9554073215), tensor(1.2540864944), tensor(1.6585892439), tensor(1.0395872593), tensor(1.2166737318)]\n",
            "c:  [tensor(0.0059349285), tensor(0.5333616138), tensor(0.5085867643), tensor(-0.0171546340), tensor(-0.0016473925), tensor(-0.0123709207), tensor(0.0055875541), tensor(0.0060448912), tensor(0.3905201554), tensor(-0.0114711672), tensor(0.0481330417), tensor(-0.1736273170), tensor(-0.0623483248), tensor(0.3725307286), tensor(-0.0057780202), tensor(0.0586752370), tensor(-0.1668414772), tensor(-0.0649880245)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.4053531885e+00, -2.5171488523e-02,  4.1858816147e-01,\n",
            "         6.2330746651e-01, -7.9510450363e-02,  1.1793422699e+00,\n",
            "         8.7745618820e-01, -3.0164003372e-02,  1.9812107086e-01,\n",
            "         6.4185154438e-01,  4.9443018436e-01,  5.4872393608e-01,\n",
            "        -2.8202044964e-01,  1.5749641657e+00,  7.3389935493e-01,\n",
            "         5.0798594952e-01,  9.5089858770e-01,  1.6296052933e-01,\n",
            "        -2.9098987579e-04,  5.3593420982e-01])\n",
            "btensor.grad: tensor([-0.9702177048, -0.6156951189, -0.2495745420, -0.9713280201,\n",
            "        -0.3979037404, -0.5745574236, -0.5922829509, -0.0598726273,\n",
            "         0.3580384851, -0.8249893785, -0.5602186918, -0.8271798491,\n",
            "        -0.4925954342, -0.3769471645, -0.6213579178,  0.0896145105,\n",
            "         0.0448858663, -0.5764481425, -0.4856047630, -0.4008710980])\n",
            "ctensor.grad: tensor([-1.8461381197e+00, -2.1566974640e+01, -2.0666603088e+01,\n",
            "        -1.7955103517e-01,  1.0614398867e-02,  1.1389467865e-01,\n",
            "         1.3707217574e-01,  5.8867346495e-02, -1.5518243790e+01,\n",
            "         1.2450745106e+00,  1.1396587372e+01,  8.1176452637e+00,\n",
            "        -5.5501413345e+00, -1.5380175591e+01,  1.1927194595e+00,\n",
            "         1.0542272568e+01,  7.8554205894e+00, -4.7603573799e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1028.9338378906, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4632321596), tensor(0.9744533300), tensor(1.0296893120), tensor(1.1409193277), tensor(1.1225675344), tensor(1.2379008532), tensor(0.9259994030), tensor(1.1479045153), tensor(0.9086789489), tensor(1.0851129293), tensor(0.9565957189), tensor(1.3894271851), tensor(0.9447897673), tensor(1.0204039812), tensor(1.0557826757), tensor(0.8494461775), tensor(0.9486248493), tensor(0.8243877888), tensor(1.0490049124), tensor(0.9256886244)]\n",
            "b:  [tensor(0.7525752783), tensor(1.1991976500), tensor(1.3384721279), tensor(1.1108957529), tensor(1.4930343628), tensor(0.8800569177), tensor(1.2670103312), tensor(1.1014283895), tensor(1.6415979862), tensor(0.9984752536), tensor(1.3453713655), tensor(0.8205329776), tensor(1.3399194479), tensor(1.3160216808), tensor(0.9566881061), tensor(0.9551236033), tensor(1.2540237904), tensor(1.6617456675), tensor(1.0420879126), tensor(1.2187364101)]\n",
            "c:  [tensor(0.0068162931), tensor(0.5439476371), tensor(0.5187299848), tensor(-0.0170507468), tensor(-0.0016537226), tensor(-0.0124373166), tensor(0.0055114916), tensor(0.0060131932), tensor(0.3982468545), tensor(-0.0121162925), tensor(0.0425495356), tensor(-0.1772324592), tensor(-0.0591509640), tensor(0.3801920712), tensor(-0.0063955612), tensor(0.0535123907), tensor(-0.1703323722), tensor(-0.0622082129)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3936853409, -0.0310219824,  0.4137533903,  0.6266077161,\n",
            "        -0.0601867437,  1.1206803322,  0.8509116769, -0.0534793139,\n",
            "         0.2040256858,  0.5906611681,  0.4322878718,  0.5009419322,\n",
            "        -0.3330292702,  1.5291594267,  0.7235177755,  0.4632179737,\n",
            "         0.9325218201,  0.1152746081, -0.0287383795,  0.5316644907])\n",
            "btensor.grad: tensor([-0.9583820701, -0.5948394537, -0.2883387804, -0.9861271381,\n",
            "        -0.4196579158, -0.5998576880, -0.6421517134, -0.1182159185,\n",
            "         0.2680795193, -0.8038171530, -0.5619810820, -0.8358777761,\n",
            "        -0.5241121054, -0.3805435896, -0.6307222843,  0.0567404628,\n",
            "         0.0125380605, -0.6312761903, -0.5001325607, -0.4125363827])\n",
            "ctensor.grad: tensor([-1.7627288103e+00, -2.1172027588e+01, -2.0286441803e+01,\n",
            "        -2.0777551830e-01,  1.2660153210e-02,  1.3279181719e-01,\n",
            "         1.5212459862e-01,  6.3395969570e-02, -1.5453418732e+01,\n",
            "         1.2902506590e+00,  1.1167011261e+01,  7.2102770805e+00,\n",
            "        -6.3947238922e+00, -1.5322656631e+01,  1.2350819111e+00,\n",
            "         1.0325691223e+01,  6.9817862511e+00, -5.5596218109e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1027.9893798828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4560655355), tensor(0.9741843939), tensor(1.0272128582), tensor(1.1373989582), tensor(1.1224131584), tensor(1.2322969437), tensor(0.9214596152), tensor(1.1478934288), tensor(0.9071298838), tensor(1.0820486546), tensor(0.9543408751), tensor(1.3868494034), tensor(0.9461959600), tensor(1.0126996040), tensor(1.0518498421), tensor(0.8468997478), tensor(0.9436983466), tensor(0.8235324025), tensor(1.0488101244), tensor(0.9226034284)]\n",
            "b:  [tensor(0.7569679022), tensor(1.2016540766), tensor(1.3397974968), tensor(1.1154991388), tensor(1.4948670864), tensor(0.8828223944), tensor(1.2701332569), tensor(1.1019209623), tensor(1.6403652430), tensor(1.0020016432), tensor(1.3478358984), tensor(0.8244494200), tensor(1.3423424959), tensor(1.3176470995), tensor(0.9595465064), tensor(0.9546264410), tensor(1.2537919283), tensor(1.6648250818), tensor(1.0443179607), tensor(1.2204712629)]\n",
            "c:  [tensor(0.0078583639), tensor(0.5543308854), tensor(0.5286764503), tensor(-0.0169400368), tensor(-0.0016611131), tensor(-0.0125230197), tensor(0.0054243566), tensor(0.0059661842), tensor(0.4053739309), tensor(-0.0127929496), tensor(0.0360544845), tensor(-0.1809034795), tensor(-0.0569053404), tensor(0.3872866929), tensor(-0.0070426282), tensor(0.0474871770), tensor(-0.1738787591), tensor(-0.0603306144)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.4333190918,  0.0537813157,  0.4952845573,  0.7040836811,\n",
            "         0.0308645368,  1.1207726002,  0.9079573750,  0.0022253990,\n",
            "         0.3098120093,  0.6128526330,  0.4509652257,  0.5155543089,\n",
            "        -0.2812339067,  1.5408710241,  0.7865685225,  0.5092833042,\n",
            "         0.9853038192,  0.1710755229,  0.0389679074,  0.6170370579])\n",
            "btensor.grad: tensor([-0.8785300255, -0.4912961721, -0.2650740147, -0.9206683636,\n",
            "        -0.3665351272, -0.5530938506, -0.6245863438, -0.0985109806,\n",
            "         0.2465420961, -0.7052689791, -0.4929047823, -0.7832927704,\n",
            "        -0.4846158624, -0.3250795603, -0.5716784596,  0.0994285345,\n",
            "         0.0463687927, -0.6158909798, -0.4460183382, -0.3469808102])\n",
            "ctensor.grad: tensor([-2.0841410160e+00, -2.0766513824e+01, -1.9892980576e+01,\n",
            "        -2.2142148018e-01,  1.4781023376e-02,  1.7140613496e-01,\n",
            "         1.7427012324e-01,  9.4017513096e-02, -1.4254142761e+01,\n",
            "         1.3533133268e+00,  1.2990104675e+01,  7.3420486450e+00,\n",
            "        -4.4912471771e+00, -1.4189238548e+01,  1.2941343784e+00,\n",
            "         1.2050424576e+01,  7.0927762985e+00, -3.7551972866e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1027.0798339844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4495350122), tensor(0.9748778343), tensor(1.0256614685), tensor(1.1346802711), tensor(1.1228990555), tensor(1.2276182175), tensor(0.9179603457), tensor(1.1487729549), tensor(0.9066303372), tensor(1.0799810886), tensor(0.9532194734), tensor(1.3851317167), tensor(0.9488590956), tensor(1.0058641434), tensor(1.0487133265), tensor(0.8454824090), tensor(0.9396383166), tensor(0.8239636421), tensor(1.0496847630), tensor(0.9205018878)]\n",
            "b:  [tensor(0.7619792223), tensor(1.2048532963), tensor(1.3419406414), tensor(1.1209313869), tensor(1.4975357056), tensor(0.8864411712), tensor(1.2741625309), tensor(1.1034467220), tensor(1.6402680874), tensor(1.0062186718), tensor(1.3510252237), tensor(0.8290310502), tensor(1.3456307650), tensor(1.3198999166), tensor(0.9631489515), tensor(0.9550537467), tensor(1.2543793917), tensor(1.6687953472), tensor(1.0473240614), tensor(1.2230408192)]\n",
            "c:  [tensor(0.0084345294), tensor(0.5645112395), tensor(0.5384330153), tensor(-0.0168011747), tensor(-0.0016695486), tensor(-0.0125933792), tensor(0.0053443392), tensor(0.0059481882), tensor(0.4136654735), tensor(-0.0134759899), tensor(0.0320012383), tensor(-0.1829765439), tensor(-0.0514828004), tensor(0.3954886794), tensor(-0.0076957652), tensor(0.0437756702), tensor(-0.1758970916), tensor(-0.0554397330)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.3061082363, -0.1386909932,  0.3102671206,  0.5437456369,\n",
            "        -0.0971730351,  0.9357414246,  0.6998519301, -0.1759051085,\n",
            "         0.0999082327,  0.4135190547,  0.2242817879,  0.3435271382,\n",
            "        -0.5326281786,  1.3670860529,  0.6273044348,  0.2834702730,\n",
            "         0.8120061159, -0.0862451792, -0.1749188900,  0.4203110933])\n",
            "btensor.grad: tensor([-1.0022616386, -0.6398381591, -0.4286252558, -1.0864410400,\n",
            "        -0.5337295532, -0.7237535119, -0.8058602214, -0.3051508665,\n",
            "         0.0194401145, -0.8433963656, -0.6378680468, -0.9163318872,\n",
            "        -0.6576519608, -0.4505583048, -0.7204939127, -0.0854582787,\n",
            "        -0.1174901426, -0.7940453887, -0.6012115479, -0.5139028430])\n",
            "ctensor.grad: tensor([-1.1523307562e+00, -2.0360763550e+01, -1.9513175964e+01,\n",
            "        -2.7772262692e-01,  1.6870815307e-02,  1.4071992040e-01,\n",
            "         1.6003450751e-01,  3.5991542041e-02, -1.6583089828e+01,\n",
            "         1.3660800457e+00,  8.1064910889e+00,  4.1461315155e+00,\n",
            "        -1.0845081329e+01, -1.6403968811e+01,  1.3062741756e+00,\n",
            "         7.4230122566e+00,  4.0366625786e+00, -9.7817602158e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1026.2537841797, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4410330057), tensor(0.9720917344), tensor(1.0207860470), tensor(1.1290314198), tensor(1.1205139160), tensor(1.2209063768), tensor(0.9113021493), tensor(1.1467542648), tensor(0.9021868110), tensor(1.0753350258), tensor(0.9492772818), tensor(1.3812445402), tensor(0.9478647709), tensor(0.9969735742), tensor(1.0427935123), tensor(0.8407832384), tensor(0.9328607321), tensor(0.8206396699), tensor(1.0470654964), tensor(0.9149073958)]\n",
            "b:  [tensor(0.7643298507), tensor(1.2047637701), tensor(1.3418889046), tensor(1.1234273911), tensor(1.4974848032), tensor(0.8874321580), tensor(1.2758702040), tensor(1.1023223400), tensor(1.6379716396), tensor(1.0073394775), tensor(1.3515026569), tensor(0.8312960267), tensor(1.3463453054), tensor(1.3198916912), tensor(0.9641594291), tensor(0.9527589083), tensor(1.2526096106), tensor(1.6704592705), tensor(1.0477538109), tensor(1.2227088213)]\n",
            "c:  [tensor(0.0105467634), tensor(0.5744436979), tensor(0.5479382277), tensor(-0.0167118087), tensor(-0.0016792914), tensor(-0.0127716092), tensor(0.0052109216), tensor(0.0057996050), tensor(0.4174379408), tensor(-0.0142480526), tensor(0.0196906514), tensor(-0.1888357401), tensor(-0.0564474165), tensor(0.3994038105), tensor(-0.0084322039), tensor(0.0322458483), tensor(-0.1815058887), tensor(-0.0603949204)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([1.7004073858, 0.5572165847, 0.9750798941, 1.1297638416, 0.4770316482,\n",
            "        1.3423769474, 1.3316354752, 0.4037352204, 0.8887006640, 0.9292110801,\n",
            "        0.7884406447, 0.7774242163, 0.1988639832, 1.7781143188, 1.1839746237,\n",
            "        0.9398384094, 1.3555126190, 0.6647991538, 0.5238421559, 1.1188960075])\n",
            "btensor.grad: tensor([-0.4701229930,  0.0179080367,  0.0103504062, -0.4991998672,\n",
            "         0.0101746321, -0.1981926560, -0.3415370882,  0.2248739004,\n",
            "         0.4592924118, -0.2241587341, -0.0954960585, -0.4529933035,\n",
            "        -0.1429156661,  0.0016449690, -0.2020997703,  0.4589717388,\n",
            "         0.3539475203, -0.3327802718, -0.0859558582,  0.0663999915])\n",
            "ctensor.grad: tensor([-4.2244682312e+00, -1.9864910126e+01, -1.9010404587e+01,\n",
            "        -1.7873068154e-01,  1.9485669211e-02,  3.5646083951e-01,\n",
            "         2.6683503389e-01,  2.9716670513e-01, -7.5449142456e+00,\n",
            "         1.5441243649e+00,  2.4621173859e+01,  1.1718403816e+01,\n",
            "         9.9292345047e+00, -7.8302578926e+00,  1.4728778601e+00,\n",
            "         2.3059642792e+01,  1.1217606544e+01,  9.9103736877e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1026.0518798828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4395527840), tensor(0.9810199738), tensor(1.0272026062), tensor(1.1332728863), tensor(1.1272041798), tensor(1.2223577499), tensor(0.9160728455), tensor(1.1546301842), tensor(0.9110532403), tensor(1.0804090500), tensor(0.9561346769), tensor(1.3854765892), tensor(0.9600208402), tensor(0.9962261915), tensor(1.0464588404), tensor(0.8480197787), tensor(0.9357432127), tensor(0.8309109807), tensor(1.0564872026), tensor(0.9212952256)]\n",
            "b:  [tensor(0.7748243213), tensor(1.2148329020), tensor(1.3497152328), tensor(1.1353954077), tensor(1.5065459013), tensor(0.8973478079), tensor(1.2860001326), tensor(1.1108525991), tensor(1.6447169781), tensor(1.0179238319), tensor(1.3607072830), tensor(0.8411249518), tensor(1.3559870720), tensor(1.3272811174), tensor(0.9736765027), tensor(0.9599514604), tensor(1.2591309547), tensor(1.6803432703), tensor(1.0567853451), tensor(1.2319666147)]\n",
            "c:  [tensor(0.0072917310), tensor(0.5836811066), tensor(0.5568383336), tensor(-0.0163654760), tensor(-0.0016890230), tensor(-0.0125472657), tensor(0.0052831783), tensor(0.0061708395), tensor(0.4366720319), tensor(-0.0148267737), tensor(0.0368583649), tensor(-0.1797706783), tensor(-0.0250046328), tensor(0.4180012047), tensor(-0.0089881448), tensor(0.0486670285), tensor(-0.1728832424), tensor(-0.0307934042)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 0.2960433364, -1.7856508493, -1.2833139896, -0.8482978344,\n",
            "        -1.3380640745, -0.2902710438, -0.9541352987, -1.5751721859,\n",
            "        -1.7732892036, -1.0148051977, -1.3714776039, -0.8464174867,\n",
            "        -2.4312112331,  0.1494746208, -0.7330602407, -1.4473074675,\n",
            "        -0.5765005350, -2.0542633533, -1.8843464851, -1.2775702477])\n",
            "btensor.grad: tensor([-2.0988936424, -2.0138158798, -1.5652697086, -2.3936018944,\n",
            "        -1.8122246265, -1.9831358194, -2.0259773731, -1.7060452700,\n",
            "        -1.3490620852, -2.1168661118, -1.8409368992, -1.9657833576,\n",
            "        -1.9283421040, -1.4778752327, -1.9034104347, -1.4385108948,\n",
            "        -1.3042749166, -1.9767942429, -1.8063113689, -1.8515551090])\n",
            "ctensor.grad: tensor([ 6.5100646019e+00, -1.8474802017e+01, -1.7800170898e+01,\n",
            "        -6.9266551733e-01,  1.9463296980e-02, -4.4868752360e-01,\n",
            "        -1.4451345801e-01, -7.4246853590e-01, -3.8468173981e+01,\n",
            "         1.1574413776e+00, -3.4335430145e+01, -1.8130134583e+01,\n",
            "        -6.2885566711e+01, -3.7194770813e+01,  1.1118819714e+00,\n",
            "        -3.2842353821e+01, -1.7245306015e+01, -5.9203029633e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1033.0974121094, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4161924124), tensor(0.9512362480), tensor(0.9974358082), tensor(1.1055490971), tensor(1.1024698019), tensor(1.1988780499), tensor(0.8854660392), tensor(1.1291024685), tensor(0.8772873282), tensor(1.0553447008), tensor(0.9296827912), tensor(1.3633679152), tensor(0.9308449030), tensor(0.9713234305), tensor(1.0196729898), tensor(0.8177515864), tensor(0.9075738788), tensor(0.7985833287), tensor(1.0272439718), tensor(0.8894068599)]\n",
            "b:  [tensor(0.7537853718), tensor(1.1876749992), tensor(1.3302620649), tensor(1.1126239300), tensor(1.4836068153), tensor(0.8755418658), tensor(1.2669163942), tensor(1.0871202946), tensor(1.6223955154), tensor(0.9925064445), tensor(1.3381427526), tensor(0.8228113055), tensor(1.3342995644), tensor(1.3082073927), tensor(0.9521822333), tensor(0.9348982573), tensor(1.2376409769), tensor(1.6615406275), tensor(1.0349470377), tensor(1.2073109150)]\n",
            "c:  [tensor(0.0210593417), tensor(0.5876980424), tensor(0.5606678128), tensor(-0.0166493021), tensor(-0.0017016034), tensor(-0.0133687863), tensor(0.0048207557), tensor(0.0051854639), tensor(0.4099117517), tensor(-0.0161073543), tensor(-0.0344963297), tensor(-0.2170285881), tensor(-0.1110190302), tensor(0.3927618265), tensor(-0.0102066211), tensor(-0.0189912543), tensor(-0.2083338499), tensor(-0.1127392575)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([4.6720647812, 5.9567494392, 5.9533658028, 5.5447516441, 4.9468865395,\n",
            "        4.6959409714, 6.1213598251, 5.1055479050, 6.7531852722, 5.0128684044,\n",
            "        5.2903804779, 4.4217290878, 5.8351821899, 4.9805498123, 5.3571748734,\n",
            "        6.0536437035, 5.6338620186, 6.4655337334, 5.8486423492, 6.3776750565])\n",
            "btensor.grad: tensor([4.2077851295, 5.4315795898, 3.8906435966, 4.5542898178, 4.5878105164,\n",
            "        4.3611869812, 3.8167400360, 4.7464618683, 4.4643011093, 5.0834765434,\n",
            "        4.5129003525, 3.6627349854, 4.3375105858, 3.8147454262, 4.2988538742,\n",
            "        5.0106363297, 4.2979907990, 3.7605311871, 4.3676567078, 4.9311351776])\n",
            "ctensor.grad: tensor([-2.7535221100e+01, -8.0338878632e+00, -7.6589407921e+00,\n",
            "         5.6765204668e-01,  2.5160798803e-02,  1.6430411339e+00,\n",
            "         9.2484515905e-01,  1.9707512856e+00,  5.3520561218e+01,\n",
            "         2.5611608028e+00,  1.4270938110e+02,  7.4515823364e+01,\n",
            "         1.7202879333e+02,  5.0478775024e+01,  2.4369528294e+00,\n",
            "         1.3531655884e+02,  7.0901206970e+01,  1.6389169312e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1112.9354248047, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4905382395), tensor(1.0812655687), tensor(1.1264876127), tensor(1.2129675150), tensor(1.1951904297), tensor(1.2888407707), tensor(1.0272123814), tensor(1.2245455980), tensor(1.0374882221), tensor(1.1693835258), tensor(1.0614172220), tensor(1.4440234900), tensor(1.0825504065), tensor(1.0778851509), tensor(1.1357928514), tensor(0.9662041068), tensor(1.0190777779), tensor(0.9666074514), tensor(1.1577339172), tensor(1.0295608044)]\n",
            "b:  [tensor(0.8136643171), tensor(1.2658963203), tensor(1.3975658417), tensor(1.1862069368), tensor(1.5584460497), tensor(0.9470328093), tensor(1.3396667242), tensor(1.1698594093), tensor(1.7009421587), tensor(1.0627571344), tensor(1.4079357386), tensor(0.8834111094), tensor(1.4091155529), tensor(1.3721172810), tensor(1.0193984509), tensor(1.0154945850), tensor(1.3104799986), tensor(1.7319805622), tensor(1.1049957275), tensor(1.2866271734)]\n",
            "c:  [tensor(-0.0428432822), tensor(0.5272340775), tensor(0.5037521720), tensor(-0.0103029516), tensor(-0.0016694596), tensor(-0.0026406413), tensor(0.0099621881), tensor(0.0177733395), tensor(0.5823815465), tensor(-0.0151779484), tensor(0.2518830001), tensor(-0.0953494534), tensor(0.1853958219), tensor(0.5625154376), tensor(-0.0092958240), tensor(0.2642610967), tensor(-0.0870130882), tensor(0.1806477308)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-14.8691596985, -26.0058650970, -25.8103675842, -21.4836902618,\n",
            "        -18.5441188812, -17.9925460815, -28.3492717743, -19.0886211395,\n",
            "        -32.0401916504, -22.8077545166, -26.3468952179, -16.1311206818,\n",
            "        -30.3410911560, -21.3123378754, -23.2239627838, -29.6905059814,\n",
            "        -22.3007907867, -33.6048240662, -26.0979843140, -28.0307865143])\n",
            "btensor.grad: tensor([-11.9757947922, -15.6442680359, -13.4607610703, -14.7166061401,\n",
            "        -14.9678544998, -14.2981939316, -14.5500640869, -16.5478324890,\n",
            "        -15.7093248367, -14.0501279831, -13.9585971832, -12.1199579239,\n",
            "        -14.9631996155, -12.7819728851, -13.4432468414, -16.1192665100,\n",
            "        -14.5677967072, -14.0879802704, -14.0097274780, -15.8632526398])\n",
            "ctensor.grad: tensor([ 1.2780524445e+02,  1.2092788696e+02,  1.1383125305e+02,\n",
            "        -1.2692701340e+01, -6.4287573099e-02, -2.1456289291e+01,\n",
            "        -1.0282864571e+01, -2.5175750732e+01, -3.4493957520e+02,\n",
            "        -1.8588118553e+00, -5.7275866699e+02, -2.4335826111e+02,\n",
            "        -5.9282965088e+02, -3.3950723267e+02, -1.8215939999e+00,\n",
            "        -5.6650469971e+02, -2.4264151001e+02, -5.8677398682e+02])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1195.2404785156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4322135448), tensor(0.9784405231), tensor(1.0398046970), tensor(1.1291855574), tensor(1.1006095409), tensor(1.2246774435), tensor(0.9414507151), tensor(1.1325864792), tensor(0.9350858331), tensor(1.0929189920), tensor(0.9780525565), tensor(1.3733943701), tensor(0.9854148030), tensor(1.0135999918), tensor(1.0573827028), tensor(0.8753933907), tensor(0.9368279576), tensor(0.8653873205), tensor(1.0658367872), tensor(0.9347462654)]\n",
            "b:  [tensor(0.6659201384), tensor(1.1398177147), tensor(1.2978893518), tensor(1.0611711740), tensor(1.4529898167), tensor(0.8222194910), tensor(1.2298774719), tensor(1.0728251934), tensor(1.6204136610), tensor(0.9224098921), tensor(1.2919007540), tensor(0.7457328439), tensor(1.3034622669), tensor(1.2561175823), tensor(0.8978978395), tensor(0.9181885719), tensor(1.2151751518), tensor(1.6308910847), tensor(0.9878883958), tensor(1.1721968651)]\n",
            "c:  [tensor(-0.0110065676), tensor(0.4439750016), tensor(0.4204669595), tensor(-0.0027663456), tensor(-0.0015160807), tensor(0.0136476057), tensor(0.0190266110), tensor(0.0384074822), tensor(0.5805925727), tensor(-0.0152676152), tensor(0.2465305328), tensor(-0.0991327316), tensor(0.1752417684), tensor(0.5608283281), tensor(-0.0093820691), tensor(0.2592713535), tensor(-0.0905300826), tensor(0.1710368693)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([11.6649274826, 20.5650119781, 17.3365879059, 16.7563915253,\n",
            "        18.9161891937, 12.8326606750, 17.1523399353, 18.3918285370,\n",
            "        20.4804744720, 15.2929067612, 16.6729297638, 14.1258172989,\n",
            "        19.4271163940, 12.8570423126, 15.6820373535, 18.1621379852,\n",
            "        16.4499626160, 20.2440299988, 18.3794288635, 18.9629039764])\n",
            "btensor.grad: tensor([29.5488414764, 25.2157154083, 19.9352989197, 25.0071525574,\n",
            "        21.0912475586, 24.9626617432, 21.9578609467, 19.4068393707,\n",
            "        16.1056900024, 28.0694465637, 23.2069950104, 27.5356578827,\n",
            "        21.1306476593, 23.1999340057, 24.3001251221, 19.4611968994,\n",
            "        19.0609703064, 20.2178897858, 23.4214706421, 22.8860530853])\n",
            "ctensor.grad: tensor([-63.6734237671, 166.5181579590, 166.5704498291, -15.0732107162,\n",
            "         -0.3067578971, -32.5764923096, -18.1288433075, -41.2682838440,\n",
            "          3.5779864788,   0.1793328673,  10.7049350739,   7.5665526390,\n",
            "         20.3081016541,   3.3741843700,   0.1724903733,   9.9794816971,\n",
            "          7.0339860916,  19.2217330933])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1107.2685546875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4087977409), tensor(0.9156310558), tensor(0.9899364710), tensor(1.0828896761), tensor(1.0436267853), tensor(1.1974836588), tensor(0.8954728842), tensor(1.0795501471), tensor(0.8703896403), tensor(1.0557460785), tensor(0.9338310361), tensor(1.3394346237), tensor(0.9257389903), tensor(0.9894306064), tensor(1.0163769722), tensor(0.8255097270), tensor(0.8944898844), tensor(0.8038726449), tensor(1.0121427774), tensor(0.8809436560)]\n",
            "b:  [tensor(0.5848808289), tensor(1.0679119825), tensor(1.2451871634), tensor(0.9899688363), tensor(1.3946294785), tensor(0.7573466301), tensor(1.1702382565), tensor(1.0289150476), tensor(1.5824553967), tensor(0.8419720531), tensor(1.2256309986), tensor(0.6726008058), tensor(1.2450326681), tensor(1.1916540861), tensor(0.8320126534), tensor(0.8757992983), tensor(1.1691561937), tensor(1.5750836134), tensor(0.9257781506), tensor(1.1108233929)]\n",
            "c:  [tensor(-0.0050946712), tensor(0.3990088701), tensor(0.3754604161), tensor(-0.0019202335), tensor(-0.0014857060), tensor(0.0159900784), tensor(0.0205334760), tensor(0.0417529345), tensor(0.5792983770), tensor(-0.0153353903), tensor(0.2424739450), tensor(-0.1019900516), tensor(0.1675361693), tensor(0.5596331358), tensor(-0.0094461255), tensor(0.2555668354), tensor(-0.0931325406), tensor(0.1638872325)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.6831650734, 12.5618953705,  9.9736433029,  9.2591695786,\n",
            "        11.3965606689,  5.4387631416,  9.1955604553, 10.6072778702,\n",
            "        12.9392414093,  7.4345912933,  8.8443040848,  6.7919492722,\n",
            "        11.9351606369,  4.8338723183,  8.2011547089,  9.9767303467,\n",
            "         8.4676113129, 12.3029394150, 10.7387914658, 10.7605199814])\n",
            "btensor.grad: tensor([16.2078647614, 14.3811416626, 10.5404348373, 14.2404699326,\n",
            "        11.6720724106, 12.9745740891, 11.9278345108,  8.7820310593,\n",
            "         7.5916576385, 16.0875682831, 13.2539634705, 14.6264114380,\n",
            "        11.6859245300, 12.8927049637, 13.1770362854,  8.4778585434,\n",
            "         9.2037982941, 11.1615018845, 12.4220466614, 12.2746849060])\n",
            "ctensor.grad: tensor([-1.1823792458e+01,  8.9932250977e+01,  9.0013069153e+01,\n",
            "        -1.6922243834e+00, -6.0749474913e-02, -4.6849446297e+00,\n",
            "        -3.0137290955e+00, -6.6909012794e+00,  2.5884199142e+00,\n",
            "         1.3555100560e-01,  8.1131744385e+00,  5.7146420479e+00,\n",
            "         1.5411188126e+01,  2.3904337883e+00,  1.2811221182e-01,\n",
            "         7.4090161324e+00,  5.2049164772e+00,  1.4299261093e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1081.9190673828, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4037693739), tensor(0.8770412207), tensor(0.9614776373), tensor(1.0580137968), tensor(1.0089246035), tensor(1.1900891066), tensor(0.8724650741), tensor(1.0492668152), tensor(0.8288412690), tensor(1.0403114557), tensor(0.9119861126), tensor(1.3254809380), tensor(0.8886072040), tensor(0.9864390492), tensor(0.9965574741), tensor(0.7997490168), tensor(0.8749678135), tensor(0.7665770054), tensor(0.9808864594), tensor(0.8513960838)]\n",
            "b:  [tensor(0.5470757484), tensor(1.0275121927), tensor(1.2181172371), tensor(0.9503194094), tensor(1.3622583151), tensor(0.7274333835), tensor(1.1386001110), tensor(1.0125815868), tensor(1.5664879084), tensor(0.7982897758), tensor(1.1877272129), tensor(0.6388677359), tensor(1.2128922939), tensor(1.1564675570), tensor(0.7986629605), tensor(0.8616864681), tensor(1.1492002010), tensor(1.5439757109), tensor(0.8949168921), tensor(1.0791250467)]\n",
            "c:  [tensor(-0.0029042261), tensor(0.3760778010), tensor(0.3524760604), tensor(-0.0018967133), tensor(-0.0014752743), tensor(0.0164223015), tensor(0.0209258515), tensor(0.0425948575), tensor(0.5782644749), tensor(-0.0153926676), tensor(0.2390730530), tensor(-0.1043809950), tensor(0.1610634774), tensor(0.5586960912), tensor(-0.0094994046), tensor(0.2525170743), tensor(-0.0952708498), tensor(0.1579870880)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([1.0056658983, 7.7179641724, 5.6917695999, 4.9751691818, 6.9404392242,\n",
            "        1.4789046049, 4.6015620232, 6.0566644669, 8.3096733093, 3.0869314671,\n",
            "        4.3689908981, 2.7907261848, 7.4263601303, 0.5983141661, 3.9639034271,\n",
            "        5.1521425247, 3.9044103622, 7.4591274261, 6.2512588501, 5.9095096588])\n",
            "btensor.grad: tensor([7.5610203743, 8.0799694061, 5.4139943123, 7.9298825264, 6.4742264748,\n",
            "        5.9826526642, 6.3276271820, 3.2666971684, 3.1934924126, 8.7364540100,\n",
            "        7.5807647705, 6.7466197014, 6.4280676842, 7.0373077393, 6.6699395180,\n",
            "        2.8225679398, 3.9911937714, 6.2215886116, 6.1722564697, 6.3396692276])\n",
            "ctensor.grad: tensor([-4.3808898926e+00,  4.5862163544e+01,  4.5968692780e+01,\n",
            "        -4.7040387988e-02, -2.0863257349e-02, -8.6444640160e-01,\n",
            "        -7.8475093842e-01, -1.6838483810e+00,  2.0677471161e+00,\n",
            "         1.1455466598e-01,  6.8017845154e+00,  4.7818908691e+00,\n",
            "         1.2945385933e+01,  1.8741255999e+00,  1.0655871779e-01,\n",
            "         6.0995039940e+00,  4.2766146660e+00,  1.1800282478e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1074.8498535156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4075018167), tensor(0.8524861336), tensor(0.9447877407), tensor(1.0446271896), tensor(0.9868870378), tensor(1.1922276020), tensor(0.8616931438), tensor(1.0315960646), tensor(0.8010446429), tensor(1.0358468294), tensor(0.9019506574), tensor(1.3216116428), tensor(0.8645405769), tensor(0.9931157827), tensor(0.9878717661), tensor(0.7871307135), tensor(0.8673787713), tensor(0.7433291078), tensor(0.9621568322), tensor(0.8352724314)]\n",
            "b:  [tensor(0.5330740213), tensor(1.0048702955), tensor(1.2045432329), tensor(0.9284971356), tensor(1.3439536095), tensor(0.7157820463), tensor(1.1220682859), tensor(1.0092228651), tensor(1.5613665581), tensor(0.7757954001), tensor(1.1656950712), tensor(0.6261101961), tensor(1.1950461864), tensor(1.1374589205), tensor(0.7829995751), tensor(0.8603226542), tensor(1.1421784163), tensor(1.5261391401), tensor(0.8806539178), tensor(1.0633026361)]\n",
            "c:  [tensor(-0.0018465305), tensor(0.3650400043), tensor(0.3413709700), tensor(-0.0019966515), tensor(-0.0014704696), tensor(0.0164673645), tensor(0.0210546516), tensor(0.0428550281), tensor(0.5773432255), tensor(-0.0154463956), tensor(0.2359226942), tensor(-0.1065964252), tensor(0.1550547779), tensor(0.5578744411), tensor(-0.0095486799), tensor(0.2497350425), tensor(-0.0972214788), tensor(0.1525925547)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-0.7464791536,  4.9110217094,  3.3379797935,  2.6773226261,\n",
            "         4.4075093269, -0.4276888371,  2.1543810368,  3.5341515541,\n",
            "         5.5593309402,  0.8929249644,  2.0070965290,  0.7738558054,\n",
            "         4.8133316040, -1.3353472948,  1.7371358871,  2.5236644745,\n",
            "         1.5178081989,  4.6495742798,  3.7459254265,  3.2247340679])\n",
            "btensor.grad: tensor([2.8003435135, 4.5283803940, 2.7148096561, 4.3644590378, 3.6609406471,\n",
            "        2.3302621841, 3.3063673973, 0.6717362404, 1.0242732763, 4.4988780022,\n",
            "        4.4064207077, 2.5515084267, 3.5692150593, 3.8017334938, 3.1326735020,\n",
            "        0.2727676630, 1.4043548107, 3.5673117638, 2.8525915146, 3.1644873619])\n",
            "ctensor.grad: tensor([-2.1153910160e+00,  2.2075597763e+01,  2.2210205078e+01,\n",
            "         1.9987657666e-01, -9.6092876047e-03, -9.0127602220e-02,\n",
            "        -2.5759825110e-01, -5.2034223080e-01,  1.8425290585e+00,\n",
            "         1.0745596141e-01,  6.3007302284e+00,  4.4308662415e+00,\n",
            "         1.2017391205e+01,  1.6432938576e+00,  9.8550610244e-02,\n",
            "         5.5640578270e+00,  3.9012610912e+00,  1.0789072037e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1072.7266845703, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4147894382), tensor(0.8358559012), tensor(0.9342796206), tensor(1.0370597839), tensor(0.9718866944), tensor(1.1982765198), tensor(0.8569172621), tensor(1.0206393003), tensor(0.7813093066), tensor(1.0363192558), tensor(0.8976433873), tensor(1.3223426342), tensor(0.8478975892), tensor(1.0032211542), tensor(0.9845842123), tensor(0.7811261415), tensor(0.8654398322), tensor(0.7279471159), tensor(0.9501625299), tensor(0.8261507154)]\n",
            "b:  [tensor(0.5297411680), tensor(0.9919264317), tensor(1.1977865696), tensor(0.9163703322), tensor(1.3331065178), tensor(0.7125663161), tensor(1.1133956909), tensor(1.0111553669), tensor(1.5611813068), tensor(0.7646722794), tensor(1.1524027586), tensor(0.6229043603), tensor(1.1847845316), tensor(1.1271624565), tensor(0.7762168646), tensor(0.8636877537), tensor(1.1410356760), tensor(1.5153121948), tensor(0.8745822310), tensor(1.0555642843)]\n",
            "c:  [tensor(-0.0012747943), tensor(0.3600635529), tensor(0.3363097310), tensor(-0.0020962672), tensor(-0.0014677903), tensor(0.0164318364), tensor(0.0211059414), tensor(0.0429494679), tensor(0.5764457583), tensor(-0.0155007150), tensor(0.2327762991), tensor(-0.1088137329), tensor(0.1490409672), tensor(0.5570854545), tensor(-0.0095978389), tensor(0.2469938695), tensor(-0.0991467759), tensor(0.1472665519)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.4575202465,  3.3260416985,  2.1016221046,  1.5134813786,\n",
            "         3.0000703335, -1.2097830772,  0.9551794529,  2.1913638115,\n",
            "         3.9470725060, -0.0944749787,  0.8614506721, -0.1462033987,\n",
            "         3.3285999298, -2.0210726261,  0.6575118303,  1.2009105682,\n",
            "         0.3877924085,  3.0763974190,  2.3988637924,  1.8243404627])\n",
            "btensor.grad: tensor([ 0.6665661931,  2.5887732506,  1.3513276577,  2.4253592491,\n",
            "         2.1694207191,  0.6431401372,  1.7345197201, -0.3865122795,\n",
            "         0.0370514393,  2.2246234417,  2.6584730148,  0.6411645412,\n",
            "         2.0523331165,  2.0592932701,  1.3565404415, -0.6730186343,\n",
            "         0.2285409868,  2.1653909683,  1.2143335342,  1.5476685762])\n",
            "ctensor.grad: tensor([-1.1434720755e+00,  9.9529027939e+00,  1.0122461319e+01,\n",
            "         1.9923134148e-01, -5.3585739806e-03,  7.1056351066e-02,\n",
            "        -1.0257837921e-01, -1.8887820840e-01,  1.7948838472e+00,\n",
            "         1.0863792151e-01,  6.2927808762e+00,  4.4346175194e+00,\n",
            "         1.2027621269e+01,  1.5779833794e+00,  9.8318710923e-02,\n",
            "         5.4823522568e+00,  3.8505992889e+00,  1.0651994705e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.8701171875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4230842590), tensor(0.8237997293), tensor(0.9270094633), tensor(1.0323741436), tensor(0.9609022141), tensor(1.2054806948), tensor(0.8548948169), tensor(1.0132615566), tensor(0.7664588690), tensor(1.0387076139), tensor(0.8959322572), tensor(1.3249186277), tensor(0.8355906606), tensor(1.0138109922), tensor(0.9837437868), tensor(0.7782728076), tensor(0.8659205437), tensor(0.7170175314), tensor(0.9418154955), tensor(0.8206046224)]\n",
            "b:  [tensor(0.5303623080), tensor(0.9842805266), tensor(1.1944178343), tensor(0.9094789028), tensor(1.3262617588), tensor(0.7128297091), tensor(1.1087831259), tensor(1.0147722960), tensor(1.5630168915), tensor(0.7593901753), tensor(1.1440064907), tensor(0.6234799027), tensor(1.1785789728), tensor(1.1215858459), tensor(0.7736581564), tensor(0.8681911826), tensor(1.1423013210), tensor(1.5082507133), tensor(0.8723495603), tensor(1.0518465042)]\n",
            "c:  [tensor(-0.0009339311), tensor(0.3581197858), tensor(0.3342583776), tensor(-0.0021769607), tensor(-0.0014660726), tensor(0.0163860209), tensor(0.0211309716), tensor(0.0429905355), tensor(0.5755184889), tensor(-0.0155581962), tensor(0.2294806540), tensor(-0.1111437902), tensor(0.1427285969), tensor(0.5562815070), tensor(-0.0096491566), tensor(0.2441601157), tensor(-0.1011426449), tensor(0.1417506635)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.6589725018,  2.4112367630,  1.4540333748,  0.9371213913,\n",
            "         2.1968929768, -1.4408400059,  0.4044926465,  1.4755506516,\n",
            "         2.9700901508, -0.4776825011,  0.3422237635, -0.5152004957,\n",
            "         2.4613857269, -2.1179735661,  0.1680907607,  0.5706620216,\n",
            "        -0.0961482525,  2.1859228611,  1.6694010496,  1.1092185974])\n",
            "btensor.grad: tensor([-0.1242251396,  1.5291850567,  0.6737414002,  1.3782804012,\n",
            "         1.3689548969, -0.0526733398,  0.9225056171, -0.7233892083,\n",
            "        -0.3671250343,  1.0564217567,  1.6792597771, -0.1151086390,\n",
            "         1.2411082983,  1.1153301001,  0.5117442608, -0.9006877542,\n",
            "        -0.2531273961,  1.4123057127,  0.4465374351,  0.7435481548])\n",
            "ctensor.grad: tensor([-6.8172651529e-01,  3.8875486851e+00,  4.1027355194e+00,\n",
            "         1.6138716042e-01, -3.4353376832e-03,  9.1629259288e-02,\n",
            "        -5.0060100853e-02, -8.2134842873e-02,  1.8545856476e+00,\n",
            "         1.1496172100e-01,  6.5912914276e+00,  4.6601214409e+00,\n",
            "         1.2624749184e+01,  1.6078429222e+00,  1.0263516009e-01,\n",
            "         5.6675186157e+00,  3.9917447567e+00,  1.1031774521e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(1071.3511962891, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(1.4312328100), tensor(0.8145466447), tensor(0.9214986563), tensor(1.0291280746), tensor(0.9523714185), tensor(1.2126003504), tensor(0.8540504575), tensor(1.0078667402), tensor(0.7547664642), tensor(1.0416114330), tensor(0.8953109980), tensor(1.3280400038), tensor(0.8259840012), tensor(1.0236593485), tensor(0.9839237928), tensor(0.7768522501), tensor(0.8672778010), tensor(0.7087340951), tensor(0.9355379939), tensor(0.8169066310)]\n",
            "b:  [tensor(0.5321349502), tensor(0.9796035290), tensor(1.1927380562), tensor(0.9054648876), tensor(1.3216512203), tensor(0.7143234015), tensor(1.1063033342), tensor(1.0185353756), tensor(1.5655057430), tensor(0.7570487857), tensor(1.1384712458), tensor(0.6253169775), tensor(1.1746186018), tensor(1.1186492443), tensor(0.7730313540), tensor(0.8724235892), tensor(1.1443713903), tensor(1.5033081770), tensor(0.8718414903), tensor(1.0501174927)]\n",
            "c:  [tensor(-0.0007067662), tensor(0.3577300906), tensor(0.3337306976), tensor(-0.0022401214), tensor(-0.0014648645), tensor(0.0163451750), tensor(0.0211456046), tensor(0.0430119671), tensor(0.5745248199), tensor(-0.0156207243), tensor(0.2259262800), tensor(-0.1136666909), tensor(0.1359056383), tensor(0.5554327369), tensor(-0.0097041475), tensor(0.2411462665), tensor(-0.1032725871), tensor(0.1358737350)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([-1.6296995878,  1.8506182432,  1.1021652222,  0.6492114067,\n",
            "         1.7061553001, -1.4239294529,  0.1688691676,  1.0789580345,\n",
            "         2.3384792805, -0.5807591677,  0.1242559552, -0.6242737174,\n",
            "         1.9213328362, -1.9696702957, -0.0359988809,  0.2841140628,\n",
            "        -0.2714501619,  1.6566822529,  1.2555036545,  0.7395961881])\n",
            "btensor.grad: tensor([-0.3545297980,  0.9354031086,  0.3359560370,  0.8028080463,\n",
            "         0.9221108556, -0.2987339497,  0.4959582686, -0.7526059151,\n",
            "        -0.4977793097,  0.4682763815,  1.1070420742, -0.3674128652,\n",
            "         0.7920650840,  0.5873254538,  0.1253566146, -0.8464795351,\n",
            "        -0.4140138328,  0.9885145426,  0.1016165614,  0.3458021879])\n",
            "ctensor.grad: tensor([-4.5432972908e-01,  7.7940952778e-01,  1.0553404093e+00,\n",
            "         1.2632146478e-01, -2.4164069910e-03,  8.1691846251e-02,\n",
            "        -2.9266985133e-02, -4.2860396206e-02,  1.9873484373e+00,\n",
            "         1.2505599856e-01,  7.1087508202e+00,  5.0458045006e+00,\n",
            "         1.3645916939e+01,  1.6975388527e+00,  1.0998262465e-01,\n",
            "         6.0276994705e+00,  4.2598915100e+00,  1.1753843307e+01])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " Score table after training\n",
            "                 Arsenal          Birmingham       Blackburn        Fulham           Leicester        Man United       Portsmouth       Charlton         Leeds            Liverpool        Bolton           Chelsea          Everton          Man City         Newcastle        Southampton      Tottenham        Wolves           Aston Villa      Middlesbrough    \n",
            "Arsenal          1.107            1.747            2.052            1.641            2.237            1.368            1.929            1.803            2.586            1.429            1.975            1.240            2.026            1.947            1.452            1.594            1.983            2.497            1.593            1.848            \n",
            "Birmingham       0.778            1.138            1.315            1.081            1.419            0.926            1.245            1.171            1.617            0.962            1.272            0.853            1.299            1.256            0.974            1.055            1.276            1.566            1.054            1.200            \n",
            "Blackburn        0.836            1.246            1.444            1.179            1.562            1.004            1.364            1.283            1.787            1.043            1.395            0.921            1.427            1.376            1.058            1.149            1.400            1.730            1.149            1.313            \n",
            "Fulham           0.893            1.351            1.572            1.276            1.704            1.080            1.483            1.392            1.955            1.124            1.517            0.989            1.553            1.497            1.141            1.243            1.523            1.891            1.242            1.426            \n",
            "Leicester        0.852            1.275            1.480            1.207            1.603            1.025            1.398            1.313            1.834            1.066            1.429            0.940            1.463            1.411            1.081            1.176            1.435            1.775            1.175            1.345            \n",
            "Man United       0.991            1.532            1.792            1.443            1.948            1.212            1.687            1.580            2.243            1.264            1.726            1.104            1.770            1.702            1.283            1.403            1.733            2.168            1.403            1.619            \n",
            "Portsmouth       0.800            1.180            1.364            1.118            1.473            0.955            1.290            1.214            1.681            0.992            1.318            0.879            1.348            1.301            1.005            1.090            1.322            1.628            1.090            1.242            \n",
            "Charlton         0.881            1.329            1.546            1.257            1.676            1.065            1.459            1.370            1.921            1.108            1.493            0.975            1.527            1.473            1.124            1.224            1.498            1.858            1.223            1.403            \n",
            "Leeds            0.747            1.081            1.245            1.027            1.341            0.884            1.179            1.112            1.525            0.917            1.204            0.816            1.230            1.189            0.928            1.003            1.208            1.478            1.003            1.137            \n",
            "Liverpool        0.900            1.365            1.588            1.289            1.722            1.090            1.498            1.406            1.976            1.134            1.531            0.997            1.569            1.511            1.151            1.254            1.537            1.911            1.254            1.439            \n",
            "Bolton           0.822            1.222            1.413            1.156            1.529            0.985            1.336            1.257            1.747            1.023            1.365            0.905            1.397            1.347            1.038            1.127            1.370            1.691            1.126            1.286            \n",
            "Chelsea          1.052            1.645            1.929            1.547            2.100            1.294            1.814            1.697            2.424            1.351            1.857            1.176            1.905            1.831            1.372            1.504            1.865            2.341            1.503            1.740            \n",
            "Everton          0.785            1.151            1.330            1.092            1.436            0.935            1.258            1.185            1.637            0.971            1.286            0.861            1.314            1.269            0.983            1.066            1.290            1.585            1.065            1.212            \n",
            "Man City         0.890            1.347            1.566            1.272            1.698            1.077            1.478            1.388            1.948            1.121            1.511            0.986            1.548            1.491            1.137            1.239            1.517            1.884            1.238            1.420            \n",
            "Newcastle        0.869            1.308            1.519            1.236            1.645            1.048            1.434            1.347            1.885            1.090            1.466            0.961            1.501            1.446            1.106            1.204            1.471            1.824            1.203            1.379            \n",
            "Southampton      0.759            1.105            1.272            1.048            1.372            0.900            1.205            1.136            1.561            0.934            1.230            0.831            1.257            1.215            0.946            1.023            1.234            1.513            1.023            1.161            \n",
            "Tottenham        0.807            1.194            1.380            1.130            1.491            0.965            1.305            1.228            1.702            1.002            1.333            0.888            1.364            1.316            1.016            1.102            1.338            1.648            1.101            1.256            \n",
            "Wolves           0.722            1.036            1.190            0.986            1.280            0.851            1.128            1.065            1.453            0.882            1.152            0.788            1.176            1.138            0.893            0.963            1.156            1.409            0.962            1.089            \n",
            "Aston Villa      0.843            1.260            1.461            1.192            1.581            1.014            1.380            1.297            1.809            1.054            1.411            0.930            1.444            1.392            1.068            1.161            1.416            1.751            1.161            1.328            \n",
            "Middlesbrough    0.780            1.145            1.320            1.085            1.425            0.929            1.249            1.177            1.624            0.964            1.276            0.856            1.305            1.259            0.977            1.058            1.280            1.573            1.058            1.203            \n",
            "\n",
            "\n",
            "\n",
            "=============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Historic Bookies Estimate:\n",
        "==========================\n",
        "\n",
        "The historic bookies estimate for goals landed by team i against team j is\n",
        "\n",
        "a_i b_j\n",
        "\n",
        "where\n",
        "\n",
        "a_i = A_i C^{-1/2}\n",
        "b_j = B_j C^{-1/2}\n",
        "\n",
        "A_i = goals landed per game by team i\n",
        "B_j = goals conceded per game by team j\n",
        "C   = average goals per game of all teams\n",
        "\n",
        "\n",
        "The historic 1980's max likelihood models\n",
        "=========================================\n",
        "\n",
        "Starting with Maher, the 1980's max likelihood model starts with this guess and\n",
        "perfects it by max likelihood, when team i lands k goals agaist team j\n",
        "loss was minus the log of the predicted probability by Poisson\n",
        "\n",
        "- log ( e^{-a_ib_j}(a_ib_j)^k /k!)\n",
        "\n",
        "\n",
        "A gitgub user said chi squared shows HST,AST,HR,AR have a significant effect\n",
        "\n",
        "HST = home shots on target\n",
        "AST = away shots on target\n",
        "HR  = home red cards\n",
        "AR  = away red cards\n",
        "HS  = home shots\n",
        "AS  = away shots\n",
        "HC  = home quarter kicks\n",
        "AC  = away corner kicks\n",
        "HF  = home fouls\n",
        "AF  = away fouls\n",
        "\n",
        "\n",
        "New Cross-entropy AI model with a neural layer\n",
        "==========================================\n",
        "\n",
        "Since gradient descent generalizes max likelihood we can replace a_ib_j by\n",
        "\n",
        "a_i b_j  +  (c_0 sigma ( c_3 HST + c_4 HR +c_5 HS + c_6 HC _c_7 HF)\n",
        "             + ...\n",
        "             +c_2 sigma(c_13 HST + c_14 HR + c_15 HS + c_16 HCC + c_17 HF) )b_j\n",
        "\n",
        "when i is the home team and\n",
        "\n",
        "a_i b_j  +  c_0 sigma ( c_1 AST + c_2 AR  ..... +AF  ) b_j\n",
        "\n",
        "when i is the away team, with sigma being the sigmoid function\n",
        "\n",
        "\n",
        "   sigma(x)=softmax(0,x) = e^x/(e^0+e^x) = 1/(1+e^{-x})\n",
        "\n",
        "\n",
        "\n",
        "This puts a *neural layer* behind the standard max likelihood model from the 1980s\n",
        "\n",
        "The weights are now the a_i,  b_i,   and c_i\n",
        "\n",
        "Since the c_i are shared by all teams the training rate for the c_i should be lower\n",
        "\n",
        "As in the model which this generalizes, the training is cross entropy versus the Poisson distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A comment about the way the databases are stored, variables like HST and AST refer to 'home'\n",
        "and 'away' team but we do not make a distinction between home versus away.\n",
        "\n",
        "This means that each row of a data table is interpreted as if it were two rows,\n",
        "one giving information about team i against team j, the other giving information\n",
        "about team j against team i.\n",
        "\n",
        "For instance to calculate the average goals scored by any team over all games\n",
        "each row gives goals scored by a home team and goals scored by an away team\n",
        "and we have to add 2 to total games.\n",
        "\n",
        "That is when we say total games it really means the sum over all teams\n",
        "of the number of games that team played in, which is twice the number\n",
        "of games.\n",
        "\n",
        "That explains the line  totalgames=totalgames+2 each time a row is read in.\n",
        "\n",
        "There is no need to change this architecture to include things causing a home\n",
        "team advantage. This starts with a constant taking the value 1 in the first line\n",
        "\n",
        "loss -=  ...\n",
        "\n",
        "and taking the value 0 in the second line\n",
        "\n",
        "loss -= ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import torch as t\n",
        "import torch.nn as n\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def sigma(x):\n",
        "  return t.exp(x)/(t.exp(t.tensor(1.0))+t.exp(x))-t.tensor(0.5)\n",
        "\n",
        "\n",
        "\n",
        "t.set_printoptions(precision=10)\n",
        "#print(\"beep boop\")\n",
        "#print(\"Aston Villa loses\")\n",
        "print(\"=============================================================\")\n",
        "\n",
        "#GITHUB LOCATION:\n",
        "#https://github.com/Pavlos01232/Match_Outcome_Prediction\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0304.csv?raw=true')\n",
        "\n",
        "\n",
        "#\"DEEP learning\" just means \"hidden\" layers\n",
        "\n",
        "\n",
        "#df.to_csv(r'C:\\Users\\Pavlos\\Desktop\\export_dataframe.csv', sep='\\t', encoding='utf-8')\n",
        "#print (df[2])\n",
        "#file_list = os.listdir('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/')\n",
        "#df = pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0405.csv?raw=true',sep='\\t', lineterminator='\\r')\n",
        "#print(df)\n",
        "\n",
        "#read function\n",
        "\n",
        "first = \"https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL\"\n",
        "last = \".csv?raw=true\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Starting with an array of data frames,\n",
        "and an array of column names, make a\n",
        "single array with the chosen columns\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine(dataFrames,columnNames):\n",
        " t=[]\n",
        " for i in range(0,len(dataFrames)):\n",
        "    theseColumns=dataFrames[i].columns.values[0].split(\",\")\n",
        "    for j in range(0 ,len(dataFrames[i])):\n",
        "      row=dataFrames[i].values[j][0].split(\",\")\n",
        "      newEntry=[]\n",
        "      for k in range(0, len(columnNames)):\n",
        "         for m in range(0, len(theseColumns)):\n",
        "             if(columnNames[k]==theseColumns[m] and m<=len(row)):\n",
        "                newEntry.append(row[m])\n",
        "      t.append(newEntry)\n",
        " return t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "can read years 3 to 23, 13*********************, there's something wrong with 14\n",
        "since the first two files in the training data are formatted incorrectly\n",
        "converts csv to dataframe\n",
        "'''\n",
        "\n",
        "df=[]\n",
        "\n",
        "for i in range(3, 4):\n",
        "  result = first + str('{:02.0f}'.format(i)) + str('{:02.0f}'.format(i+1)) + last\n",
        "  x = pd.read_csv(result, sep='\\t', encoding = 'unicode_escape', lineterminator='\\r')\n",
        "  df.append(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get an array with each (home) team listed once\n",
        "from an array of data frames with column \"HomeTeam\"\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getTeams(df):\n",
        " teams=[]\n",
        " homeTeams=combine(df,[\"HomeTeam\"])\n",
        " for i in range(len(homeTeams)):\n",
        "  if(len(homeTeams[i])>0):\n",
        "   found=False\n",
        "   for j in range(len(teams)):\n",
        "    if homeTeams[i][0] == teams[j]:\n",
        "      found=True\n",
        "      break\n",
        "   if found:\n",
        "    continue\n",
        "   teams.append(homeTeams[i][0])\n",
        " return teams\n",
        "\n",
        "\n",
        "'''\n",
        "Get the list of teams from the array of data frames called df\n",
        "and print it to the console\n",
        "'''\n",
        "\n",
        "\n",
        "teams=getTeams(df)\n",
        "print(\"\\nteams:\")\n",
        "print(teams)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Create an array called Data with just the team names and scores\n",
        "from the data framees in the array of frames df, and print it\n",
        "\n",
        "The list [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\"] can be\n",
        "made longer if other columnts may be useful to use\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Data=combine(df, [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\", \"HS\",\"AS\",\"HC\",\"AC\",\"HF\",\"AF\"])\n",
        "#print(\"\\n\\ndata: (team names respective goals scored, respective shots on target, respective red cards, respective shots, respective corner-kicks, respective fouls)\")\n",
        "#print(Data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get the numerical index of a team name x in the array teams\n",
        "otherwise just return x\n",
        "'''\n",
        "\n",
        "def getIndex(x,teams):\n",
        "  for i in range(len(teams)):\n",
        "   if(teams[i]==x):\n",
        "    return i\n",
        "  return x\n",
        "\n",
        "\n",
        "#print(\"\\n\\nIndex assigned to Everton:\")\n",
        "#print(getIndex(\"Everton\",teams))\n",
        "\n",
        "\n",
        "'''\n",
        "Replace any occurrence of names from the array teams\n",
        "which occur anywhere in A by their actual  numbers\n",
        "'''\n",
        "\n",
        "def teamsToNumbers(A,teams):\n",
        "  B=[]\n",
        "  for i in range(len(A)):\n",
        "    B.append([])\n",
        "    for j in range(len(A[i])):\n",
        "      B[i].append(getIndex(A[i][j],teams))\n",
        "  return B\n",
        "\n",
        "\n",
        "'''\n",
        "Create Data2 which is a copy of Data but with team names\n",
        "replaced by their index\n",
        "'''\n",
        "\n",
        "#print(\"\\n\\ndata2, using the team's index number instead of name\")\n",
        "Data2=teamsToNumbers(Data,teams)\n",
        "print(len(Data2))\n",
        "\n",
        "\n",
        "'''\n",
        "remove game 12 from Data2\n",
        "Caution: these indices must be high enough not to disrupt the indices of team names\n",
        "'''\n",
        "\n",
        "excluded=[137, 141, 144, 150, 153, 163, 191, 191, 201]\n",
        "if(len(excluded)>0):\n",
        "  print(\"excluded: \"+str(excluded))\n",
        "  print(\"\\nCAUTION: check that the teams list was not disrupted by exclusions:\\n\"+str(teams))\n",
        "validateGame=[]\n",
        "for i in range(len(excluded)):\n",
        " validateGame.append(Data2.pop(i))\n",
        "\n",
        "'''\n",
        "\n",
        "Assume Data2 has team indices in column 0 and 1 and scores\n",
        "in cols 2 and 3\n",
        "\n",
        "A[i] is array of average goals landed per game by tean i\n",
        "B[i] is array of average goals conceded per game by team i\n",
        "C is total games played by all teams (twide the number of games)\n",
        "games[i]=total games played by team i\n",
        "a[i]*b[j]=first approx of expected goals landed by i when playing\n",
        "    against j\n",
        "'''\n",
        "\n",
        "A=[0]*len(teams)\n",
        "B=[0]*len(teams)\n",
        "games=[0]*len(teams)\n",
        "a=[0]*len(teams)\n",
        "b=[0]*len(teams)\n",
        "C=0\n",
        "totalGames=0\n",
        "\n",
        "for i in range(len(Data2)):\n",
        "  if(len(Data2[i])<2):\n",
        "    continue\n",
        "  games[Data2[i][0]]+=1\n",
        "  games[Data2[i][1]]+=1\n",
        "  A[Data2[i][0]]+=int(Data2[i][2])\n",
        "  B[Data2[i][0]]+=int(Data2[i][3])\n",
        "  A[Data2[i][1]]+=int(Data2[i][3])\n",
        "  B[Data2[i][1]]+=int(Data2[i][2])\n",
        "  C+=int(Data2[i][2])+int(Data2[i][3])\n",
        "  totalGames+=2\n",
        "\n",
        "\n",
        "'''\n",
        "Initial estimates of a,b,c\n",
        "'''\n",
        "\n",
        "for i in range(len(A)):\n",
        "  a[i]=A[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "for i in range(len(B)):\n",
        "  b[i]=B[i]/games[i]*(C/totalGames)**(-1/2)\n",
        "\n",
        "#c is another set of hidden weights for our weightrix. they are initally nonzero to avoid a stationary point.\n",
        "\n",
        "c=[0.001,0.002,0.004,-0.001,-0.002,-0.004,0.0005,0.0001,0.01,0.002,0.005,0.007,0.009,0.003,0.007,0.009,0.002,-0.001]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "AI training function\n",
        "\n",
        "The training is by gradient descent, the loss\n",
        "function will be cross entropy loss function against Poisson\n",
        "using loss.backward()\n",
        "\n",
        "The hidden weights at the moment are the entries of a,b,c\n",
        "the array c  is shared for all teams.\n",
        "These enter into the calculation of mu (which we\n",
        "call muHome or muAway during training) and are\n",
        "hidden as they have no direct meaning.\n",
        "\n",
        "Thus mu as a function of the entries of a,b,c is\n",
        "learned by training, the weights are the entries\n",
        "of the three arrays.\n",
        "\n",
        "\n",
        "\n",
        "tau is the training rate for c which should be small\n",
        "compared to eta\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "def elementOf(i,A):\n",
        "  for j in range(len(A)):\n",
        "    if(A[j]==i):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def train(eta,tau):\n",
        "  loss=t.tensor(0.0)\n",
        "\n",
        "\n",
        "\n",
        "  atensor=t.tensor(a,requires_grad=True)\n",
        "  btensor=t.tensor(b,requires_grad=True)\n",
        "  ctensor=t.tensor(c,requires_grad=True)\n",
        "\n",
        "\n",
        "  for i in range(len(Data2)):\n",
        "    if(len(Data2[i])==0):\n",
        "      continue\n",
        "    homeTeam=int(Data2[i][0])\n",
        "    awayTeam=int(Data2[i][1])\n",
        "    homeGoals=int(Data2[i][2])\n",
        "    awayGoals=int(Data2[i][3])\n",
        "    HST=t.tensor(float(Data2[i][4]))\n",
        "    AST=t.tensor(float(Data2[i][5]))\n",
        "    HR=t.tensor(float(Data2[i][6]))\n",
        "    AR=t.tensor(float(Data2[i][7]))\n",
        "    HS=t.tensor(float(Data2[i][8]))\n",
        "    AS=t.tensor(float(Data2[i][9]))\n",
        "    HC=t.tensor(float(Data2[i][10]))\n",
        "    AC=t.tensor(float(Data2[i][11]))\n",
        "    HF=t.tensor(float(Data2[i][12]))\n",
        "    AF=t.tensor(float(Data2[i][13]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    The reason there are two lines of code for the loss is that\n",
        "    each game can be thought of as  two 'rows' of data where we label\n",
        "    the home team as team i  or team j.\n",
        "\n",
        "    Thus teams are interpreted symmetrically and there is not yet any\n",
        "    home team advantage but this can be put in\n",
        "    without modifying the architecture as a constant which is 1 in the\n",
        "    first line and 0 in the second\n",
        "    '''\n",
        "\n",
        "\n",
        "    muHome=atensor[homeTeam]*btensor[awayTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*HST+ctensor[4]*HR+ctensor[5]*HS+ctensor[6]*HC+ctensor[7]*HF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*HST+ctensor[9]*HR+ctensor[10]*HS+ctensor[11]*HC+ctensor[12]*HF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*HST+ctensor[14]*HR+ctensor[15]*HS+ctensor[16]*HC+ctensor[17]*HF)\n",
        "    muHome=muHome+neural*btensor[awayTeam]\n",
        "\n",
        "    muAway=atensor[awayTeam]*btensor[homeTeam]\n",
        "    neural=ctensor[0]*sigma(ctensor[3]*AST+ctensor[4]*AR+ctensor[5]*AS+ctensor[6]*AC+ctensor[7]*AF)\n",
        "    neural+=ctensor[1]*sigma(ctensor[8]*AST+ctensor[9]*AR+ctensor[10]*AS+ctensor[11]*AC+ctensor[12]*AF)\n",
        "    neural+=ctensor[2]*sigma(ctensor[13]*AST+ctensor[14]*AR+ctensor[15]*AS+ctensor[16]*AC+ctensor[17]*AF)\n",
        "    muAway=muAway+neural*btensor[homeTeam]\n",
        "\n",
        "\n",
        "    loss-=t.log(t.exp(-muHome)*t.pow(muHome,homeGoals)/math.factorial(homeGoals))\n",
        "    loss-=t.log(t.exp(-muAway)*t.pow(muAway,awayGoals)/math.factorial(awayGoals))\n",
        "\n",
        "  loss.backward()\n",
        "  for i in range(len(c)):\n",
        "    c[i]=c[i]-tau*ctensor.grad[i]\n",
        "  for i in range(len(a)):\n",
        "    a[i]=a[i]-eta*atensor.grad[i]\n",
        "  for i in range(len(b)):\n",
        "    b[i]=b[i]-eta*btensor.grad[i]\n",
        "  print(\"\\n\\nCross-entropy loss vs Poisson: \"+str(loss))\n",
        "  print(\"\\n\\nWeights:\\n\\na:  \"+str(a))\n",
        "  print(\"b:  \"+str(b))\n",
        "  print(\"c:  \"+str(c))\n",
        "  print(\"\\n\\n\\n\")\n",
        "  print(\"Partial derivatives of loss w/r to hidden weights:\")\n",
        "  print(\"\\natensor.grad: \"+str(atensor.grad))\n",
        "  print(\"btensor.grad: \"+str(btensor.grad))\n",
        "  print(\"ctensor.grad: \"+str(ctensor.grad))\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Use weights to construct predicted expected goals scored by i\n",
        "against j and then find probability of k goals scored using\n",
        "Poisson when given values of R S ST  C F are provided\n",
        "'''\n",
        "\n",
        "# non-tensor version of sigma for using in the field\n",
        "\n",
        "def mathsigma(x):\n",
        "  return math.exp(x)/(math.exp(0)+math.exp(x))-0.5\n",
        "\n",
        "def goalProb(i,j,k,ST,R,S,C,F):\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return math.exp(-mu)*mu**k/math.factorial(k)\n",
        "\n",
        "\n",
        "def goalProb2(i,j,k):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  #print(\"\\nNaive prediction of ST, R, S, C, F: \"+str(ST)+\", \"+str(R)+\", \"+str(S)+\", \"+str(C)+\", \"+str(F))\n",
        "  return goalProb(i,j,k,ST,R,S,C,F)\n",
        "\n",
        "\n",
        "'''\n",
        "Pre-training estimates of ST, R, S, C, F\n",
        "'''\n",
        "\n",
        "def preTrain(i,j,u,v):\n",
        "  X=0\n",
        "  Y=0\n",
        "  Z=0\n",
        "  for s in range(len(Data2)):\n",
        "    if(len(Data2[s])<2):\n",
        "      continue\n",
        "    if(Data2[s][0]==i):\n",
        "      X+=int(Data2[s][u])\n",
        "    if(Data2[s][0]==j):\n",
        "      Y+=int(Data2[s][u])\n",
        "    if(Data2[s][1]==i):\n",
        "      X+=int(Data2[s][v])\n",
        "    if(Data2[s][1]==j):\n",
        "      Y+=int(Data2[s][v])\n",
        "    Z+=int(Data2[s][u])+int(Data2[s][v])\n",
        "  return (X/games[i])*(Y/games[j])/(Z/totalGames)\n",
        "\n",
        "def preTrainST(i,j):\n",
        "  return preTrain(i,j,4,5)\n",
        "\n",
        "def preTrainR(i,j):\n",
        "  return preTrain(i,j,6,7)\n",
        "\n",
        "def preTrainS(i,j):\n",
        "  return preTrain(i,j,8,9)\n",
        "\n",
        "def preTrainC(i,j):\n",
        "  return preTrain(i,j,10,11)\n",
        "\n",
        "def preTrainF(i,j):\n",
        "  return preTrain(i,j,12,13)\n",
        "\n",
        "\n",
        "def expectedGoals(i,j):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return mu\n",
        "\n",
        "'''\n",
        "Test training a bit\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "If training iterations is set to zero this is used\n",
        "'''\n",
        "\n",
        "\n",
        "import numpy\n",
        "\n",
        "def readWeights():\n",
        " aLocal = numpy.fromfile('a.bin', dtype=numpy.float32)\n",
        " bLocal = numpy.fromfile('b.bin', dtype=numpy.float32)\n",
        " cLocal = numpy.fromfile('c.bin', dtype=numpy.float32)\n",
        " for i in range(len(a)):\n",
        "  a[i]=aLocal[i]\n",
        " for i in range(len(b)):\n",
        "  b[i]=bLocal[i]\n",
        " for i in range(len(c)):\n",
        "  c[i]=cLocal[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n\\n Pre-training score table\")\n",
        "st=\"{:<17}\".format(\"\")\n",
        "for i in range(len(teams)):\n",
        "  st+=\"{:<17}\".format(teams[i])\n",
        "print (st)\n",
        "for i in range(len(teams)):\n",
        "  st=\"{:<17}\".format(teams[i])\n",
        "  for j in range(len(teams)):\n",
        "    st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "  print(st)\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def  validate():\n",
        "  MSE1=0.0\n",
        "  MSE2=0.0\n",
        "  loss1=0.0\n",
        "  loss2=0.0\n",
        "  for s in range(len(validateGame)):\n",
        "    ii=validateGame[s][0]\n",
        "    jj=validateGame[s][1]\n",
        "    print(\"\\nvalidating excluded game:\"+teams[ii]+\" vs \" +teams[jj])\n",
        "    print(\"pre-training prediction: \"+str(expectedGoals(ii,jj))+\" to \"+str(expectedGoals(jj,ii)))\n",
        "    MSE1+=(float(validateGame[s][2])-expectedGoals(ii,jj))**2\n",
        "    MSE1+=(float(validateGame[s][3])-expectedGoals(jj,ii))**2\n",
        "    loss1-=math.log(goalProb2(ii,jj,int(validateGame[s][2])))\n",
        "    print(\"*\"+str(loss1))\n",
        "    loss1-=math.log(goalProb2(jj,ii,int(validateGame[s][3])))\n",
        "    print(\"*\"+str(loss1))\n",
        "\n",
        "  readWeights()\n",
        "  print(\"\\n\\n\")\n",
        "  for s in range(len(validateGame)):\n",
        "    ii=validateGame[s][0]\n",
        "    jj=validateGame[s][1]\n",
        "    print(\"\\nblinded saved-weights prediction for \"+teams[ii]+ \" vs \"+teams[jj]+\": \"+str(expectedGoals(ii,jj))+\" to \"+str(expectedGoals(jj,ii)))\n",
        "    print(\"actual score of that game: \" +str(validateGame[s][2])+ \" to \"+str(validateGame[s][3]))\n",
        "    MSE2+=(float(validateGame[s][2])-expectedGoals(jj,ii))**2\n",
        "    MSE2+=(float(validateGame[s][3])-expectedGoals(jj,ii))**2\n",
        "    loss2-=math.log(goalProb2(ii,jj,int(validateGame[s][2])))\n",
        "    loss2-=math.log(goalProb2(jj,ii,int(validateGame[s][3])))\n",
        "  print(\"\\n\\npre-training MSE: \"+str(MSE1/len(validateGame))+\", post-training MSE: \"+str(MSE2/len(validateGame)))\n",
        "  print(\"\\n\\npre-training loss: \"+str(loss1)+\", post-training loss: \"+str(loss2))\n",
        "\n",
        "# TRAINING\n",
        "#NUMBER OF ITRATIONS,\n",
        "#TRAINING RATE FOR ETA, TAU.\n",
        "\n",
        "iterations=80\n",
        "\n",
        "\n",
        "for i in range(iterations):\n",
        "    train(0.005,0.0005)\n",
        "\n",
        "\n",
        "if(iterations>0):\n",
        "    print(\"\\n\\n Score table after training\")\n",
        "    st=\"{:<17}\".format(\"\")\n",
        "    for i in range(len(teams)):\n",
        "       st+=\"{:<17}\".format(teams[i])\n",
        "    print (st)\n",
        "    for i in range(len(teams)):\n",
        "      st=\"{:<17}\".format(teams[i])\n",
        "      for j in range(len(teams)):\n",
        "          st+=\"{:<17}\".format(       \"%2.3f\" % expectedGoals(i,j))\n",
        "      print(st)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "'''\n",
        "save weights\n",
        "'''\n",
        "\n",
        "\n",
        "def writeWeights():\n",
        " float_array = numpy.array(a, dtype=numpy.float32)\n",
        " float_array.tofile('a.bin')\n",
        "\n",
        "\n",
        " float_array = numpy.array(b, dtype=numpy.float32)\n",
        " float_array.tofile('b.bin')\n",
        "\n",
        " float_array = numpy.array(c, dtype=numpy.float32)\n",
        " float_array.tofile('c.bin')\n",
        "\n",
        "\n",
        "if(iterations == 0):\n",
        "  validate()\n",
        "\n",
        "if(iterations>0):\n",
        "  writeWeights()\n",
        "\n",
        "print(\"=============================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting actual goals for each match in the dataset\n",
        "# Match[2] = home goals\n",
        "# Match[3] = away goals\n",
        "goals_true = []\n",
        "\n",
        "for match in Data2:\n",
        "  if len(match) >=4:\n",
        "    goals_true.append([int(match[2]),int(match[3])])\n",
        "\n",
        "# Extracting values from tensors a, b, and c\n",
        "a_values = []\n",
        "b_values = []\n",
        "c_values = []\n",
        "\n",
        "for value in a:\n",
        "    a_values.append(float(value))\n",
        "\n",
        "for value in b:\n",
        "    b_values.append(float(value))\n",
        "\n",
        "for value in c:\n",
        "    c_values.append(float(value))\n",
        "\n",
        "\n",
        "def predict_goals(home_team, away_team, a_values, b_values, c_values, teams):\n",
        "    # Indices of the home and away teams\n",
        "    home_team_index = getIndex(home_team, teams)\n",
        "    away_team_index = getIndex(away_team, teams)\n",
        "    predicted_home_goals=expectedGoals(home_team_index,away_team_index)\n",
        "    predicted_away_goals=expectedGoals(away_team_index,home_team_index)\n",
        "    return [predicted_home_goals, predicted_away_goals]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_goals = []\n",
        "actual_goals = []\n",
        "\n",
        "# Go over each match in Data2\n",
        "for match in Data2:\n",
        "    if len(match) >= 4:\n",
        "        home_team_index = match[0]\n",
        "        away_team_index = match[1]\n",
        "\n",
        "        # Call the predict_goals function to get the predicted goals\n",
        "        predicted_match_goals = predict_goals(teams[home_team_index], teams[away_team_index], a_values, b_values, c_values, teams)\n",
        "\n",
        "        predicted_goals.append(predicted_match_goals)\n",
        "        actual_goals.append([int(match[2]), int(match[3])])\n",
        "\n",
        "\n",
        "total_matches = len(actual_goals)\n",
        "correct_predictions = 0\n",
        "\n",
        "\n",
        "absolute_errors = []\n",
        "\n",
        "# Iterate over each match to calculate absolute errors\n",
        "for i in range(total_matches):\n",
        "    absolute_error_match = [] # Each match\n",
        "    #print(\"Actual goals: \", actual_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][0] - predicted_goals[i][0]))\n",
        "\n",
        "    #print(\"Predicted goals: \", predicted_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][1] - predicted_goals[i][1]))\n",
        "\n",
        "    absolute_errors.append(absolute_error_match)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = 0\n",
        "for i in range(total_matches):\n",
        "    if actual_goals[i] == predicted_goals[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_matches\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "\n",
        "# Iterate over each match to calculate sum of absolute errors\n",
        "sum_abs_errors = 0\n",
        "for error_p_match in absolute_errors:\n",
        "    sum_abs_errors += sum(error_p_match)\n",
        "\n",
        "# Calculate mean absolute error\n",
        "mean_absolute_error = sum_abs_errors / total_matches\n",
        "\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten\n",
        "true_goals_flat = []\n",
        "for match in actual_goals:\n",
        "    for goal in match:\n",
        "        true_goals_flat.append(goal)\n",
        "\n",
        "predicted_goals_flat = []\n",
        "for match in predicted_goals:\n",
        "    for goal in match:\n",
        "        predicted_goals_flat.append(round(goal.item()))\n",
        "\n",
        "\n",
        "# Histogram of Predicted Goals vs. Actual Goals\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist([true_goals_flat, predicted_goals_flat], bins=10, label=['True Goals', 'Predicted Goals'])\n",
        "plt.xlabel('Goals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Predicted Goals vs. True Goals')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "e3tJxBFgMOLT",
        "outputId": "e27e40ee-bf81-45c8-98d0-4a125f9d1228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.0\n",
            "Mean Absolute Error:  tensor(1.8534085751)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}