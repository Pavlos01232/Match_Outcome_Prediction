{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pavlos01232/Match_Outcome_Prediction/blob/main/Another_copy_of_Game_prediction_MSE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Historic Bookies Estimate:\n",
        "==========================\n",
        "\n",
        "The historic bookies estimate for goals landed by team i against team j is\n",
        "\n",
        "a_i b_j\n",
        "\n",
        "where\n",
        "\n",
        "a_i = A_i C^{-1/2}\n",
        "b_j = B_j C^{-1/2}\n",
        "\n",
        "A_i = goals landed per game by team i\n",
        "B_j = goals conceded per game by team j\n",
        "C   = average goals per game of all teams\n",
        "\n",
        "\n",
        "The historic 1980's max likelihood models\n",
        "=========================================\n",
        "\n",
        "Starting with Maher, the 1980's max likelihood model starts with this guess and\n",
        "perfects it by max likelihood, when team i lands k goals agaist team j\n",
        "loss was minus the log of the predicted probability by Poisson\n",
        "\n",
        "- log ( e^{-a_ib_j}(a_ib_j)^k /k!)\n",
        "\n",
        "\n",
        "A gitgub user said chi squared shows HST,AST,HR,AR have a significant effect\n",
        "\n",
        "HST = home shots on target\n",
        "AST = away shots on target\n",
        "HR  = home red cards\n",
        "AR  = away red cards\n",
        "\n",
        "\n",
        "New Cross-entropy AI model with a neural layer\n",
        "==========================================\n",
        "\n",
        "Since gradient descent generalizes max likelihood we can replace a_ib_j by\n",
        "\n",
        "a_i b_j  +  c_0 sigma ( c_1 HST + c_2 HR ) b_j\n",
        "\n",
        "when i is the home team and\n",
        "\n",
        "a_i b_j  +  c_0 sigma ( c_1 AST + c_2 AR ) b_j\n",
        "\n",
        "when i is the away team, with sigma being the sigmoid function\n",
        "\n",
        "\n",
        "   sigma(x)=softmax(0,x) = e^x/(e^0+e^x) = 1/(1=e^{-x})\n",
        "\n",
        "\n",
        "\n",
        "This puts a *neural layer* behind the standard max likelihood model from the 1980s\n",
        "\n",
        "The weights are now the a_i,  b_i,   and c_i\n",
        "\n",
        "Since the c_i are shared by all teams the training rate for the c_i should be lower\n",
        "\n",
        "As in the model which this generalizes, the training is cross entropy versus the Poisson distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A comment about the way the databases are stored, variables like HST and AST refer to 'home'\n",
        "and 'away' team but we do not make a distinction between home versus away.\n",
        "\n",
        "This means that each row of a data table is interpreted as if it were two rows,\n",
        "one giving information about team i against team j, the other giving information\n",
        "about team j against team i.\n",
        "\n",
        "For instance to calculate the average goals scored by any team over all games\n",
        "each row gives goals scored by a home team and goals scored by an away team\n",
        "and we have to add 2 to total games.\n",
        "\n",
        "That is when we say total games it really means the sum over all teams\n",
        "of the number of games that team played in, which is twice the number\n",
        "of games.\n",
        "\n",
        "That explains the line  totalgames=totalgames+2 each time a row is read in.\n",
        "\n",
        "There is no need to change this architecture to include things causing a home\n",
        "team advantage. This starts with a constant taking the value 1 in the first line\n",
        "\n",
        "loss -=  ...\n",
        "\n",
        "and taking the value 0 in the second line\n",
        "\n",
        "loss -= ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "import torch as t\n",
        "import torch.nn as n\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "def sigma(x):\n",
        "  return t.exp(x)/(t.exp(t.tensor(1.0))+t.exp(x))\n",
        "\n",
        "\n",
        "\n",
        "t.set_printoptions(precision=10)\n",
        "#print(\"beep boop\")\n",
        "#print(\"Aston Villa loses\")\n",
        "print(\"=============================================================\")\n",
        "\n",
        "#GITHUB LOCATION:\n",
        "#https://github.com/Pavlos01232/Match_Outcome_Prediction\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0304.csv?raw=true')\n",
        "\n",
        "\n",
        "#\"DEEP learning\" just means \"hidden\" layers\n",
        "\n",
        "\n",
        "#df.to_csv(r'C:\\Users\\Pavlos\\Desktop\\export_dataframe.csv', sep='\\t', encoding='utf-8')\n",
        "#print (df[2])\n",
        "#file_list = os.listdir('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/')\n",
        "#df = pd.read_csv('https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL0405.csv?raw=true',sep='\\t', lineterminator='\\r')\n",
        "#print(df)\n",
        "\n",
        "#read function\n",
        "\n",
        "first = \"https://raw.githubusercontent.com/Pavlos01232/Match_Outcome_Prediction/main/TRAINING%20DATA/PL\"\n",
        "last = \".csv?raw=true\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Starting with an array of data frames,\n",
        "and an array of column names, make a\n",
        "single array with the chosen columns\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine(dataFrames,columnNames):\n",
        " t=[]\n",
        " for i in range(0,len(dataFrames)):\n",
        "    theseColumns=dataFrames[i].columns.values[0].split(\",\")\n",
        "    for j in range(0 ,len(dataFrames[i])):\n",
        "      row=dataFrames[i].values[j][0].split(\",\")\n",
        "      newEntry=[]\n",
        "      for k in range(0, len(columnNames)):\n",
        "         for m in range(0, len(theseColumns)):\n",
        "             if(columnNames[k]==theseColumns[m] and m<=len(row)):\n",
        "                newEntry.append(row[m])\n",
        "      t.append(newEntry)\n",
        " return t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "can read years 3 to 23, 13*********************, there's something wrong with 14\n",
        "since the first two files in the training data are formatted incorrectly\n",
        "converts csv to dataframe\n",
        "'''\n",
        "\n",
        "df=[]\n",
        "\n",
        "for i in range(3, 5):\n",
        "  result = first + str('{:02.0f}'.format(i)) + str('{:02.0f}'.format(i+1)) + last\n",
        "  x = pd.read_csv(result, sep='\\t', encoding = 'unicode_escape', lineterminator='\\r')\n",
        "  df.append(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get an array with each (home) team listed once\n",
        "from an array of data frames with column \"HomeTeam\"\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getTeams(df):\n",
        " teams=[]\n",
        " homeTeams=combine(df,[\"HomeTeam\"])\n",
        " for i in range(len(homeTeams)):\n",
        "  if(len(homeTeams[i])>0):\n",
        "   found=False\n",
        "   for j in range(len(teams)):\n",
        "    if homeTeams[i][0] == teams[j]:\n",
        "      found=True\n",
        "      break\n",
        "   if found:\n",
        "    continue\n",
        "   teams.append(homeTeams[i][0])\n",
        " return teams\n",
        "\n",
        "\n",
        "'''\n",
        "Get the list of teams from the array of data frames called df\n",
        "and print it to the console\n",
        "'''\n",
        "\n",
        "\n",
        "teams=getTeams(df)\n",
        "print(\"\\nteams:\")\n",
        "print(teams)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Create an array called Data with just the team names and scores\n",
        "from the data framees in the array of frames df, and print it\n",
        "\n",
        "The list [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\"] can be\n",
        "made longer if other columnts may be useful to use\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "Data=combine(df, [\"HomeTeam\",\"AwayTeam\",\"FTHG\",\"FTAG\",\"HST\",\"AST\",\"HR\",\"AR\", \"HS\",\"AS\",\"HC\",\"AC\",\"HF\",\"AF\"])\n",
        "print(\"\\n\\ndata: (team names respective goals scored, respective shots on target, respective red cards)\")\n",
        "print(Data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Get the numerical index of a team name x in the array teams\n",
        "otherwise just return x\n",
        "'''\n",
        "\n",
        "def getIndex(x,teams):\n",
        "  for i in range(len(teams)):\n",
        "   if(teams[i]==x):\n",
        "    return i\n",
        "  return x\n",
        "\n",
        "\n",
        "print(\"\\n\\nIndex assigned to Everton:\")\n",
        "print(getIndex(\"Everton\",teams))\n",
        "\n",
        "\n",
        "'''\n",
        "Replace any occurrence of names from the array teams\n",
        "which occur anywhere in A by their actual  numbers\n",
        "'''\n",
        "\n",
        "def teamsToNumbers(A,teams):\n",
        "  B=[]\n",
        "  for i in range(len(A)):\n",
        "    B.append([])\n",
        "    for j in range(len(A[i])):\n",
        "      B[i].append(getIndex(A[i][j],teams))\n",
        "  return B\n",
        "\n",
        "\n",
        "'''\n",
        "Create Data2 which is a copy of Data but with team names\n",
        "replaced by their index\n",
        "'''\n",
        "\n",
        "print(\"\\n\\ndata2, using the team's index number instead of name\")\n",
        "Data2=teamsToNumbers(Data,teams)\n",
        "print(Data2)\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "Assume Data2 has team indices in column 0 and 1 and scores\n",
        "in cols 2 and 3\n",
        "\n",
        "A[i] is array of average goals landed per game by tean i\n",
        "B[i] is array of average goals conceded per game by team i\n",
        "C is total games played by all teams (twide the number of games)\n",
        "games[i]=total games played by team i\n",
        "a[i]*b[j]=first approx of expected goals landed by i when playing\n",
        "    against j\n",
        "'''\n",
        "\n",
        "A=[0]*len(teams)\n",
        "B=[0]*len(teams)\n",
        "games=[0]*len(teams)\n",
        "a=[0]*len(teams)\n",
        "b=[0]*len(teams)\n",
        "C=0\n",
        "totalGames=0\n",
        "\n",
        "for i in range(len(Data2)):\n",
        "  if(len(Data2[i])<2):\n",
        "    continue\n",
        "  games[Data2[i][0]]+=1\n",
        "  games[Data2[i][1]]+=1\n",
        "  A[Data2[i][0]]+=int(Data2[i][2])\n",
        "  B[Data2[i][0]]+=int(Data2[i][3])\n",
        "  A[Data2[i][1]]+=int(Data2[i][3])\n",
        "  B[Data2[i][1]]+=int(Data2[i][2])\n",
        "  C+=int(Data2[i][2])+int(Data2[i][3])\n",
        "  totalGames+=2\n",
        "\n",
        "\n",
        "'''\n",
        "Initial estimates of a,b,c\n",
        "'''\n",
        "\n",
        "for i in range(len(A)):\n",
        "  a[i]=A[i]*C**(-1/2)\n",
        "\n",
        "for i in range(len(B)):\n",
        "   b[i]=B[i]*C**(-1/2)\n",
        "\n",
        "#c is another set of hidden weights for our weightrix. they are initally nonzero to avoid a stationary point.\n",
        "\n",
        "c=[0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001,0.001]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "AI training function\n",
        "\n",
        "The training is by gradient descent, the loss\n",
        "function will be cross entropy loss function against Poisson\n",
        "using loss.backward()\n",
        "\n",
        "The hidden weights at the moment are the entries of a,b,c\n",
        "the array c  is shared for all teams.\n",
        "These enter into the calculation of mu (which we\n",
        "call muHome or muAway during training) and are\n",
        "hidden as they have no direct meaning.\n",
        "\n",
        "Thus mu as a function of the entries of a,b,c is\n",
        "learned by training, the weights are the entries\n",
        "of the three arrays.\n",
        "\n",
        "\n",
        "\n",
        "tau is the training rate for c which should be small\n",
        "compared to eta\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "flag=0   exclude nothing\n",
        "flag=1   exclude entries of exclusions\n",
        "flag=2   exclude entries not in exclusions\n",
        "'''\n",
        "\n",
        "def elementOf(i,A):\n",
        "  for j in range(len(A)):\n",
        "    if(A[j]==i):\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def train(eta,tau):\n",
        "  loss=t.tensor(0.0)\n",
        "\n",
        "\n",
        "\n",
        "  atensor=t.tensor(a,requires_grad=True)\n",
        "  btensor=t.tensor(b,requires_grad=True)\n",
        "  ctensor=t.tensor(c,requires_grad=True)\n",
        "\n",
        "\n",
        "  for i in range(len(Data2)):\n",
        "    if(len(Data2[i])==0):\n",
        "      continue\n",
        "    homeTeam=int(Data2[i][0])\n",
        "    awayTeam=int(Data2[i][1])\n",
        "    homeGoals=int(Data2[i][2])\n",
        "    awayGoals=int(Data2[i][3])\n",
        "    HST=t.tensor(float(Data2[i][4]))\n",
        "    AST=t.tensor(float(Data2[i][5]))\n",
        "    HR=t.tensor(float(Data2[i][6]))\n",
        "    AR=t.tensor(float(Data2[i][7]))\n",
        "    HS=t.tensor(float(Data2[i][8]))\n",
        "    AS=t.tensor(float(Data2[i][9]))\n",
        "    HC=t.tensor(float(Data2[i][10]))\n",
        "    AC=t.tensor(float(Data2[i][11]))\n",
        "    HF=t.tensor(float(Data2[i][12]))\n",
        "    AF=t.tensor(float(Data2[i][13]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    The reason there are two lines of code for the loss is that\n",
        "    each game can be thought of as  two 'rows' of data where we label\n",
        "    the home team as team i  or team j.\n",
        "\n",
        "    Thus teams are interpreted symmetrically and there is not yet any\n",
        "    home team advantage but this can be put in\n",
        "    without modifying the architecture as a constant which is 1 in the\n",
        "    first line and 0 in the second\n",
        "    '''\n",
        "\n",
        "    muHome=atensor[homeTeam]*btensor[awayTeam]\n",
        "    muHome+=+ctensor[0]*sigma(ctensor[3]*HST+ctensor[4]*HR+ctensor[5]*HS+ctensor[6]*HC+ctensor[7]*HF)\n",
        "    muHome+=+ctensor[1]*sigma(ctensor[8]*HST+ctensor[9]*HR+ctensor[10]*HS+ctensor[11]*HC+ctensor[12]*HF)\n",
        "    muHome+=+ctensor[2]*sigma(ctensor[13]*HST+ctensor[14]*HR+ctensor[15]*HS+ctensor[16]*HC+ctensor[17]*HF)\n",
        "    muHome=muHome*btensor[awayTeam]\n",
        "\n",
        "    muAway=atensor[awayTeam]*btensor[awayTeam]\n",
        "    muAway+=+ctensor[0]*sigma(ctensor[3]*AST+ctensor[4]*AR+ctensor[5]*AS+ctensor[6]*AC+ctensor[7]*AF)\n",
        "    muAway+=+ctensor[1]*sigma(ctensor[8]*AST+ctensor[9]*AR+ctensor[10]*AS+ctensor[11]*AC+ctensor[12]*AF)\n",
        "    muAway+=+ctensor[2]*sigma(ctensor[13]*AST+ctensor[14]*AR+ctensor[15]*AS+ctensor[16]*AC+ctensor[17]*AF)\n",
        "    muAway=muAway*btensor[homeTeam]\n",
        "\n",
        "\n",
        "    loss-=t.log(t.exp(-muHome)*t.pow(muHome,homeGoals)/math.factorial(homeGoals))\n",
        "    loss-=t.log(t.exp(-muAway)*t.pow(muAway,awayGoals)/math.factorial(awayGoals))\n",
        "\n",
        "  loss.backward()\n",
        "  for i in range(len(c)):\n",
        "    c[i]=c[i]-tau*ctensor.grad[i]\n",
        "  for i in range(len(a)):\n",
        "    a[i]=a[i]-eta*atensor.grad[i]\n",
        "  for i in range(len(b)):\n",
        "    b[i]=b[i]-eta*btensor.grad[i]\n",
        "  print(\"\\n\\nCross-entropy loss vs Poisson: \"+str(loss))\n",
        "  print(\"\\n\\nWeights:\\n\\na:  \"+str(a))\n",
        "  print(\"b:  \"+str(b))\n",
        "  print(\"c:  \"+str(c))\n",
        "  print(\"\\n\\n\\n\")\n",
        "  print(\"Partial derivatives of loss w/r to hidden weights:\")\n",
        "  print(\"\\natensor.grad: \"+str(atensor.grad))\n",
        "  print(\"btensor.grad: \"+str(btensor.grad))\n",
        "  print(\"ctensor.grad: \"+str(ctensor.grad))\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Use weights to construct predicted expected goals scored by i\n",
        "against j and then find probability of k goals scored using\n",
        "Poisson when given values of R S ST  C F are provided\n",
        "'''\n",
        "\n",
        "# non-tensor version of sigma for using in the field\n",
        "\n",
        "def mathsigma(x):\n",
        "  return math.exp(x)/(math.exp(0)+math.exp(x))\n",
        "\n",
        "def goalProb(i,j,k,ST,R,S,C,F):\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return math.exp(-mu)*mu**k/math.factorial(k)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preTrain(i,j,u,v):\n",
        "  X=0\n",
        "  Y=0\n",
        "  Z=0\n",
        "  for s in range(len(Data2)):\n",
        "    if(len(Data2[s])<2):\n",
        "      continue\n",
        "    if(Data2[s][0]==i):\n",
        "      X+=int(Data2[s][u])\n",
        "    if(Data2[s][0]==j):\n",
        "      Y+=int(Data2[s][u])\n",
        "    if(Data2[s][1]==i):\n",
        "      X+=int(Data2[s][v])\n",
        "    if(Data2[s][1]==j):\n",
        "      Y+=int(Data2[s][v])\n",
        "    Z+=int(Data2[s][u])+int(Data2[s][v])\n",
        "  return (X/games[i])*(Y/games[j])/(Z/totalGames)\n",
        "\n",
        "def preTrainST(i,j):\n",
        "  return preTrain(i,j,4,5)\n",
        "\n",
        "def preTrainR(i,j):\n",
        "  return preTrain(i,j,6,7)\n",
        "\n",
        "def preTrainS(i,j):\n",
        "  return preTrain(i,j,8,9)\n",
        "\n",
        "def preTrainC(i,j):\n",
        "  return preTrain(i,j,10,11)\n",
        "\n",
        "def preTrainF(i,j):\n",
        "  return preTrain(i,j,12,13)\n",
        "\n",
        "\n",
        "def expectedGoals(i,j):\n",
        "  ST=preTrainST(i,j)\n",
        "  R=preTrainR(i,j)\n",
        "  S=preTrainS(i,j)\n",
        "  C=preTrainC(i,j)\n",
        "  F=preTrainF(i,j)\n",
        "  mu=a[i]*b[j]\n",
        "  mu+=c[0]*mathsigma(c[3]*ST+c[4]*R+c[5]*S+c[6]*C+c[7]*F)\n",
        "  mu+=c[1]*mathsigma(c[8]*ST+c[9]*R+c[10]*S+c[11]*C+c[12]*F)\n",
        "  mu+=c[2]*mathsigma(c[13]*ST+c[14]*R+c[15]*S+c[16]*C+c[17]*F)\n",
        "  return mu\n",
        "\n",
        "'''\n",
        "Test training a bit\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "for i in range(80):\n",
        "    train(0.001,0.00005)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"=============================================================\")\n",
        "\n",
        "# Extracting actual goals for each match in the dataset\n",
        "# Match[2] = home goals\n",
        "# Match[3] = away goals\n",
        "goals_true = []\n",
        "\n",
        "for match in Data2:\n",
        "  if len(match) >=4:\n",
        "    goals_true.append([int(match[2]),int(match[3])])\n",
        "\n",
        "# Extracting values from tensors a, b, and c\n",
        "a_values = []\n",
        "b_values = []\n",
        "c_values = []\n",
        "\n",
        "for value in a:\n",
        "    a_values.append(float(value))\n",
        "\n",
        "for value in b:\n",
        "    b_values.append(float(value))\n",
        "\n",
        "for value in c:\n",
        "    c_values.append(float(value))\n",
        "\n",
        "\n",
        "def predict_goals(home_team, away_team, a_values, b_values, c_values, teams):\n",
        "    # Indices of the home and away teams\n",
        "    home_team_index = getIndex(home_team, teams)\n",
        "    away_team_index = getIndex(away_team, teams)\n",
        "    predicted_home_goals=expectedGoals(home_team_index,away_team_index)\n",
        "    predicted_away_goals=expectedGoals(away_team_index,home_team_index)\n",
        "    return [predicted_home_goals, predicted_away_goals]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predicted_goals = []\n",
        "actual_goals = []\n",
        "\n",
        "# Go over each match in Data2\n",
        "for match in Data2:\n",
        "    if len(match) >= 4:\n",
        "        home_team_index = match[0]\n",
        "        away_team_index = match[1]\n",
        "\n",
        "        # Call the predict_goals function to get the predicted goals\n",
        "        predicted_match_goals = predict_goals(teams[home_team_index], teams[away_team_index], a_values, b_values, c_values, teams)\n",
        "\n",
        "        predicted_goals.append(predicted_match_goals)\n",
        "        actual_goals.append([int(match[2]), int(match[3])])\n",
        "\n",
        "\n",
        "total_matches = len(actual_goals)\n",
        "correct_predictions = 0\n",
        "\n",
        "\n",
        "absolute_errors = []\n",
        "\n",
        "# Iterate over each match to calculate absolute errors\n",
        "for i in range(total_matches):\n",
        "    absolute_error_match = [] # Each match\n",
        "    print(\"Actual goals: \", actual_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][0] - predicted_goals[i][0]))\n",
        "\n",
        "    print(\"Predicted goals: \", predicted_goals[i])\n",
        "    absolute_error_match.append(abs(actual_goals[i][1] - predicted_goals[i][1]))\n",
        "\n",
        "    absolute_errors.append(absolute_error_match)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = 0\n",
        "for i in range(total_matches):\n",
        "    if actual_goals[i] == predicted_goals[i]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_matches\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "\n",
        "# Iterate over each match to calculate sum of absolute errors\n",
        "sum_abs_errors = 0\n",
        "for error_p_match in absolute_errors:\n",
        "    sum_abs_errors += sum(error_p_match)\n",
        "\n",
        "# Calculate mean absolute error\n",
        "mean_absolute_error = sum_abs_errors / total_matches\n",
        "\n",
        "print(\"Mean Absolute Error: \", mean_absolute_error)\n"
      ],
      "metadata": {
        "id": "spZN7MlA9Zyl",
        "outputId": "0de8fbad-1b86-41eb-e26d-c3fdc07ba779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================================\n",
            "\n",
            "teams:\n",
            "['Arsenal', 'Birmingham', 'Blackburn', 'Fulham', 'Leicester', 'Man United', 'Portsmouth', 'Charlton', 'Leeds', 'Liverpool', 'Bolton', 'Chelsea', 'Everton', 'Man City', 'Newcastle', 'Southampton', 'Tottenham', 'Wolves', 'Aston Villa', 'Middlesbrough', 'Norwich', 'Crystal Palace', 'West Brom']\n",
            "\n",
            "\n",
            "data: (team names respective goals scored, respective shots on target, respective red cards)\n",
            "[['Arsenal', 'Everton', '2', '1', '5', '7', '1', '1', '11', '13', '6', '9', '8', '15'], ['Birmingham', 'Tottenham', '1', '0', '5', '7', '0', '0', '10', '15', '1', '4', '20', '27'], ['Blackburn', 'Wolves', '5', '1', '13', '5', '0', '0', '25', '8', '6', '2', '8', '14'], ['Fulham', 'Middlesbrough', '3', '2', '9', '5', '0', '0', '17', '8', '7', '6', '18', '16'], ['Leicester', 'Southampton', '2', '2', '7', '10', '0', '0', '12', '13', '2', '7', '27', '15'], ['Man United', 'Bolton', '4', '0', '6', '5', '0', '0', '13', '15', '8', '4', '12', '8'], ['Portsmouth', 'Aston Villa', '2', '1', '3', '5', '0', '1', '4', '9', '7', '9', '18', '22'], ['Charlton', 'Man City', '0', '3', '5', '8', '1', '0', '10', '14', '4', '7', '12', '16'], ['Leeds', 'Newcastle', '2', '2', '3', '10', '0', '0', '8', '19', '4', '8', '19', '16'], ['Liverpool', 'Chelsea', '1', '2', '7', '6', '0', '0', '12', '10', '8', '6', '13', '12'], ['Bolton', 'Blackburn', '2', '2', '10', '5', '0', '1', '16', '10', '9', '2', '11', '15'], ['Chelsea', 'Leicester', '2', '1', '7', '1', '1', '2', '17', '5', '7', '2', '12', '18'], ['Everton', 'Fulham', '3', '1', '8', '15', '0', '0', '10', '23', '4', '10', '15', '14'], ['Man City', 'Portsmouth', '1', '1', '11', '2', '0', '0', '21', '4', '11', '7', '8', '22'], ['Newcastle', 'Man United', '1', '2', '5', '8', '0', '0', '7', '11', '2', '4', '18', '14'], ['Southampton', 'Birmingham', '0', '0', '7', '5', '0', '0', '14', '12', '2', '7', '15', '19'], ['Tottenham', 'Leeds', '2', '1', '4', '2', '0', '0', '20', '5', '6', '0', '14', '14'], ['Wolves', 'Charlton', '0', '4', '4', '6', '0', '1', '10', '8', '5', '2', '11', '7'], ['Aston Villa', 'Liverpool', '0', '0', '9', '9', '0', '0', '14', '15', '8', '9', '16', '7'], ['Middlesbrough', 'Arsenal', '0', '4', '4', '10', '0', '0', '10', '14', '3', '5', '11', '7'], ['Blackburn', 'Man City', '2', '3', '10', '5', '0', '0', '14', '13', '9', '0', '14', '10'], ['Charlton', 'Everton', '2', '2', '4', '4', '0', '0', '10', '13', '6', '8', '8', '8'], ['Leeds', 'Southampton', '0', '0', '6', '8', '0', '0', '15', '14', '6', '10', '10', '11'], ['Leicester', 'Middlesbrough', '0', '0', '4', '3', '0', '0', '12', '13', '6', '4', '17', '16'], ['Portsmouth', 'Bolton', '4', '0', '8', '4', '0', '0', '13', '13', '0', '4', '12', '14'], ['Arsenal', 'Aston Villa', '2', '0', '6', '3', '0', '0', '14', '6', '2', '6', '16', '16'], ['Liverpool', 'Tottenham', '0', '0', '10', '4', '0', '0', '22', '9', '8', '5', '20', '27'], ['Man United', 'Wolves', '1', '0', '5', '2', '0', '0', '14', '12', '10', '2', '6', '12'], ['Aston Villa', 'Leicester', '3', '1', '8', '4', '0', '1', '10', '5', '11', '1', '18', '17'], ['Bolton', 'Charlton', '0', '0', '9', '1', '0', '0', '13', '7', '12', '3', '18', '16'], ['Chelsea', 'Blackburn', '2', '2', '5', '4', '0', '0', '12', '7', '3', '4', '11', '19'], ['Everton', 'Liverpool', '0', '3', '5', '11', '0', '0', '11', '18', '9', '5', '22', '17'], ['Middlesbrough', 'Leeds', '2', '3', '16', '4', '0', '0', '27', '8', '6', '3', '17', '21'], ['Newcastle', 'Birmingham', '0', '1', '10', '3', '0', '0', '19', '7', '9', '6', '15', '15'], ['Tottenham', 'Fulham', '0', '3', '6', '5', '0', '0', '18', '9', '9', '4', '15', '13'], ['Wolves', 'Portsmouth', '0', '0', '2', '2', '0', '0', '11', '9', '4', '6', '15', '16'], ['Man City', 'Arsenal', '1', '2', '7', '5', '0', '0', '11', '10', '5', '5', '12', '11'], ['Southampton', 'Man United', '1', '0', '5', '9', '0', '0', '12', '10', '9', '5', '8', '10'], ['Arsenal', 'Portsmouth', '1', '1', '7', '2', '0', '0', '12', '8', '7', '1', '11', '19'], ['Blackburn', 'Liverpool', '1', '3', '5', '14', '1', '0', '8', '21', '5', '10', '13', '11'], ['Bolton', 'Middlesbrough', '2', '0', '8', '6', '0', '0', '13', '7', '10', '1', '17', '14'], ['Charlton', 'Man United', '0', '2', '3', '9', '1', '0', '8', '14', '3', '3', '19', '18'], ['Chelsea', 'Tottenham', '4', '2', '6', '3', '0', '0', '12', '9', '10', '3', '16', '20'], ['Everton', 'Newcastle', '2', '2', '7', '2', '1', '1', '9', '4', '6', '3', '20', '21'], ['Southampton', 'Wolves', '2', '0', '10', '3', '0', '0', '15', '10', '7', '3', '16', '20'], ['Birmingham', 'Fulham', '2', '2', '9', '6', '1', '1', '15', '8', '8', '1', '16', '21'], ['Man City', 'Aston Villa', '4', '1', '9', '4', '0', '0', '17', '8', '4', '3', '11', '11'], ['Leicester', 'Leeds', '4', '0', '11', '2', '0', '0', '19', '14', '9', '5', '10', '10'], ['Aston Villa', 'Charlton', '2', '1', '6', '3', '0', '0', '15', '8', '10', '2', '13', '13'], ['Fulham', 'Man City', '2', '2', '9', '6', '0', '0', '13', '7', '2', '4', '14', '19'], ['Leeds', 'Birmingham', '0', '2', '6', '3', '1', '0', '12', '7', '6', '1', '15', '14'], ['Liverpool', 'Leicester', '2', '1', '8', '3', '0', '0', '19', '9', '4', '4', '10', '9'], ['Newcastle', 'Bolton', '0', '0', '7', '6', '0', '0', '14', '8', '7', '5', '15', '12'], ['Portsmouth', 'Blackburn', '1', '2', '6', '10', '0', '0', '9', '12', '7', '6', '17', '18'], ['Tottenham', 'Southampton', '1', '3', '6', '10', '0', '0', '13', '12', '4', '7', '15', '14'], ['Wolves', 'Chelsea', '0', '5', '4', '8', '0', '0', '8', '14', '1', '3', '15', '15'], ['Man United', 'Arsenal', '0', '0', '7', '1', '0', '1', '9', '6', '4', '3', '11', '15'], ['Middlesbrough', 'Everton', '1', '0', '4', '5', '0', '0', '9', '9', '4', '6', '12', '14'], ['Arsenal', 'Newcastle', '3', '2', '7', '4', '0', '0', '11', '5', '3', '4', '13', '17'], ['Birmingham', 'Portsmouth', '2', '0', '5', '5', '0', '0', '7', '10', '4', '4', '10', '16'], ['Bolton', 'Wolves', '1', '1', '14', '4', '0', '0', '22', '8', '12', '3', '16', '4'], ['Chelsea', 'Aston Villa', '1', '0', '5', '1', '0', '0', '10', '7', '6', '5', '7', '9'], ['Leicester', 'Man United', '1', '4', '4', '9', '0', '0', '10', '11', '7', '3', '7', '11'], ['Southampton', 'Middlesbrough', '0', '1', '6', '9', '1', '0', '10', '12', '4', '4', '12', '19'], ['Blackburn', 'Fulham', '0', '2', '0', '2', '0', '0', '6', '7', '5', '5', '14', '11'], ['Charlton', 'Liverpool', '3', '2', '5', '9', '0', '0', '14', '17', '7', '5', '11', '9'], ['Everton', 'Leeds', '4', '0', '9', '2', '0', '0', '20', '6', '11', '4', '11', '15'], ['Man City', 'Tottenham', '0', '0', '10', '2', '0', '0', '21', '4', '6', '6', '7', '15'], ['Fulham', 'Leicester', '2', '0', '8', '4', '0', '0', '16', '8', '4', '4', '14', '10'], ['Leeds', 'Blackburn', '2', '1', '5', '9', '0', '0', '15', '17', '2', '5', '18', '20'], ['Liverpool', 'Arsenal', '1', '2', '7', '8', '0', '0', '13', '12', '11', '3', '12', '20'], ['Man United', 'Birmingham', '3', '0', '8', '2', '0', '1', '10', '5', '5', '1', '13', '11'], ['Newcastle', 'Southampton', '1', '0', '9', '4', '0', '0', '14', '7', '8', '4', '14', '10'], ['Portsmouth', 'Charlton', '1', '2', '8', '6', '0', '0', '16', '11', '6', '9', '20', '14'], ['Tottenham', 'Everton', '3', '0', '7', '4', '0', '0', '13', '12', '2', '6', '15', '14'], ['Wolves', 'Man City', '1', '0', '4', '5', '0', '0', '8', '9', '3', '10', '13', '7'], ['Aston Villa', 'Bolton', '1', '1', '8', '5', '0', '0', '14', '15', '9', '5', '14', '16'], ['Middlesbrough', 'Chelsea', '1', '2', '6', '6', '0', '0', '15', '10', '2', '8', '14', '18'], ['Birmingham', 'Chelsea', '0', '0', '2', '8', '0', '0', '6', '13', '2', '18', '10', '14'], ['Arsenal', 'Chelsea', '2', '1', '9', '3', '0', '0', '13', '7', '6', '3', '11', '13'], ['Fulham', 'Wolves', '0', '0', '4', '6', '0', '0', '9', '14', '4', '11', '12', '11'], ['Leeds', 'Man United', '0', '1', '2', '10', '0', '0', '3', '18', '7', '2', '19', '8'], ['Man City', 'Bolton', '6', '2', '13', '8', '1', '0', '16', '12', '7', '4', '11', '8'], ['Middlesbrough', 'Newcastle', '0', '1', '7', '3', '0', '0', '11', '4', '8', '2', '17', '12'], ['Portsmouth', 'Liverpool', '1', '0', '6', '7', '0', '0', '10', '13', '5', '3', '22', '10'], ['Birmingham', 'Aston Villa', '0', '0', '3', '1', '0', '0', '4', '2', '3', '4', '16', '20'], ['Everton', 'Southampton', '0', '0', '6', '7', '0', '0', '13', '19', '8', '6', '11', '6'], ['Leicester', 'Tottenham', '1', '2', '15', '4', '0', '0', '22', '7', '5', '1', '12', '19'], ['Blackburn', 'Charlton', '0', '1', '6', '5', '0', '0', '15', '10', '5', '3', '17', '12'], ['Fulham', 'Newcastle', '2', '3', '4', '7', '0', '0', '4', '11', '4', '8', '14', '16'], ['Aston Villa', 'Everton', '0', '0', '6', '2', '0', '0', '16', '8', '9', '3', '11', '12'], ['Bolton', 'Birmingham', '0', '1', '7', '3', '0', '0', '16', '7', '7', '3', '14', '16'], ['Chelsea', 'Man City', '1', '0', '4', '6', '0', '0', '7', '9', '1', '5', '8', '8'], ['Liverpool', 'Leeds', '3', '1', '9', '5', '0', '0', '20', '7', '9', '3', '11', '13'], ['Man United', 'Fulham', '1', '3', '9', '7', '0', '0', '17', '12', '10', '2', '10', '14'], ['Newcastle', 'Portsmouth', '3', '0', '8', '4', '0', '0', '10', '9', '6', '6', '12', '14'], ['Southampton', 'Blackburn', '2', '0', '15', '3', '0', '1', '22', '6', '5', '3', '17', '14'], ['Wolves', 'Leicester', '4', '3', '8', '5', '0', '0', '12', '7', '2', '6', '6', '13'], ['Charlton', 'Arsenal', '1', '1', '4', '6', '0', '0', '6', '11', '7', '5', '15', '13'], ['Tottenham', 'Middlesbrough', '0', '0', '8', '10', '0', '0', '12', '16', '8', '5', '14', '10'], ['Everton', 'Chelsea', '0', '1', '4', '7', '0', '0', '12', '17', '3', '1', '5', '11'], ['Leeds', 'Arsenal', '1', '4', '5', '9', '0', '0', '9', '13', '4', '3', '12', '11'], ['Man United', 'Portsmouth', '3', '0', '4', '3', '0', '0', '10', '9', '3', '3', '13', '12'], ['Middlesbrough', 'Wolves', '2', '0', '11', '2', '0', '1', '14', '9', '7', '3', '8', '12'], ['Newcastle', 'Aston Villa', '1', '1', '13', '6', '0', '1', '22', '11', '15', '3', '9', '17'], ['Southampton', 'Man City', '0', '2', '5', '9', '0', '0', '8', '13', '4', '6', '10', '16'], ['Tottenham', 'Bolton', '0', '1', '6', '15', '0', '0', '10', '25', '7', '5', '18', '6'], ['Fulham', 'Liverpool', '1', '2', '6', '6', '1', '0', '10', '10', '4', '8', '13', '10'], ['Leicester', 'Blackburn', '2', '0', '3', '2', '0', '0', '4', '10', '2', '6', '11', '11'], ['Birmingham', 'Charlton', '1', '2', '13', '9', '0', '0', '14', '13', '15', '8', '14', '18'], ['Arsenal', 'Tottenham', '2', '1', '3', '2', '0', '0', '9', '5', '9', '5', '8', '22'], ['Aston Villa', 'Middlesbrough', '0', '2', '4', '3', '0', '0', '12', '8', '5', '2', '19', '16'], ['Bolton', 'Southampton', '0', '0', '11', '5', '0', '1', '18', '8', '8', '6', '10', '14'], ['Charlton', 'Fulham', '3', '1', '7', '7', '0', '0', '12', '11', '1', '5', '10', '12'], ['Portsmouth', 'Leeds', '6', '1', '6', '5', '0', '0', '14', '6', '3', '2', '9', '11'], ['Wolves', 'Birmingham', '1', '1', '4', '2', '0', '0', '11', '4', '8', '1', '13', '12'], ['Chelsea', 'Newcastle', '5', '0', '8', '3', '0', '1', '15', '4', '2', '0', '13', '5'], ['Liverpool', 'Man United', '1', '2', '4', '4', '0', '0', '10', '6', '2', '6', '16', '9'], ['Man City', 'Leicester', '0', '3', '4', '3', '0', '0', '14', '6', '12', '3', '7', '12'], ['Blackburn', 'Everton', '2', '1', '12', '2', '0', '0', '18', '8', '12', '3', '16', '9'], ['Birmingham', 'Arsenal', '0', '3', '1', '6', '0', '0', '7', '9', '8', '6', '8', '11'], ['Everton', 'Wolves', '2', '0', '15', '4', '0', '0', '23', '8', '8', '3', '7', '23'], ['Leeds', 'Bolton', '0', '2', '10', '5', '0', '0', '13', '10', '8', '2', '15', '11'], ['Leicester', 'Charlton', '1', '1', '7', '4', '0', '0', '13', '7', '3', '3', '15', '15'], ['Man United', 'Blackburn', '2', '1', '8', '6', '0', '0', '14', '13', '7', '4', '6', '15'], ['Middlesbrough', 'Liverpool', '0', '0', '3', '1', '0', '0', '8', '8', '3', '4', '16', '15'], ['Newcastle', 'Man City', '3', '0', '10', '1', '0', '0', '17', '5', '4', '2', '13', '16'], ['Southampton', 'Chelsea', '0', '1', '3', '4', '0', '0', '13', '8', '3', '3', '7', '13'], ['Tottenham', 'Aston Villa', '2', '1', '5', '6', '0', '0', '10', '13', '3', '3', '14', '15'], ['Fulham', 'Portsmouth', '2', '0', '4', '6', '0', '1', '8', '12', '2', '11', '9', '16'], ['Aston Villa', 'Southampton', '1', '0', '12', '3', '0', '0', '22', '7', '9', '4', '13', '13'], ['Blackburn', 'Tottenham', '1', '0', '12', '3', '0', '0', '18', '9', '9', '6', '10', '13'], ['Bolton', 'Everton', '2', '0', '6', '3', '0', '0', '13', '7', '7', '7', '11', '18'], ['Charlton', 'Leeds', '0', '1', '6', '6', '0', '0', '13', '12', '3', '4', '11', '17'], ['Portsmouth', 'Leicester', '0', '2', '7', '3', '0', '0', '12', '7', '11', '4', '12', '14'], ['Wolves', 'Newcastle', '1', '1', '4', '4', '0', '0', '10', '7', '5', '3', '14', '14'], ['Arsenal', 'Fulham', '0', '0', '14', '1', '0', '0', '22', '7', '9', '0', '13', '11'], ['Chelsea', 'Man United', '1', '0', '4', '3', '0', '0', '12', '5', '2', '4', '11', '12'], ['Liverpool', 'Birmingham', '3', '1', '7', '4', '0', '0', '10', '7', '6', '4', '24', '17'], ['Man City', 'Middlesbrough', '0', '1', '14', '0', '0', '0', '24', '2', '12', '0', '15', '12'], ['Birmingham', 'Blackburn', '0', '4', '2', '7', '1', '0', '6', '11', '10', '7', '23', '15'], ['Fulham', 'Bolton', '2', '1', '3', '5', '0', '0', '10', '10', '3', '2', '15', '11'], ['Leeds', 'Chelsea', '1', '1', '4', '10', '0', '0', '8', '19', '2', '8', '14', '11'], ['Leicester', 'Arsenal', '1', '1', '1', '5', '0', '1', '4', '8', '7', '5', '13', '12'], ['Man United', 'Aston Villa', '4', '0', '8', '3', '0', '0', '12', '10', '6', '5', '6', '11'], ['Middlesbrough', 'Portsmouth', '0', '0', '3', '2', '0', '1', '5', '6', '6', '5', '17', '20'], ['Newcastle', 'Liverpool', '1', '1', '6', '3', '0', '0', '13', '7', '8', '1', '13', '13'], ['Tottenham', 'Wolves', '5', '2', '10', '8', '0', '0', '12', '16', '4', '5', '14', '8'], ['Everton', 'Man City', '0', '0', '2', '7', '0', '0', '9', '15', '3', '5', '11', '13'], ['Southampton', 'Charlton', '3', '2', '14', '14', '0', '0', '18', '20', '8', '9', '12', '12'], ['Chelsea', 'Bolton', '1', '2', '5', '2', '0', '0', '9', '4', '8', '3', '13', '11'], ['Leicester', 'Birmingham', '0', '2', '5', '4', '2', '0', '8', '9', '6', '7', '19', '16'], ['Liverpool', 'Southampton', '1', '2', '10', '4', '0', '0', '18', '8', '9', '5', '6', '8'], ['Man United', 'Man City', '3', '1', '7', '9', '0', '0', '10', '12', '2', '5', '8', '10'], ['Middlesbrough', 'Charlton', '0', '0', '6', '3', '0', '0', '12', '10', '3', '7', '13', '11'], ['Newcastle', 'Tottenham', '4', '0', '11', '4', '0', '0', '19', '4', '11', '7', '10', '12'], ['Portsmouth', 'Everton', '1', '2', '10', '4', '0', '0', '12', '9', '7', '10', '13', '13'], ['Arsenal', 'Blackburn', '1', '0', '7', '1', '0', '0', '11', '5', '5', '6', '14', '14'], ['Aston Villa', 'Wolves', '3', '2', '8', '5', '0', '0', '11', '8', '4', '6', '12', '14'], ['Leeds', 'Fulham', '3', '2', '11', '7', '0', '0', '14', '10', '6', '5', '17', '11'], ['Blackburn', 'Aston Villa', '0', '2', '5', '7', '0', '0', '12', '15', '9', '1', '7', '11'], ['Bolton', 'Arsenal', '1', '1', '13', '4', '0', '0', '19', '9', '8', '1', '11', '13'], ['Charlton', 'Newcastle', '0', '0', '10', '5', '0', '0', '18', '11', '7', '9', '11', '9'], ['Everton', 'Leicester', '3', '2', '8', '4', '0', '0', '15', '6', '11', '7', '13', '18'], ['Fulham', 'Chelsea', '0', '1', '2', '9', '0', '0', '7', '11', '2', '8', '11', '10'], ['Southampton', 'Portsmouth', '3', '0', '7', '7', '0', '0', '12', '11', '8', '5', '7', '21'], ['Tottenham', 'Man United', '1', '2', '8', '7', '0', '0', '14', '12', '7', '8', '12', '8'], ['Man City', 'Leeds', '1', '1', '11', '4', '0', '0', '19', '6', '8', '5', '7', '8'], ['Arsenal', 'Wolves', '3', '0', '10', '1', '0', '0', '15', '5', '14', '3', '15', '17'], ['Birmingham', 'Man City', '2', '1', '3', '5', '0', '0', '5', '9', '9', '3', '18', '14'], ['Blackburn', 'Middlesbrough', '2', '2', '10', '3', '0', '0', '13', '5', '14', '1', '11', '10'], ['Charlton', 'Chelsea', '4', '2', '4', '8', '0', '0', '8', '15', '8', '6', '14', '10'], ['Fulham', 'Southampton', '2', '0', '7', '2', '0', '0', '14', '6', '11', '5', '13', '12'], ['Leeds', 'Aston Villa', '0', '0', '3', '4', '0', '0', '6', '13', '7', '5', '15', '13'], ['Leicester', 'Newcastle', '1', '1', '5', '6', '0', '0', '6', '12', '6', '7', '11', '10'], ['Liverpool', 'Bolton', '3', '1', '8', '2', '0', '0', '13', '7', '7', '1', '10', '18'], ['Man United', 'Everton', '3', '2', '11', '6', '0', '0', '19', '10', '7', '2', '8', '10'], ['Portsmouth', 'Tottenham', '2', '0', '8', '3', '0', '0', '10', '5', '3', '6', '10', '10'], ['Aston Villa', 'Fulham', '3', '0', '8', '3', '0', '0', '12', '9', '8', '3', '17', '7'], ['Bolton', 'Leicester', '2', '2', '8', '6', '0', '0', '15', '7', '3', '7', '16', '17'], ['Chelsea', 'Portsmouth', '3', '0', '8', '3', '0', '0', '15', '7', '7', '1', '8', '14'], ['Everton', 'Birmingham', '1', '0', '6', '2', '0', '0', '16', '5', '5', '2', '7', '13'], ['Man City', 'Liverpool', '2', '2', '8', '9', '0', '0', '16', '11', '7', '7', '6', '14'], ['Middlesbrough', 'Man United', '0', '1', '2', '6', '0', '1', '11', '10', '4', '2', '11', '15'], ['Newcastle', 'Blackburn', '0', '1', '11', '3', '0', '0', '15', '11', '8', '8', '6', '13'], ['Tottenham', 'Charlton', '0', '1', '9', '8', '0', '0', '18', '10', '11', '11', '8', '13'], ['Wolves', 'Leeds', '3', '1', '4', '4', '0', '1', '11', '9', '6', '10', '11', '14'], ['Southampton', 'Arsenal', '0', '1', '3', '9', '0', '0', '6', '17', '2', '3', '14', '10'], ['Aston Villa', 'Portsmouth', '2', '1', '13', '4', '0', '0', '18', '11', '9', '6', '17', '14'], ['Bolton', 'Man United', '1', '2', '5', '7', '0', '0', '15', '11', '10', '6', '4', '8'], ['Chelsea', 'Liverpool', '0', '1', '2', '1', '0', '1', '9', '4', '8', '3', '15', '9'], ['Everton', 'Arsenal', '1', '1', '4', '5', '0', '0', '8', '9', '5', '4', '13', '16'], ['Man City', 'Charlton', '1', '1', '10', '4', '0', '0', '18', '9', '2', '2', '9', '11'], ['Middlesbrough', 'Fulham', '2', '1', '5', '4', '0', '0', '10', '9', '9', '1', '14', '17'], ['Newcastle', 'Leeds', '1', '0', '6', '5', '0', '0', '13', '9', '5', '3', '14', '17'], ['Southampton', 'Leicester', '0', '0', '14', '8', '0', '0', '14', '9', '4', '5', '12', '15'], ['Tottenham', 'Birmingham', '4', '1', '7', '3', '0', '0', '11', '9', '1', '6', '15', '6'], ['Wolves', 'Blackburn', '2', '2', '5', '5', '0', '0', '10', '9', '2', '7', '13', '20'], ['Arsenal', 'Middlesbrough', '4', '1', '13', '4', '0', '0', '20', '5', '7', '3', '12', '21'], ['Birmingham', 'Southampton', '2', '1', '6', '6', '0', '1', '11', '9', '7', '10', '14', '23'], ['Blackburn', 'Bolton', '3', '4', '3', '10', '0', '0', '8', '16', '4', '7', '15', '13'], ['Charlton', 'Wolves', '2', '0', '4', '4', '0', '0', '10', '7', '6', '5', '6', '12'], ['Fulham', 'Everton', '2', '1', '10', '5', '0', '0', '14', '11', '5', '7', '19', '9'], ['Leeds', 'Tottenham', '0', '1', '1', '8', '0', '0', '7', '15', '4', '5', '17', '13'], ['Liverpool', 'Aston Villa', '1', '0', '4', '2', '0', '0', '12', '8', '3', '6', '14', '15'], ['Portsmouth', 'Man City', '4', '2', '8', '6', '0', '0', '11', '15', '2', '7', '7', '8'], ['Leicester', 'Chelsea', '0', '4', '3', '6', '0', '0', '6', '11', '6', '2', '17', '7'], ['Man United', 'Newcastle', '0', '0', '7', '3', '0', '0', '12', '7', '2', '3', '13', '11'], ['Bolton', 'Portsmouth', '1', '0', '6', '3', '0', '1', '13', '8', '2', '2', '9', '13'], ['Everton', 'Charlton', '0', '1', '7', '4', '0', '0', '17', '7', '10', '5', '6', '14'], ['Man City', 'Blackburn', '1', '1', '7', '3', '0', '0', '10', '6', '4', '2', '7', '13'], ['Middlesbrough', 'Leicester', '3', '3', '7', '8', '0', '0', '13', '14', '6', '6', '15', '12'], ['Southampton', 'Leeds', '2', '1', '6', '6', '0', '0', '18', '11', '1', '6', '16', '8'], ['Tottenham', 'Liverpool', '2', '1', '3', '4', '0', '0', '7', '10', '5', '6', '18', '9'], ['Wolves', 'Man United', '1', '0', '6', '6', '0', '0', '11', '15', '4', '12', '13', '5'], ['Aston Villa', 'Arsenal', '0', '2', '5', '9', '0', '0', '12', '13', '3', '2', '17', '21'], ['Chelsea', 'Birmingham', '0', '0', '9', '1', '0', '0', '16', '2', '6', '2', '13', '8'], ['Newcastle', 'Fulham', '3', '1', '9', '6', '0', '0', '10', '13', '7', '4', '10', '8'], ['Wolves', 'Liverpool', '1', '1', '6', '10', '0', '0', '9', '14', '7', '5', '14', '16'], ['Birmingham', 'Newcastle', '1', '1', '5', '2', '0', '0', '15', '4', '2', '3', '13', '11'], ['Charlton', 'Bolton', '1', '2', '7', '10', '0', '0', '14', '14', '5', '6', '9', '9'], ['Fulham', 'Tottenham', '2', '1', '10', '4', '0', '0', '16', '4', '10', '13', '6', '12'], ['Leeds', 'Middlesbrough', '0', '3', '4', '6', '1', '0', '8', '15', '7', '6', '14', '6'], ['Leicester', 'Aston Villa', '0', '5', '9', '10', '0', '0', '13', '14', '5', '9', '12', '7'], ['Liverpool', 'Everton', '0', '0', '9', '4', '0', '0', '21', '6', '8', '3', '7', '11'], ['Man United', 'Southampton', '3', '2', '7', '11', '0', '0', '12', '17', '1', '10', '9', '11'], ['Portsmouth', 'Wolves', '0', '0', '10', '4', '0', '0', '17', '8', '6', '6', '9', '6'], ['Arsenal', 'Man City', '2', '1', '9', '8', '0', '1', '11', '9', '5', '4', '13', '13'], ['Blackburn', 'Chelsea', '2', '3', '7', '9', '0', '0', '13', '17', '5', '6', '9', '14'], ['Aston Villa', 'Leeds', '2', '0', '7', '4', '0', '0', '15', '7', '3', '10', '15', '16'], ['Bolton', 'Liverpool', '2', '2', '6', '6', '0', '0', '12', '11', '3', '5', '15', '15'], ['Everton', 'Man United', '3', '4', '8', '9', '0', '0', '12', '19', '7', '7', '11', '14'], ['Middlesbrough', 'Blackburn', '0', '1', '7', '2', '0', '0', '15', '6', '8', '3', '6', '12'], ['Newcastle', 'Leicester', '3', '1', '9', '3', '0', '0', '15', '7', '9', '1', '17', '11'], ['Southampton', 'Fulham', '0', '0', '6', '8', '0', '0', '11', '11', '17', '3', '12', '18'], ['Tottenham', 'Portsmouth', '4', '3', '13', '9', '0', '0', '15', '16', '6', '5', '16', '13'], ['Wolves', 'Arsenal', '1', '3', '5', '7', '0', '0', '12', '10', '6', '4', '18', '18'], ['Chelsea', 'Charlton', '1', '0', '6', '0', '0', '0', '16', '5', '4', '3', '9', '17'], ['Man City', 'Birmingham', '0', '0', '12', '2', '0', '0', '21', '7', '11', '5', '11', '10'], ['Arsenal', 'Southampton', '2', '0', '6', '6', '0', '0', '11', '10', '5', '9', '13', '12'], ['Leeds', 'Wolves', '4', '1', '8', '5', '0', '0', '20', '11', '4', '3', '15', '14'], ['Leicester', 'Bolton', '1', '1', '15', '9', '0', '0', '15', '9', '8', '7', '16', '13'], ['Birmingham', 'Everton', '3', '0', '11', '3', '0', '0', '15', '5', '8', '11', '10', '14'], ['Blackburn', 'Newcastle', '1', '1', '7', '7', '0', '0', '17', '14', '8', '6', '12', '10'], ['Charlton', 'Tottenham', '2', '4', '14', '10', '0', '0', '18', '12', '10', '4', '13', '14'], ['Fulham', 'Aston Villa', '1', '2', '6', '3', '1', '0', '13', '10', '6', '5', '16', '17'], ['Liverpool', 'Man City', '2', '1', '7', '3', '0', '0', '12', '6', '6', '3', '7', '8'], ['Man United', 'Middlesbrough', '2', '3', '13', '8', '0', '0', '23', '12', '10', '5', '13', '17'], ['Portsmouth', 'Chelsea', '0', '2', '5', '4', '0', '0', '11', '10', '9', '10', '8', '13'], ['Bolton', 'Man City', '1', '3', '11', '13', '0', '0', '16', '17', '12', '11', '9', '10'], ['Charlton', 'Blackburn', '3', '2', '7', '12', '0', '0', '11', '18', '8', '12', '7', '6'], ['Chelsea', 'Arsenal', '1', '2', '4', '7', '1', '0', '10', '9', '11', '5', '16', '18'], ['Man United', 'Leeds', '1', '1', '7', '3', '0', '0', '24', '7', '12', '7', '12', '17'], ['Newcastle', 'Middlesbrough', '2', '1', '9', '4', '0', '0', '16', '8', '7', '3', '20', '21'], ['Southampton', 'Everton', '3', '3', '5', '6', '0', '0', '9', '12', '6', '5', '14', '19'], ['Wolves', 'Fulham', '2', '1', '10', '5', '0', '0', '12', '9', '8', '6', '13', '16'], ['Aston Villa', 'Birmingham', '2', '2', '8', '9', '0', '0', '11', '13', '7', '5', '15', '14'], ['Tottenham', 'Leicester', '4', '4', '9', '7', '0', '1', '10', '14', '4', '3', '9', '14'], ['Arsenal', 'Charlton', '2', '1', '11', '3', '0', '0', '21', '6', '12', '3', '8', '10'], ['Blackburn', 'Southampton', '1', '1', '7', '6', '0', '0', '16', '10', '6', '9', '10', '8'], ['Everton', 'Aston Villa', '2', '0', '7', '8', '0', '0', '15', '12', '5', '11', '13', '18'], ['Fulham', 'Man United', '1', '1', '3', '7', '0', '0', '7', '13', '4', '3', '13', '19'], ['Leicester', 'Wolves', '0', '0', '6', '3', '0', '0', '10', '6', '6', '3', '7', '9'], ['Man City', 'Chelsea', '0', '1', '4', '4', '0', '0', '14', '9', '7', '1', '8', '11'], ['Leeds', 'Liverpool', '2', '2', '6', '13', '0', '0', '12', '22', '6', '8', '13', '11'], ['Portsmouth', 'Newcastle', '1', '1', '8', '2', '0', '0', '18', '5', '8', '6', '9', '9'], ['Birmingham', 'Middlesbrough', '3', '1', '5', '6', '0', '1', '9', '8', '4', '5', '13', '20'], ['Birmingham', 'Bolton', '2', '0', '6', '2', '0', '0', '10', '10', '3', '5', '10', '15'], ['Middlesbrough', 'Tottenham', '1', '0', '6', '2', '0', '0', '10', '10', '3', '5', '10', '15'], ['Birmingham', 'Leicester', '0', '1', '4', '2', '0', '0', '6', '4', '8', '6', '12', '26'], ['Blackburn', 'Arsenal', '0', '2', '5', '5', '0', '0', '10', '14', '8', '3', '12', '11'], ['Bolton', 'Chelsea', '0', '2', '11', '8', '0', '0', '20', '12', '8', '6', '8', '13'], ['Charlton', 'Middlesbrough', '1', '0', '7', '9', '0', '0', '14', '15', '4', '7', '13', '9'], ['Everton', 'Portsmouth', '1', '0', '6', '3', '0', '0', '13', '5', '6', '4', '10', '12'], ['Fulham', 'Leeds', '2', '0', '10', '6', '0', '0', '19', '9', '4', '7', '14', '13'], ['Man City', 'Man United', '4', '1', '11', '9', '0', '0', '17', '17', '7', '4', '18', '14'], ['Southampton', 'Liverpool', '2', '0', '3', '12', '0', '0', '7', '18', '1', '7', '20', '8'], ['Tottenham', 'Newcastle', '1', '0', '4', '4', '0', '0', '11', '6', '3', '12', '13', '13'], ['Wolves', 'Aston Villa', '0', '4', '1', '7', '0', '0', '9', '15', '8', '13', '10', '14'], ['Liverpool', 'Portsmouth', '3', '0', '13', '4', '0', '0', '20', '4', '10', '8', '9', '20'], ['Arsenal', 'Bolton', '2', '1', '8', '2', '0', '0', '11', '8', '6', '1', '12', '14'], ['Aston Villa', 'Blackburn', '0', '2', '7', '7', '0', '0', '10', '16', '4', '6', '11', '14'], ['Chelsea', 'Fulham', '2', '1', '8', '1', '0', '0', '13', '5', '9', '1', '12', '13'], ['Leicester', 'Everton', '1', '1', '4', '3', '0', '1', '10', '7', '9', '2', '12', '21'], ['Liverpool', 'Wolves', '1', '0', '4', '6', '0', '0', '12', '8', '14', '4', '5', '12'], ['Man United', 'Tottenham', '3', '0', '6', '3', '0', '0', '11', '8', '4', '2', '14', '10'], ['Middlesbrough', 'Birmingham', '5', '3', '9', '12', '0', '0', '13', '18', '4', '4', '11', '13'], ['Newcastle', 'Charlton', '3', '1', '10', '2', '0', '0', '15', '6', '6', '6', '11', '12'], ['Portsmouth', 'Southampton', '1', '0', '8', '8', '0', '0', '16', '13', '6', '4', '16', '12'], ['Leeds', 'Man City', '2', '1', '3', '14', '0', '1', '4', '21', '4', '5', '13', '14'], ['Birmingham', 'Leeds', '4', '1', '7', '7', '0', '0', '17', '11', '7', '7', '11', '18'], ['Blackburn', 'Portsmouth', '1', '2', '9', '4', '0', '0', '14', '8', '5', '3', '11', '10'], ['Charlton', 'Aston Villa', '1', '2', '9', '11', '0', '0', '11', '19', '3', '12', '11', '14'], ['Chelsea', 'Wolves', '5', '2', '10', '4', '0', '0', '15', '9', '7', '4', '6', '12'], ['Everton', 'Middlesbrough', '1', '1', '7', '5', '0', '0', '14', '10', '8', '4', '16', '13'], ['Man City', 'Fulham', '0', '0', '4', '2', '0', '0', '12', '5', '3', '1', '12', '10'], ['Southampton', 'Tottenham', '1', '0', '11', '9', '0', '0', '18', '15', '10', '6', '8', '16'], ['Arsenal', 'Man United', '1', '1', '10', '8', '0', '0', '17', '13', '8', '7', '15', '18'], ['Bolton', 'Newcastle', '1', '0', '12', '7', '0', '0', '20', '15', '7', '7', '11', '10'], ['Leicester', 'Liverpool', '0', '0', '9', '12', '0', '0', '13', '19', '7', '10', '15', '20'], ['Fulham', 'Birmingham', '0', '0', '5', '1', '0', '0', '6', '6', '4', '2', '24', '22'], ['Middlesbrough', 'Bolton', '2', '0', '6', '8', '0', '0', '10', '13', '7', '8', '13', '15'], ['Newcastle', 'Everton', '4', '2', '8', '6', '0', '0', '12', '13', '5', '9', '11', '7'], ['Tottenham', 'Chelsea', '0', '1', '7', '7', '0', '0', '12', '10', '5', '4', '10', '10'], ['Wolves', 'Southampton', '1', '4', '3', '10', '0', '0', '8', '13', '3', '6', '12', '8'], ['Aston Villa', 'Man City', '1', '1', '3', '4', '0', '0', '6', '7', '5', '4', '13', '13'], ['Liverpool', 'Blackburn', '4', '0', '11', '2', '0', '0', '24', '9', '6', '4', '8', '10'], ['Leeds', 'Leicester', '3', '2', '10', '9', '1', '0', '15', '14', '7', '11', '19', '14'], ['Arsenal', 'Liverpool', '4', '2', '7', '5', '0', '0', '12', '12', '2', '8', '16', '9'], ['Everton', 'Tottenham', '3', '1', '10', '7', '0', '1', '13', '12', '8', '4', '9', '14'], ['Birmingham', 'Man United', '1', '2', '8', '6', '0', '0', '11', '10', '0', '3', '11', '10'], ['Blackburn', 'Leeds', '1', '2', '7', '3', '0', '0', '9', '8', '6', '4', '14', '13'], ['Bolton', 'Aston Villa', '2', '2', '7', '3', '0', '0', '16', '7', '6', '4', '10', '12'], ['Charlton', 'Portsmouth', '1', '1', '3', '8', '0', '0', '9', '10', '6', '2', '14', '8'], ['Chelsea', 'Middlesbrough', '0', '0', '9', '5', '0', '0', '19', '9', '6', '2', '10', '8'], ['Leicester', 'Fulham', '0', '2', '7', '4', '0', '0', '10', '9', '15', '4', '13', '15'], ['Man City', 'Wolves', '3', '3', '8', '9', '0', '0', '14', '15', '5', '6', '9', '12'], ['Newcastle', 'Arsenal', '0', '0', '5', '5', '0', '0', '9', '11', '5', '4', '14', '12'], ['Aston Villa', 'Chelsea', '3', '2', '3', '2', '0', '0', '5', '6', '7', '2', '8', '14'], ['Fulham', 'Blackburn', '3', '4', '7', '7', '0', '0', '10', '11', '6', '1', '12', '14'], ['Liverpool', 'Charlton', '0', '1', '16', '3', '0', '0', '25', '6', '7', '4', '12', '17'], ['Middlesbrough', 'Southampton', '3', '1', '18', '8', '0', '0', '25', '16', '6', '3', '11', '14'], ['Portsmouth', 'Birmingham', '3', '1', '6', '2', '0', '1', '12', '6', '4', '4', '19', '11'], ['Tottenham', 'Man City', '1', '1', '10', '8', '0', '0', '15', '9', '4', '3', '10', '21'], ['Wolves', 'Bolton', '1', '2', '3', '4', '0', '0', '8', '13', '6', '9', '12', '11'], ['Leeds', 'Everton', '1', '1', '7', '4', '0', '0', '14', '8', '10', '5', '12', '12'], ['Man United', 'Leicester', '1', '0', '12', '5', '0', '0', '16', '12', '7', '4', '11', '11'], ['Arsenal', 'Leeds', '5', '0', '8', '2', '0', '0', '15', '7', '3', '8', '10', '10'], ['Blackburn', 'Leicester', '1', '0', '4', '4', '0', '0', '12', '8', '2', '4', '16', '14'], ['Bolton', 'Tottenham', '2', '0', '10', '0', '0', '0', '15', '4', '11', '5', '18', '21'], ['Charlton', 'Birmingham', '1', '1', '5', '4', '0', '0', '12', '7', '9', '3', '14', '9'], ['Chelsea', 'Everton', '0', '0', '6', '4', '0', '0', '16', '5', '8', '4', '11', '10'], ['Liverpool', 'Fulham', '0', '0', '10', '7', '0', '0', '18', '11', '14', '7', '16', '10'], ['Man City', 'Southampton', '1', '3', '7', '5', '0', '0', '15', '9', '6', '3', '11', '9'], ['Portsmouth', 'Man United', '1', '0', '4', '12', '0', '0', '9', '19', '3', '13', '14', '8'], ['Wolves', 'Middlesbrough', '2', '0', '8', '10', '0', '0', '12', '16', '6', '8', '10', '10'], ['Aston Villa', 'Newcastle', '0', '0', '1', '4', '0', '1', '10', '5', '7', '3', '17', '13'], ['Man United', 'Charlton', '2', '0', '6', '0', '0', '0', '10', '5', '8', '3', '10', '8'], ['Everton', 'Blackburn', '0', '1', '9', '7', '0', '0', '16', '10', '8', '5', '8', '22'], ['Fulham', 'Charlton', '2', '0', '6', '3', '0', '0', '8', '8', '4', '7', '15', '13'], ['Leicester', 'Man City', '1', '1', '5', '5', '0', '0', '12', '11', '4', '2', '13', '14'], ['Man United', 'Liverpool', '0', '1', '4', '4', '0', '0', '11', '9', '3', '4', '19', '15'], ['Middlesbrough', 'Aston Villa', '1', '2', '21', '9', '0', '1', '31', '12', '12', '8', '12', '15'], ['Southampton', 'Bolton', '1', '2', '3', '8', '0', '0', '9', '11', '2', '2', '13', '16'], ['Birmingham', 'Wolves', '2', '2', '9', '7', '0', '0', '14', '11', '9', '2', '11', '13'], ['Leeds', 'Portsmouth', '1', '2', '6', '5', '0', '0', '14', '7', '9', '2', '9', '21'], ['Newcastle', 'Chelsea', '2', '1', '12', '6', '0', '0', '21', '17', '7', '5', '12', '12'], ['Tottenham', 'Arsenal', '2', '2', '7', '7', '0', '0', '13', '9', '7', '4', '14', '15'], ['Arsenal', 'Birmingham', '0', '0', '1', '3', '0', '0', '7', '3', '2', '3', '13', '16'], ['Blackburn', 'Man United', '1', '0', '8', '5', '0', '0', '12', '9', '5', '4', '11', '10'], ['Charlton', 'Leicester', '2', '2', '7', '5', '0', '1', '8', '10', '5', '4', '15', '7'], ['Chelsea', 'Southampton', '4', '0', '7', '3', '0', '0', '12', '5', '12', '3', '6', '5'], ['Man City', 'Newcastle', '1', '0', '3', '6', '0', '0', '7', '7', '7', '5', '7', '9'], ['Portsmouth', 'Fulham', '1', '1', '7', '7', '0', '0', '14', '11', '10', '3', '14', '13'], ['Wolves', 'Everton', '2', '1', '11', '6', '0', '0', '20', '10', '12', '7', '7', '12'], ['Aston Villa', 'Tottenham', '1', '0', '6', '5', '0', '0', '11', '10', '5', '5', '15', '20'], ['Bolton', 'Leeds', '4', '1', '14', '4', '0', '1', '21', '6', '9', '3', '14', '20'], ['Liverpool', 'Middlesbrough', '2', '0', '8', '6', '0', '0', '15', '12', '7', '3', '12', '14'], ['Portsmouth', 'Arsenal', '1', '1', '6', '9', '0', '0', '11', '20', '5', '12', '13', '10'], ['Birmingham', 'Liverpool', '0', '3', '2', '8', '1', '0', '3', '15', '3', '5', '11', '12'], ['Everton', 'Bolton', '1', '2', '9', '3', '0', '0', '16', '10', '7', '3', '14', '9'], ['Leeds', 'Charlton', '3', '3', '7', '6', '0', '0', '13', '12', '3', '2', '12', '9'], ['Leicester', 'Portsmouth', '3', '1', '9', '7', '0', '0', '13', '14', '8', '3', '11', '15'], ['Man United', 'Chelsea', '1', '1', '7', '7', '0', '1', '12', '11', '6', '2', '9', '19'], ['Middlesbrough', 'Man City', '2', '1', '7', '6', '0', '0', '12', '14', '5', '4', '9', '14'], ['Southampton', 'Aston Villa', '1', '1', '10', '7', '0', '0', '18', '14', '4', '4', '10', '13'], ['Tottenham', 'Blackburn', '1', '0', '13', '2', '0', '0', '16', '14', '7', '7', '9', '17'], ['Fulham', 'Arsenal', '0', '1', '5', '5', '0', '0', '12', '7', '6', '7', '13', '19'], ['Newcastle', 'Wolves', '1', '1', '8', '7', '0', '0', '19', '15', '13', '3', '13', '11'], ['Southampton', 'Newcastle', '3', '3', '12', '12', '0', '0', '15', '22', '3', '5', '13', '16'], ['Arsenal', 'Leicester', '2', '1', '10', '1', '0', '0', '17', '2', '7', '5', '15', '13'], ['Aston Villa', 'Man United', '0', '2', '8', '8', '0', '2', '16', '12', '9', '4', '21', '15'], ['Blackburn', 'Birmingham', '1', '1', '5', '5', '0', '0', '10', '9', '6', '8', '11', '12'], ['Bolton', 'Fulham', '0', '2', '9', '6', '0', '0', '17', '10', '11', '3', '9', '14'], ['Charlton', 'Southampton', '2', '1', '5', '4', '0', '0', '10', '12', '7', '10', '13', '10'], ['Chelsea', 'Leeds', '1', '0', '9', '0', '0', '0', '19', '4', '9', '2', '5', '9'], ['Liverpool', 'Newcastle', '1', '1', '4', '5', '0', '0', '8', '6', '8', '1', '11', '17'], ['Man City', 'Everton', '5', '1', '11', '0', '0', '0', '13', '2', '7', '3', '5', '8'], ['Portsmouth', 'Middlesbrough', '5', '1', '10', '4', '0', '0', '16', '19', '7', '2', '12', '8'], ['Wolves', 'Tottenham', '0', '2', '10', '12', '1', '0', '18', '14', '5', '3', '6', '11'], [], ['Aston Villa', 'Southampton', '2', '0', '5', '2', '0', '0', '14', '6', '12', '6', '14', '9'], ['Blackburn', 'West Brom', '1', '1', '4', '2', '0', '0', '12', '4', '4', '5', '15', '17'], ['Bolton', 'Charlton', '4', '1', '11', '5', '0', '0', '21', '9', '9', '5', '10', '12'], ['Man City', 'Fulham', '1', '1', '5', '2', '0', '0', '12', '4', '9', '4', '14', '12'], ['Middlesbrough', 'Newcastle', '2', '2', '8', '4', '0', '0', '15', '11', '6', '7', '16', '13'], ['Norwich', 'Crystal Palace', '1', '1', '10', '8', '0', '0', '14', '14', '6', '11', '14', '16'], ['Portsmouth', 'Birmingham', '1', '1', '8', '11', '0', '0', '14', '16', '8', '4', '13', '16'], ['Tottenham', 'Liverpool', '1', '1', '7', '8', '0', '0', '14', '16', '3', '8', '17', '11'], ['Chelsea', 'Man United', '1', '0', '5', '3', '0', '0', '8', '11', '2', '3', '17', '8'], ['Everton', 'Arsenal', '1', '4', '5', '14', '0', '0', '9', '18', '0', '7', '14', '19'], ['Birmingham', 'Chelsea', '0', '1', '3', '3', '0', '0', '9', '7', '8', '2', '10', '16'], ['Charlton', 'Portsmouth', '2', '1', '14', '6', '0', '0', '18', '10', '5', '6', '9', '11'], ['Crystal Palace', 'Everton', '1', '3', '9', '7', '0', '1', '15', '14', '6', '6', '6', '18'], ['Fulham', 'Bolton', '2', '0', '12', '4', '0', '0', '23', '10', '8', '4', '11', '13'], ['Liverpool', 'Man City', '2', '1', '10', '4', '0', '1', '15', '7', '8', '3', '9', '17'], ['Man United', 'Norwich', '2', '1', '7', '2', '0', '0', '17', '7', '11', '5', '8', '23'], ['Newcastle', 'Tottenham', '0', '1', '6', '8', '0', '0', '13', '15', '13', '3', '10', '12'], ['Southampton', 'Blackburn', '3', '2', '7', '6', '0', '0', '13', '10', '3', '6', '12', '18'], ['Arsenal', 'Middlesbrough', '5', '3', '11', '5', '0', '0', '16', '9', '7', '5', '8', '14'], ['West Brom', 'Aston Villa', '1', '1', '5', '4', '0', '0', '14', '9', '12', '4', '12', '20'], ['Birmingham', 'Man City', '1', '0', '2', '1', '0', '0', '6', '4', '2', '5', '20', '13'], ['Crystal Palace', 'Chelsea', '0', '2', '4', '10', '0', '0', '7', '16', '1', '6', '9', '12'], ['Arsenal', 'Blackburn', '3', '0', '9', '0', '0', '0', '15', '3', '6', '5', '11', '18'], ['Charlton', 'Aston Villa', '3', '0', '5', '4', '0', '0', '7', '9', '4', '12', '6', '7'], ['Fulham', 'Middlesbrough', '0', '2', '2', '5', '0', '0', '6', '10', '1', '6', '10', '8'], ['Newcastle', 'Norwich', '2', '2', '15', '5', '0', '0', '23', '13', '12', '6', '17', '10'], ['Southampton', 'Bolton', '1', '2', '6', '5', '0', '0', '10', '9', '9', '6', '9', '8'], ['West Brom', 'Tottenham', '1', '1', '2', '6', '0', '0', '4', '12', '4', '5', '13', '10'], ['Aston Villa', 'Newcastle', '4', '2', '13', '10', '0', '0', '16', '12', '9', '4', '23', '11'], ['Blackburn', 'Man United', '1', '1', '4', '15', '1', '0', '8', '24', '4', '14', '25', '17'], ['Chelsea', 'Southampton', '2', '1', '7', '2', '0', '0', '16', '6', '9', '0', '11', '12'], ['Everton', 'West Brom', '2', '1', '10', '8', '0', '0', '16', '12', '7', '6', '11', '16'], ['Man City', 'Charlton', '4', '0', '10', '3', '0', '0', '16', '6', '6', '4', '11', '8'], ['Middlesbrough', 'Crystal Palace', '2', '1', '15', '2', '0', '0', '24', '6', '4', '2', '16', '16'], ['Norwich', 'Arsenal', '1', '4', '5', '15', '0', '0', '5', '18', '4', '10', '15', '5'], ['Tottenham', 'Birmingham', '1', '0', '3', '2', '0', '0', '6', '10', '2', '8', '15', '5'], ['Bolton', 'Liverpool', '1', '0', '8', '2', '0', '0', '10', '7', '5', '2', '15', '14'], ['Man United', 'Everton', '0', '0', '6', '2', '0', '0', '14', '7', '8', '2', '8', '13'], ['Portsmouth', 'Fulham', '4', '3', '13', '10', '0', '0', '21', '18', '6', '8', '12', '12'], ['Aston Villa', 'Chelsea', '0', '0', '5', '10', '0', '0', '11', '18', '2', '9', '9', '12'], ['Bolton', 'Man United', '2', '2', '7', '7', '0', '0', '12', '12', '4', '7', '22', '15'], ['Fulham', 'Arsenal', '0', '3', '7', '7', '0', '0', '9', '12', '9', '4', '16', '10'], ['Liverpool', 'West Brom', '3', '0', '9', '5', '0', '0', '16', '9', '8', '6', '9', '11'], ['Man City', 'Everton', '0', '1', '4', '5', '0', '1', '13', '6', '9', '6', '14', '9'], ['Middlesbrough', 'Birmingham', '2', '1', '6', '3', '0', '0', '14', '5', '6', '3', '16', '19'], ['Newcastle', 'Blackburn', '3', '0', '4', '1', '0', '0', '7', '5', '8', '5', '10', '19'], ['Portsmouth', 'Crystal Palace', '3', '1', '7', '3', '0', '0', '16', '6', '8', '7', '14', '21'], ['Tottenham', 'Norwich', '0', '0', '7', '2', '0', '0', '15', '7', '13', '5', '17', '9'], ['Charlton', 'Southampton', '0', '0', '4', '7', '0', '0', '8', '12', '9', '10', '12', '13'], ['Arsenal', 'Bolton', '2', '2', '6', '5', '0', '0', '11', '8', '3', '8', '13', '17'], ['Birmingham', 'Charlton', '1', '1', '5', '1', '1', '0', '7', '2', '12', '3', '12', '16'], ['Blackburn', 'Portsmouth', '1', '0', '3', '2', '0', '0', '13', '6', '13', '4', '13', '15'], ['Crystal Palace', 'Man City', '1', '2', '1', '10', '0', '0', '3', '14', '2', '7', '14', '15'], ['Norwich', 'Aston Villa', '0', '0', '3', '9', '0', '0', '14', '18', '1', '6', '10', '13'], ['West Brom', 'Fulham', '1', '1', '5', '3', '1', '2', '8', '9', '11', '9', '11', '6'], ['Chelsea', 'Tottenham', '0', '0', '8', '3', '0', '0', '20', '6', '9', '6', '16', '13'], ['Everton', 'Middlesbrough', '1', '0', '9', '11', '0', '0', '11', '16', '6', '3', '13', '12'], ['Southampton', 'Newcastle', '1', '2', '6', '3', '0', '0', '11', '9', '7', '6', '15', '11'], ['Man United', 'Liverpool', '2', '1', '9', '3', '0', '0', '14', '6', '7', '2', '22', '15'], ['Aston Villa', 'Crystal Palace', '1', '1', '5', '8', '0', '0', '10', '13', '5', '8', '10', '15'], ['Bolton', 'Birmingham', '1', '1', '12', '2', '0', '0', '16', '6', '7', '2', '7', '18'], ['Fulham', 'Southampton', '1', '0', '6', '5', '0', '0', '11', '15', '5', '4', '11', '19'], ['Liverpool', 'Norwich', '3', '0', '12', '1', '0', '0', '20', '2', '4', '2', '10', '12'], ['Man City', 'Arsenal', '0', '1', '7', '9', '0', '0', '11', '12', '8', '1', '16', '8'], ['Middlesbrough', 'Chelsea', '0', '1', '4', '12', '0', '0', '8', '17', '3', '7', '10', '15'], ['Newcastle', 'West Brom', '3', '1', '10', '3', '0', '1', '31', '7', '8', '3', '15', '15'], ['Tottenham', 'Man United', '0', '1', '5', '4', '0', '0', '11', '6', '3', '3', '9', '10'], ['Portsmouth', 'Everton', '0', '1', '6', '2', '0', '0', '11', '9', '3', '6', '12', '16'], ['Charlton', 'Blackburn', '1', '0', '3', '7', '0', '0', '6', '12', '6', '10', '18', '14'], ['Arsenal', 'Charlton', '4', '0', '9', '0', '0', '0', '15', '4', '6', '1', '13', '15'], ['Blackburn', 'Aston Villa', '2', '2', '7', '9', '0', '0', '13', '15', '5', '8', '14', '17'], ['Everton', 'Tottenham', '0', '1', '4', '2', '0', '0', '10', '8', '11', '2', '9', '13'], ['Norwich', 'Portsmouth', '2', '2', '9', '5', '0', '0', '17', '13', '8', '2', '13', '7'], ['Southampton', 'Man City', '0', '0', '11', '11', '0', '0', '16', '21', '6', '14', '13', '11'], ['West Brom', 'Bolton', '2', '1', '7', '7', '0', '0', '13', '16', '6', '5', '18', '15'], ['Birmingham', 'Newcastle', '2', '2', '6', '3', '0', '0', '10', '6', '6', '5', '17', '14'], ['Chelsea', 'Liverpool', '1', '0', '5', '3', '0', '0', '13', '5', '11', '2', '14', '20'], ['Man United', 'Middlesbrough', '1', '1', '10', '9', '0', '0', '22', '13', '10', '2', '8', '13'], ['Crystal Palace', 'Fulham', '2', '0', '10', '5', '0', '1', '20', '8', '10', '3', '10', '10'], ['Arsenal', 'Aston Villa', '3', '1', '15', '2', '0', '0', '18', '5', '6', '3', '11', '22'], ['Birmingham', 'Man United', '0', '0', '6', '7', '0', '0', '14', '12', '3', '3', '16', '11'], ['Blackburn', 'Middlesbrough', '0', '4', '5', '10', '1', '0', '12', '16', '2', '10', '12', '14'], ['Bolton', 'Crystal Palace', '1', '0', '7', '2', '0', '0', '21', '7', '7', '3', '19', '14'], ['Everton', 'Southampton', '1', '0', '5', '4', '0', '0', '14', '8', '5', '6', '5', '14'], ['Fulham', 'Liverpool', '2', '4', '3', '3', '0', '1', '5', '10', '3', '6', '12', '14'], ['Man City', 'Chelsea', '1', '0', '1', '3', '0', '0', '4', '9', '0', '12', '6', '7'], ['West Brom', 'Norwich', '0', '0', '2', '5', '0', '0', '9', '12', '2', '3', '13', '21'], ['Charlton', 'Newcastle', '1', '1', '6', '8', '0', '0', '11', '15', '8', '9', '15', '7'], ['Portsmouth', 'Tottenham', '1', '0', '6', '7', '0', '0', '11', '16', '3', '5', '12', '11'], ['Aston Villa', 'Fulham', '2', '0', '5', '1', '0', '0', '11', '7', '8', '4', '21', '14'], ['Chelsea', 'Blackburn', '4', '0', '13', '4', '0', '0', '17', '7', '12', '3', '11', '11'], ['Crystal Palace', 'West Brom', '3', '0', '4', '6', '0', '0', '11', '11', '7', '10', '11', '15'], ['Liverpool', 'Charlton', '2', '0', '7', '1', '0', '0', '17', '3', '12', '0', '9', '11'], ['Norwich', 'Everton', '2', '3', '10', '9', '0', '0', '22', '13', '11', '4', '8', '11'], ['Tottenham', 'Bolton', '1', '2', '5', '4', '0', '0', '8', '6', '2', '3', '14', '17'], ['Man United', 'Arsenal', '2', '0', '5', '6', '0', '0', '11', '12', '3', '3', '20', '23'], ['Middlesbrough', 'Portsmouth', '1', '1', '11', '1', '0', '0', '26', '3', '9', '0', '5', '5'], ['Newcastle', 'Man City', '4', '3', '11', '6', '0', '0', '16', '11', '7', '5', '13', '14'], ['Southampton', 'Birmingham', '0', '0', '1', '2', '0', '0', '6', '8', '4', '4', '11', '11'], ['Arsenal', 'Southampton', '2', '2', '9', '4', '0', '0', '14', '7', '7', '7', '8', '21'], ['Birmingham', 'Crystal Palace', '0', '1', '11', '1', '0', '0', '16', '4', '11', '3', '8', '14'], ['Blackburn', 'Liverpool', '2', '2', '7', '9', '0', '0', '8', '13', '6', '6', '14', '11'], ['Charlton', 'Middlesbrough', '1', '2', '7', '7', '0', '0', '8', '11', '3', '4', '14', '8'], ['Everton', 'Aston Villa', '1', '1', '8', '5', '0', '0', '20', '9', '8', '3', '10', '19'], ['Fulham', 'Tottenham', '2', '0', '4', '1', '0', '0', '8', '5', '5', '4', '11', '9'], ['Portsmouth', 'Man United', '2', '0', '7', '9', '0', '0', '11', '20', '3', '9', '19', '9'], ['West Brom', 'Chelsea', '1', '4', '9', '9', '0', '0', '12', '19', '2', '6', '12', '8'], ['Bolton', 'Newcastle', '2', '1', '11', '4', '0', '0', '17', '5', '4', '5', '14', '11'], ['Man City', 'Norwich', '1', '1', '11', '5', '0', '0', '19', '10', '9', '4', '10', '13'], ['Aston Villa', 'Portsmouth', '3', '0', '9', '2', '0', '0', '10', '7', '6', '4', '8', '13'], ['Chelsea', 'Everton', '1', '0', '3', '1', '0', '0', '12', '3', '16', '3', '10', '15'], ['Crystal Palace', 'Arsenal', '1', '1', '1', '13', '0', '0', '3', '19', '1', '8', '10', '8'], ['Liverpool', 'Birmingham', '0', '1', '8', '1', '0', '0', '17', '6', '4', '5', '19', '17'], ['Norwich', 'Blackburn', '1', '1', '5', '9', '0', '1', '12', '15', '7', '4', '18', '12'], ['Southampton', 'West Brom', '2', '2', '6', '5', '0', '0', '11', '10', '5', '1', '13', '14'], ['Tottenham', 'Charlton', '2', '3', '11', '3', '0', '1', '19', '3', '5', '1', '6', '13'], ['Man United', 'Man City', '0', '0', '10', '1', '1', '0', '15', '3', '16', '1', '10', '10'], ['Middlesbrough', 'Bolton', '1', '1', '5', '4', '0', '1', '14', '7', '3', '3', '13', '12'], ['Newcastle', 'Fulham', '1', '4', '20', '7', '0', '0', '26', '11', '19', '0', '12', '16'], ['Birmingham', 'Everton', '0', '1', '4', '8', '1', '0', '15', '9', '5', '3', '13', '12'], ['Bolton', 'Aston Villa', '1', '2', '7', '5', '0', '0', '15', '10', '9', '5', '10', '17'], ['Charlton', 'Norwich', '4', '0', '11', '4', '0', '0', '16', '8', '1', '5', '14', '11'], ['Fulham', 'Chelsea', '1', '4', '2', '11', '0', '0', '7', '16', '1', '5', '13', '13'], ['Liverpool', 'Crystal Palace', '3', '2', '5', '4', '0', '0', '18', '4', '7', '1', '13', '17'], ['Man City', 'Blackburn', '1', '1', '7', '2', '1', '0', '8', '11', '3', '5', '19', '13'], ['Southampton', 'Portsmouth', '2', '1', '5', '6', '0', '0', '11', '13', '4', '8', '14', '15'], ['Tottenham', 'Arsenal', '4', '5', '7', '6', '0', '0', '10', '10', '3', '4', '20', '9'], ['Newcastle', 'Man United', '1', '3', '7', '7', '0', '0', '13', '12', '2', '7', '11', '16'], ['West Brom', 'Middlesbrough', '1', '2', '6', '7', '0', '0', '16', '11', '7', '10', '14', '14'], ['Arsenal', 'West Brom', '1', '1', '4', '2', '0', '0', '11', '4', '12', '3', '9', '12'], ['Chelsea', 'Bolton', '2', '2', '11', '5', '0', '0', '17', '10', '9', '2', '18', '11'], ['Crystal Palace', 'Newcastle', '0', '2', '5', '9', '0', '0', '7', '17', '3', '5', '13', '10'], ['Everton', 'Fulham', '1', '0', '7', '4', '0', '0', '13', '6', '12', '7', '11', '24'], ['Man United', 'Charlton', '2', '0', '11', '2', '0', '0', '20', '5', '6', '3', '8', '13'], ['Middlesbrough', 'Liverpool', '2', '0', '6', '10', '0', '0', '14', '17', '3', '5', '14', '10'], ['Norwich', 'Southampton', '2', '1', '8', '7', '0', '0', '9', '14', '4', '6', '19', '14'], ['Portsmouth', 'Man City', '1', '3', '7', '13', '0', '0', '9', '18', '8', '11', '8', '14'], ['Blackburn', 'Birmingham', '3', '3', '10', '5', '0', '0', '15', '8', '6', '9', '12', '9'], ['Aston Villa', 'Tottenham', '1', '0', '5', '6', '0', '0', '9', '11', '5', '5', '17', '11'], ['Birmingham', 'Norwich', '1', '1', '10', '3', '0', '0', '16', '7', '6', '1', '14', '12'], ['Bolton', 'Portsmouth', '0', '1', '10', '4', '0', '0', '19', '8', '7', '6', '13', '14'], ['Charlton', 'Chelsea', '0', '4', '4', '12', '0', '0', '8', '16', '7', '10', '13', '11'], ['Fulham', 'Blackburn', '0', '2', '6', '4', '1', '0', '12', '17', '6', '6', '9', '13'], ['Man City', 'Aston Villa', '2', '0', '6', '3', '0', '1', '11', '7', '8', '8', '14', '19'], ['Southampton', 'Crystal Palace', '2', '2', '8', '8', '0', '0', '15', '11', '4', '8', '7', '11'], ['West Brom', 'Man United', '0', '3', '2', '9', '0', '0', '3', '18', '1', '8', '13', '11'], ['Liverpool', 'Arsenal', '2', '1', '7', '2', '0', '0', '11', '3', '6', '0', '6', '16'], ['Newcastle', 'Everton', '1', '1', '9', '3', '0', '0', '19', '7', '8', '4', '14', '19'], ['Tottenham', 'Middlesbrough', '2', '0', '9', '3', '0', '1', '14', '8', '5', '7', '11', '13'], ['Arsenal', 'Birmingham', '3', '0', '4', '1', '0', '0', '7', '2', '4', '2', '7', '15'], ['Aston Villa', 'Liverpool', '1', '1', '2', '7', '0', '0', '3', '12', '3', '9', '10', '10'], ['Blackburn', 'Tottenham', '0', '1', '7', '8', '0', '0', '12', '10', '7', '5', '11', '11'], ['Chelsea', 'Newcastle', '4', '0', '9', '5', '0', '0', '14', '9', '3', '2', '12', '15'], ['Everton', 'Bolton', '3', '2', '3', '6', '0', '0', '12', '10', '4', '4', '14', '18'], ['Man United', 'Southampton', '3', '0', '14', '0', '0', '0', '34', '4', '16', '2', '5', '10'], ['Norwich', 'Fulham', '0', '1', '4', '9', '0', '0', '10', '15', '5', '4', '22', '22'], ['Portsmouth', 'West Brom', '3', '2', '10', '4', '0', '0', '14', '10', '8', '3', '14', '14'], ['Crystal Palace', 'Charlton', '0', '1', '3', '7', '0', '0', '5', '11', '5', '3', '16', '14'], ['Middlesbrough', 'Man City', '3', '2', '7', '6', '0', '0', '11', '9', '6', '4', '14', '18'], ['Crystal Palace', 'Blackburn', '0', '0', '1', '8', '0', '1', '8', '16', '2', '1', '19', '20'], ['Everton', 'Liverpool', '1', '0', '2', '5', '0', '0', '6', '16', '2', '7', '12', '14'], ['Man City', 'Tottenham', '0', '1', '3', '2', '0', '0', '8', '5', '6', '2', '8', '9'], ['Newcastle', 'Portsmouth', '1', '1', '2', '3', '0', '0', '15', '6', '6', '2', '12', '15'], ['Norwich', 'Bolton', '3', '2', '4', '3', '0', '0', '9', '4', '4', '4', '10', '26'], ['Southampton', 'Middlesbrough', '2', '2', '5', '7', '0', '0', '16', '15', '4', '6', '11', '12'], ['West Brom', 'Charlton', '0', '1', '4', '2', '0', '0', '7', '8', '6', '6', '16', '13'], ['Arsenal', 'Chelsea', '2', '2', '5', '5', '0', '0', '10', '10', '4', '6', '11', '17'], ['Aston Villa', 'Birmingham', '1', '2', '3', '5', '0', '0', '9', '9', '6', '0', '9', '28'], ['Fulham', 'Man United', '1', '1', '3', '6', '0', '0', '7', '14', '6', '7', '15', '11'], ['Liverpool', 'Portsmouth', '1', '1', '7', '4', '0', '0', '12', '7', '6', '4', '6', '9'], ['Birmingham', 'West Brom', '4', '0', '5', '9', '0', '0', '7', '11', '2', '2', '14', '20'], ['Blackburn', 'Everton', '0', '0', '9', '2', '0', '0', '13', '5', '7', '3', '25', '21'], ['Bolton', 'Man City', '0', '1', '6', '2', '0', '0', '14', '6', '8', '1', '17', '12'], ['Chelsea', 'Norwich', '4', '0', '8', '0', '0', '0', '12', '1', '7', '3', '12', '11'], ['Man United', 'Crystal Palace', '5', '2', '14', '3', '0', '0', '19', '4', '15', '3', '12', '8'], ['Middlesbrough', 'Aston Villa', '3', '0', '8', '6', '0', '0', '9', '14', '5', '6', '6', '18'], ['Tottenham', 'Southampton', '5', '1', '12', '4', '0', '0', '18', '7', '6', '3', '9', '13'], ['Liverpool', 'Newcastle', '3', '1', '5', '3', '0', '1', '15', '7', '5', '5', '12', '19'], ['Portsmouth', 'Arsenal', '0', '1', '5', '5', '0', '0', '11', '13', '5', '7', '13', '9'], ['Charlton', 'Fulham', '2', '1', '5', '4', '0', '0', '8', '6', '5', '3', '14', '16'], ['Arsenal', 'Fulham', '2', '0', '8', '1', '0', '0', '11', '2', '7', '6', '12', '9'], ['Birmingham', 'Middlesbrough', '2', '0', '10', '6', '0', '0', '11', '9', '5', '3', '17', '9'], ['Blackburn', 'Newcastle', '2', '2', '11', '3', '0', '0', '21', '5', '7', '5', '18', '19'], ['Chelsea', 'Aston Villa', '1', '0', '7', '7', '0', '0', '14', '9', '6', '3', '12', '13'], ['Crystal Palace', 'Portsmouth', '0', '1', '10', '6', '0', '0', '18', '11', '9', '7', '10', '14'], ['Everton', 'Man City', '2', '1', '6', '6', '0', '1', '9', '10', '4', '4', '14', '21'], ['Man United', 'Bolton', '2', '0', '7', '1', '0', '0', '14', '8', '9', '0', '12', '14'], ['Norwich', 'Tottenham', '0', '2', '8', '9', '0', '0', '13', '16', '10', '8', '11', '9'], ['Southampton', 'Charlton', '0', '0', '4', '5', '0', '0', '12', '15', '5', '11', '7', '12'], ['West Brom', 'Liverpool', '0', '5', '1', '8', '1', '0', '2', '16', '4', '11', '12', '13'], ['Aston Villa', 'Man United', '0', '1', '6', '7', '0', '0', '8', '15', '6', '9', '14', '10'], ['Bolton', 'Blackburn', '0', '1', '5', '5', '0', '0', '13', '7', '5', '1', '13', '10'], ['Charlton', 'Everton', '2', '0', '8', '3', '0', '1', '12', '5', '8', '2', '13', '11'], ['Fulham', 'Birmingham', '2', '3', '7', '5', '0', '0', '10', '6', '7', '3', '9', '13'], ['Liverpool', 'Southampton', '1', '0', '11', '2', '0', '0', '19', '3', '7', '3', '6', '12'], ['Man City', 'West Brom', '1', '1', '4', '0', '0', '1', '11', '0', '11', '0', '7', '10'], ['Middlesbrough', 'Norwich', '2', '0', '11', '2', '0', '0', '17', '10', '8', '4', '13', '10'], ['Portsmouth', 'Chelsea', '0', '2', '2', '9', '0', '0', '4', '12', '5', '4', '12', '15'], ['Tottenham', 'Crystal Palace', '1', '1', '14', '7', '0', '0', '17', '11', '5', '6', '10', '15'], ['Newcastle', 'Arsenal', '0', '1', '5', '6', '0', '0', '9', '13', '4', '8', '10', '21'], ['Aston Villa', 'Blackburn', '1', '0', '3', '4', '0', '0', '11', '9', '6', '7', '9', '9'], ['Bolton', 'West Brom', '1', '1', '15', '7', '0', '0', '25', '11', '7', '3', '13', '15'], ['Charlton', 'Arsenal', '1', '3', '4', '5', '0', '0', '7', '8', '1', '2', '13', '15'], ['Fulham', 'Crystal Palace', '3', '1', '6', '3', '0', '0', '12', '7', '4', '2', '11', '18'], ['Liverpool', 'Chelsea', '0', '1', '7', '7', '0', '0', '14', '8', '3', '2', '14', '16'], ['Man City', 'Southampton', '2', '1', '8', '1', '0', '0', '15', '6', '9', '8', '12', '9'], ['Middlesbrough', 'Man United', '0', '2', '2', '7', '0', '0', '6', '13', '10', '4', '13', '9'], ['Newcastle', 'Birmingham', '2', '1', '4', '8', '0', '0', '8', '10', '6', '5', '8', '14'], ['Portsmouth', 'Norwich', '1', '1', '6', '5', '0', '1', '14', '6', '7', '5', '13', '6'], ['Tottenham', 'Everton', '5', '2', '12', '3', '0', '0', '14', '6', '6', '4', '8', '13'], ['Blackburn', 'Charlton', '1', '0', '11', '6', '0', '0', '16', '9', '6', '8', '11', '14'], ['Crystal Palace', 'Aston Villa', '2', '0', '9', '6', '0', '0', '14', '12', '7', '7', '8', '14'], ['Norwich', 'Liverpool', '1', '2', '4', '6', '0', '0', '7', '11', '5', '2', '16', '19'], ['West Brom', 'Newcastle', '0', '0', '5', '7', '0', '0', '8', '12', '1', '6', '9', '7'], ['Arsenal', 'Man City', '1', '1', '9', '4', '0', '0', '13', '5', '12', '5', '10', '13'], ['Birmingham', 'Bolton', '1', '2', '8', '6', '0', '0', '14', '11', '6', '5', '12', '12'], ['Chelsea', 'Middlesbrough', '2', '0', '10', '1', '0', '0', '14', '2', '4', '3', '15', '9'], ['Everton', 'Portsmouth', '2', '1', '7', '6', '0', '0', '17', '9', '8', '1', '5', '9'], ['Man United', 'Tottenham', '0', '0', '16', '5', '0', '0', '23', '7', '8', '2', '11', '13'], ['Southampton', 'Fulham', '3', '3', '5', '7', '0', '0', '8', '12', '2', '10', '16', '16'], ['Aston Villa', 'Norwich', '3', '0', '10', '1', '0', '0', '22', '7', '7', '5', '15', '12'], ['Bolton', 'Arsenal', '1', '0', '3', '7', '0', '0', '9', '11', '8', '7', '8', '8'], ['Charlton', 'Birmingham', '3', '1', '6', '6', '0', '0', '10', '10', '4', '4', '11', '10'], ['Liverpool', 'Man United', '0', '1', '5', '2', '0', '1', '13', '5', '9', '4', '12', '25'], ['Man City', 'Crystal Palace', '3', '1', '9', '2', '0', '0', '16', '7', '7', '7', '10', '11'], ['Newcastle', 'Southampton', '2', '1', '17', '6', '0', '0', '22', '9', '14', '4', '12', '20'], ['Portsmouth', 'Blackburn', '0', '1', '1', '6', '2', '0', '11', '14', '5', '6', '13', '23'], ['Tottenham', 'Chelsea', '0', '2', '4', '5', '0', '0', '12', '12', '4', '7', '14', '12'], ['Fulham', 'West Brom', '1', '0', '4', '6', '0', '0', '8', '7', '7', '5', '12', '12'], ['Middlesbrough', 'Everton', '1', '1', '12', '5', '0', '0', '18', '13', '9', '6', '12', '10'], ['Birmingham', 'Fulham', '1', '2', '5', '3', '0', '0', '8', '6', '5', '3', '16', '11'], ['Chelsea', 'Portsmouth', '3', '0', '9', '5', '0', '0', '14', '9', '2', '3', '2', '9'], ['Crystal Palace', 'Tottenham', '3', '0', '7', '6', '0', '0', '8', '12', '5', '4', '9', '14'], ['Everton', 'Charlton', '0', '1', '7', '3', '0', '0', '12', '4', '9', '5', '13', '10'], ['Man United', 'Aston Villa', '3', '1', '11', '6', '0', '0', '18', '10', '4', '2', '11', '21'], ['Norwich', 'Middlesbrough', '4', '4', '9', '15', '0', '0', '17', '22', '5', '8', '11', '16'], ['Southampton', 'Liverpool', '2', '0', '7', '2', '0', '0', '11', '11', '6', '3', '10', '12'], ['West Brom', 'Man City', '2', '0', '6', '5', '0', '0', '13', '11', '3', '11', '9', '12'], ['Arsenal', 'Newcastle', '1', '0', '8', '1', '0', '0', '13', '4', '11', '1', '17', '19'], ['Blackburn', 'Bolton', '0', '1', '4', '3', '0', '0', '9', '6', '7', '7', '16', '19'], ['Arsenal', 'Man United', '2', '4', '10', '9', '0', '1', '11', '10', '6', '7', '20', '16'], ['Bolton', 'Tottenham', '3', '1', '9', '5', '0', '1', '15', '11', '4', '6', '9', '17'], ['Charlton', 'Liverpool', '1', '2', '3', '7', '0', '0', '4', '19', '3', '7', '11', '15'], ['Portsmouth', 'Middlesbrough', '2', '1', '5', '2', '0', '0', '14', '9', '9', '5', '9', '8'], ['West Brom', 'Crystal Palace', '2', '2', '7', '4', '0', '1', '12', '4', '8', '4', '17', '11'], ['Birmingham', 'Southampton', '2', '1', '4', '3', '0', '0', '12', '6', '8', '1', '11', '18'], ['Blackburn', 'Chelsea', '0', '1', '2', '3', '0', '0', '11', '6', '4', '4', '24', '15'], ['Everton', 'Norwich', '1', '0', '4', '6', '0', '0', '18', '11', '3', '3', '12', '16'], ['Fulham', 'Aston Villa', '1', '1', '3', '2', '0', '0', '12', '12', '4', '8', '12', '17'], ['Man City', 'Newcastle', '1', '1', '1', '1', '0', '0', '5', '3', '3', '1', '11', '18'], ['Aston Villa', 'Arsenal', '1', '3', '1', '6', '0', '0', '5', '15', '5', '2', '13', '11'], ['Crystal Palace', 'Bolton', '0', '1', '7', '6', '0', '0', '17', '9', '6', '4', '16', '10'], ['Liverpool', 'Fulham', '3', '1', '7', '2', '0', '0', '10', '6', '4', '3', '12', '13'], ['Man United', 'Birmingham', '2', '0', '11', '1', '0', '0', '17', '7', '5', '2', '10', '16'], ['Middlesbrough', 'Blackburn', '1', '0', '7', '3', '1', '0', '13', '6', '8', '10', '14', '10'], ['Newcastle', 'Charlton', '1', '1', '10', '2', '0', '0', '21', '8', '7', '2', '10', '12'], ['Norwich', 'West Brom', '3', '2', '7', '8', '0', '0', '13', '15', '3', '3', '18', '13'], ['Tottenham', 'Portsmouth', '3', '1', '10', '3', '0', '0', '18', '6', '9', '2', '10', '8'], ['Chelsea', 'Man City', '0', '0', '8', '2', '0', '0', '11', '5', '7', '1', '9', '11'], ['Southampton', 'Everton', '2', '2', '9', '4', '0', '0', '21', '8', '8', '3', '15', '11'], ['Birmingham', 'Liverpool', '2', '0', '3', '2', '0', '0', '5', '6', '10', '4', '16', '10'], ['Blackburn', 'Norwich', '3', '0', '13', '6', '0', '0', '18', '9', '12', '3', '15', '14'], ['Bolton', 'Middlesbrough', '0', '0', '8', '2', '0', '0', '13', '5', '3', '2', '11', '15'], ['Everton', 'Chelsea', '0', '1', '2', '11', '1', '0', '4', '25', '3', '9', '14', '9'], ['Portsmouth', 'Aston Villa', '1', '2', '3', '4', '0', '0', '9', '6', '6', '3', '14', '22'], ['Man City', 'Man United', '0', '2', '3', '3', '0', '0', '9', '10', '4', '6', '9', '14'], ['Arsenal', 'Crystal Palace', '5', '1', '12', '3', '0', '0', '17', '6', '5', '1', '8', '17'], ['West Brom', 'Southampton', '0', '0', '4', '7', '0', '0', '9', '11', '7', '3', '15', '15'], ['Aston Villa', 'Everton', '1', '3', '6', '9', '0', '0', '9', '15', '3', '7', '22', '17'], ['Crystal Palace', 'Birmingham', '2', '0', '2', '5', '0', '0', '8', '9', '4', '7', '23', '15'], ['Man United', 'Portsmouth', '2', '1', '6', '4', '0', '0', '13', '8', '4', '3', '15', '15'], ['Southampton', 'Arsenal', '1', '1', '3', '12', '1', '1', '10', '17', '3', '5', '9', '13'], ['Tottenham', 'Fulham', '2', '0', '8', '5', '0', '0', '18', '8', '13', '2', '6', '13'], ['Middlesbrough', 'Charlton', '2', '2', '18', '2', '0', '0', '21', '5', '10', '1', '10', '15'], ['Newcastle', 'Bolton', '2', '1', '9', '2', '0', '0', '14', '4', '6', '3', '12', '19'], ['Norwich', 'Man City', '2', '3', '4', '10', '1', '0', '5', '17', '3', '12', '12', '7'], ['Arsenal', 'Portsmouth', '3', '0', '6', '2', '0', '0', '11', '5', '7', '3', '12', '15'], ['Aston Villa', 'Middlesbrough', '2', '0', '4', '1', '0', '0', '8', '5', '5', '4', '8', '6'], ['Crystal Palace', 'Man United', '0', '0', '1', '9', '1', '0', '2', '16', '2', '7', '12', '9'], ['Fulham', 'Charlton', '0', '0', '5', '6', '0', '0', '10', '9', '5', '5', '9', '12'], ['Newcastle', 'Liverpool', '1', '0', '4', '3', '0', '0', '13', '6', '5', '4', '11', '16'], ['Norwich', 'Chelsea', '1', '3', '5', '13', '0', '0', '8', '23', '7', '7', '14', '15'], ['Southampton', 'Tottenham', '1', '0', '4', '8', '0', '0', '9', '13', '4', '10', '7', '11'], ['Everton', 'Blackburn', '0', '1', '4', '5', '0', '0', '12', '11', '6', '3', '15', '20'], ['West Brom', 'Birmingham', '2', '0', '7', '0', '0', '0', '12', '7', '13', '3', '16', '18'], ['Man City', 'Bolton', '0', '1', '3', '3', '0', '0', '8', '11', '5', '5', '10', '21'], ['Chelsea', 'West Brom', '1', '0', '11', '4', '0', '0', '23', '7', '8', '3', '13', '9'], ['Charlton', 'Tottenham', '2', '0', '2', '4', '0', '0', '5', '7', '5', '2', '15', '16'], ['Liverpool', 'Blackburn', '0', '0', '2', '2', '0', '0', '7', '5', '11', '3', '12', '13'], ['Blackburn', 'Arsenal', '0', '1', '2', '8', '0', '0', '9', '10', '4', '6', '21', '8'], ['Bolton', 'Norwich', '1', '0', '10', '2', '0', '0', '16', '6', '7', '6', '20', '16'], ['Charlton', 'West Brom', '1', '4', '5', '9', '1', '0', '8', '14', '3', '9', '11', '8'], ['Chelsea', 'Crystal Palace', '4', '1', '13', '3', '0', '0', '20', '6', '10', '4', '13', '16'], ['Man United', 'Fulham', '1', '0', '12', '6', '0', '0', '19', '10', '18', '3', '7', '11'], ['Portsmouth', 'Newcastle', '1', '1', '6', '6', '0', '0', '13', '11', '4', '3', '15', '19'], ['Tottenham', 'Man City', '2', '1', '9', '7', '0', '0', '12', '11', '6', '4', '13', '13'], ['Birmingham', 'Aston Villa', '2', '0', '4', '4', '0', '0', '10', '5', '2', '3', '12', '20'], ['Liverpool', 'Everton', '2', '1', '8', '2', '1', '0', '15', '4', '5', '1', '13', '15'], ['Middlesbrough', 'Southampton', '1', '3', '8', '7', '0', '0', '14', '12', '3', '6', '20', '17'], ['Arsenal', 'Norwich', '4', '1', '15', '1', '0', '0', '21', '3', '10', '3', '8', '11'], ['Birmingham', 'Tottenham', '1', '1', '7', '5', '0', '0', '14', '9', '3', '3', '6', '10'], ['Charlton', 'Man City', '2', '2', '3', '8', '0', '0', '9', '16', '6', '7', '6', '8'], ['Crystal Palace', 'Middlesbrough', '0', '1', '13', '5', '0', '0', '23', '7', '8', '3', '6', '19'], ['Liverpool', 'Bolton', '1', '0', '7', '5', '0', '0', '10', '13', '4', '12', '6', '13'], ['Man United', 'Blackburn', '0', '0', '9', '3', '0', '0', '23', '9', '9', '3', '21', '18'], ['Newcastle', 'Aston Villa', '0', '3', '12', '5', '3', '0', '20', '8', '8', '6', '14', '24'], ['Southampton', 'Chelsea', '1', '3', '3', '8', '0', '0', '4', '12', '4', '3', '11', '10'], ['Fulham', 'Portsmouth', '3', '1', '7', '4', '0', '0', '12', '8', '5', '5', '14', '10'], ['West Brom', 'Everton', '1', '0', '4', '2', '0', '0', '8', '8', '7', '7', '15', '20'], ['Blackburn', 'Southampton', '3', '0', '8', '3', '0', '0', '15', '9', '4', '2', '19', '14'], ['Bolton', 'Fulham', '3', '1', '10', '4', '0', '1', '15', '7', '4', '6', '6', '14'], ['Chelsea', 'Birmingham', '1', '1', '12', '1', '0', '0', '19', '2', '8', '5', '17', '12'], ['Man City', 'Liverpool', '1', '0', '8', '1', '0', '0', '11', '6', '5', '2', '8', '11'], ['Middlesbrough', 'Arsenal', '0', '1', '6', '1', '0', '0', '9', '7', '5', '3', '11', '9'], ['Norwich', 'Man United', '2', '0', '5', '8', '0', '0', '7', '16', '3', '4', '19', '18'], ['Portsmouth', 'Charlton', '4', '2', '9', '4', '0', '0', '13', '8', '5', '4', '11', '14'], ['Aston Villa', 'West Brom', '1', '1', '7', '5', '1', '1', '14', '7', '5', '5', '6', '10'], ['Everton', 'Crystal Palace', '4', '0', '8', '3', '0', '0', '13', '11', '4', '9', '10', '14'], ['Tottenham', 'Newcastle', '1', '0', '6', '3', '0', '0', '19', '6', '4', '7', '9', '15'], ['Birmingham', 'Portsmouth', '0', '0', '4', '4', '0', '0', '9', '5', '7', '5', '16', '12'], ['Charlton', 'Bolton', '1', '2', '10', '5', '0', '0', '18', '11', '8', '2', '10', '14'], ['Crystal Palace', 'Norwich', '3', '3', '10', '8', '0', '0', '12', '12', '7', '3', '17', '21'], ['Fulham', 'Man City', '1', '1', '3', '3', '0', '0', '8', '7', '6', '8', '9', '19'], ['Liverpool', 'Tottenham', '2', '2', '7', '6', '0', '0', '18', '9', '12', '3', '5', '7'], ['Southampton', 'Aston Villa', '2', '3', '8', '7', '0', '0', '16', '11', '5', '8', '10', '17'], ['Bolton', 'Southampton', '1', '1', '16', '4', '0', '0', '22', '5', '5', '2', '9', '9'], ['Middlesbrough', 'Fulham', '1', '1', '4', '2', '0', '0', '8', '7', '2', '6', '11', '12'], ['Aston Villa', 'Charlton', '0', '0', '5', '3', '0', '0', '13', '12', '6', '5', '14', '9'], ['Blackburn', 'Crystal Palace', '1', '0', '7', '3', '0', '0', '18', '4', '11', '6', '11', '19'], ['Chelsea', 'Arsenal', '0', '0', '3', '4', '0', '0', '9', '6', '2', '2', '10', '16'], ['Everton', 'Man United', '1', '0', '3', '11', '0', '2', '7', '16', '4', '5', '18', '17'], ['Man City', 'Birmingham', '3', '0', '4', '1', '0', '0', '8', '5', '8', '3', '11', '9'], ['Norwich', 'Newcastle', '2', '1', '8', '10', '0', '0', '15', '24', '6', '6', '12', '11'], ['Portsmouth', 'Liverpool', '1', '2', '6', '12', '0', '0', '8', '19', '5', '4', '10', '8'], ['Tottenham', 'West Brom', '1', '1', '5', '4', '0', '0', '15', '10', '7', '3', '12', '12'], ['Aston Villa', 'Bolton', '1', '1', '10', '6', '0', '0', '13', '12', '7', '5', '17', '13'], ['Blackburn', 'Man City', '0', '0', '5', '1', '0', '0', '11', '7', '6', '2', '20', '17'], ['Chelsea', 'Fulham', '3', '1', '7', '4', '0', '0', '12', '10', '4', '6', '7', '11'], ['Crystal Palace', 'Liverpool', '1', '0', '3', '7', '0', '0', '6', '9', '4', '1', '19', '14'], ['Everton', 'Birmingham', '1', '1', '6', '4', '0', '0', '11', '9', '9', '5', '18', '18'], ['Middlesbrough', 'West Brom', '4', '0', '8', '9', '0', '0', '12', '15', '1', '4', '12', '12'], ['Norwich', 'Charlton', '1', '0', '6', '6', '0', '0', '13', '11', '7', '8', '8', '9'], ['Man United', 'Newcastle', '2', '1', '10', '5', '0', '0', '18', '11', '9', '4', '8', '20'], ['Portsmouth', 'Southampton', '4', '1', '8', '8', '0', '0', '18', '11', '4', '7', '15', '11'], ['Arsenal', 'Tottenham', '1', '0', '3', '1', '0', '0', '12', '7', '4', '7', '17', '15'], ['West Brom', 'Blackburn', '1', '1', '3', '7', '0', '0', '9', '14', '6', '4', '13', '15'], ['Newcastle', 'Middlesbrough', '0', '0', '5', '3', '0', '0', '11', '5', '7', '1', '8', '9'], ['Birmingham', 'Blackburn', '2', '1', '6', '1', '0', '0', '10', '5', '6', '1', '15', '16'], ['Bolton', 'Chelsea', '0', '2', '6', '5', '0', '0', '14', '8', '7', '2', '17', '14'], ['Fulham', 'Everton', '2', '0', '6', '4', '1', '0', '11', '11', '3', '7', '18', '16'], ['Liverpool', 'Middlesbrough', '1', '1', '6', '3', '0', '0', '12', '6', '7', '4', '10', '20'], ['Man City', 'Portsmouth', '2', '0', '7', '3', '0', '0', '13', '7', '7', '5', '14', '6'], ['Newcastle', 'Crystal Palace', '0', '0', '9', '3', '0', '0', '16', '7', '5', '2', '16', '15'], ['Southampton', 'Norwich', '4', '3', '9', '7', '0', '0', '15', '14', '6', '8', '18', '13'], ['Charlton', 'Man United', '0', '4', '4', '12', '1', '0', '6', '18', '1', '7', '6', '15'], ['Tottenham', 'Aston Villa', '5', '1', '14', '3', '0', '0', '20', '4', '9', '3', '13', '17'], ['West Brom', 'Arsenal', '0', '2', '3', '5', '0', '0', '7', '8', '9', '0', '12', '11'], ['Fulham', 'Newcastle', '1', '3', '3', '3', '0', '0', '6', '4', '1', '2', '11', '13'], ['Aston Villa', 'Man City', '1', '2', '8', '4', '0', '0', '15', '10', '6', '2', '8', '18'], ['Blackburn', 'Fulham', '1', '3', '6', '6', '1', '1', '13', '8', '8', '5', '11', '23'], ['Chelsea', 'Charlton', '1', '0', '6', '3', '0', '0', '15', '6', '2', '5', '7', '13'], ['Crystal Palace', 'Southampton', '2', '2', '6', '3', '1', '1', '14', '7', '5', '2', '16', '18'], ['Everton', 'Newcastle', '2', '0', '6', '4', '0', '1', '8', '9', '3', '2', '22', '11'], ['Man United', 'West Brom', '1', '1', '14', '0', '0', '0', '27', '2', '12', '0', '10', '13'], ['Middlesbrough', 'Tottenham', '1', '0', '8', '2', '0', '0', '16', '5', '4', '3', '14', '10'], ['Norwich', 'Birmingham', '1', '0', '4', '9', '0', '1', '8', '22', '1', '7', '20', '10'], ['Portsmouth', 'Bolton', '1', '1', '4', '4', '0', '0', '9', '12', '2', '9', '12', '23'], ['Arsenal', 'Liverpool', '3', '1', '8', '6', '0', '0', '11', '11', '1', '4', '13', '10'], ['Man United', 'Chelsea', '1', '3', '9', '6', '0', '0', '15', '7', '7', '0', '11', '14'], ['Arsenal', 'Everton', '7', '0', '12', '3', '0', '0', '19', '7', '4', '2', '9', '12'], ['Birmingham', 'Arsenal', '2', '1', '4', '5', '0', '0', '9', '11', '3', '2', '10', '8'], ['Bolton', 'Everton', '3', '2', '9', '5', '1', '0', '16', '10', '5', '6', '12', '17'], ['Charlton', 'Crystal Palace', '2', '2', '5', '6', '0', '0', '8', '9', '3', '8', '14', '14'], ['Fulham', 'Norwich', '6', '0', '9', '0', '0', '0', '13', '2', '4', '13', '11', '12'], ['Liverpool', 'Aston Villa', '2', '1', '7', '4', '0', '0', '15', '13', '9', '5', '10', '16'], ['Man City', 'Middlesbrough', '1', '1', '5', '3', '0', '0', '10', '4', '10', '3', '12', '15'], ['Newcastle', 'Chelsea', '1', '1', '4', '7', '0', '0', '8', '12', '6', '4', '11', '17'], ['Southampton', 'Man United', '1', '2', '3', '8', '0', '0', '11', '16', '5', '3', '14', '13'], ['Tottenham', 'Blackburn', '0', '0', '7', '8', '0', '0', '11', '12', '5', '7', '10', '14'], ['West Brom', 'Portsmouth', '2', '0', '5', '2', '0', '0', '9', '8', '6', '1', '12', '13'], []]\n",
            "\n",
            "\n",
            "Index assigned to Everton:\n",
            "12\n",
            "\n",
            "\n",
            "data2, using the team's index number instead of name\n",
            "[[0, 12, '2', '1', '5', '7', '1', '1', '11', '13', '6', '9', '8', '15'], [1, 16, '1', '0', '5', '7', '0', '0', '10', '15', '1', '4', '20', '27'], [2, 17, '5', '1', '13', '5', '0', '0', '25', '8', '6', '2', '8', '14'], [3, 19, '3', '2', '9', '5', '0', '0', '17', '8', '7', '6', '18', '16'], [4, 15, '2', '2', '7', '10', '0', '0', '12', '13', '2', '7', '27', '15'], [5, 10, '4', '0', '6', '5', '0', '0', '13', '15', '8', '4', '12', '8'], [6, 18, '2', '1', '3', '5', '0', '1', '4', '9', '7', '9', '18', '22'], [7, 13, '0', '3', '5', '8', '1', '0', '10', '14', '4', '7', '12', '16'], [8, 14, '2', '2', '3', '10', '0', '0', '8', '19', '4', '8', '19', '16'], [9, 11, '1', '2', '7', '6', '0', '0', '12', '10', '8', '6', '13', '12'], [10, 2, '2', '2', '10', '5', '0', '1', '16', '10', '9', '2', '11', '15'], [11, 4, '2', '1', '7', '1', '1', '2', '17', '5', '7', '2', '12', '18'], [12, 3, '3', '1', '8', '15', '0', '0', '10', '23', '4', '10', '15', '14'], [13, 6, '1', '1', '11', '2', '0', '0', '21', '4', '11', '7', '8', '22'], [14, 5, '1', '2', '5', '8', '0', '0', '7', '11', '2', '4', '18', '14'], [15, 1, '0', '0', '7', '5', '0', '0', '14', '12', '2', '7', '15', '19'], [16, 8, '2', '1', '4', '2', '0', '0', '20', '5', '6', '0', '14', '14'], [17, 7, '0', '4', '4', '6', '0', '1', '10', '8', '5', '2', '11', '7'], [18, 9, '0', '0', '9', '9', '0', '0', '14', '15', '8', '9', '16', '7'], [19, 0, '0', '4', '4', '10', '0', '0', '10', '14', '3', '5', '11', '7'], [2, 13, '2', '3', '10', '5', '0', '0', '14', '13', '9', '0', '14', '10'], [7, 12, '2', '2', '4', '4', '0', '0', '10', '13', '6', '8', '8', '8'], [8, 15, '0', '0', '6', '8', '0', '0', '15', '14', '6', '10', '10', '11'], [4, 19, '0', '0', '4', '3', '0', '0', '12', '13', '6', '4', '17', '16'], [6, 10, '4', '0', '8', '4', '0', '0', '13', '13', '0', '4', '12', '14'], [0, 18, '2', '0', '6', '3', '0', '0', '14', '6', '2', '6', '16', '16'], [9, 16, '0', '0', '10', '4', '0', '0', '22', '9', '8', '5', '20', '27'], [5, 17, '1', '0', '5', '2', '0', '0', '14', '12', '10', '2', '6', '12'], [18, 4, '3', '1', '8', '4', '0', '1', '10', '5', '11', '1', '18', '17'], [10, 7, '0', '0', '9', '1', '0', '0', '13', '7', '12', '3', '18', '16'], [11, 2, '2', '2', '5', '4', '0', '0', '12', '7', '3', '4', '11', '19'], [12, 9, '0', '3', '5', '11', '0', '0', '11', '18', '9', '5', '22', '17'], [19, 8, '2', '3', '16', '4', '0', '0', '27', '8', '6', '3', '17', '21'], [14, 1, '0', '1', '10', '3', '0', '0', '19', '7', '9', '6', '15', '15'], [16, 3, '0', '3', '6', '5', '0', '0', '18', '9', '9', '4', '15', '13'], [17, 6, '0', '0', '2', '2', '0', '0', '11', '9', '4', '6', '15', '16'], [13, 0, '1', '2', '7', '5', '0', '0', '11', '10', '5', '5', '12', '11'], [15, 5, '1', '0', '5', '9', '0', '0', '12', '10', '9', '5', '8', '10'], [0, 6, '1', '1', '7', '2', '0', '0', '12', '8', '7', '1', '11', '19'], [2, 9, '1', '3', '5', '14', '1', '0', '8', '21', '5', '10', '13', '11'], [10, 19, '2', '0', '8', '6', '0', '0', '13', '7', '10', '1', '17', '14'], [7, 5, '0', '2', '3', '9', '1', '0', '8', '14', '3', '3', '19', '18'], [11, 16, '4', '2', '6', '3', '0', '0', '12', '9', '10', '3', '16', '20'], [12, 14, '2', '2', '7', '2', '1', '1', '9', '4', '6', '3', '20', '21'], [15, 17, '2', '0', '10', '3', '0', '0', '15', '10', '7', '3', '16', '20'], [1, 3, '2', '2', '9', '6', '1', '1', '15', '8', '8', '1', '16', '21'], [13, 18, '4', '1', '9', '4', '0', '0', '17', '8', '4', '3', '11', '11'], [4, 8, '4', '0', '11', '2', '0', '0', '19', '14', '9', '5', '10', '10'], [18, 7, '2', '1', '6', '3', '0', '0', '15', '8', '10', '2', '13', '13'], [3, 13, '2', '2', '9', '6', '0', '0', '13', '7', '2', '4', '14', '19'], [8, 1, '0', '2', '6', '3', '1', '0', '12', '7', '6', '1', '15', '14'], [9, 4, '2', '1', '8', '3', '0', '0', '19', '9', '4', '4', '10', '9'], [14, 10, '0', '0', '7', '6', '0', '0', '14', '8', '7', '5', '15', '12'], [6, 2, '1', '2', '6', '10', '0', '0', '9', '12', '7', '6', '17', '18'], [16, 15, '1', '3', '6', '10', '0', '0', '13', '12', '4', '7', '15', '14'], [17, 11, '0', '5', '4', '8', '0', '0', '8', '14', '1', '3', '15', '15'], [5, 0, '0', '0', '7', '1', '0', '1', '9', '6', '4', '3', '11', '15'], [19, 12, '1', '0', '4', '5', '0', '0', '9', '9', '4', '6', '12', '14'], [0, 14, '3', '2', '7', '4', '0', '0', '11', '5', '3', '4', '13', '17'], [1, 6, '2', '0', '5', '5', '0', '0', '7', '10', '4', '4', '10', '16'], [10, 17, '1', '1', '14', '4', '0', '0', '22', '8', '12', '3', '16', '4'], [11, 18, '1', '0', '5', '1', '0', '0', '10', '7', '6', '5', '7', '9'], [4, 5, '1', '4', '4', '9', '0', '0', '10', '11', '7', '3', '7', '11'], [15, 19, '0', '1', '6', '9', '1', '0', '10', '12', '4', '4', '12', '19'], [2, 3, '0', '2', '0', '2', '0', '0', '6', '7', '5', '5', '14', '11'], [7, 9, '3', '2', '5', '9', '0', '0', '14', '17', '7', '5', '11', '9'], [12, 8, '4', '0', '9', '2', '0', '0', '20', '6', '11', '4', '11', '15'], [13, 16, '0', '0', '10', '2', '0', '0', '21', '4', '6', '6', '7', '15'], [3, 4, '2', '0', '8', '4', '0', '0', '16', '8', '4', '4', '14', '10'], [8, 2, '2', '1', '5', '9', '0', '0', '15', '17', '2', '5', '18', '20'], [9, 0, '1', '2', '7', '8', '0', '0', '13', '12', '11', '3', '12', '20'], [5, 1, '3', '0', '8', '2', '0', '1', '10', '5', '5', '1', '13', '11'], [14, 15, '1', '0', '9', '4', '0', '0', '14', '7', '8', '4', '14', '10'], [6, 7, '1', '2', '8', '6', '0', '0', '16', '11', '6', '9', '20', '14'], [16, 12, '3', '0', '7', '4', '0', '0', '13', '12', '2', '6', '15', '14'], [17, 13, '1', '0', '4', '5', '0', '0', '8', '9', '3', '10', '13', '7'], [18, 10, '1', '1', '8', '5', '0', '0', '14', '15', '9', '5', '14', '16'], [19, 11, '1', '2', '6', '6', '0', '0', '15', '10', '2', '8', '14', '18'], [1, 11, '0', '0', '2', '8', '0', '0', '6', '13', '2', '18', '10', '14'], [0, 11, '2', '1', '9', '3', '0', '0', '13', '7', '6', '3', '11', '13'], [3, 17, '0', '0', '4', '6', '0', '0', '9', '14', '4', '11', '12', '11'], [8, 5, '0', '1', '2', '10', '0', '0', '3', '18', '7', '2', '19', '8'], [13, 10, '6', '2', '13', '8', '1', '0', '16', '12', '7', '4', '11', '8'], [19, 14, '0', '1', '7', '3', '0', '0', '11', '4', '8', '2', '17', '12'], [6, 9, '1', '0', '6', '7', '0', '0', '10', '13', '5', '3', '22', '10'], [1, 18, '0', '0', '3', '1', '0', '0', '4', '2', '3', '4', '16', '20'], [12, 15, '0', '0', '6', '7', '0', '0', '13', '19', '8', '6', '11', '6'], [4, 16, '1', '2', '15', '4', '0', '0', '22', '7', '5', '1', '12', '19'], [2, 7, '0', '1', '6', '5', '0', '0', '15', '10', '5', '3', '17', '12'], [3, 14, '2', '3', '4', '7', '0', '0', '4', '11', '4', '8', '14', '16'], [18, 12, '0', '0', '6', '2', '0', '0', '16', '8', '9', '3', '11', '12'], [10, 1, '0', '1', '7', '3', '0', '0', '16', '7', '7', '3', '14', '16'], [11, 13, '1', '0', '4', '6', '0', '0', '7', '9', '1', '5', '8', '8'], [9, 8, '3', '1', '9', '5', '0', '0', '20', '7', '9', '3', '11', '13'], [5, 3, '1', '3', '9', '7', '0', '0', '17', '12', '10', '2', '10', '14'], [14, 6, '3', '0', '8', '4', '0', '0', '10', '9', '6', '6', '12', '14'], [15, 2, '2', '0', '15', '3', '0', '1', '22', '6', '5', '3', '17', '14'], [17, 4, '4', '3', '8', '5', '0', '0', '12', '7', '2', '6', '6', '13'], [7, 0, '1', '1', '4', '6', '0', '0', '6', '11', '7', '5', '15', '13'], [16, 19, '0', '0', '8', '10', '0', '0', '12', '16', '8', '5', '14', '10'], [12, 11, '0', '1', '4', '7', '0', '0', '12', '17', '3', '1', '5', '11'], [8, 0, '1', '4', '5', '9', '0', '0', '9', '13', '4', '3', '12', '11'], [5, 6, '3', '0', '4', '3', '0', '0', '10', '9', '3', '3', '13', '12'], [19, 17, '2', '0', '11', '2', '0', '1', '14', '9', '7', '3', '8', '12'], [14, 18, '1', '1', '13', '6', '0', '1', '22', '11', '15', '3', '9', '17'], [15, 13, '0', '2', '5', '9', '0', '0', '8', '13', '4', '6', '10', '16'], [16, 10, '0', '1', '6', '15', '0', '0', '10', '25', '7', '5', '18', '6'], [3, 9, '1', '2', '6', '6', '1', '0', '10', '10', '4', '8', '13', '10'], [4, 2, '2', '0', '3', '2', '0', '0', '4', '10', '2', '6', '11', '11'], [1, 7, '1', '2', '13', '9', '0', '0', '14', '13', '15', '8', '14', '18'], [0, 16, '2', '1', '3', '2', '0', '0', '9', '5', '9', '5', '8', '22'], [18, 19, '0', '2', '4', '3', '0', '0', '12', '8', '5', '2', '19', '16'], [10, 15, '0', '0', '11', '5', '0', '1', '18', '8', '8', '6', '10', '14'], [7, 3, '3', '1', '7', '7', '0', '0', '12', '11', '1', '5', '10', '12'], [6, 8, '6', '1', '6', '5', '0', '0', '14', '6', '3', '2', '9', '11'], [17, 1, '1', '1', '4', '2', '0', '0', '11', '4', '8', '1', '13', '12'], [11, 14, '5', '0', '8', '3', '0', '1', '15', '4', '2', '0', '13', '5'], [9, 5, '1', '2', '4', '4', '0', '0', '10', '6', '2', '6', '16', '9'], [13, 4, '0', '3', '4', '3', '0', '0', '14', '6', '12', '3', '7', '12'], [2, 12, '2', '1', '12', '2', '0', '0', '18', '8', '12', '3', '16', '9'], [1, 0, '0', '3', '1', '6', '0', '0', '7', '9', '8', '6', '8', '11'], [12, 17, '2', '0', '15', '4', '0', '0', '23', '8', '8', '3', '7', '23'], [8, 10, '0', '2', '10', '5', '0', '0', '13', '10', '8', '2', '15', '11'], [4, 7, '1', '1', '7', '4', '0', '0', '13', '7', '3', '3', '15', '15'], [5, 2, '2', '1', '8', '6', '0', '0', '14', '13', '7', '4', '6', '15'], [19, 9, '0', '0', '3', '1', '0', '0', '8', '8', '3', '4', '16', '15'], [14, 13, '3', '0', '10', '1', '0', '0', '17', '5', '4', '2', '13', '16'], [15, 11, '0', '1', '3', '4', '0', '0', '13', '8', '3', '3', '7', '13'], [16, 18, '2', '1', '5', '6', '0', '0', '10', '13', '3', '3', '14', '15'], [3, 6, '2', '0', '4', '6', '0', '1', '8', '12', '2', '11', '9', '16'], [18, 15, '1', '0', '12', '3', '0', '0', '22', '7', '9', '4', '13', '13'], [2, 16, '1', '0', '12', '3', '0', '0', '18', '9', '9', '6', '10', '13'], [10, 12, '2', '0', '6', '3', '0', '0', '13', '7', '7', '7', '11', '18'], [7, 8, '0', '1', '6', '6', '0', '0', '13', '12', '3', '4', '11', '17'], [6, 4, '0', '2', '7', '3', '0', '0', '12', '7', '11', '4', '12', '14'], [17, 14, '1', '1', '4', '4', '0', '0', '10', '7', '5', '3', '14', '14'], [0, 3, '0', '0', '14', '1', '0', '0', '22', '7', '9', '0', '13', '11'], [11, 5, '1', '0', '4', '3', '0', '0', '12', '5', '2', '4', '11', '12'], [9, 1, '3', '1', '7', '4', '0', '0', '10', '7', '6', '4', '24', '17'], [13, 19, '0', '1', '14', '0', '0', '0', '24', '2', '12', '0', '15', '12'], [1, 2, '0', '4', '2', '7', '1', '0', '6', '11', '10', '7', '23', '15'], [3, 10, '2', '1', '3', '5', '0', '0', '10', '10', '3', '2', '15', '11'], [8, 11, '1', '1', '4', '10', '0', '0', '8', '19', '2', '8', '14', '11'], [4, 0, '1', '1', '1', '5', '0', '1', '4', '8', '7', '5', '13', '12'], [5, 18, '4', '0', '8', '3', '0', '0', '12', '10', '6', '5', '6', '11'], [19, 6, '0', '0', '3', '2', '0', '1', '5', '6', '6', '5', '17', '20'], [14, 9, '1', '1', '6', '3', '0', '0', '13', '7', '8', '1', '13', '13'], [16, 17, '5', '2', '10', '8', '0', '0', '12', '16', '4', '5', '14', '8'], [12, 13, '0', '0', '2', '7', '0', '0', '9', '15', '3', '5', '11', '13'], [15, 7, '3', '2', '14', '14', '0', '0', '18', '20', '8', '9', '12', '12'], [11, 10, '1', '2', '5', '2', '0', '0', '9', '4', '8', '3', '13', '11'], [4, 1, '0', '2', '5', '4', '2', '0', '8', '9', '6', '7', '19', '16'], [9, 15, '1', '2', '10', '4', '0', '0', '18', '8', '9', '5', '6', '8'], [5, 13, '3', '1', '7', '9', '0', '0', '10', '12', '2', '5', '8', '10'], [19, 7, '0', '0', '6', '3', '0', '0', '12', '10', '3', '7', '13', '11'], [14, 16, '4', '0', '11', '4', '0', '0', '19', '4', '11', '7', '10', '12'], [6, 12, '1', '2', '10', '4', '0', '0', '12', '9', '7', '10', '13', '13'], [0, 2, '1', '0', '7', '1', '0', '0', '11', '5', '5', '6', '14', '14'], [18, 17, '3', '2', '8', '5', '0', '0', '11', '8', '4', '6', '12', '14'], [8, 3, '3', '2', '11', '7', '0', '0', '14', '10', '6', '5', '17', '11'], [2, 18, '0', '2', '5', '7', '0', '0', '12', '15', '9', '1', '7', '11'], [10, 0, '1', '1', '13', '4', '0', '0', '19', '9', '8', '1', '11', '13'], [7, 14, '0', '0', '10', '5', '0', '0', '18', '11', '7', '9', '11', '9'], [12, 4, '3', '2', '8', '4', '0', '0', '15', '6', '11', '7', '13', '18'], [3, 11, '0', '1', '2', '9', '0', '0', '7', '11', '2', '8', '11', '10'], [15, 6, '3', '0', '7', '7', '0', '0', '12', '11', '8', '5', '7', '21'], [16, 5, '1', '2', '8', '7', '0', '0', '14', '12', '7', '8', '12', '8'], [13, 8, '1', '1', '11', '4', '0', '0', '19', '6', '8', '5', '7', '8'], [0, 17, '3', '0', '10', '1', '0', '0', '15', '5', '14', '3', '15', '17'], [1, 13, '2', '1', '3', '5', '0', '0', '5', '9', '9', '3', '18', '14'], [2, 19, '2', '2', '10', '3', '0', '0', '13', '5', '14', '1', '11', '10'], [7, 11, '4', '2', '4', '8', '0', '0', '8', '15', '8', '6', '14', '10'], [3, 15, '2', '0', '7', '2', '0', '0', '14', '6', '11', '5', '13', '12'], [8, 18, '0', '0', '3', '4', '0', '0', '6', '13', '7', '5', '15', '13'], [4, 14, '1', '1', '5', '6', '0', '0', '6', '12', '6', '7', '11', '10'], [9, 10, '3', '1', '8', '2', '0', '0', '13', '7', '7', '1', '10', '18'], [5, 12, '3', '2', '11', '6', '0', '0', '19', '10', '7', '2', '8', '10'], [6, 16, '2', '0', '8', '3', '0', '0', '10', '5', '3', '6', '10', '10'], [18, 3, '3', '0', '8', '3', '0', '0', '12', '9', '8', '3', '17', '7'], [10, 4, '2', '2', '8', '6', '0', '0', '15', '7', '3', '7', '16', '17'], [11, 6, '3', '0', '8', '3', '0', '0', '15', '7', '7', '1', '8', '14'], [12, 1, '1', '0', '6', '2', '0', '0', '16', '5', '5', '2', '7', '13'], [13, 9, '2', '2', '8', '9', '0', '0', '16', '11', '7', '7', '6', '14'], [19, 5, '0', '1', '2', '6', '0', '1', '11', '10', '4', '2', '11', '15'], [14, 2, '0', '1', '11', '3', '0', '0', '15', '11', '8', '8', '6', '13'], [16, 7, '0', '1', '9', '8', '0', '0', '18', '10', '11', '11', '8', '13'], [17, 8, '3', '1', '4', '4', '0', '1', '11', '9', '6', '10', '11', '14'], [15, 0, '0', '1', '3', '9', '0', '0', '6', '17', '2', '3', '14', '10'], [18, 6, '2', '1', '13', '4', '0', '0', '18', '11', '9', '6', '17', '14'], [10, 5, '1', '2', '5', '7', '0', '0', '15', '11', '10', '6', '4', '8'], [11, 9, '0', '1', '2', '1', '0', '1', '9', '4', '8', '3', '15', '9'], [12, 0, '1', '1', '4', '5', '0', '0', '8', '9', '5', '4', '13', '16'], [13, 7, '1', '1', '10', '4', '0', '0', '18', '9', '2', '2', '9', '11'], [19, 3, '2', '1', '5', '4', '0', '0', '10', '9', '9', '1', '14', '17'], [14, 8, '1', '0', '6', '5', '0', '0', '13', '9', '5', '3', '14', '17'], [15, 4, '0', '0', '14', '8', '0', '0', '14', '9', '4', '5', '12', '15'], [16, 1, '4', '1', '7', '3', '0', '0', '11', '9', '1', '6', '15', '6'], [17, 2, '2', '2', '5', '5', '0', '0', '10', '9', '2', '7', '13', '20'], [0, 19, '4', '1', '13', '4', '0', '0', '20', '5', '7', '3', '12', '21'], [1, 15, '2', '1', '6', '6', '0', '1', '11', '9', '7', '10', '14', '23'], [2, 10, '3', '4', '3', '10', '0', '0', '8', '16', '4', '7', '15', '13'], [7, 17, '2', '0', '4', '4', '0', '0', '10', '7', '6', '5', '6', '12'], [3, 12, '2', '1', '10', '5', '0', '0', '14', '11', '5', '7', '19', '9'], [8, 16, '0', '1', '1', '8', '0', '0', '7', '15', '4', '5', '17', '13'], [9, 18, '1', '0', '4', '2', '0', '0', '12', '8', '3', '6', '14', '15'], [6, 13, '4', '2', '8', '6', '0', '0', '11', '15', '2', '7', '7', '8'], [4, 11, '0', '4', '3', '6', '0', '0', '6', '11', '6', '2', '17', '7'], [5, 14, '0', '0', '7', '3', '0', '0', '12', '7', '2', '3', '13', '11'], [10, 6, '1', '0', '6', '3', '0', '1', '13', '8', '2', '2', '9', '13'], [12, 7, '0', '1', '7', '4', '0', '0', '17', '7', '10', '5', '6', '14'], [13, 2, '1', '1', '7', '3', '0', '0', '10', '6', '4', '2', '7', '13'], [19, 4, '3', '3', '7', '8', '0', '0', '13', '14', '6', '6', '15', '12'], [15, 8, '2', '1', '6', '6', '0', '0', '18', '11', '1', '6', '16', '8'], [16, 9, '2', '1', '3', '4', '0', '0', '7', '10', '5', '6', '18', '9'], [17, 5, '1', '0', '6', '6', '0', '0', '11', '15', '4', '12', '13', '5'], [18, 0, '0', '2', '5', '9', '0', '0', '12', '13', '3', '2', '17', '21'], [11, 1, '0', '0', '9', '1', '0', '0', '16', '2', '6', '2', '13', '8'], [14, 3, '3', '1', '9', '6', '0', '0', '10', '13', '7', '4', '10', '8'], [17, 9, '1', '1', '6', '10', '0', '0', '9', '14', '7', '5', '14', '16'], [1, 14, '1', '1', '5', '2', '0', '0', '15', '4', '2', '3', '13', '11'], [7, 10, '1', '2', '7', '10', '0', '0', '14', '14', '5', '6', '9', '9'], [3, 16, '2', '1', '10', '4', '0', '0', '16', '4', '10', '13', '6', '12'], [8, 19, '0', '3', '4', '6', '1', '0', '8', '15', '7', '6', '14', '6'], [4, 18, '0', '5', '9', '10', '0', '0', '13', '14', '5', '9', '12', '7'], [9, 12, '0', '0', '9', '4', '0', '0', '21', '6', '8', '3', '7', '11'], [5, 15, '3', '2', '7', '11', '0', '0', '12', '17', '1', '10', '9', '11'], [6, 17, '0', '0', '10', '4', '0', '0', '17', '8', '6', '6', '9', '6'], [0, 13, '2', '1', '9', '8', '0', '1', '11', '9', '5', '4', '13', '13'], [2, 11, '2', '3', '7', '9', '0', '0', '13', '17', '5', '6', '9', '14'], [18, 8, '2', '0', '7', '4', '0', '0', '15', '7', '3', '10', '15', '16'], [10, 9, '2', '2', '6', '6', '0', '0', '12', '11', '3', '5', '15', '15'], [12, 5, '3', '4', '8', '9', '0', '0', '12', '19', '7', '7', '11', '14'], [19, 2, '0', '1', '7', '2', '0', '0', '15', '6', '8', '3', '6', '12'], [14, 4, '3', '1', '9', '3', '0', '0', '15', '7', '9', '1', '17', '11'], [15, 3, '0', '0', '6', '8', '0', '0', '11', '11', '17', '3', '12', '18'], [16, 6, '4', '3', '13', '9', '0', '0', '15', '16', '6', '5', '16', '13'], [17, 0, '1', '3', '5', '7', '0', '0', '12', '10', '6', '4', '18', '18'], [11, 7, '1', '0', '6', '0', '0', '0', '16', '5', '4', '3', '9', '17'], [13, 1, '0', '0', '12', '2', '0', '0', '21', '7', '11', '5', '11', '10'], [0, 15, '2', '0', '6', '6', '0', '0', '11', '10', '5', '9', '13', '12'], [8, 17, '4', '1', '8', '5', '0', '0', '20', '11', '4', '3', '15', '14'], [4, 10, '1', '1', '15', '9', '0', '0', '15', '9', '8', '7', '16', '13'], [1, 12, '3', '0', '11', '3', '0', '0', '15', '5', '8', '11', '10', '14'], [2, 14, '1', '1', '7', '7', '0', '0', '17', '14', '8', '6', '12', '10'], [7, 16, '2', '4', '14', '10', '0', '0', '18', '12', '10', '4', '13', '14'], [3, 18, '1', '2', '6', '3', '1', '0', '13', '10', '6', '5', '16', '17'], [9, 13, '2', '1', '7', '3', '0', '0', '12', '6', '6', '3', '7', '8'], [5, 19, '2', '3', '13', '8', '0', '0', '23', '12', '10', '5', '13', '17'], [6, 11, '0', '2', '5', '4', '0', '0', '11', '10', '9', '10', '8', '13'], [10, 13, '1', '3', '11', '13', '0', '0', '16', '17', '12', '11', '9', '10'], [7, 2, '3', '2', '7', '12', '0', '0', '11', '18', '8', '12', '7', '6'], [11, 0, '1', '2', '4', '7', '1', '0', '10', '9', '11', '5', '16', '18'], [5, 8, '1', '1', '7', '3', '0', '0', '24', '7', '12', '7', '12', '17'], [14, 19, '2', '1', '9', '4', '0', '0', '16', '8', '7', '3', '20', '21'], [15, 12, '3', '3', '5', '6', '0', '0', '9', '12', '6', '5', '14', '19'], [17, 3, '2', '1', '10', '5', '0', '0', '12', '9', '8', '6', '13', '16'], [18, 1, '2', '2', '8', '9', '0', '0', '11', '13', '7', '5', '15', '14'], [16, 4, '4', '4', '9', '7', '0', '1', '10', '14', '4', '3', '9', '14'], [0, 7, '2', '1', '11', '3', '0', '0', '21', '6', '12', '3', '8', '10'], [2, 15, '1', '1', '7', '6', '0', '0', '16', '10', '6', '9', '10', '8'], [12, 18, '2', '0', '7', '8', '0', '0', '15', '12', '5', '11', '13', '18'], [3, 5, '1', '1', '3', '7', '0', '0', '7', '13', '4', '3', '13', '19'], [4, 17, '0', '0', '6', '3', '0', '0', '10', '6', '6', '3', '7', '9'], [13, 11, '0', '1', '4', '4', '0', '0', '14', '9', '7', '1', '8', '11'], [8, 9, '2', '2', '6', '13', '0', '0', '12', '22', '6', '8', '13', '11'], [6, 14, '1', '1', '8', '2', '0', '0', '18', '5', '8', '6', '9', '9'], [1, 19, '3', '1', '5', '6', '0', '1', '9', '8', '4', '5', '13', '20'], [1, 10, '2', '0', '6', '2', '0', '0', '10', '10', '3', '5', '10', '15'], [19, 16, '1', '0', '6', '2', '0', '0', '10', '10', '3', '5', '10', '15'], [1, 4, '0', '1', '4', '2', '0', '0', '6', '4', '8', '6', '12', '26'], [2, 0, '0', '2', '5', '5', '0', '0', '10', '14', '8', '3', '12', '11'], [10, 11, '0', '2', '11', '8', '0', '0', '20', '12', '8', '6', '8', '13'], [7, 19, '1', '0', '7', '9', '0', '0', '14', '15', '4', '7', '13', '9'], [12, 6, '1', '0', '6', '3', '0', '0', '13', '5', '6', '4', '10', '12'], [3, 8, '2', '0', '10', '6', '0', '0', '19', '9', '4', '7', '14', '13'], [13, 5, '4', '1', '11', '9', '0', '0', '17', '17', '7', '4', '18', '14'], [15, 9, '2', '0', '3', '12', '0', '0', '7', '18', '1', '7', '20', '8'], [16, 14, '1', '0', '4', '4', '0', '0', '11', '6', '3', '12', '13', '13'], [17, 18, '0', '4', '1', '7', '0', '0', '9', '15', '8', '13', '10', '14'], [9, 6, '3', '0', '13', '4', '0', '0', '20', '4', '10', '8', '9', '20'], [0, 10, '2', '1', '8', '2', '0', '0', '11', '8', '6', '1', '12', '14'], [18, 2, '0', '2', '7', '7', '0', '0', '10', '16', '4', '6', '11', '14'], [11, 3, '2', '1', '8', '1', '0', '0', '13', '5', '9', '1', '12', '13'], [4, 12, '1', '1', '4', '3', '0', '1', '10', '7', '9', '2', '12', '21'], [9, 17, '1', '0', '4', '6', '0', '0', '12', '8', '14', '4', '5', '12'], [5, 16, '3', '0', '6', '3', '0', '0', '11', '8', '4', '2', '14', '10'], [19, 1, '5', '3', '9', '12', '0', '0', '13', '18', '4', '4', '11', '13'], [14, 7, '3', '1', '10', '2', '0', '0', '15', '6', '6', '6', '11', '12'], [6, 15, '1', '0', '8', '8', '0', '0', '16', '13', '6', '4', '16', '12'], [8, 13, '2', '1', '3', '14', '0', '1', '4', '21', '4', '5', '13', '14'], [1, 8, '4', '1', '7', '7', '0', '0', '17', '11', '7', '7', '11', '18'], [2, 6, '1', '2', '9', '4', '0', '0', '14', '8', '5', '3', '11', '10'], [7, 18, '1', '2', '9', '11', '0', '0', '11', '19', '3', '12', '11', '14'], [11, 17, '5', '2', '10', '4', '0', '0', '15', '9', '7', '4', '6', '12'], [12, 19, '1', '1', '7', '5', '0', '0', '14', '10', '8', '4', '16', '13'], [13, 3, '0', '0', '4', '2', '0', '0', '12', '5', '3', '1', '12', '10'], [15, 16, '1', '0', '11', '9', '0', '0', '18', '15', '10', '6', '8', '16'], [0, 5, '1', '1', '10', '8', '0', '0', '17', '13', '8', '7', '15', '18'], [10, 14, '1', '0', '12', '7', '0', '0', '20', '15', '7', '7', '11', '10'], [4, 9, '0', '0', '9', '12', '0', '0', '13', '19', '7', '10', '15', '20'], [3, 1, '0', '0', '5', '1', '0', '0', '6', '6', '4', '2', '24', '22'], [19, 10, '2', '0', '6', '8', '0', '0', '10', '13', '7', '8', '13', '15'], [14, 12, '4', '2', '8', '6', '0', '0', '12', '13', '5', '9', '11', '7'], [16, 11, '0', '1', '7', '7', '0', '0', '12', '10', '5', '4', '10', '10'], [17, 15, '1', '4', '3', '10', '0', '0', '8', '13', '3', '6', '12', '8'], [18, 13, '1', '1', '3', '4', '0', '0', '6', '7', '5', '4', '13', '13'], [9, 2, '4', '0', '11', '2', '0', '0', '24', '9', '6', '4', '8', '10'], [8, 4, '3', '2', '10', '9', '1', '0', '15', '14', '7', '11', '19', '14'], [0, 9, '4', '2', '7', '5', '0', '0', '12', '12', '2', '8', '16', '9'], [12, 16, '3', '1', '10', '7', '0', '1', '13', '12', '8', '4', '9', '14'], [1, 5, '1', '2', '8', '6', '0', '0', '11', '10', '0', '3', '11', '10'], [2, 8, '1', '2', '7', '3', '0', '0', '9', '8', '6', '4', '14', '13'], [10, 18, '2', '2', '7', '3', '0', '0', '16', '7', '6', '4', '10', '12'], [7, 6, '1', '1', '3', '8', '0', '0', '9', '10', '6', '2', '14', '8'], [11, 19, '0', '0', '9', '5', '0', '0', '19', '9', '6', '2', '10', '8'], [4, 3, '0', '2', '7', '4', '0', '0', '10', '9', '15', '4', '13', '15'], [13, 17, '3', '3', '8', '9', '0', '0', '14', '15', '5', '6', '9', '12'], [14, 0, '0', '0', '5', '5', '0', '0', '9', '11', '5', '4', '14', '12'], [18, 11, '3', '2', '3', '2', '0', '0', '5', '6', '7', '2', '8', '14'], [3, 2, '3', '4', '7', '7', '0', '0', '10', '11', '6', '1', '12', '14'], [9, 7, '0', '1', '16', '3', '0', '0', '25', '6', '7', '4', '12', '17'], [19, 15, '3', '1', '18', '8', '0', '0', '25', '16', '6', '3', '11', '14'], [6, 1, '3', '1', '6', '2', '0', '1', '12', '6', '4', '4', '19', '11'], [16, 13, '1', '1', '10', '8', '0', '0', '15', '9', '4', '3', '10', '21'], [17, 10, '1', '2', '3', '4', '0', '0', '8', '13', '6', '9', '12', '11'], [8, 12, '1', '1', '7', '4', '0', '0', '14', '8', '10', '5', '12', '12'], [5, 4, '1', '0', '12', '5', '0', '0', '16', '12', '7', '4', '11', '11'], [0, 8, '5', '0', '8', '2', '0', '0', '15', '7', '3', '8', '10', '10'], [2, 4, '1', '0', '4', '4', '0', '0', '12', '8', '2', '4', '16', '14'], [10, 16, '2', '0', '10', '0', '0', '0', '15', '4', '11', '5', '18', '21'], [7, 1, '1', '1', '5', '4', '0', '0', '12', '7', '9', '3', '14', '9'], [11, 12, '0', '0', '6', '4', '0', '0', '16', '5', '8', '4', '11', '10'], [9, 3, '0', '0', '10', '7', '0', '0', '18', '11', '14', '7', '16', '10'], [13, 15, '1', '3', '7', '5', '0', '0', '15', '9', '6', '3', '11', '9'], [6, 5, '1', '0', '4', '12', '0', '0', '9', '19', '3', '13', '14', '8'], [17, 19, '2', '0', '8', '10', '0', '0', '12', '16', '6', '8', '10', '10'], [18, 14, '0', '0', '1', '4', '0', '1', '10', '5', '7', '3', '17', '13'], [5, 7, '2', '0', '6', '0', '0', '0', '10', '5', '8', '3', '10', '8'], [12, 2, '0', '1', '9', '7', '0', '0', '16', '10', '8', '5', '8', '22'], [3, 7, '2', '0', '6', '3', '0', '0', '8', '8', '4', '7', '15', '13'], [4, 13, '1', '1', '5', '5', '0', '0', '12', '11', '4', '2', '13', '14'], [5, 9, '0', '1', '4', '4', '0', '0', '11', '9', '3', '4', '19', '15'], [19, 18, '1', '2', '21', '9', '0', '1', '31', '12', '12', '8', '12', '15'], [15, 10, '1', '2', '3', '8', '0', '0', '9', '11', '2', '2', '13', '16'], [1, 17, '2', '2', '9', '7', '0', '0', '14', '11', '9', '2', '11', '13'], [8, 6, '1', '2', '6', '5', '0', '0', '14', '7', '9', '2', '9', '21'], [14, 11, '2', '1', '12', '6', '0', '0', '21', '17', '7', '5', '12', '12'], [16, 0, '2', '2', '7', '7', '0', '0', '13', '9', '7', '4', '14', '15'], [0, 1, '0', '0', '1', '3', '0', '0', '7', '3', '2', '3', '13', '16'], [2, 5, '1', '0', '8', '5', '0', '0', '12', '9', '5', '4', '11', '10'], [7, 4, '2', '2', '7', '5', '0', '1', '8', '10', '5', '4', '15', '7'], [11, 15, '4', '0', '7', '3', '0', '0', '12', '5', '12', '3', '6', '5'], [13, 14, '1', '0', '3', '6', '0', '0', '7', '7', '7', '5', '7', '9'], [6, 3, '1', '1', '7', '7', '0', '0', '14', '11', '10', '3', '14', '13'], [17, 12, '2', '1', '11', '6', '0', '0', '20', '10', '12', '7', '7', '12'], [18, 16, '1', '0', '6', '5', '0', '0', '11', '10', '5', '5', '15', '20'], [10, 8, '4', '1', '14', '4', '0', '1', '21', '6', '9', '3', '14', '20'], [9, 19, '2', '0', '8', '6', '0', '0', '15', '12', '7', '3', '12', '14'], [6, 0, '1', '1', '6', '9', '0', '0', '11', '20', '5', '12', '13', '10'], [1, 9, '0', '3', '2', '8', '1', '0', '3', '15', '3', '5', '11', '12'], [12, 10, '1', '2', '9', '3', '0', '0', '16', '10', '7', '3', '14', '9'], [8, 7, '3', '3', '7', '6', '0', '0', '13', '12', '3', '2', '12', '9'], [4, 6, '3', '1', '9', '7', '0', '0', '13', '14', '8', '3', '11', '15'], [5, 11, '1', '1', '7', '7', '0', '1', '12', '11', '6', '2', '9', '19'], [19, 13, '2', '1', '7', '6', '0', '0', '12', '14', '5', '4', '9', '14'], [15, 18, '1', '1', '10', '7', '0', '0', '18', '14', '4', '4', '10', '13'], [16, 2, '1', '0', '13', '2', '0', '0', '16', '14', '7', '7', '9', '17'], [3, 0, '0', '1', '5', '5', '0', '0', '12', '7', '6', '7', '13', '19'], [14, 17, '1', '1', '8', '7', '0', '0', '19', '15', '13', '3', '13', '11'], [15, 14, '3', '3', '12', '12', '0', '0', '15', '22', '3', '5', '13', '16'], [0, 4, '2', '1', '10', '1', '0', '0', '17', '2', '7', '5', '15', '13'], [18, 5, '0', '2', '8', '8', '0', '2', '16', '12', '9', '4', '21', '15'], [2, 1, '1', '1', '5', '5', '0', '0', '10', '9', '6', '8', '11', '12'], [10, 3, '0', '2', '9', '6', '0', '0', '17', '10', '11', '3', '9', '14'], [7, 15, '2', '1', '5', '4', '0', '0', '10', '12', '7', '10', '13', '10'], [11, 8, '1', '0', '9', '0', '0', '0', '19', '4', '9', '2', '5', '9'], [9, 14, '1', '1', '4', '5', '0', '0', '8', '6', '8', '1', '11', '17'], [13, 12, '5', '1', '11', '0', '0', '0', '13', '2', '7', '3', '5', '8'], [6, 19, '5', '1', '10', '4', '0', '0', '16', '19', '7', '2', '12', '8'], [17, 16, '0', '2', '10', '12', '1', '0', '18', '14', '5', '3', '6', '11'], [], [18, 15, '2', '0', '5', '2', '0', '0', '14', '6', '12', '6', '14', '9'], [2, 22, '1', '1', '4', '2', '0', '0', '12', '4', '4', '5', '15', '17'], [10, 7, '4', '1', '11', '5', '0', '0', '21', '9', '9', '5', '10', '12'], [13, 3, '1', '1', '5', '2', '0', '0', '12', '4', '9', '4', '14', '12'], [19, 14, '2', '2', '8', '4', '0', '0', '15', '11', '6', '7', '16', '13'], [20, 21, '1', '1', '10', '8', '0', '0', '14', '14', '6', '11', '14', '16'], [6, 1, '1', '1', '8', '11', '0', '0', '14', '16', '8', '4', '13', '16'], [16, 9, '1', '1', '7', '8', '0', '0', '14', '16', '3', '8', '17', '11'], [11, 5, '1', '0', '5', '3', '0', '0', '8', '11', '2', '3', '17', '8'], [12, 0, '1', '4', '5', '14', '0', '0', '9', '18', '0', '7', '14', '19'], [1, 11, '0', '1', '3', '3', '0', '0', '9', '7', '8', '2', '10', '16'], [7, 6, '2', '1', '14', '6', '0', '0', '18', '10', '5', '6', '9', '11'], [21, 12, '1', '3', '9', '7', '0', '1', '15', '14', '6', '6', '6', '18'], [3, 10, '2', '0', '12', '4', '0', '0', '23', '10', '8', '4', '11', '13'], [9, 13, '2', '1', '10', '4', '0', '1', '15', '7', '8', '3', '9', '17'], [5, 20, '2', '1', '7', '2', '0', '0', '17', '7', '11', '5', '8', '23'], [14, 16, '0', '1', '6', '8', '0', '0', '13', '15', '13', '3', '10', '12'], [15, 2, '3', '2', '7', '6', '0', '0', '13', '10', '3', '6', '12', '18'], [0, 19, '5', '3', '11', '5', '0', '0', '16', '9', '7', '5', '8', '14'], [22, 18, '1', '1', '5', '4', '0', '0', '14', '9', '12', '4', '12', '20'], [1, 13, '1', '0', '2', '1', '0', '0', '6', '4', '2', '5', '20', '13'], [21, 11, '0', '2', '4', '10', '0', '0', '7', '16', '1', '6', '9', '12'], [0, 2, '3', '0', '9', '0', '0', '0', '15', '3', '6', '5', '11', '18'], [7, 18, '3', '0', '5', '4', '0', '0', '7', '9', '4', '12', '6', '7'], [3, 19, '0', '2', '2', '5', '0', '0', '6', '10', '1', '6', '10', '8'], [14, 20, '2', '2', '15', '5', '0', '0', '23', '13', '12', '6', '17', '10'], [15, 10, '1', '2', '6', '5', '0', '0', '10', '9', '9', '6', '9', '8'], [22, 16, '1', '1', '2', '6', '0', '0', '4', '12', '4', '5', '13', '10'], [18, 14, '4', '2', '13', '10', '0', '0', '16', '12', '9', '4', '23', '11'], [2, 5, '1', '1', '4', '15', '1', '0', '8', '24', '4', '14', '25', '17'], [11, 15, '2', '1', '7', '2', '0', '0', '16', '6', '9', '0', '11', '12'], [12, 22, '2', '1', '10', '8', '0', '0', '16', '12', '7', '6', '11', '16'], [13, 7, '4', '0', '10', '3', '0', '0', '16', '6', '6', '4', '11', '8'], [19, 21, '2', '1', '15', '2', '0', '0', '24', '6', '4', '2', '16', '16'], [20, 0, '1', '4', '5', '15', '0', '0', '5', '18', '4', '10', '15', '5'], [16, 1, '1', '0', '3', '2', '0', '0', '6', '10', '2', '8', '15', '5'], [10, 9, '1', '0', '8', '2', '0', '0', '10', '7', '5', '2', '15', '14'], [5, 12, '0', '0', '6', '2', '0', '0', '14', '7', '8', '2', '8', '13'], [6, 3, '4', '3', '13', '10', '0', '0', '21', '18', '6', '8', '12', '12'], [18, 11, '0', '0', '5', '10', '0', '0', '11', '18', '2', '9', '9', '12'], [10, 5, '2', '2', '7', '7', '0', '0', '12', '12', '4', '7', '22', '15'], [3, 0, '0', '3', '7', '7', '0', '0', '9', '12', '9', '4', '16', '10'], [9, 22, '3', '0', '9', '5', '0', '0', '16', '9', '8', '6', '9', '11'], [13, 12, '0', '1', '4', '5', '0', '1', '13', '6', '9', '6', '14', '9'], [19, 1, '2', '1', '6', '3', '0', '0', '14', '5', '6', '3', '16', '19'], [14, 2, '3', '0', '4', '1', '0', '0', '7', '5', '8', '5', '10', '19'], [6, 21, '3', '1', '7', '3', '0', '0', '16', '6', '8', '7', '14', '21'], [16, 20, '0', '0', '7', '2', '0', '0', '15', '7', '13', '5', '17', '9'], [7, 15, '0', '0', '4', '7', '0', '0', '8', '12', '9', '10', '12', '13'], [0, 10, '2', '2', '6', '5', '0', '0', '11', '8', '3', '8', '13', '17'], [1, 7, '1', '1', '5', '1', '1', '0', '7', '2', '12', '3', '12', '16'], [2, 6, '1', '0', '3', '2', '0', '0', '13', '6', '13', '4', '13', '15'], [21, 13, '1', '2', '1', '10', '0', '0', '3', '14', '2', '7', '14', '15'], [20, 18, '0', '0', '3', '9', '0', '0', '14', '18', '1', '6', '10', '13'], [22, 3, '1', '1', '5', '3', '1', '2', '8', '9', '11', '9', '11', '6'], [11, 16, '0', '0', '8', '3', '0', '0', '20', '6', '9', '6', '16', '13'], [12, 19, '1', '0', '9', '11', '0', '0', '11', '16', '6', '3', '13', '12'], [15, 14, '1', '2', '6', '3', '0', '0', '11', '9', '7', '6', '15', '11'], [5, 9, '2', '1', '9', '3', '0', '0', '14', '6', '7', '2', '22', '15'], [18, 21, '1', '1', '5', '8', '0', '0', '10', '13', '5', '8', '10', '15'], [10, 1, '1', '1', '12', '2', '0', '0', '16', '6', '7', '2', '7', '18'], [3, 15, '1', '0', '6', '5', '0', '0', '11', '15', '5', '4', '11', '19'], [9, 20, '3', '0', '12', '1', '0', '0', '20', '2', '4', '2', '10', '12'], [13, 0, '0', '1', '7', '9', '0', '0', '11', '12', '8', '1', '16', '8'], [19, 11, '0', '1', '4', '12', '0', '0', '8', '17', '3', '7', '10', '15'], [14, 22, '3', '1', '10', '3', '0', '1', '31', '7', '8', '3', '15', '15'], [16, 5, '0', '1', '5', '4', '0', '0', '11', '6', '3', '3', '9', '10'], [6, 12, '0', '1', '6', '2', '0', '0', '11', '9', '3', '6', '12', '16'], [7, 2, '1', '0', '3', '7', '0', '0', '6', '12', '6', '10', '18', '14'], [0, 7, '4', '0', '9', '0', '0', '0', '15', '4', '6', '1', '13', '15'], [2, 18, '2', '2', '7', '9', '0', '0', '13', '15', '5', '8', '14', '17'], [12, 16, '0', '1', '4', '2', '0', '0', '10', '8', '11', '2', '9', '13'], [20, 6, '2', '2', '9', '5', '0', '0', '17', '13', '8', '2', '13', '7'], [15, 13, '0', '0', '11', '11', '0', '0', '16', '21', '6', '14', '13', '11'], [22, 10, '2', '1', '7', '7', '0', '0', '13', '16', '6', '5', '18', '15'], [1, 14, '2', '2', '6', '3', '0', '0', '10', '6', '6', '5', '17', '14'], [11, 9, '1', '0', '5', '3', '0', '0', '13', '5', '11', '2', '14', '20'], [5, 19, '1', '1', '10', '9', '0', '0', '22', '13', '10', '2', '8', '13'], [21, 3, '2', '0', '10', '5', '0', '1', '20', '8', '10', '3', '10', '10'], [0, 18, '3', '1', '15', '2', '0', '0', '18', '5', '6', '3', '11', '22'], [1, 5, '0', '0', '6', '7', '0', '0', '14', '12', '3', '3', '16', '11'], [2, 19, '0', '4', '5', '10', '1', '0', '12', '16', '2', '10', '12', '14'], [10, 21, '1', '0', '7', '2', '0', '0', '21', '7', '7', '3', '19', '14'], [12, 15, '1', '0', '5', '4', '0', '0', '14', '8', '5', '6', '5', '14'], [3, 9, '2', '4', '3', '3', '0', '1', '5', '10', '3', '6', '12', '14'], [13, 11, '1', '0', '1', '3', '0', '0', '4', '9', '0', '12', '6', '7'], [22, 20, '0', '0', '2', '5', '0', '0', '9', '12', '2', '3', '13', '21'], [7, 14, '1', '1', '6', '8', '0', '0', '11', '15', '8', '9', '15', '7'], [6, 16, '1', '0', '6', '7', '0', '0', '11', '16', '3', '5', '12', '11'], [18, 3, '2', '0', '5', '1', '0', '0', '11', '7', '8', '4', '21', '14'], [11, 2, '4', '0', '13', '4', '0', '0', '17', '7', '12', '3', '11', '11'], [21, 22, '3', '0', '4', '6', '0', '0', '11', '11', '7', '10', '11', '15'], [9, 7, '2', '0', '7', '1', '0', '0', '17', '3', '12', '0', '9', '11'], [20, 12, '2', '3', '10', '9', '0', '0', '22', '13', '11', '4', '8', '11'], [16, 10, '1', '2', '5', '4', '0', '0', '8', '6', '2', '3', '14', '17'], [5, 0, '2', '0', '5', '6', '0', '0', '11', '12', '3', '3', '20', '23'], [19, 6, '1', '1', '11', '1', '0', '0', '26', '3', '9', '0', '5', '5'], [14, 13, '4', '3', '11', '6', '0', '0', '16', '11', '7', '5', '13', '14'], [15, 1, '0', '0', '1', '2', '0', '0', '6', '8', '4', '4', '11', '11'], [0, 15, '2', '2', '9', '4', '0', '0', '14', '7', '7', '7', '8', '21'], [1, 21, '0', '1', '11', '1', '0', '0', '16', '4', '11', '3', '8', '14'], [2, 9, '2', '2', '7', '9', '0', '0', '8', '13', '6', '6', '14', '11'], [7, 19, '1', '2', '7', '7', '0', '0', '8', '11', '3', '4', '14', '8'], [12, 18, '1', '1', '8', '5', '0', '0', '20', '9', '8', '3', '10', '19'], [3, 16, '2', '0', '4', '1', '0', '0', '8', '5', '5', '4', '11', '9'], [6, 5, '2', '0', '7', '9', '0', '0', '11', '20', '3', '9', '19', '9'], [22, 11, '1', '4', '9', '9', '0', '0', '12', '19', '2', '6', '12', '8'], [10, 14, '2', '1', '11', '4', '0', '0', '17', '5', '4', '5', '14', '11'], [13, 20, '1', '1', '11', '5', '0', '0', '19', '10', '9', '4', '10', '13'], [18, 6, '3', '0', '9', '2', '0', '0', '10', '7', '6', '4', '8', '13'], [11, 12, '1', '0', '3', '1', '0', '0', '12', '3', '16', '3', '10', '15'], [21, 0, '1', '1', '1', '13', '0', '0', '3', '19', '1', '8', '10', '8'], [9, 1, '0', '1', '8', '1', '0', '0', '17', '6', '4', '5', '19', '17'], [20, 2, '1', '1', '5', '9', '0', '1', '12', '15', '7', '4', '18', '12'], [15, 22, '2', '2', '6', '5', '0', '0', '11', '10', '5', '1', '13', '14'], [16, 7, '2', '3', '11', '3', '0', '1', '19', '3', '5', '1', '6', '13'], [5, 13, '0', '0', '10', '1', '1', '0', '15', '3', '16', '1', '10', '10'], [19, 10, '1', '1', '5', '4', '0', '1', '14', '7', '3', '3', '13', '12'], [14, 3, '1', '4', '20', '7', '0', '0', '26', '11', '19', '0', '12', '16'], [1, 12, '0', '1', '4', '8', '1', '0', '15', '9', '5', '3', '13', '12'], [10, 18, '1', '2', '7', '5', '0', '0', '15', '10', '9', '5', '10', '17'], [7, 20, '4', '0', '11', '4', '0', '0', '16', '8', '1', '5', '14', '11'], [3, 11, '1', '4', '2', '11', '0', '0', '7', '16', '1', '5', '13', '13'], [9, 21, '3', '2', '5', '4', '0', '0', '18', '4', '7', '1', '13', '17'], [13, 2, '1', '1', '7', '2', '1', '0', '8', '11', '3', '5', '19', '13'], [15, 6, '2', '1', '5', '6', '0', '0', '11', '13', '4', '8', '14', '15'], [16, 0, '4', '5', '7', '6', '0', '0', '10', '10', '3', '4', '20', '9'], [14, 5, '1', '3', '7', '7', '0', '0', '13', '12', '2', '7', '11', '16'], [22, 19, '1', '2', '6', '7', '0', '0', '16', '11', '7', '10', '14', '14'], [0, 22, '1', '1', '4', '2', '0', '0', '11', '4', '12', '3', '9', '12'], [11, 10, '2', '2', '11', '5', '0', '0', '17', '10', '9', '2', '18', '11'], [21, 14, '0', '2', '5', '9', '0', '0', '7', '17', '3', '5', '13', '10'], [12, 3, '1', '0', '7', '4', '0', '0', '13', '6', '12', '7', '11', '24'], [5, 7, '2', '0', '11', '2', '0', '0', '20', '5', '6', '3', '8', '13'], [19, 9, '2', '0', '6', '10', '0', '0', '14', '17', '3', '5', '14', '10'], [20, 15, '2', '1', '8', '7', '0', '0', '9', '14', '4', '6', '19', '14'], [6, 13, '1', '3', '7', '13', '0', '0', '9', '18', '8', '11', '8', '14'], [2, 1, '3', '3', '10', '5', '0', '0', '15', '8', '6', '9', '12', '9'], [18, 16, '1', '0', '5', '6', '0', '0', '9', '11', '5', '5', '17', '11'], [1, 20, '1', '1', '10', '3', '0', '0', '16', '7', '6', '1', '14', '12'], [10, 6, '0', '1', '10', '4', '0', '0', '19', '8', '7', '6', '13', '14'], [7, 11, '0', '4', '4', '12', '0', '0', '8', '16', '7', '10', '13', '11'], [3, 2, '0', '2', '6', '4', '1', '0', '12', '17', '6', '6', '9', '13'], [13, 18, '2', '0', '6', '3', '0', '1', '11', '7', '8', '8', '14', '19'], [15, 21, '2', '2', '8', '8', '0', '0', '15', '11', '4', '8', '7', '11'], [22, 5, '0', '3', '2', '9', '0', '0', '3', '18', '1', '8', '13', '11'], [9, 0, '2', '1', '7', '2', '0', '0', '11', '3', '6', '0', '6', '16'], [14, 12, '1', '1', '9', '3', '0', '0', '19', '7', '8', '4', '14', '19'], [16, 19, '2', '0', '9', '3', '0', '1', '14', '8', '5', '7', '11', '13'], [0, 1, '3', '0', '4', '1', '0', '0', '7', '2', '4', '2', '7', '15'], [18, 9, '1', '1', '2', '7', '0', '0', '3', '12', '3', '9', '10', '10'], [2, 16, '0', '1', '7', '8', '0', '0', '12', '10', '7', '5', '11', '11'], [11, 14, '4', '0', '9', '5', '0', '0', '14', '9', '3', '2', '12', '15'], [12, 10, '3', '2', '3', '6', '0', '0', '12', '10', '4', '4', '14', '18'], [5, 15, '3', '0', '14', '0', '0', '0', '34', '4', '16', '2', '5', '10'], [20, 3, '0', '1', '4', '9', '0', '0', '10', '15', '5', '4', '22', '22'], [6, 22, '3', '2', '10', '4', '0', '0', '14', '10', '8', '3', '14', '14'], [21, 7, '0', '1', '3', '7', '0', '0', '5', '11', '5', '3', '16', '14'], [19, 13, '3', '2', '7', '6', '0', '0', '11', '9', '6', '4', '14', '18'], [21, 2, '0', '0', '1', '8', '0', '1', '8', '16', '2', '1', '19', '20'], [12, 9, '1', '0', '2', '5', '0', '0', '6', '16', '2', '7', '12', '14'], [13, 16, '0', '1', '3', '2', '0', '0', '8', '5', '6', '2', '8', '9'], [14, 6, '1', '1', '2', '3', '0', '0', '15', '6', '6', '2', '12', '15'], [20, 10, '3', '2', '4', '3', '0', '0', '9', '4', '4', '4', '10', '26'], [15, 19, '2', '2', '5', '7', '0', '0', '16', '15', '4', '6', '11', '12'], [22, 7, '0', '1', '4', '2', '0', '0', '7', '8', '6', '6', '16', '13'], [0, 11, '2', '2', '5', '5', '0', '0', '10', '10', '4', '6', '11', '17'], [18, 1, '1', '2', '3', '5', '0', '0', '9', '9', '6', '0', '9', '28'], [3, 5, '1', '1', '3', '6', '0', '0', '7', '14', '6', '7', '15', '11'], [9, 6, '1', '1', '7', '4', '0', '0', '12', '7', '6', '4', '6', '9'], [1, 22, '4', '0', '5', '9', '0', '0', '7', '11', '2', '2', '14', '20'], [2, 12, '0', '0', '9', '2', '0', '0', '13', '5', '7', '3', '25', '21'], [10, 13, '0', '1', '6', '2', '0', '0', '14', '6', '8', '1', '17', '12'], [11, 20, '4', '0', '8', '0', '0', '0', '12', '1', '7', '3', '12', '11'], [5, 21, '5', '2', '14', '3', '0', '0', '19', '4', '15', '3', '12', '8'], [19, 18, '3', '0', '8', '6', '0', '0', '9', '14', '5', '6', '6', '18'], [16, 15, '5', '1', '12', '4', '0', '0', '18', '7', '6', '3', '9', '13'], [9, 14, '3', '1', '5', '3', '0', '1', '15', '7', '5', '5', '12', '19'], [6, 0, '0', '1', '5', '5', '0', '0', '11', '13', '5', '7', '13', '9'], [7, 3, '2', '1', '5', '4', '0', '0', '8', '6', '5', '3', '14', '16'], [0, 3, '2', '0', '8', '1', '0', '0', '11', '2', '7', '6', '12', '9'], [1, 19, '2', '0', '10', '6', '0', '0', '11', '9', '5', '3', '17', '9'], [2, 14, '2', '2', '11', '3', '0', '0', '21', '5', '7', '5', '18', '19'], [11, 18, '1', '0', '7', '7', '0', '0', '14', '9', '6', '3', '12', '13'], [21, 6, '0', '1', '10', '6', '0', '0', '18', '11', '9', '7', '10', '14'], [12, 13, '2', '1', '6', '6', '0', '1', '9', '10', '4', '4', '14', '21'], [5, 10, '2', '0', '7', '1', '0', '0', '14', '8', '9', '0', '12', '14'], [20, 16, '0', '2', '8', '9', '0', '0', '13', '16', '10', '8', '11', '9'], [15, 7, '0', '0', '4', '5', '0', '0', '12', '15', '5', '11', '7', '12'], [22, 9, '0', '5', '1', '8', '1', '0', '2', '16', '4', '11', '12', '13'], [18, 5, '0', '1', '6', '7', '0', '0', '8', '15', '6', '9', '14', '10'], [10, 2, '0', '1', '5', '5', '0', '0', '13', '7', '5', '1', '13', '10'], [7, 12, '2', '0', '8', '3', '0', '1', '12', '5', '8', '2', '13', '11'], [3, 1, '2', '3', '7', '5', '0', '0', '10', '6', '7', '3', '9', '13'], [9, 15, '1', '0', '11', '2', '0', '0', '19', '3', '7', '3', '6', '12'], [13, 22, '1', '1', '4', '0', '0', '1', '11', '0', '11', '0', '7', '10'], [19, 20, '2', '0', '11', '2', '0', '0', '17', '10', '8', '4', '13', '10'], [6, 11, '0', '2', '2', '9', '0', '0', '4', '12', '5', '4', '12', '15'], [16, 21, '1', '1', '14', '7', '0', '0', '17', '11', '5', '6', '10', '15'], [14, 0, '0', '1', '5', '6', '0', '0', '9', '13', '4', '8', '10', '21'], [18, 2, '1', '0', '3', '4', '0', '0', '11', '9', '6', '7', '9', '9'], [10, 22, '1', '1', '15', '7', '0', '0', '25', '11', '7', '3', '13', '15'], [7, 0, '1', '3', '4', '5', '0', '0', '7', '8', '1', '2', '13', '15'], [3, 21, '3', '1', '6', '3', '0', '0', '12', '7', '4', '2', '11', '18'], [9, 11, '0', '1', '7', '7', '0', '0', '14', '8', '3', '2', '14', '16'], [13, 15, '2', '1', '8', '1', '0', '0', '15', '6', '9', '8', '12', '9'], [19, 5, '0', '2', '2', '7', '0', '0', '6', '13', '10', '4', '13', '9'], [14, 1, '2', '1', '4', '8', '0', '0', '8', '10', '6', '5', '8', '14'], [6, 20, '1', '1', '6', '5', '0', '1', '14', '6', '7', '5', '13', '6'], [16, 12, '5', '2', '12', '3', '0', '0', '14', '6', '6', '4', '8', '13'], [2, 7, '1', '0', '11', '6', '0', '0', '16', '9', '6', '8', '11', '14'], [21, 18, '2', '0', '9', '6', '0', '0', '14', '12', '7', '7', '8', '14'], [20, 9, '1', '2', '4', '6', '0', '0', '7', '11', '5', '2', '16', '19'], [22, 14, '0', '0', '5', '7', '0', '0', '8', '12', '1', '6', '9', '7'], [0, 13, '1', '1', '9', '4', '0', '0', '13', '5', '12', '5', '10', '13'], [1, 10, '1', '2', '8', '6', '0', '0', '14', '11', '6', '5', '12', '12'], [11, 19, '2', '0', '10', '1', '0', '0', '14', '2', '4', '3', '15', '9'], [12, 6, '2', '1', '7', '6', '0', '0', '17', '9', '8', '1', '5', '9'], [5, 16, '0', '0', '16', '5', '0', '0', '23', '7', '8', '2', '11', '13'], [15, 3, '3', '3', '5', '7', '0', '0', '8', '12', '2', '10', '16', '16'], [18, 20, '3', '0', '10', '1', '0', '0', '22', '7', '7', '5', '15', '12'], [10, 0, '1', '0', '3', '7', '0', '0', '9', '11', '8', '7', '8', '8'], [7, 1, '3', '1', '6', '6', '0', '0', '10', '10', '4', '4', '11', '10'], [9, 5, '0', '1', '5', '2', '0', '1', '13', '5', '9', '4', '12', '25'], [13, 21, '3', '1', '9', '2', '0', '0', '16', '7', '7', '7', '10', '11'], [14, 15, '2', '1', '17', '6', '0', '0', '22', '9', '14', '4', '12', '20'], [6, 2, '0', '1', '1', '6', '2', '0', '11', '14', '5', '6', '13', '23'], [16, 11, '0', '2', '4', '5', '0', '0', '12', '12', '4', '7', '14', '12'], [3, 22, '1', '0', '4', '6', '0', '0', '8', '7', '7', '5', '12', '12'], [19, 12, '1', '1', '12', '5', '0', '0', '18', '13', '9', '6', '12', '10'], [1, 3, '1', '2', '5', '3', '0', '0', '8', '6', '5', '3', '16', '11'], [11, 6, '3', '0', '9', '5', '0', '0', '14', '9', '2', '3', '2', '9'], [21, 16, '3', '0', '7', '6', '0', '0', '8', '12', '5', '4', '9', '14'], [12, 7, '0', '1', '7', '3', '0', '0', '12', '4', '9', '5', '13', '10'], [5, 18, '3', '1', '11', '6', '0', '0', '18', '10', '4', '2', '11', '21'], [20, 19, '4', '4', '9', '15', '0', '0', '17', '22', '5', '8', '11', '16'], [15, 9, '2', '0', '7', '2', '0', '0', '11', '11', '6', '3', '10', '12'], [22, 13, '2', '0', '6', '5', '0', '0', '13', '11', '3', '11', '9', '12'], [0, 14, '1', '0', '8', '1', '0', '0', '13', '4', '11', '1', '17', '19'], [2, 10, '0', '1', '4', '3', '0', '0', '9', '6', '7', '7', '16', '19'], [0, 5, '2', '4', '10', '9', '0', '1', '11', '10', '6', '7', '20', '16'], [10, 16, '3', '1', '9', '5', '0', '1', '15', '11', '4', '6', '9', '17'], [7, 9, '1', '2', '3', '7', '0', '0', '4', '19', '3', '7', '11', '15'], [6, 19, '2', '1', '5', '2', '0', '0', '14', '9', '9', '5', '9', '8'], [22, 21, '2', '2', '7', '4', '0', '1', '12', '4', '8', '4', '17', '11'], [1, 15, '2', '1', '4', '3', '0', '0', '12', '6', '8', '1', '11', '18'], [2, 11, '0', '1', '2', '3', '0', '0', '11', '6', '4', '4', '24', '15'], [12, 20, '1', '0', '4', '6', '0', '0', '18', '11', '3', '3', '12', '16'], [3, 18, '1', '1', '3', '2', '0', '0', '12', '12', '4', '8', '12', '17'], [13, 14, '1', '1', '1', '1', '0', '0', '5', '3', '3', '1', '11', '18'], [18, 0, '1', '3', '1', '6', '0', '0', '5', '15', '5', '2', '13', '11'], [21, 10, '0', '1', '7', '6', '0', '0', '17', '9', '6', '4', '16', '10'], [9, 3, '3', '1', '7', '2', '0', '0', '10', '6', '4', '3', '12', '13'], [5, 1, '2', '0', '11', '1', '0', '0', '17', '7', '5', '2', '10', '16'], [19, 2, '1', '0', '7', '3', '1', '0', '13', '6', '8', '10', '14', '10'], [14, 7, '1', '1', '10', '2', '0', '0', '21', '8', '7', '2', '10', '12'], [20, 22, '3', '2', '7', '8', '0', '0', '13', '15', '3', '3', '18', '13'], [16, 6, '3', '1', '10', '3', '0', '0', '18', '6', '9', '2', '10', '8'], [11, 13, '0', '0', '8', '2', '0', '0', '11', '5', '7', '1', '9', '11'], [15, 12, '2', '2', '9', '4', '0', '0', '21', '8', '8', '3', '15', '11'], [1, 9, '2', '0', '3', '2', '0', '0', '5', '6', '10', '4', '16', '10'], [2, 20, '3', '0', '13', '6', '0', '0', '18', '9', '12', '3', '15', '14'], [10, 19, '0', '0', '8', '2', '0', '0', '13', '5', '3', '2', '11', '15'], [12, 11, '0', '1', '2', '11', '1', '0', '4', '25', '3', '9', '14', '9'], [6, 18, '1', '2', '3', '4', '0', '0', '9', '6', '6', '3', '14', '22'], [13, 5, '0', '2', '3', '3', '0', '0', '9', '10', '4', '6', '9', '14'], [0, 21, '5', '1', '12', '3', '0', '0', '17', '6', '5', '1', '8', '17'], [22, 15, '0', '0', '4', '7', '0', '0', '9', '11', '7', '3', '15', '15'], [18, 12, '1', '3', '6', '9', '0', '0', '9', '15', '3', '7', '22', '17'], [21, 1, '2', '0', '2', '5', '0', '0', '8', '9', '4', '7', '23', '15'], [5, 6, '2', '1', '6', '4', '0', '0', '13', '8', '4', '3', '15', '15'], [15, 0, '1', '1', '3', '12', '1', '1', '10', '17', '3', '5', '9', '13'], [16, 3, '2', '0', '8', '5', '0', '0', '18', '8', '13', '2', '6', '13'], [19, 7, '2', '2', '18', '2', '0', '0', '21', '5', '10', '1', '10', '15'], [14, 10, '2', '1', '9', '2', '0', '0', '14', '4', '6', '3', '12', '19'], [20, 13, '2', '3', '4', '10', '1', '0', '5', '17', '3', '12', '12', '7'], [0, 6, '3', '0', '6', '2', '0', '0', '11', '5', '7', '3', '12', '15'], [18, 19, '2', '0', '4', '1', '0', '0', '8', '5', '5', '4', '8', '6'], [21, 5, '0', '0', '1', '9', '1', '0', '2', '16', '2', '7', '12', '9'], [3, 7, '0', '0', '5', '6', '0', '0', '10', '9', '5', '5', '9', '12'], [14, 9, '1', '0', '4', '3', '0', '0', '13', '6', '5', '4', '11', '16'], [20, 11, '1', '3', '5', '13', '0', '0', '8', '23', '7', '7', '14', '15'], [15, 16, '1', '0', '4', '8', '0', '0', '9', '13', '4', '10', '7', '11'], [12, 2, '0', '1', '4', '5', '0', '0', '12', '11', '6', '3', '15', '20'], [22, 1, '2', '0', '7', '0', '0', '0', '12', '7', '13', '3', '16', '18'], [13, 10, '0', '1', '3', '3', '0', '0', '8', '11', '5', '5', '10', '21'], [11, 22, '1', '0', '11', '4', '0', '0', '23', '7', '8', '3', '13', '9'], [7, 16, '2', '0', '2', '4', '0', '0', '5', '7', '5', '2', '15', '16'], [9, 2, '0', '0', '2', '2', '0', '0', '7', '5', '11', '3', '12', '13'], [2, 0, '0', '1', '2', '8', '0', '0', '9', '10', '4', '6', '21', '8'], [10, 20, '1', '0', '10', '2', '0', '0', '16', '6', '7', '6', '20', '16'], [7, 22, '1', '4', '5', '9', '1', '0', '8', '14', '3', '9', '11', '8'], [11, 21, '4', '1', '13', '3', '0', '0', '20', '6', '10', '4', '13', '16'], [5, 3, '1', '0', '12', '6', '0', '0', '19', '10', '18', '3', '7', '11'], [6, 14, '1', '1', '6', '6', '0', '0', '13', '11', '4', '3', '15', '19'], [16, 13, '2', '1', '9', '7', '0', '0', '12', '11', '6', '4', '13', '13'], [1, 18, '2', '0', '4', '4', '0', '0', '10', '5', '2', '3', '12', '20'], [9, 12, '2', '1', '8', '2', '1', '0', '15', '4', '5', '1', '13', '15'], [19, 15, '1', '3', '8', '7', '0', '0', '14', '12', '3', '6', '20', '17'], [0, 20, '4', '1', '15', '1', '0', '0', '21', '3', '10', '3', '8', '11'], [1, 16, '1', '1', '7', '5', '0', '0', '14', '9', '3', '3', '6', '10'], [7, 13, '2', '2', '3', '8', '0', '0', '9', '16', '6', '7', '6', '8'], [21, 19, '0', '1', '13', '5', '0', '0', '23', '7', '8', '3', '6', '19'], [9, 10, '1', '0', '7', '5', '0', '0', '10', '13', '4', '12', '6', '13'], [5, 2, '0', '0', '9', '3', '0', '0', '23', '9', '9', '3', '21', '18'], [14, 18, '0', '3', '12', '5', '3', '0', '20', '8', '8', '6', '14', '24'], [15, 11, '1', '3', '3', '8', '0', '0', '4', '12', '4', '3', '11', '10'], [3, 6, '3', '1', '7', '4', '0', '0', '12', '8', '5', '5', '14', '10'], [22, 12, '1', '0', '4', '2', '0', '0', '8', '8', '7', '7', '15', '20'], [2, 15, '3', '0', '8', '3', '0', '0', '15', '9', '4', '2', '19', '14'], [10, 3, '3', '1', '10', '4', '0', '1', '15', '7', '4', '6', '6', '14'], [11, 1, '1', '1', '12', '1', '0', '0', '19', '2', '8', '5', '17', '12'], [13, 9, '1', '0', '8', '1', '0', '0', '11', '6', '5', '2', '8', '11'], [19, 0, '0', '1', '6', '1', '0', '0', '9', '7', '5', '3', '11', '9'], [20, 5, '2', '0', '5', '8', '0', '0', '7', '16', '3', '4', '19', '18'], [6, 7, '4', '2', '9', '4', '0', '0', '13', '8', '5', '4', '11', '14'], [18, 22, '1', '1', '7', '5', '1', '1', '14', '7', '5', '5', '6', '10'], [12, 21, '4', '0', '8', '3', '0', '0', '13', '11', '4', '9', '10', '14'], [16, 14, '1', '0', '6', '3', '0', '0', '19', '6', '4', '7', '9', '15'], [1, 6, '0', '0', '4', '4', '0', '0', '9', '5', '7', '5', '16', '12'], [7, 10, '1', '2', '10', '5', '0', '0', '18', '11', '8', '2', '10', '14'], [21, 20, '3', '3', '10', '8', '0', '0', '12', '12', '7', '3', '17', '21'], [3, 13, '1', '1', '3', '3', '0', '0', '8', '7', '6', '8', '9', '19'], [9, 16, '2', '2', '7', '6', '0', '0', '18', '9', '12', '3', '5', '7'], [15, 18, '2', '3', '8', '7', '0', '0', '16', '11', '5', '8', '10', '17'], [10, 15, '1', '1', '16', '4', '0', '0', '22', '5', '5', '2', '9', '9'], [19, 3, '1', '1', '4', '2', '0', '0', '8', '7', '2', '6', '11', '12'], [18, 7, '0', '0', '5', '3', '0', '0', '13', '12', '6', '5', '14', '9'], [2, 21, '1', '0', '7', '3', '0', '0', '18', '4', '11', '6', '11', '19'], [11, 0, '0', '0', '3', '4', '0', '0', '9', '6', '2', '2', '10', '16'], [12, 5, '1', '0', '3', '11', '0', '2', '7', '16', '4', '5', '18', '17'], [13, 1, '3', '0', '4', '1', '0', '0', '8', '5', '8', '3', '11', '9'], [20, 14, '2', '1', '8', '10', '0', '0', '15', '24', '6', '6', '12', '11'], [6, 9, '1', '2', '6', '12', '0', '0', '8', '19', '5', '4', '10', '8'], [16, 22, '1', '1', '5', '4', '0', '0', '15', '10', '7', '3', '12', '12'], [18, 10, '1', '1', '10', '6', '0', '0', '13', '12', '7', '5', '17', '13'], [2, 13, '0', '0', '5', '1', '0', '0', '11', '7', '6', '2', '20', '17'], [11, 3, '3', '1', '7', '4', '0', '0', '12', '10', '4', '6', '7', '11'], [21, 9, '1', '0', '3', '7', '0', '0', '6', '9', '4', '1', '19', '14'], [12, 1, '1', '1', '6', '4', '0', '0', '11', '9', '9', '5', '18', '18'], [19, 22, '4', '0', '8', '9', '0', '0', '12', '15', '1', '4', '12', '12'], [20, 7, '1', '0', '6', '6', '0', '0', '13', '11', '7', '8', '8', '9'], [5, 14, '2', '1', '10', '5', '0', '0', '18', '11', '9', '4', '8', '20'], [6, 15, '4', '1', '8', '8', '0', '0', '18', '11', '4', '7', '15', '11'], [0, 16, '1', '0', '3', '1', '0', '0', '12', '7', '4', '7', '17', '15'], [22, 2, '1', '1', '3', '7', '0', '0', '9', '14', '6', '4', '13', '15'], [14, 19, '0', '0', '5', '3', '0', '0', '11', '5', '7', '1', '8', '9'], [1, 2, '2', '1', '6', '1', '0', '0', '10', '5', '6', '1', '15', '16'], [10, 11, '0', '2', '6', '5', '0', '0', '14', '8', '7', '2', '17', '14'], [3, 12, '2', '0', '6', '4', '1', '0', '11', '11', '3', '7', '18', '16'], [9, 19, '1', '1', '6', '3', '0', '0', '12', '6', '7', '4', '10', '20'], [13, 6, '2', '0', '7', '3', '0', '0', '13', '7', '7', '5', '14', '6'], [14, 21, '0', '0', '9', '3', '0', '0', '16', '7', '5', '2', '16', '15'], [15, 20, '4', '3', '9', '7', '0', '0', '15', '14', '6', '8', '18', '13'], [7, 5, '0', '4', '4', '12', '1', '0', '6', '18', '1', '7', '6', '15'], [16, 18, '5', '1', '14', '3', '0', '0', '20', '4', '9', '3', '13', '17'], [22, 0, '0', '2', '3', '5', '0', '0', '7', '8', '9', '0', '12', '11'], [3, 14, '1', '3', '3', '3', '0', '0', '6', '4', '1', '2', '11', '13'], [18, 13, '1', '2', '8', '4', '0', '0', '15', '10', '6', '2', '8', '18'], [2, 3, '1', '3', '6', '6', '1', '1', '13', '8', '8', '5', '11', '23'], [11, 7, '1', '0', '6', '3', '0', '0', '15', '6', '2', '5', '7', '13'], [21, 15, '2', '2', '6', '3', '1', '1', '14', '7', '5', '2', '16', '18'], [12, 14, '2', '0', '6', '4', '0', '1', '8', '9', '3', '2', '22', '11'], [5, 22, '1', '1', '14', '0', '0', '0', '27', '2', '12', '0', '10', '13'], [19, 16, '1', '0', '8', '2', '0', '0', '16', '5', '4', '3', '14', '10'], [20, 1, '1', '0', '4', '9', '0', '1', '8', '22', '1', '7', '20', '10'], [6, 10, '1', '1', '4', '4', '0', '0', '9', '12', '2', '9', '12', '23'], [0, 9, '3', '1', '8', '6', '0', '0', '11', '11', '1', '4', '13', '10'], [5, 11, '1', '3', '9', '6', '0', '0', '15', '7', '7', '0', '11', '14'], [0, 12, '7', '0', '12', '3', '0', '0', '19', '7', '4', '2', '9', '12'], [1, 0, '2', '1', '4', '5', '0', '0', '9', '11', '3', '2', '10', '8'], [10, 12, '3', '2', '9', '5', '1', '0', '16', '10', '5', '6', '12', '17'], [7, 21, '2', '2', '5', '6', '0', '0', '8', '9', '3', '8', '14', '14'], [3, 20, '6', '0', '9', '0', '0', '0', '13', '2', '4', '13', '11', '12'], [9, 18, '2', '1', '7', '4', '0', '0', '15', '13', '9', '5', '10', '16'], [13, 19, '1', '1', '5', '3', '0', '0', '10', '4', '10', '3', '12', '15'], [14, 11, '1', '1', '4', '7', '0', '0', '8', '12', '6', '4', '11', '17'], [15, 5, '1', '2', '3', '8', '0', '0', '11', '16', '5', '3', '14', '13'], [16, 2, '0', '0', '7', '8', '0', '0', '11', '12', '5', '7', '10', '14'], [22, 6, '2', '0', '5', '2', '0', '0', '9', '8', '6', '1', '12', '13'], []]\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(9525.9560546875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.3646006584), tensor(1.5900309086), tensor(1.5788823366), tensor(2.0445148945), tensor(0.9833196402), tensor(2.5136582851), tensor(1.7210109234), tensor(1.7936770916), tensor(0.7932867408), tensor(2.1515111923), tensor(1.8957189322), tensor(2.9202263355), tensor(1.7345434427), tensor(2.0176846981), tensor(1.9447606802), tensor(1.7012517452), tensor(1.8311960697), tensor(0.7499025464), tensor(1.8115577698), tensor(1.8984978199), tensor(0.8419135213), tensor(0.8307094574), tensor(0.7193021178)]\n",
            "b:  [tensor(0.8600127697), tensor(1.5603754520), tensor(1.7083460093), tensor(1.7623870373), tensor(1.3061623573), tensor(0.8953070641), tensor(1.9072767496), tensor(1.8255419731), tensor(1.5914800167), tensor(1.2333235741), tensor(1.6619225740), tensor(0.5920216441), tensor(1.7264685631), tensor(1.5216593742), tensor(1.5988225937), tensor(1.8736026287), tensor(1.6130354404), tensor(1.5491509438), tensor(1.5904839039), tensor(1.6202241182), tensor(1.5483609438), tensor(1.2511587143), tensor(1.2299714088)]\n",
            "c:  [tensor(-0.0332295299), tensor(-0.0332295299), tensor(-0.0332295299), tensor(0.0008501226), tensor(0.0009980786), tensor(0.0007173247), tensor(0.0008605699), tensor(0.0006793492), tensor(0.0008501226), tensor(0.0009980786), tensor(0.0007173247), tensor(0.0008605699), tensor(0.0006793492), tensor(0.0008501226), tensor(0.0009980786), tensor(0.0007173247), tensor(0.0008605699), tensor(0.0006793492)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([224.7927703857, 271.9668579102, 283.1153869629, 288.5907287598,\n",
            "         93.4983444214, 223.2542419434, 298.0227355957, 292.6577758789,\n",
            "        104.0616073608, 248.8955993652, 280.3508300781, 198.0591430664,\n",
            "        284.4902038574, 270.5535583496, 276.1763305664, 295.3483276367,\n",
            "        277.5723571777, 102.5783386230, 274.7772216797, 277.5719299316,\n",
            "        100.3022384644,  89.0725860596,  88.3113937378])\n",
            "btensor.grad: tensor([530.8770751953, 548.3929443359, 579.8922729492, 615.5860595703,\n",
            "        152.0287170410, 473.1491394043, 627.7321166992, 619.7321166992,\n",
            "        180.7828826904, 516.5057373047, 581.4481811523, 417.4951782227,\n",
            "        584.2034301758, 564.6755371094, 577.2471313477, 616.5389404297,\n",
            "        585.4679565430, 178.2445220947, 563.1519775391, 578.2793579102,\n",
            "        179.0345916748, 139.7311096191, 138.4848480225])\n",
            "ctensor.grad: tensor([6.8459057617e+02, 6.8459057617e+02, 6.8459057617e+02, 2.9975488186e+00,\n",
            "        3.8428589702e-02, 5.6535072327e+00, 2.7886037827e+00, 6.4130172729e+00,\n",
            "        2.9975488186e+00, 3.8428589702e-02, 5.6535072327e+00, 2.7886037827e+00,\n",
            "        6.4130172729e+00, 2.9975488186e+00, 3.8428589702e-02, 5.6535072327e+00,\n",
            "        2.7886037827e+00, 6.4130172729e+00])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(4297.1401367188, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.2706599236), tensor(1.4660351276), tensor(1.4483847618), tensor(1.9096733332), tensor(0.9499953985), tensor(2.4189620018), tensor(1.5811865330), tensor(1.6570593119), tensor(0.7548744678), tensor(2.0407607555), tensor(1.7658392191), tensor(2.8409230709), tensor(1.6024708748), tensor(1.8937478065), tensor(1.8175595999), tensor(1.5629482269), tensor(1.7037862539), tensor(0.7128118873), tensor(1.6852158308), tensor(1.7704807520), tensor(0.8057584167), tensor(0.8012103438), tensor(0.6912742853)]\n",
            "b:  [tensor(0.6360687017), tensor(1.2685390711), tensor(1.3903243542), tensor(1.4255836010), tensor(1.2342574596), tensor(0.6955699921), tensor(1.5539164543), tensor(1.4786784649), tensor(1.4926357269), tensor(0.9817082286), tensor(1.3513333797), tensor(0.4821570218), tensor(1.4105387926), tensor(1.2263766527), tensor(1.2911190987), tensor(1.5297617912), tensor(1.2926542759), tensor(1.4516004324), tensor(1.2926840782), tensor(1.3108496666), tensor(1.4505656958), tensor(1.1876651049), tensor(1.1660237312)]\n",
            "c:  [tensor(-0.0528135374), tensor(-0.0528135374), tensor(-0.0528135374), tensor(0.0036384647), tensor(0.0010367045), tensor(0.0061056460), tensor(0.0035732924), tensor(0.0067750080), tensor(0.0036384647), tensor(0.0010367045), tensor(0.0061056460), tensor(0.0035732924), tensor(0.0067750080), tensor(0.0036384647), tensor(0.0010367045), tensor(0.0061056460), tensor(0.0035732924), tensor(0.0067750080)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 93.9406585693, 123.9957199097, 130.4975738525, 134.8415527344,\n",
            "         33.3242111206,  94.6962585449, 139.8243408203, 136.6177978516,\n",
            "         38.4122657776, 110.7503814697, 129.8796997070,  79.3032760620,\n",
            "        132.0725097656, 123.9369277954, 127.2010574341, 138.3034667969,\n",
            "        127.4097900391,  37.0906410217, 126.3419189453, 128.0170898438,\n",
            "         36.1550788879,  29.4991016388,  28.0278511047])\n",
            "btensor.grad: tensor([223.9440917969, 291.8363952637, 318.0216674805, 336.8034667969,\n",
            "         71.9049224854, 199.7370452881, 353.3603515625, 346.8635559082,\n",
            "         98.8442230225, 251.6153411865, 310.5891113281, 109.8646240234,\n",
            "        315.9297180176, 295.2827148438, 307.7035522461, 343.8408203125,\n",
            "        320.3811645508,  97.5505523682, 297.7998352051, 309.3744812012,\n",
            "         97.7952651978,  63.4935874939,  63.9476814270])\n",
            "ctensor.grad: tensor([ 391.6801147461,  391.6801147461,  391.6801147461,  -55.7668418884,\n",
            "          -0.7725191116, -107.7664337158,  -54.2544479370, -121.9131851196,\n",
            "         -55.7668418884,   -0.7725191116, -107.7664337158,  -54.2544479370,\n",
            "        -121.9131851196,  -55.7668418884,   -0.7725191116, -107.7664337158,\n",
            "         -54.2544479370, -121.9131851196])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2880.6142578125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.2267224789), tensor(1.4041569233), tensor(1.3825659752), tensor(1.8391461372), tensor(0.9426275492), tensor(2.3742313385), tensor(1.5086272955), tensor(1.5864769220), tensor(0.7456502914), tensor(1.9858896732), tensor(1.6987806559), tensor(2.8042759895), tensor(1.5345317125), tensor(1.8304295540), tensor(1.7523696423), tensor(1.4913152456), tensor(1.6393277645), tensor(0.7048004270), tensor(1.6207761765), tensor(1.7048208714), tensor(0.7979248762), tensor(0.7973669767), tensor(0.6893994212)]\n",
            "b:  [tensor(0.5783935785), tensor(1.1033569574), tensor(1.2017312050), tensor(1.2279480696), tensor(1.1988755465), tensor(0.6353786588), tensor(1.3366348743), tensor(1.2673898935), tensor(1.4322063923), tensor(0.8629961014), tensor(1.1752570868), tensor(0.5109541416), tensor(1.2274355888), tensor(1.0654442310), tensor(1.1176728010), tensor(1.3211036921), tensor(1.1042810678), tensor(1.3918352127), tensor(1.1263964176), tensor(1.1352764368), tensor(1.3908511400), tensor(1.1583632231), tensor(1.1356031895)]\n",
            "c:  [tensor(-0.0649826080), tensor(-0.0649826080), tensor(-0.0649826080), tensor(0.0060922415), tensor(0.0010774161), tensor(0.0111313984), tensor(0.0062165475), tensor(0.0124812759), tensor(0.0060922415), tensor(0.0010774161), tensor(0.0111313984), tensor(0.0062165475), tensor(0.0124812759), tensor(0.0060922415), tensor(0.0010774161), tensor(0.0111313984), tensor(0.0062165475), tensor(0.0124812759)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([43.9374198914, 61.8782119751, 65.8187866211, 70.5271377563,\n",
            "         7.3678584099, 44.7306098938, 72.5592880249, 70.5823745728,\n",
            "         9.2241668701, 54.8710594177, 67.0585174561, 36.6469764709,\n",
            "        67.9391098022, 63.3182487488, 65.1899185181, 71.6329956055,\n",
            "        64.4585037231,  8.0114870071, 64.4396209717, 65.6598968506,\n",
            "         7.8335614204,  3.8433384895,  1.8748484850])\n",
            "btensor.grad: tensor([ 57.6751403809, 165.1820983887, 188.5932006836, 197.6355285645,\n",
            "         35.3819465637,  60.1913528442, 217.2815551758, 211.2885589600,\n",
            "         60.4292755127, 118.7121353149, 176.0762939453, -28.7971343994,\n",
            "        183.1031494141, 160.9324798584, 173.4462585449, 208.6581115723,\n",
            "        188.3731842041,  59.7652435303, 166.2876739502, 175.5731964111,\n",
            "         59.7145996094,  29.3018302917,  30.4205131531])\n",
            "ctensor.grad: tensor([ 243.3813629150,  243.3813629150,  243.3813629150,  -49.0755386353,\n",
            "          -0.8142330050, -100.5150451660,  -52.8651046753, -114.1253738403,\n",
            "         -49.0755386353,   -0.8142330050, -100.5150451660,  -52.8651046753,\n",
            "        -114.1253738403,  -49.0755386353,   -0.8142330050, -100.5150451660,\n",
            "         -52.8651046753, -114.1253738403])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2433.9069824219, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.2035279274), tensor(1.3723571301), tensor(1.3484522104), tensor(1.7996919155), tensor(0.9477806687), tensor(2.3511402607), tensor(1.4692094326), tensor(1.5482164621), tensor(0.7508122921), tensor(1.9568198919), tensor(1.6619917154), tensor(2.7841422558), tensor(1.4977531433), tensor(1.7959610224), tensor(1.7169456482), tensor(1.4524413347), tensor(1.6053916216), tensor(0.7110559940), tensor(1.5860902071), tensor(1.6691589355), tensor(0.8039948940), tensor(0.8057483435), tensor(0.6999220848)]\n",
            "b:  [tensor(0.5907270908), tensor(1.0103638172), tensor(1.0882503986), tensor(1.1113777161), tensor(1.1818172932), tensor(0.6369206309), tensor(1.1994278431), tensor(1.1358535290), tensor(1.3924037218), tensor(0.8154874444), tensor(1.0760940313), tensor(0.5545518994), tensor(1.1207349300), tensor(0.9805378914), tensor(1.0211627483), tensor(1.1916098595), tensor(0.9929741025), tensor(1.3523443937), tensor(1.0348018408), tensor(1.0365166664), tensor(1.3515638113), tensor(1.1458268166), tensor(1.1217560768)]\n",
            "c:  [tensor(-0.0719564557), tensor(-0.0719564557), tensor(-0.0719564557), tensor(0.0074419249), tensor(0.0011109451), tensor(0.0143618574), tensor(0.0080807656), tensor(0.0162703972), tensor(0.0074419249), tensor(0.0011109451), tensor(0.0143618574), tensor(0.0080807656), tensor(0.0162703972), tensor(0.0074419249), tensor(0.0011109451), tensor(0.0143618574), tensor(0.0080807656), tensor(0.0162703972)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 23.1946411133,  31.7998199463,  34.1137466431,  39.4542579651,\n",
            "         -5.1531066895,  23.0909824371,  39.4178352356,  38.2604560852,\n",
            "         -5.1619911194,  29.0698299408,  36.7888870239,  20.1336727142,\n",
            "         36.7785682678,  34.4685478210,  35.4240341187,  38.8739166260,\n",
            "         33.9361038208,  -6.2555851936,  34.6860008240,  35.6619148254,\n",
            "         -6.0700469017,  -8.3813676834, -10.5226430893])\n",
            "btensor.grad: tensor([-12.3335027695,  92.9931488037, 113.4808044434, 116.5703430176,\n",
            "         17.0582256317,  -1.5419760942, 137.2070159912, 131.5363922119,\n",
            "         39.8026199341,  47.5086402893,  99.1630783081, -43.5977783203,\n",
            "        106.7006759644,  84.9063415527,  96.5101089478, 129.4938659668,\n",
            "        111.3069839478,  39.4908485413,  91.5945510864,  98.7597274780,\n",
            "         39.2873802185,  12.5364055634,  13.8471651077])\n",
            "ctensor.grad: tensor([139.4769897461, 139.4769897461, 139.4769897461, -26.9936733246,\n",
            "         -0.6705809236, -64.6091766357, -37.2843704224, -75.7824401855,\n",
            "        -26.9936733246,  -0.6705809236, -64.6091766357, -37.2843704224,\n",
            "        -75.7824401855, -26.9936733246,  -0.6705809236, -64.6091766357,\n",
            "        -37.2843704224, -75.7824401855])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2283.2170410156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1891691685), tensor(1.3560289145), tensor(1.3309286833), tensor(1.7763174772), tensor(0.9589764476), tensor(2.3377642632), tensor(1.4473333359), tensor(1.5269289017), tensor(0.7629610300), tensor(1.9402428865), tensor(1.6407558918), tensor(2.7714958191), tensor(1.4771267176), tensor(1.7760695219), tensor(1.6967433691), tensor(1.4308339357), tensor(1.5872423649), tensor(0.7241316438), tensor(1.5666307211), tensor(1.6488871574), tensor(0.8167846203), tensor(0.8197975159), tensor(0.7160212994)]\n",
            "b:  [tensor(0.6155858040), tensor(0.9595787525), tensor(1.0206747055), tensor(1.0440595150), tensor(1.1740223169), tensor(0.6552033424), tensor(1.1127671003), tensor(1.0543845892), tensor(1.3644148111), tensor(0.8023799658), tensor(1.0219941139), tensor(0.5884825587), tensor(1.0596822500), tensor(0.9385663271), tensor(0.9695951939), tensor(1.1114836931), tensor(0.9287047982), tensor(1.3244702816), tensor(0.9863255024), tensor(0.9829079509), tensor(1.3239538670), tensor(1.1414572001), tensor(1.1160845757)]\n",
            "c:  [tensor(-0.0753606632), tensor(-0.0753606632), tensor(-0.0753606632), tensor(0.0077386536), tensor(0.0011367794), tensor(0.0158268753), tensor(0.0091508776), tensor(0.0182294808), tensor(0.0077386536), tensor(0.0011367794), tensor(0.0158268753), tensor(0.0091508776), tensor(0.0182294808), tensor(0.0077386536), tensor(0.0011367794), tensor(0.0158268753), tensor(0.0091508776), tensor(0.0182294808)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 14.3587379456,  16.3282451630,  17.5235443115,  23.3744697571,\n",
            "        -11.1957778931,  13.3761062622,  21.8760967255,  21.2875576019,\n",
            "        -12.1487560272,  16.5769577026,  21.2358684540,  12.6463375092,\n",
            "         20.6264266968,  19.8915367126,  20.2023239136,  21.6073837280,\n",
            "         18.1493053436, -13.0756483078,  19.4595184326,  20.2718372345,\n",
            "        -12.7897071838, -14.0491437912, -16.0992412567])\n",
            "btensor.grad: tensor([-24.8587303162,  50.7850837708,  67.5757293701,  67.3181610107,\n",
            "          7.7949576378, -18.2827129364,  86.6607513428,  81.4689712524,\n",
            "         27.9889240265,  13.1074829102,  54.0998992920, -33.9306449890,\n",
            "         61.0526695251,  41.9715538025,  51.5675239563,  80.1261291504,\n",
            "         64.2692871094,  27.8741188049,  48.4763221741,  53.6087188721,\n",
            "         27.6099033356,   4.3695840836,   5.6714487076])\n",
            "ctensor.grad: tensor([ 68.0842132568,  68.0842132568,  68.0842132568,  -5.9345750809,\n",
            "         -0.5166857839, -29.3003559113, -21.4022388458, -39.1816558838,\n",
            "         -5.9345750809,  -0.5166857839, -29.3003559113, -21.4022388458,\n",
            "        -39.1816558838,  -5.9345750809,  -0.5166857839, -29.3003559113,\n",
            "        -21.4022388458, -39.1816558838])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2230.8110351562, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1788098812), tensor(1.3477603197), tensor(1.3222330809), tensor(1.7614974976), tensor(0.9728093147), tensor(2.3288600445), tensor(1.4350106716), tensor(1.5148144960), tensor(0.7780858874), tensor(1.9297567606), tensor(1.6276807785), tensor(2.7624726295), tensor(1.4650534391), tensor(1.7636638880), tensor(1.6844757795), tensor(1.4185774326), tensor(1.5774133205), tensor(0.7399986982), tensor(1.5551072359), tensor(1.6366699934), tensor(0.8324100971), tensor(0.8360443711), tensor(0.7340561748)]\n",
            "b:  [tensor(0.6365296245), tensor(0.9326040745), tensor(0.9809164405), tensor(1.0060687065), tensor(1.1706779003), tensor(0.6733364463), tensor(1.0584061146), tensor(1.0044970512), tensor(1.3434495926), tensor(0.8029732704), tensor(0.9933784008), tensor(0.6122795343), tensor(1.0253435373), tensor(0.9192776680), tensor(0.9431450963), tensor(1.0622820854), tensor(0.8925397992), tensor(1.3035168648), tensor(0.9616569281), tensor(0.9548103213), tensor(1.3032674789), tensor(1.1407459974), tensor(1.1141549349)]\n",
            "c:  [tensor(-0.0765668005), tensor(-0.0765668005), tensor(-0.0765668005), tensor(0.0073083164), tensor(0.0011572227), tensor(0.0160546694), tensor(0.0096556023), tensor(0.0189418681), tensor(0.0073083164), tensor(0.0011572227), tensor(0.0160546694), tensor(0.0096556023), tensor(0.0189418681), tensor(0.0073083164), tensor(0.0011572227), tensor(0.0160546694), tensor(0.0096556023), tensor(0.0189418681)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 10.3592233658,   8.2686166763,   8.6955490112,  14.8199939728,\n",
            "        -13.8328599930,   8.9042949677,  12.3226690292,  12.1143817902,\n",
            "        -15.1248531342,  10.4861383438,  13.0751552582,   9.0232210159,\n",
            "         12.0733108521,  12.4056539536,  12.2675609589,  12.2565450668,\n",
            "          9.8290710449, -15.8670282364,  11.5234384537,  12.2171287537,\n",
            "        -15.6254568100, -16.2468643188, -18.0348606110])\n",
            "btensor.grad: tensor([-20.9437999725,  26.9746608734,  39.7582740784,  37.9908218384,\n",
            "          3.3444418907, -18.1330890656,  54.3609695435,  49.8875427246,\n",
            "         20.9651794434,  -0.5933147073,  28.6157054901, -23.7969951630,\n",
            "         34.3386878967,  19.2886829376,  26.4501094818,  49.2015762329,\n",
            "         36.1650085449,  20.9533996582,  24.6685428619,  28.0976161957,\n",
            "         20.6863937378,   0.7112075686,   1.9295907021])\n",
            "ctensor.grad: tensor([ 24.1227817535,  24.1227817535,  24.1227817535,   8.6067399979,\n",
            "         -0.4088645577,  -4.5558671951, -10.0944967270, -14.2477302551,\n",
            "          8.6067399979,  -0.4088645577,  -4.5558671951, -10.0944967270,\n",
            "        -14.2477302551,   8.6067399979,  -0.4088645577,  -4.5558671951,\n",
            "        -10.0944967270, -14.2477302551])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2211.3510742188, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1703407764), tensor(1.3436512947), tensor(1.3181988001), tensor(1.7512809038), tensor(0.9874941707), tensor(2.3220474720), tensor(1.4279329777), tensor(1.5077024698), tensor(0.7940620184), tensor(1.9222335815), tensor(1.6189076900), tensor(2.7552089691), tensor(1.4575271606), tensor(1.7551088333), tensor(1.6763521433), tensor(1.4114204645), tensor(1.5719598532), tensor(0.7565529943), tensor(1.5477215052), tensor(1.6286777258), tensor(0.8488208055), tensor(0.8526954651), tensor(0.7521656752)]\n",
            "b:  [tensor(0.6513435245), tensor(0.9183722138), tensor(0.9575163722), tensor(0.9848373532), tensor(1.1692578793), tensor(0.6871722937), tensor(1.0244013071), tensor(0.9741066098), tensor(1.3268338442), tensor(0.8074980378), tensor(0.9784511328), tensor(0.6280363798), tensor(1.0060541630), tensor(0.9110065103), tensor(0.9298927784), tensor(1.0320857763), tensor(0.8724242449), tensor(1.2868652344), tensor(0.9493629336), tensor(0.9403249025), tensor(1.2868539095), tensor(1.1413764954), tensor(1.1136748791)]\n",
            "c:  [tensor(-0.0765859336), tensor(-0.0765859336), tensor(-0.0765859336), tensor(0.0064653005), tensor(0.0011745922), tensor(0.0155692464), tensor(0.0098297857), tensor(0.0189586673), tensor(0.0064653005), tensor(0.0011745922), tensor(0.0155692464), tensor(0.0098297857), tensor(0.0189586673), tensor(0.0064653005), tensor(0.0011745922), tensor(0.0155692464), tensor(0.0098297857), tensor(0.0189586673)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  8.4690074921,   4.1090440750,   4.0342912674,  10.2165870667,\n",
            "        -14.6848297119,   6.8126778603,   7.0776715279,   7.1119775772,\n",
            "        -15.9760999680,   7.5231637955,   8.7731008530,   7.2637243271,\n",
            "          7.5263152122,   8.5550947189,   8.1236591339,   7.1569981575,\n",
            "          5.4534387589, -16.5542716980,   7.3857679367,   7.9922289848,\n",
            "        -16.4107227325, -16.6510944366, -18.1094951630])\n",
            "btensor.grad: tensor([-14.8139181137,  14.2318687439,  23.4000568390,  21.2313346863,\n",
            "          1.4200255871, -13.8358211517,  34.0047492981,  30.3904380798,\n",
            "         16.6157703400,  -4.5247488022,  14.9272823334, -15.7568664551,\n",
            "         19.2893772125,   8.2711353302,  13.2523164749,  30.1963577271,\n",
            "         20.1155242920,  16.6516742706,  12.2939805984,  14.4854259491,\n",
            "         16.4135341644,  -0.6305195093,   0.4800080657])\n",
            "ctensor.grad: tensor([ 0.3825930059,  0.3825930059,  0.3825930059, 16.8603172302,\n",
            "        -0.3473911583,  9.7084655762, -3.4836645126, -0.3359973431,\n",
            "        16.8603172302, -0.3473911583,  9.7084655762, -3.4836645126,\n",
            "        -0.3359973431, 16.8603172302, -0.3473911583,  9.7084655762,\n",
            "        -3.4836645126, -0.3359973431])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2202.9472656250, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1628115177), tensor(1.3416774273), tensor(1.3166066408), tensor(1.7435951233), tensor(1.0021523237), tensor(2.3162448406), tensor(1.4237636328), tensor(1.5033506155), tensor(0.8098604083), tensor(1.9161796570), tensor(1.6124393940), tensor(2.7488257885), tensor(1.4524483681), tensor(1.7485678196), tensor(1.6704213619), tensor(1.4070729017), tensor(1.5688116550), tensor(0.7727984786), tensor(1.5425168276), tensor(1.6229304075), tensor(0.8650273085), tensor(0.8689097166), tensor(0.7695165277)]\n",
            "b:  [tensor(0.6610525846), tensor(0.9107031822), tensor(0.9435273409), tensor(0.9728934765), tensor(1.1685224771), tensor(0.6966685057), tensor(1.0030401945), tensor(0.9555126429), tensor(1.3130880594), tensor(0.8121588826), tensor(0.9706056714), tensor(0.6380289197), tensor(0.9950272441), tensor(0.9076991081), tensor(0.9232523441), tensor(1.0133860111), tensor(0.8611503243), tensor(1.2730662823), tensor(0.9432073832), tensor(0.9327963591), tensor(1.2732511759), tensor(1.1422548294), tensor(1.1135559082)]\n",
            "c:  [tensor(-0.0760382265), tensor(-0.0760382265), tensor(-0.0760382265), tensor(0.0054197963), tensor(0.0011904064), tensor(0.0147238625), tensor(0.0098334504), tensor(0.0186401866), tensor(0.0054197963), tensor(0.0011904064), tensor(0.0147238625), tensor(0.0098334504), tensor(0.0186401866), tensor(0.0054197963), tensor(0.0011904064), tensor(0.0147238625), tensor(0.0098334504), tensor(0.0186401866)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  7.5292868614,   1.9738527536,   1.5922071934,   7.6858124733,\n",
            "        -14.6581001282,   5.8025889397,   4.1693663597,   4.3518595695,\n",
            "        -15.7983961105,   6.0539250374,   6.4682469368,   6.3831791878,\n",
            "          5.0787367821,   6.5410022736,   5.9308199883,   4.3475971222,\n",
            "          3.1481609344, -16.2454738617,   5.2047333717,   5.7473111153,\n",
            "        -16.2065296173, -16.2142581940, -17.3508529663])\n",
            "btensor.grad: tensor([-9.7090654373,  7.6690015793, 13.9890232086, 11.9438581467,\n",
            "         0.7354071736, -9.4961986542, 21.3611545563, 18.5939712524,\n",
            "        13.7457418442, -4.6608691216,  7.8454599380, -9.9925355911,\n",
            "        11.0268983841,  3.3073968887,  6.6404557228, 18.6998157501,\n",
            "        11.2739219666, 13.7989416122,  6.1555304527,  7.5285434723,\n",
            "        13.6026782990, -0.8783245683,  0.1189369261])\n",
            "ctensor.grad: tensor([-10.9541559219, -10.9541559219, -10.9541559219,  20.9100780487,\n",
            "         -0.3162839115,  16.9076728821,  -0.0733008534,   6.3696041107,\n",
            "         20.9100780487,  -0.3162839115,  16.9076728821,  -0.0733008534,\n",
            "          6.3696041107,  20.9100780487,  -0.3162839115,  16.9076728821,\n",
            "         -0.0733008534,   6.3696041107])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2198.4375000000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1558003426), tensor(1.3408098221), tensor(1.3162943125), tensor(1.7373622656), tensor(1.0163880587), tensor(2.3109734058), tensor(1.4212392569), tensor(1.5005578995), tensor(0.8250390291), tensor(1.9109005928), tensor(1.6072577238), tensor(2.7429268360), tensor(1.4487307072), tensor(1.7431315184), tensor(1.6656966209), tensor(1.4043078423), tensor(1.5668987036), tensor(0.7883230448), tensor(1.5385035276), tensor(1.6184214354), tensor(0.8806068301), tensor(0.8843484521), tensor(0.7858107090)]\n",
            "b:  [tensor(0.6671404839), tensor(0.9063602686), tensor(0.9349247813), tensor(0.9660381079), tensor(1.1679146290), tensor(0.7028064728), tensor(0.9894808531), tensor(0.9439903498), tensor(1.3013882637), tensor(0.8158324361), tensor(0.9663646221), tensor(0.6441516280), tensor(0.9885106087), tensor(0.9064782858), tensor(0.9198372364), tensor(1.0016105175), tensor(0.8546687961), tensor(1.2613116503), tensor(0.9400165677), tensor(0.9287490845), tensor(1.2616487741), tensor(1.1429270506), tensor(1.1133413315)]\n",
            "c:  [tensor(-0.0752560720), tensor(-0.0752560720), tensor(-0.0752560720), tensor(0.0042894497), tensor(0.0012054798), tensor(0.0137170814), tensor(0.0097570550), tensor(0.0181826465), tensor(0.0042894497), tensor(0.0012054798), tensor(0.0137170814), tensor(0.0097570550), tensor(0.0181826465), tensor(0.0042894497), tensor(0.0012054798), tensor(0.0137170814), tensor(0.0097570550), tensor(0.0181826465)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  7.0112452507,   0.8676294684,   0.3122689724,   6.2328724861,\n",
            "        -14.2357807159,   5.2715353966,   2.5243918896,   2.7927391529,\n",
            "        -15.1785917282,   5.2790455818,   5.1817045212,   5.8988771439,\n",
            "          3.7176642418,   5.4363470078,   4.7246823311,   2.7650825977,\n",
            "          1.9129500389, -15.5245447159,   4.0132613182,   4.5089254379,\n",
            "        -15.5795059204, -15.4387121201, -16.2942104340])\n",
            "btensor.grad: tensor([-6.0878725052,  4.3428897858,  8.6025571823,  6.8553938866,\n",
            "         0.6079006195, -6.1379599571, 13.5593242645, 11.5222806931,\n",
            "        11.6997432709, -3.6735539436,  4.2410564423, -6.1227092743,\n",
            "         6.5166630745,  1.2208147049,  3.4150791168, 11.7755155563,\n",
            "         6.4815492630, 11.7546329498,  3.1908433437,  4.0472459793,\n",
            "        11.6024017334, -0.6721698642,  0.2145186365])\n",
            "ctensor.grad: tensor([-15.6431198120, -15.6431198120, -15.6431198120,  22.6069374084,\n",
            "         -0.3014681637,  20.1356315613,   1.5279157162,   9.1508131027,\n",
            "         22.6069374084,  -0.3014681637,  20.1356315613,   1.5279157162,\n",
            "          9.1508131027,  22.6069374084,  -0.3014681637,  20.1356315613,\n",
            "          1.5279157162,   9.1508131027])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2195.4665527344, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1491231918), tensor(1.3405292034), tensor(1.3166562319), tensor(1.7320196629), tensor(1.0300459862), tensor(2.3060262203), tensor(1.4196722507), tensor(1.4986772537), tensor(0.8394414783), tensor(1.9060785770), tensor(1.6028437614), tensor(2.7373392582), tensor(1.4458110332), tensor(1.7383517027), tensor(1.6616806984), tensor(1.4024645090), tensor(1.5656697750), tensor(0.8029935360), tensor(1.5351831913), tensor(1.6146404743), tensor(0.8954118490), tensor(0.8989139199), tensor(0.8009976745)]\n",
            "b:  [tensor(0.6708502769), tensor(0.9037081003), tensor(0.9294317961), tensor(0.9619829059), tensor(1.1672124863), tensor(0.7066074014), tensor(0.9807401299), tensor(0.9367103577), tensor(1.2912541628), tensor(0.8184098601), tensor(0.9639593959), tensor(0.6477957368), tensor(0.9844812155), tensor(0.9060626626), tensor(0.9179797769), tensor(0.9940269589), tensor(0.8507867455), tensor(1.2511284351), tensor(0.9382416010), tensor(0.9264386296), tensor(1.2515770197), tensor(1.1432460546), tensor(1.1128776073)]\n",
            "c:  [tensor(-0.0744034424), tensor(-0.0744034424), tensor(-0.0744034424), tensor(0.0031342283), tensor(0.0012202036), tensor(0.0126506276), tensor(0.0096471887), tensor(0.0176827107), tensor(0.0031342283), tensor(0.0012202036), tensor(0.0126506276), tensor(0.0096471887), tensor(0.0176827107), tensor(0.0031342283), tensor(0.0012202036), tensor(0.0126506276), tensor(0.0096471887), tensor(0.0176827107)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  6.6771826744,   0.2806694508,  -0.3619184494,   5.3426160812,\n",
            "        -13.6578826904,   4.9470734596,   1.5670462847,   1.8807027340,\n",
            "        -14.4024343491,   4.8220529556,   4.4139542580,   5.5876660347,\n",
            "          2.9196608067,   4.7798161507,   4.0158934593,   1.8433768749,\n",
            "          1.2289372683, -14.6704759598,   3.3202784061,   3.7809185982,\n",
            "        -14.8050165176, -14.5654411316, -15.1869506836])\n",
            "btensor.grad: tensor([-3.7097632885,  2.6521973610,  5.4929852486,  4.0552306175,\n",
            "         0.7021911144, -3.8009381294,  8.7407503128,  7.2799687386,\n",
            "        10.1340475082, -2.5774250031,  2.4052500725, -3.6440834999,\n",
            "         4.0294218063,  0.4156385660,  1.8574789762,  7.5835447311,\n",
            "         3.8820765018, 10.1832275391,  1.7749719620,  2.3104391098,\n",
            "        10.0717191696, -0.3189821839,  0.4637208283])\n",
            "ctensor.grad: tensor([-17.0525398254, -17.0525398254, -17.0525398254,  23.1044311523,\n",
            "         -0.2944741547,  21.3290786743,   2.1973249912,   9.9987001419,\n",
            "         23.1044311523,  -0.2944741547,  21.3290786743,   2.1973249912,\n",
            "          9.9987001419,  23.1044311523,  -0.2944741547,  21.3290786743,\n",
            "          2.1973249912,   9.9987001419])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2193.2138671875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1426990032), tensor(1.3405710459), tensor(1.3173737526), tensor(1.7272676229), tensor(1.0430830717), tensor(2.3013153076), tensor(1.4186815023), tensor(1.4973536730), tensor(0.8530405164), tensor(1.9015675783), tensor(1.5989286900), tensor(2.7319869995), tensor(1.4433922768), tensor(1.7340044975), tensor(1.6581192017), tensor(1.4011805058), tensor(1.5648376942), tensor(0.8167998195), tensor(1.5323011875), tensor(1.6113247871), tensor(0.9094190001), tensor(0.9126144052), tensor(0.8151283264)]\n",
            "b:  [tensor(0.6730700731), tensor(0.9019311666), tensor(0.9257650375), tensor(0.9594909549), tensor(1.1663483381), tensor(0.7088732123), tensor(0.9749923348), tensor(0.9319934249), tensor(1.2823832035), tensor(0.8200921416), tensor(0.9625012875), tensor(0.6499069333), tensor(0.9818519354), tensor(0.9059138298), tensor(0.9168758392), tensor(0.9890085459), tensor(0.8483321667), tensor(1.2422170639), tensor(0.9371430278), tensor(0.9250038862), tensor(1.2427419424), tensor(1.1431999207), tensor(1.1121443510)]\n",
            "c:  [tensor(-0.0735569075), tensor(-0.0735569075), tensor(-0.0735569075), tensor(0.0019830568), tensor(0.0012347520), tensor(0.0115740513), tensor(0.0095267314), tensor(0.0171850380), tensor(0.0019830568), tensor(0.0012347520), tensor(0.0115740513), tensor(0.0095267314), tensor(0.0171850380), tensor(0.0019830568), tensor(0.0012347520), tensor(0.0115740513), tensor(0.0095267314), tensor(0.0171850380)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  6.4242973328,  -0.0418953151,  -0.7175109386,   4.7520728111,\n",
            "        -13.0370445251,   4.7108597755,   0.9907273054,   1.3235635757,\n",
            "        -13.5990276337,   4.5109891891,   3.9150514603,   5.3522119522,\n",
            "          2.4187765121,   4.3472480774,   3.5615189075,   1.2840592861,\n",
            "          0.8321188092, -13.8062562943,   2.8819956779,   3.3157312870,\n",
            "        -14.0071411133, -13.7004814148, -14.1306810379])\n",
            "btensor.grad: tensor([-2.2198207378,  1.7769362926,  3.6667625904,  2.4919455051,\n",
            "         0.8642019629, -2.2657840252,  5.7477836609,  4.7169179916,\n",
            "         8.8709278107, -1.6822798252,  1.4581124783, -2.1112241745,\n",
            "         2.6292955875,  0.1488254070,  1.1039385796,  5.0184388161,\n",
            "         2.4545726776,  8.9113235474,  1.0985500813,  1.4347304106,\n",
            "         8.8351078033,  0.0461875722,  0.7333130836])\n",
            "ctensor.grad: tensor([-16.9307708740, -16.9307708740, -16.9307708740,  23.0234317780,\n",
            "         -0.2909693718,  21.5315189362,   2.4091489315,   9.9534673691,\n",
            "         23.0234317780,  -0.2909693718,  21.5315189362,   2.4091489315,\n",
            "          9.9534673691,  23.0234317780,  -0.2909693718,  21.5315189362,\n",
            "          2.4091489315,   9.9534673691])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2191.3676757812, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1364891529), tensor(1.3407979012), tensor(1.3182762861), tensor(1.7229408026), tensor(1.0555065870), tensor(2.2968015671), tensor(1.4180506468), tensor(1.4963873625), tensor(0.8658630848), tensor(1.8972986937), tensor(1.5953685045), tensor(2.7268350124), tensor(1.4413124323), tensor(1.7299740314), tensor(1.6548776627), tensor(1.4002516270), tensor(1.5642491579), tensor(0.8297813535), tensor(1.5297231674), tensor(1.6083344221), tensor(0.9226567149), tensor(0.9255000949), tensor(0.8282884359)]\n",
            "b:  [tensor(0.6743864417), tensor(0.9006226063), tensor(0.9231948256), tensor(0.9578896165), tensor(1.1653205156), tensor(0.7101676464), tensor(0.9711192846), tensor(0.9288417101), tensor(1.2745667696), tensor(0.8211253285), tensor(0.9615430832), tensor(0.6510961056), tensor(0.9800324440), tensor(0.9058224559), tensor(0.9161413908), tensor(0.9855810404), tensor(0.8466776013), tensor(1.2343698740), tensor(0.9363718033), tensor(0.9240210652), tensor(1.2349416018), tensor(1.1428295374), tensor(1.1111733913)]\n",
            "c:  [tensor(-0.0727502555), tensor(-0.0727502555), tensor(-0.0727502555), tensor(0.0008492060), tensor(0.0012491978), tensor(0.0105105974), tensor(0.0094065769), tensor(0.0167089663), tensor(0.0008492060), tensor(0.0012491978), tensor(0.0105105974), tensor(0.0094065769), tensor(0.0167089663), tensor(0.0008492060), tensor(0.0012491978), tensor(0.0105105974), tensor(0.0094065769), tensor(0.0167089663)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  6.2097311020,  -0.2268566191,  -0.9025242329,   4.3268046379,\n",
            "        -12.4234819412,   4.5136308670,   0.6308996081,   0.9663032293,\n",
            "        -12.8225688934,   4.2689380646,   3.5602426529,   5.1520452499,\n",
            "          2.0798251629,   4.0304884911,   3.2414915562,   0.9288996458,\n",
            "          0.5885288715, -12.9815483093,   2.5780138969,   2.9903087616,\n",
            "        -13.2377052307, -12.8857030869, -13.1601085663])\n",
            "btensor.grad: tensor([-1.3163473606,  1.3085354567,  2.5701889992,  1.6013230085,\n",
            "         1.0277800560, -1.2944437265,  3.8730721474,  3.1517221928,\n",
            "         7.8163762093, -1.0331943035,  0.9582152367, -1.1891454458,\n",
            "         1.8194854259,  0.0913879871,  0.7344512939,  3.4274988174,\n",
            "         1.6545853615,  7.8471307755,  0.7712488174,  0.9827988148,\n",
            "         7.8003640175,  0.3704196811,  0.9709764123])\n",
            "ctensor.grad: tensor([-16.1329936981, -16.1329936981, -16.1329936981,  22.6770153046,\n",
            "         -0.2889141738,  21.2690753937,   2.4030961990,   9.5214433670,\n",
            "         22.6770153046,  -0.2889141738,  21.2690753937,   2.4030961990,\n",
            "          9.5214433670,  22.6770153046,  -0.2889141738,  21.2690753937,\n",
            "          2.4030961990,   9.5214433670])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2189.7897949219, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1304731369), tensor(1.3411357403), tensor(1.3192703724), tensor(1.7189437151), tensor(1.0673445463), tensor(2.2924659252), tensor(1.4176529646), tensor(1.4956619740), tensor(0.8779562712), tensor(1.8932367563), tensor(1.5920819044), tensor(2.7218639851), tensor(1.4394793510), tensor(1.7261970043), tensor(1.6518816948), tensor(1.3995591402), tensor(1.5638197660), tensor(0.8419945836), tensor(1.5273748636), tensor(1.6055915356), tensor(0.9351723790), tensor(0.9376342297), tensor(0.8405697942)]\n",
            "b:  [tensor(0.6751685143), tensor(0.8995775580), tensor(0.9213001728), tensor(0.9568080902), tensor(1.1641529799), tensor(0.7108647227), tensor(0.9684323668), tensor(0.9266582131), tensor(1.2676490545), tensor(0.8217171431), tensor(0.9608576298), tensor(0.6517435312), tensor(0.9786964655), tensor(0.9057182074), tensor(0.9155936241), tensor(0.9831552505), tensor(0.8454834819), tensor(1.2274308205), tensor(0.9357641935), tensor(0.9232805967), tensor(1.2280253172), tensor(1.1421917677), tensor(1.1100126505)]\n",
            "c:  [tensor(-0.0719969943), tensor(-0.0719969943), tensor(-0.0719969943), tensor(-0.0002617256), tensor(0.0012635677), tensor(0.0094706006), tensor(0.0092917262), tensor(0.0162619576), tensor(-0.0002617256), tensor(0.0012635677), tensor(0.0094706006), tensor(0.0092917262), tensor(0.0162619576), tensor(-0.0002617256), tensor(0.0012635677), tensor(0.0094706006), tensor(0.0092917262), tensor(0.0162619576)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  6.0160946846,  -0.3378131092,  -0.9941425323,   3.9970812798,\n",
            "        -11.8379583359,   4.3357510567,   0.3976943493,   0.7253317833,\n",
            "        -12.0931682587,   4.0619692802,   3.2865443230,   4.9709939957,\n",
            "          1.8331007957,   3.7770609856,   2.9959304333,   0.6925421953,\n",
            "          0.4294490814, -12.2132358551,   2.3483285904,   2.7428617477,\n",
            "        -12.5156612396, -12.1341247559, -12.2813425064])\n",
            "btensor.grad: tensor([-0.7820911407,  1.0450712442,  1.8946571350,  1.0815274715,\n",
            "         1.1675478220, -0.6970529556,  2.6868946552,  2.1835198402,\n",
            "         6.9177017212, -0.5917890668,  0.6854779720, -0.6474099159,\n",
            "         1.3359839916,  0.1042653322,  0.5477635860,  2.4257631302,\n",
            "         1.1941052675,  6.9390573502,  0.6076200008,  0.7404625416,\n",
            "         6.9162464142,  0.6378177404,  1.1607279778])\n",
            "ctensor.grad: tensor([-15.0652856827, -15.0652856827, -15.0652856827,  22.2186336517,\n",
            "         -0.2874003053,  20.7999458313,   2.2970061302,   8.9401826859,\n",
            "         22.2186336517,  -0.2874003053,  20.7999458313,   2.2970061302,\n",
            "          8.9401826859,  22.2186336517,  -0.2874003053,  20.7999458313,\n",
            "          2.2970061302,   8.9401826859])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2188.4147949219, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1246366501), tensor(1.3415430784), tensor(1.3203037977), tensor(1.7152178288), tensor(1.0786322355), tensor(2.2882962227), tensor(1.4174121618), tensor(1.4951074123), tensor(0.8893722296), tensor(1.8893615007), tensor(1.5890204906), tensor(2.7170612812), tensor(1.4378376007), tensor(1.7226359844), tensor(1.6490873098), tensor(1.3990312815), tensor(1.5635007620), tensor(0.8534988165), tensor(1.5252124071), tensor(1.6030498743), tensor(0.9470179677), tensor(0.9490806460), tensor(0.8520590663)]\n",
            "b:  [tensor(0.6756414771), tensor(0.8986911178), tensor(0.9198337197), tensor(0.9560384154), tensor(1.1628766060), tensor(0.7112035155), tensor(0.9665046930), tensor(0.9250825047), tensor(1.2615063190), tensor(0.8220213652), tensor(0.9603279233), tensor(0.6520798802), tensor(0.9776597619), tensor(0.9055859447), tensor(0.9151456952), tensor(0.9813707471), tensor(0.8445635438), tensor(1.2212753296), tensor(0.9352440238), tensor(0.9226778150), tensor(1.2218735218), tensor(1.1413434744), tensor(1.1087106466)]\n",
            "c:  [tensor(-0.0713011622), tensor(-0.0713011622), tensor(-0.0713011622), tensor(-0.0013478426), tensor(0.0012778700), tensor(0.0084581375), tensor(0.0091843167), tensor(0.0158460401), tensor(-0.0013478426), tensor(0.0012778700), tensor(0.0084581375), tensor(0.0091843167), tensor(0.0158460401), tensor(-0.0013478426), tensor(0.0012778700), tensor(0.0084581375), tensor(0.0091843167), tensor(0.0158460401)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  5.8363685608,  -0.4072926044,  -1.0334553719,   3.7258610725,\n",
            "        -11.2876653671,   4.1697673798,   0.2408366799,   0.5545076132,\n",
            "        -11.4159517288,   3.8752291203,   3.0614671707,   4.8026914597,\n",
            "          1.6417803764,   3.5610637665,   2.7943611145,   0.5278108120,\n",
            "          0.3189464808, -11.5042114258,   2.1624436378,   2.5417175293,\n",
            "        -11.8456134796, -11.4463930130, -11.4893007278])\n",
            "btensor.grad: tensor([-0.4729912281,  0.8864691257,  1.4664490223,  0.7697043419,\n",
            "         1.2763509750, -0.3387897015,  1.9276604652,  1.5756980181,\n",
            "         6.1427421570, -0.3042155504,  0.5297305584, -0.3363730609,\n",
            "         1.0367321968,  0.1322814226,  0.4479094744,  1.7845255136,\n",
            "         0.9199371338,  6.1555094719,  0.5201575756,  0.6027821302,\n",
            "         6.1518115997,  0.8482857943,  1.3019952774])\n",
            "ctensor.grad: tensor([-13.9166822433, -13.9166822433, -13.9166822433,  21.7223396301,\n",
            "         -0.2860455811,  20.2492580414,   2.1481974125,   8.3183317184,\n",
            "         21.7223396301,  -0.2860455811,  20.2492580414,   2.1481974125,\n",
            "          8.3183317184,  21.7223396301,  -0.2860455811,  20.2492580414,\n",
            "          2.1481974125,   8.3183317184])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2187.2001953125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1189692020), tensor(1.3419954777), tensor(1.3213466406), tensor(1.7117249966), tensor(1.0894060135), tensor(2.2842833996), tensor(1.4172806740), tensor(1.4946796894), tensor(0.9001620412), tensor(1.8856593370), tensor(1.5861527920), tensor(2.7124166489), tensor(1.4363517761), tensor(1.7192666531), tensor(1.6464664936), tensor(1.3986233473), tensor(1.5632630587), tensor(0.8643505573), tensor(1.5232080221), tensor(1.6006797552), tensor(0.9582441449), tensor(0.9598989487), tensor(0.8628342748)]\n",
            "b:  [tensor(0.6759395599), tensor(0.8979084492), tensor(0.9186476469), tensor(0.9554616809), tensor(1.1615219116), tensor(0.7113332152), tensor(0.9650695324), tensor(0.9238950014), tensor(1.2560364008), tensor(0.8221448064), tensor(0.9598926306), tensor(0.6522423625), tensor(0.9768158793), tensor(0.9054298997), tensor(0.9147563577), tensor(0.9800043106), tensor(0.8438137174), tensor(1.2158002853), tensor(0.9347761273), tensor(0.9221596718), tensor(1.2163870335), tensor(1.1403357983), tensor(1.1073105335)]\n",
            "c:  [tensor(-0.0706625059), tensor(-0.0706625059), tensor(-0.0706625059), tensor(-0.0024090230), tensor(0.0012921053), tensor(0.0074742800), tensor(0.0090850992), tensor(0.0154608460), tensor(-0.0024090230), tensor(0.0012921053), tensor(0.0074742800), tensor(0.0090850992), tensor(0.0154608460), tensor(-0.0024090230), tensor(0.0012921053), tensor(0.0074742800), tensor(0.0090850992), tensor(0.0154608460)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  5.6675558090,  -0.4523673654,  -1.0428338051,   3.4928565025,\n",
            "        -10.7737207413,   4.0129375458,   0.1314896345,   0.4276870489,\n",
            "        -10.7898015976,   3.7021679878,   2.8677353859,   4.6445398331,\n",
            "          1.4857906103,   3.3693442345,   2.6208379269,   0.4078857303,\n",
            "          0.2377251387, -10.8517236710,   2.0043971539,   2.3701567650,\n",
            "        -11.2261486053, -10.8183240891, -10.7752170563])\n",
            "btensor.grad: tensor([-0.2980574071,  0.7826634645,  1.1860853434,  0.5767216086,\n",
            "         1.3546997309, -0.1297016144,  1.4351327419,  1.1875318289,\n",
            "         5.4699378014, -0.1234300137,  0.4352691174, -0.1624719501,\n",
            "         0.8438857198,  0.1560387611,  0.3893231153,  1.3664492369,\n",
            "         0.7498098016,  5.4751033783,  0.4678746462,  0.5181171894,\n",
            "         5.4864373207,  1.0077054501,  1.4000738859])\n",
            "ctensor.grad: tensor([-12.7730808258, -12.7730808258, -12.7730808258,  21.2236080170,\n",
            "         -0.2847045958,  19.6771545410,   1.9843585491,   7.7038865089,\n",
            "         21.2236080170,  -0.2847045958,  19.6771545410,   1.9843585491,\n",
            "          7.7038865089,  21.2236080170,  -0.2847045958,  19.6771545410,\n",
            "          1.9843585491,   7.7038865089])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2186.1254882812, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1134610176), tensor(1.3424777985), tensor(1.3223813772), tensor(1.7084383965), tensor(1.0997006893), tensor(2.2804191113), tensor(1.4172279835), tensor(1.4943500757), tensor(0.9103732705), tensor(1.8821195364), tensor(1.5834569931), tensor(2.7079212666), tensor(1.4349980354), tensor(1.7160717249), tensor(1.6439998150), tensor(1.3983062506), tensor(1.5630879402), tensor(0.8746016622), tensor(1.5213425159), tensor(1.5984607935), tensor(0.9688977599), tensor(0.9701431394), tensor(0.8729640245)]\n",
            "b:  [tensor(0.6761411428), tensor(0.8972000480), tensor(0.9176517725), tensor(0.9550087452), tensor(1.1601158381), tensor(0.7113450766), tensor(0.9639590383), tensor(0.9229603410), tensor(1.2511528730), tensor(0.8221587539), tensor(0.9595189691), tensor(0.6523110271), tensor(0.9761018753), tensor(0.9052592516), tensor(0.9144059420), tensor(0.9789162278), tensor(0.8431748152), tensor(1.2109181881), tensor(0.9343443513), tensor(0.9216987491), tensor(1.2114819288), tensor(1.1392120123), tensor(1.1058486700)]\n",
            "c:  [tensor(-0.0700789317), tensor(-0.0700789317), tensor(-0.0700789317), tensor(-0.0034459643), tensor(0.0013062720), tensor(0.0065186732), tensor(0.0089941658), tensor(0.0151050137), tensor(-0.0034459643), tensor(0.0013062720), tensor(0.0065186732), tensor(0.0089941658), tensor(0.0151050137), tensor(-0.0034459643), tensor(0.0013062720), tensor(0.0065186732), tensor(0.0089941658), tensor(0.0151050137)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([  5.5081853867,  -0.4822860658,  -1.0347919464,   3.2865500450,\n",
            "        -10.2946271896,   3.8642311096,   0.0526595116,   0.3296396136,\n",
            "        -10.2112092972,   3.5398159027,   2.6958134174,   4.4952998161,\n",
            "          1.3537880182,   3.1949739456,   2.4667022228,   0.3171076179,\n",
            "          0.1751306653, -10.2511224747,   1.8655353785,   2.2190098763,\n",
            "        -10.6536083221, -10.2441740036, -10.1297693253])\n",
            "btensor.grad: tensor([-0.2016032338,  0.7084218264,  0.9958490729,  0.4529119730,\n",
            "         1.4060795307, -0.0118652582,  1.1105049849,  0.9346376657,\n",
            "         4.8834686279, -0.0139252543,  0.3736720085, -0.0686401725,\n",
            "         0.7139855027,  0.1706444025,  0.3504418135,  1.0880980492,\n",
            "         0.6388866305,  4.8820819855,  0.4317870736,  0.4609130621,\n",
            "         4.9050974846,  1.1237480640,  1.4619128704])\n",
            "ctensor.grad: tensor([-11.6715478897, -11.6715478897, -11.6715478897,  20.7388248444,\n",
            "         -0.2833347321,  19.1121349335,   1.8186677694,   7.1166543961,\n",
            "         20.7388248444,  -0.2833347321,  19.1121349335,   1.8186677694,\n",
            "          7.1166543961,  20.7388248444,  -0.2833347321,  19.1121349335,\n",
            "          1.8186677694,   7.1166543961])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2185.1589355469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1081037521), tensor(1.3429800272), tensor(1.3233978748), tensor(1.7053382397), tensor(1.1095485687), tensor(2.2766959667), tensor(1.4172339439), tensor(1.4940989017), tensor(0.9200492501), tensor(1.8787328005), tensor(1.5809167624), tensor(2.7035670280), tensor(1.4337589741), tensor(1.7130376101), tensor(1.6416728497), tensor(1.3980602026), tensor(1.5629628897), tensor(0.8842990398), tensor(1.5196015835), tensor(1.5963777304), tensor(0.9790214896), tensor(0.9798611999), tensor(0.8825083375)]\n",
            "b:  [tensor(0.6762916446), tensor(0.8965492249), tensor(0.9167902470), tensor(0.9546384811), tensor(1.1586810350), tensor(0.7112940550), tensor(0.9630666375), tensor(0.9221945405), tensor(1.2467819452), tensor(0.8221094012), tensor(0.9591887593), tensor(0.6523319483), tensor(0.9754796028), tensor(0.9050827026), tensor(0.9140847921), tensor(0.9780180454), tensor(0.8426124454), tensor(1.2065541744), tensor(0.9339411259), tensor(0.9212803245), tensor(1.2070859671), tensor(1.1380079985), tensor(1.1043542624)]\n",
            "c:  [tensor(-0.0695476234), tensor(-0.0695476234), tensor(-0.0695476234), tensor(-0.0044597220), tensor(0.0013203688), tensor(0.0055903257), tensor(0.0089113070), tensor(0.0147768417), tensor(-0.0044597220), tensor(0.0013203688), tensor(0.0055903257), tensor(0.0089113070), tensor(0.0147768417), tensor(-0.0044597220), tensor(0.0013203688), tensor(0.0055903257), tensor(0.0089113070), tensor(0.0147768417)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 5.3573074341e+00, -5.0225389004e-01, -1.0165137053e+00,\n",
            "         3.1001126766e+00, -9.8479070663e+00,  3.7231311798e+00,\n",
            "        -5.9129595757e-03,  2.5122064352e-01, -9.6759834290e+00,\n",
            "         3.3867070675e+00,  2.5401849747e+00,  4.3541836739e+00,\n",
            "         1.2390918732e+00,  3.0340757370e+00,  2.3270184994e+00,\n",
            "         2.4603170156e-01,  1.2505865097e-01, -9.6973609924e+00,\n",
            "         1.7409228086e+00,  2.0830125809e+00, -1.0123739243e+01,\n",
            "        -9.7180585861e+00, -9.5442934036e+00])\n",
            "btensor.grad: tensor([-0.1504961997,  0.6508017778,  0.8615223169,  0.3702457547,\n",
            "         1.4347672462,  0.0510243177,  0.8924186230,  0.7658269405,\n",
            "         4.3709311485,  0.0493378639,  0.3301925659, -0.0209359527,\n",
            "         0.6222487092,  0.1765509844,  0.3211582899,  0.8981876373,\n",
            "         0.5623977780,  4.3640289307,  0.4032303095,  0.4184182882,\n",
            "         4.3959946632,  1.2039769888,  1.4944021702])\n",
            "ctensor.grad: tensor([-10.6261911392, -10.6261911392, -10.6261911392,  20.2751483917,\n",
            "         -0.2819358110,  18.5669517517,   1.6571800709,   6.5634412766,\n",
            "         20.2751483917,  -0.2819358110,  18.5669517517,   1.6571800709,\n",
            "          6.5634412766,  20.2751483917,  -0.2819358110,  18.5669517517,\n",
            "          1.6571800709,   6.5634412766])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2184.2888183594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.1028895378), tensor(1.3434953690), tensor(1.3243900537), tensor(1.7024089098), tensor(1.1189793348), tensor(2.2731068134), tensor(1.4172844887), tensor(1.4939121008), tensor(0.9292291999), tensor(1.8754907846), tensor(1.5785192251), tensor(2.6993465424), tensor(1.4326213598), tensor(1.7101532221), tensor(1.6394740343), tensor(1.3978713751), tensor(1.5628789663), tensor(0.8934846520), tensor(1.5179740191), tensor(1.5944187641), tensor(0.9886538386), tensor(0.9890956879), tensor(0.8915194869)]\n",
            "b:  [tensor(0.6764168143), tensor(0.8959461451), tensor(0.9160275459), tensor(0.9543259144), tensor(1.1572360992), tensor(0.7112126946), tensor(0.9623241425), tensor(0.9215446711), tensor(1.2428597212), tensor(0.8220262527), tensor(0.9588916302), tensor(0.6523313522), tensor(0.9749252796), tensor(0.9049069285), tensor(0.9137880206), tensor(0.9772531390), tensor(0.8421059847), tensor(1.2026433945), tensor(0.9335627556), tensor(0.9208960533), tensor(1.2031364441), tensor(1.1367528439), tensor(1.1028505564)]\n",
            "c:  [tensor(-0.0690656081), tensor(-0.0690656081), tensor(-0.0690656081), tensor(-0.0054514790), tensor(0.0013343950), tensor(0.0046879817), tensor(0.0088361846), tensor(0.0144745838), tensor(-0.0054514790), tensor(0.0013343950), tensor(0.0046879817), tensor(0.0088361846), tensor(0.0144745838), tensor(-0.0054514790), tensor(0.0013343950), tensor(0.0046879817), tensor(0.0088361846), tensor(0.0144745838)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 5.2141842842, -0.5153348446, -0.9922075272,  2.9293360710,\n",
            "        -9.4308195114,  3.5892457962, -0.0505854487,  0.1867852211,\n",
            "        -9.1799640656,  3.2419757843,  2.3974905014,  4.2205495834,\n",
            "         1.1375865936,  2.8843293190,  2.1988146305,  0.1888100207,\n",
            "         0.0838782191, -9.1856269836,  1.6275837421,  1.9589812756,\n",
            "        -9.6323709488, -9.2345075607, -9.0111379623])\n",
            "btensor.grad: tensor([-1.2517195940e-01,  6.0310888290e-01,  7.6270091534e-01,\n",
            "         3.1259161234e-01,  1.4449552298e+00,  8.1347584724e-02,\n",
            "         7.4247634411e-01,  6.4988601208e-01,  3.9222288132e+00,\n",
            "         8.3178162575e-02,  2.9711389542e-01,  5.8311223984e-04,\n",
            "         5.5431705713e-01,  1.7576479912e-01,  2.9676103592e-01,\n",
            "         7.6489102840e-01,  5.0645798445e-01,  3.9107267857e+00,\n",
            "         3.7835907936e-01,  3.8427460194e-01,  3.9494864941e+00,\n",
            "         1.2551984787e+00,  1.5036553144e+00])\n",
            "ctensor.grad: tensor([-9.6403541565, -9.6403541565, -9.6403541565, 19.8351421356,\n",
            "        -0.2805226743, 18.0468769073,  1.5024535656,  6.0451645851,\n",
            "        19.8351421356, -0.2805226743, 18.0468769073,  1.5024535656,\n",
            "         6.0451645851, 19.8351421356, -0.2805226743, 18.0468769073,\n",
            "         1.5024535656,  6.0451645851])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2183.5026855469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0978114605), tensor(1.3440188169), tensor(1.3253544569), tensor(1.6996374130), tensor(1.1280200481), tensor(2.2696447372), tensor(1.4173698425), tensor(1.4937794209), tensor(0.9379485250), tensor(1.8723857403), tensor(1.5762536526), tensor(2.6952526569), tensor(1.4315747023), tensor(1.7074090242), tensor(1.6373938322), tensor(1.3977296352), tensor(1.5628296137), tensor(0.9021962285), tensor(1.5164504051), tensor(1.5925738811), tensor(0.9978294969), tensor(0.9978843331), tensor(0.9000431895)]\n",
            "b:  [tensor(0.6765310764), tensor(0.8953843117), tensor(0.9153407216), tensor(0.9540554285), tensor(1.1557956934), tensor(0.7111199498), tensor(0.9616876245), tensor(0.9209771156), tensor(1.2393308878), tensor(0.8219275475), tensor(0.9586212635), tensor(0.6523237824), tensor(0.9744235873), tensor(0.9047365785), tensor(0.9135128856), tensor(0.9765847921), tensor(0.8416428566), tensor(1.1991298199), tensor(0.9332072139), tensor(0.9205408096), tensor(1.1995791197), tensor(1.1354695559), tensor(1.1013556719)]\n",
            "c:  [tensor(-0.0686299801), tensor(-0.0686299801), tensor(-0.0686299801), tensor(-0.0064224396), tensor(0.0013483508), tensor(0.0038103121), tensor(0.0087684188), tensor(0.0141965710), tensor(-0.0064224396), tensor(0.0013483508), tensor(0.0038103121), tensor(0.0087684188), tensor(0.0141965710), tensor(-0.0064224396), tensor(0.0013483508), tensor(0.0038103121), tensor(0.0087684188), tensor(0.0141965710)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 5.0781722069, -0.5234051347, -0.9644205570,  2.7714605331,\n",
            "        -9.0406999588,  3.4621858597, -0.0853926539,  0.1327092052,\n",
            "        -8.7193307877,  3.1049926281,  2.2655575275,  4.0938153267,\n",
            "         1.0465981960,  2.7442183495,  2.0801768303,  0.1417084038,\n",
            "         0.0493419766, -8.7115831375,  1.5236012936,  1.8448594809,\n",
            "        -9.1756439209, -8.7886629105, -8.5237312317])\n",
            "btensor.grad: tensor([-0.1142890304,  0.5618382692,  0.6868255138,  0.2705108523,\n",
            "         1.4404102564,  0.0927414894,  0.6364936829,  0.5675344467,\n",
            "         3.5287795067,  0.0987070203,  0.2703373432,  0.0075580180,\n",
            "         0.5016744733,  0.1703393459,  0.2751230001,  0.6683188677,\n",
            "         0.4631522298,  3.5135338306,  0.3555305004,  0.3552187681,\n",
            "         3.5573546886,  1.2832796574,  1.4948649406])\n",
            "ctensor.grad: tensor([-8.7126235962, -8.7126235962, -8.7126235962, 19.4192161560,\n",
            "        -0.2791144848, 17.5533885956,  1.3553167582,  5.5602502823,\n",
            "        19.4192161560, -0.2791144848, 17.5533885956,  1.3553167582,\n",
            "         5.5602502823, 19.4192161560, -0.2791144848, 17.5533885956,\n",
            "         1.3553167582,  5.5602502823])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2182.7890625000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0928628445), tensor(1.3445465565), tensor(1.3262891769), tensor(1.6970127821), tensor(1.1366951466), tensor(2.2663033009), tensor(1.4174828529), tensor(1.4936927557), tensor(0.9462391734), tensor(1.8694105148), tensor(1.5741107464), tensor(2.6912791729), tensor(1.4306104183), tensor(1.7047963142), tensor(1.6354240179), tensor(1.3976273537), tensor(1.5628095865), tensor(0.9104676247), tensor(1.5150227547), tensor(1.5908346176), tensor(1.0065796375), tensor(1.0062606335), tensor(0.9081196189)]\n",
            "b:  [tensor(0.6766424179), tensor(0.8948593140), tensor(0.9147144556), tensor(0.9538170695), tensor(1.1543713808), tensor(0.7110265493), tensor(0.9611284137), tensor(0.9204702973), tensor(1.2361476421), tensor(0.8218244314), tensor(0.9583736658), tensor(0.6523171663), tensor(0.9739644527), tensor(0.9045745730), tensor(0.9132575989), tensor(0.9759889245), tensor(0.8412149549), tensor(1.1959648132), tensor(0.9328731298), tensor(0.9202112556), tensor(1.1963665485), tensor(1.1341764927), tensor(1.0998833179)]\n",
            "c:  [tensor(-0.0682380050), tensor(-0.0682380050), tensor(-0.0682380050), tensor(-0.0073737730), tensor(0.0013622373), tensor(0.0029560053), tensor(0.0087076295), tensor(0.0139412647), tensor(-0.0073737730), tensor(0.0013622373), tensor(0.0029560053), tensor(0.0087076295), tensor(0.0139412647), tensor(-0.0073737730), tensor(0.0013622373), tensor(0.0029560053), tensor(0.0087076295), tensor(0.0139412647)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.9486794472, -0.5277067423, -0.9347062111,  2.6245901585,\n",
            "        -8.6750650406,  3.3415398598, -0.1129765511,  0.0866184235,\n",
            "        -8.2906312943,  2.9752476215,  2.1428918839,  3.9734327793,\n",
            "         0.9643059373,  2.6126780510,  1.9697954655,  0.1022581756,\n",
            "         0.0199810863, -8.2714023590,  1.4276512861,  1.7392356396,\n",
            "        -8.7501354218, -8.3762760162, -8.0764532089])\n",
            "btensor.grad: tensor([-0.1113616675,  0.5250163078,  0.6262765527,  0.2383772731,\n",
            "         1.4243543148,  0.0933721066,  0.5592103004,  0.5068135262,\n",
            "         3.1833050251,  0.1030863523,  0.2476212978,  0.0066329241,\n",
            "         0.4591639638,  0.1620062590,  0.2553163767,  0.5958901644,\n",
            "         0.4279063344,  3.1650583744,  0.3341031671,  0.3295556307,\n",
            "         3.2126019001,  1.2930839062,  1.4723763466])\n",
            "ctensor.grad: tensor([-7.8395609856, -7.8395609856, -7.8395609856, 19.0266723633,\n",
            "        -0.2777293921, 17.0861396790,  1.2157796621,  5.1061272621,\n",
            "        19.0266723633, -0.2777293921, 17.0861396790,  1.2157796621,\n",
            "         5.1061272621, 19.0266723633, -0.2777293921, 17.0861396790,\n",
            "         1.2157796621,  5.1061272621])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2182.1408691406, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0880377293), tensor(1.3450756073), tensor(1.3271932602), tensor(1.6945254803), tensor(1.1450268030), tensor(2.2630763054), tensor(1.4176179171), tensor(1.4936459064), tensor(0.9541299939), tensor(1.8665581942), tensor(1.5720824003), tensor(2.6874203682), tensor(1.4297209978), tensor(1.7023074627), tensor(1.6335573196), tensor(1.3975585699), tensor(1.5628148317), tensor(0.9183293581), tensor(1.5136840343), tensor(1.5891935825), tensor(1.0149325132), tensor(1.0142543316), tensor(0.9157841802)]\n",
            "b:  [tensor(0.6767550707), tensor(0.8943677545), tensor(0.9141382575), tensor(0.9536042213), tensor(1.1529718637), tensor(0.7109384537), tensor(0.9606275558), tensor(0.9200100303), tensor(1.2332681417), tensor(0.8217235804), tensor(0.9581459165), tensor(0.6523155570), tensor(0.9735407829), tensor(0.9044225216), tensor(0.9130206704), tensor(0.9754493237), tensor(0.8408169150), tensor(1.1931059361), tensor(0.9325592518), tensor(0.9199048281), tensor(1.1934574842), tensor(1.1328878403), tensor(1.0984436274)]\n",
            "c:  [tensor(-0.0678871572), tensor(-0.0678871572), tensor(-0.0678871572), tensor(-0.0083065890), tensor(0.0013760563), tensor(0.0021238066), tensor(0.0086534563), tensor(0.0137072634), tensor(-0.0083065890), tensor(0.0013760563), tensor(0.0021238066), tensor(0.0086534563), tensor(0.0137072634), tensor(-0.0083065890), tensor(0.0013760563), tensor(0.0021238066), tensor(0.0086534563), tensor(0.0137072634)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.8251767159e+00, -5.2907061577e-01, -9.0404224396e-01,\n",
            "         2.4873559475e+00, -8.3316831589e+00,  3.2268874645e+00,\n",
            "        -1.3508063555e-01,  4.6883285046e-02, -7.8908329010e+00,\n",
            "         2.8522717953e+00,  2.0283911228e+00,  3.8588984013e+00,\n",
            "         8.8942861557e-01,  2.4888839722e+00,  1.8667192459e+00,\n",
            "         6.8799972534e-02, -5.1895380020e-03, -7.8617224693e+00,\n",
            "         1.3387699127e+00,  1.6410697699e+00, -8.3528232574e+00,\n",
            "        -7.9936599731e+00, -7.6645379066e+00])\n",
            "btensor.grad: tensor([-1.1267182231e-01,  4.9157261848e-01,  5.7617902756e-01,\n",
            "         2.1283322573e-01,  1.3994600773e+00,  8.8085532188e-02,\n",
            "         5.0088399649e-01,  4.6027290821e-01,  2.8795318604e+00,\n",
            "         1.0085225105e-01,  2.2774243355e-01,  1.6070604324e-03,\n",
            "         4.2364305258e-01,  1.5205717087e-01,  2.3695373535e-01,\n",
            "         5.3962194920e-01,  3.9806330204e-01,  2.8589262962e+00,\n",
            "         3.1385254860e-01,  3.0643332005e-01,  2.9091143608e+00,\n",
            "         1.2886667252e+00,  1.4396723509e+00])\n",
            "ctensor.grad: tensor([-7.0169916153, -7.0169916153, -7.0169916153, 18.6563282013,\n",
            "        -0.2763826549, 16.6439762115,  1.0834641457,  4.6800231934,\n",
            "        18.6563282013, -0.2763826549, 16.6439762115,  1.0834641457,\n",
            "         4.6800231934, 18.6563282013, -0.2763826549, 16.6439762115,\n",
            "         1.0834641457,  4.6800231934])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2181.5476074219, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0833306313), tensor(1.3456037045), tensor(1.3280663490), tensor(1.6921668053), tensor(1.1530354023), tensor(2.2599585056), tensor(1.4177708626), tensor(1.4936335087), tensor(0.9616472721), tensor(1.8638225794), tensor(1.5701612234), tensor(2.6836705208), tensor(1.4289000034), tensor(1.6999353170), tensor(1.6317871809), tensor(1.3975183964), tensor(1.5628417730), tensor(0.9258089662), tensor(1.5124278069), tensor(1.5876439810), tensor(1.0229135752), tensor(1.0218919516), tensor(0.9230681062)]\n",
            "b:  [tensor(0.6768711805), tensor(0.8939068913), tensor(0.9136047363), tensor(0.9534124732), tensor(1.1516039371), tensor(0.7108586431), tensor(0.9601722956), tensor(0.9195868373), tensor(1.2306561470), tensor(0.8216286898), tensor(0.9579359889), tensor(0.6523209214), tensor(0.9731476903), tensor(0.9042811990), tensor(0.9128009081), tensor(0.9749549627), tensor(0.8404449224), tensor(1.1905163527), tensor(0.9322645664), tensor(0.9196195006), tensor(1.1908159256), tensor(1.1316145658), tensor(1.0970441103)]\n",
            "c:  [tensor(-0.0675751194), tensor(-0.0675751194), tensor(-0.0675751194), tensor(-0.0092219403), tensor(0.0013898107), tensor(0.0013125367), tensor(0.0086055631), tensor(0.0134933023), tensor(-0.0092219403), tensor(0.0013898107), tensor(0.0013125367), tensor(0.0086055631), tensor(0.0134933023), tensor(-0.0092219403), tensor(0.0013898107), tensor(0.0013125367), tensor(0.0086055631), tensor(0.0134933023)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.7071704865, -0.5281145573, -0.8730823994,  2.3587203026,\n",
            "        -8.0085735321,  3.1178178787, -0.1529458761,  0.0123448968,\n",
            "        -7.5172581673,  2.7356071472,  1.9212181568,  3.7497584820,\n",
            "         0.8209643960,  2.3721675873,  1.7701922655,  0.0401355326,\n",
            "        -0.0268864632, -7.4796266556,  1.2561906576,  1.5495636463,\n",
            "        -7.9810442924, -7.6376399994, -7.2839250565])\n",
            "btensor.grad: tensor([-0.1161345392,  0.4608607292,  0.5334989429,  0.1917735934,\n",
            "         1.3679335117,  0.0797808170,  0.4552764297,  0.4232095480,\n",
            "         2.6119689941,  0.0948722959,  0.2099506855, -0.0053645968,\n",
            "         0.3930917978,  0.1413357258,  0.2197841406,  0.4943656921,\n",
            "         0.3719959557,  2.5895609856,  0.2947078347,  0.2853221893,\n",
            "         2.6416122913,  1.2733106613,  1.3995683193])\n",
            "ctensor.grad: tensor([-6.2407712936, -6.2407712936, -6.2407712936, 18.3070259094,\n",
            "        -0.2750867903, 16.2253990173,  0.9578591585,  4.2792310715,\n",
            "        18.3070259094, -0.2750867903, 16.2253990173,  0.9578591585,\n",
            "         4.2792310715, 18.3070259094, -0.2750867903, 16.2253990173,\n",
            "         0.9578591585,  4.2792310715])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2181.0029296875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0787363052), tensor(1.3461289406), tensor(1.3289085627), tensor(1.6899290085), tensor(1.1607393026), tensor(2.2569446564), tensor(1.4179382324), tensor(1.4936513901), tensor(0.9688148499), tensor(1.8611977100), tensor(1.5683405399), tensor(2.6800248623), tensor(1.4281418324), tensor(1.6976733208), tensor(1.6301075220), tensor(1.3975030184), tensor(1.5628874302), tensor(0.9329315424), tensor(1.5112484694), tensor(1.5861798525), tensor(1.0305460691), tensor(1.0291974545), tensor(0.9299992323)]\n",
            "b:  [tensor(0.6769917607), tensor(0.8934744596), tensor(0.9131085277), tensor(0.9532385468), tensor(1.1502723694), tensor(0.7107885480), tensor(0.9597539306), tensor(0.9191942215), tensor(1.2282801867), tensor(0.8215417862), tensor(0.9577421546), tensor(0.6523340940), tensor(0.9727814198), tensor(0.9041507840), tensor(0.9125971794), tensor(0.9744981527), tensor(0.8400962353), tensor(1.1881642342), tensor(0.9319879413), tensor(0.9193536043), tensor(1.1884105206), tensor(1.1303648949), tensor(1.0956897736)]\n",
            "c:  [tensor(-0.0672997683), tensor(-0.0672997683), tensor(-0.0672997683), tensor(-0.0101208081), tensor(0.0014035034), tensor(0.0005210912), tensor(0.0085636433), tensor(0.0132982405), tensor(-0.0101208081), tensor(0.0014035034), tensor(0.0005210912), tensor(0.0085636433), tensor(0.0132982405), tensor(-0.0101208081), tensor(0.0014035034), tensor(0.0005210912), tensor(0.0085636433), tensor(0.0132982405)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.5942358971, -0.5252919793, -0.8422262669,  2.2378442287,\n",
            "        -7.7039589882,  3.0139508247, -0.1674194336, -0.0178461671,\n",
            "        -7.1675481796,  2.6248507500,  1.8206810951,  3.6455757618,\n",
            "         0.7581521273,  2.2619576454,  1.6796027422,  0.0154238045,\n",
            "        -0.0456475019, -7.1225533485,  1.1792978048,  1.4640698433,\n",
            "        -7.6324853897, -7.3054838181, -6.9311542511])\n",
            "btensor.grad: tensor([-0.1205702126,  0.4324492216,  0.4962281287,  0.1739093065,\n",
            "         1.3315794468,  0.0700937510,  0.4183430374,  0.3925899267,\n",
            "         2.3759057522,  0.0869249701,  0.1938421726, -0.0131829679,\n",
            "         0.3662495017,  0.1304173470,  0.2037398815,  0.4567836523,\n",
            "         0.3486939669,  2.3521537781,  0.2766503692,  0.2659090757,\n",
            "         2.4054584503,  1.2496873140,  1.3543230295])\n",
            "ctensor.grad: tensor([-5.5069575310, -5.5069575310, -5.5069575310, 17.9773540497,\n",
            "        -0.2738520801, 15.8289089203,  0.8384035826,  3.9012432098,\n",
            "        17.9773540497, -0.2738520801, 15.8289089203,  0.8384035826,\n",
            "         3.9012432098, 17.9773540497, -0.2738520801, 15.8289089203,\n",
            "         0.8384035826,  3.9012432098])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2180.5046386719, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0742502213), tensor(1.3466498852), tensor(1.3297203779), tensor(1.6878049374), tensor(1.1681555510), tensor(2.2540297508), tensor(1.4181174040), tensor(1.4936957359), tensor(0.9756544828), tensor(1.8586781025), tensor(1.5666143894), tensor(2.6764788628), tensor(1.4274414778), tensor(1.6955155134), tensor(1.6285130978), tensor(1.3975089788), tensor(1.5629492998), tensor(0.9397197962), tensor(1.5101408958), tensor(1.5847958326), tensor(1.0378512144), tensor(1.0361922979), tensor(0.9366024733)]\n",
            "b:  [tensor(0.6771171093), tensor(0.8930684328), tensor(0.9126454592), tensor(0.9530801177), tensor(1.1489804983), tensor(0.7107285857), tensor(0.9593665004), tensor(0.9188277125), tensor(1.2261129618), tensor(0.8214637041), tensor(0.9575630426), tensor(0.6523551941), tensor(0.9724391699), tensor(0.9040310979), tensor(0.9124084115), tensor(0.9740734696), tensor(0.8397686481), tensor(1.1860216856), tensor(0.9317283034), tensor(0.9191056490), tensor(1.1862138510), tensor(1.1291449070), tensor(1.0943840742)]\n",
            "c:  [tensor(-0.0670591742), tensor(-0.0670591742), tensor(-0.0670591742), tensor(-0.0110041136), tensor(0.0014171377), tensor(-0.0002515553), tensor(0.0085274158), tensor(0.0131210499), tensor(-0.0110041136), tensor(0.0014171377), tensor(-0.0002515553), tensor(0.0085274158), tensor(0.0131210499), tensor(-0.0110041136), tensor(0.0014171377), tensor(-0.0002515553), tensor(0.0085274158), tensor(0.0131210499)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.4859747887e+00, -5.2097105980e-01, -8.1176280975e-01,\n",
            "         2.1240484715e+00, -7.4162759781e+00,  2.9149155617e+00,\n",
            "        -1.7915534973e-01, -4.4339716434e-02, -6.8396234512e+00,\n",
            "         2.5196096897e+00,  1.7262065411e+00,  3.5459835529e+00,\n",
            "         7.0037746429e-01,  2.1577672958e+00,  1.5944416523e+00,\n",
            "        -5.9876441956e-03, -6.1890006065e-02, -6.7882561684e+00,\n",
            "         1.1075671911e+00,  1.3840432167e+00, -7.3050947189e+00,\n",
            "        -6.9948048592e+00, -6.6032667160e+00])\n",
            "btensor.grad: tensor([-0.1253451258,  0.4060534239,  0.4630928338,  0.1584014893,\n",
            "         1.2918572426,  0.0599758625,  0.3874527514,  0.3664984703,\n",
            "         2.1672291756,  0.0780597925,  0.1790857315, -0.0211124122,\n",
            "         0.3422656655,  0.1196652651,  0.1887488365,  0.4246921539,\n",
            "         0.3275676966,  2.1425504684,  0.2596287727,  0.2479847670,\n",
            "         2.1966366768,  1.2199457884,  1.3056704998])\n",
            "ctensor.grad: tensor([-4.8119344711, -4.8119344711, -4.8119344711, 17.6661128998,\n",
            "        -0.2726859450, 15.4529323578,  0.7245443463,  3.5438175201,\n",
            "        17.6661128998, -0.2726859450, 15.4529323578,  0.7245443463,\n",
            "         3.5438175201, 17.6661128998, -0.2726859450, 15.4529323578,\n",
            "         0.7245443463,  3.5438175201])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2180.0441894531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0698680878), tensor(1.3471653461), tensor(1.3305022717), tensor(1.6857881546), tensor(1.1752996445), tensor(2.2512092590), tensor(1.4183059931), tensor(1.4937633276), tensor(0.9821861386), tensor(1.8562586308), tensor(1.5649770498), tensor(2.6730282307), tensor(1.4267944098), tensor(1.6934564114), tensor(1.6269987822), tensor(1.3975335360), tensor(1.5630252361), tensor(0.9461945295), tensor(1.5091003180), tensor(1.5834867954), tensor(1.0448483229), tensor(1.0428959131), tensor(0.9429001808)]\n",
            "b:  [tensor(0.6772472262), tensor(0.8926869631), tensor(0.9122123122), tensor(0.9529354572), tensor(1.1477305889), tensor(0.7106786370), tensor(0.9590055943), tensor(0.9184839725), tensor(1.2241305113), tensor(0.8213948011), tensor(0.9573975801), tensor(0.6523840427), tensor(0.9721186161), tensor(0.9039218426), tensor(0.9122337103), tensor(0.9736768007), tensor(0.8394604921), tensor(1.1840646267), tensor(0.9314846992), tensor(0.9188742638), tensor(1.1842021942), tensor(1.1279591322), tensor(1.0931290388)]\n",
            "c:  [tensor(-0.0668515563), tensor(-0.0668515563), tensor(-0.0668515563), tensor(-0.0118727172), tensor(0.0014307174), tensor(-0.0010063604), tensor(0.0084966272), tensor(0.0129608018), tensor(-0.0118727172), tensor(0.0014307174), tensor(-0.0010063604), tensor(0.0084966272), tensor(0.0129608018), tensor(-0.0118727172), tensor(0.0014307174), tensor(-0.0010063604), tensor(0.0084966272), tensor(0.0129608018)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.3820409775, -0.5154260397, -0.7818639278,  2.0167498589,\n",
            "        -7.1441340446,  2.8203759193, -0.1886149943, -0.0676452518,\n",
            "        -6.5316500664,  2.4195168018,  1.6373050213,  3.4506382942,\n",
            "         0.6471193433,  2.0591583252,  1.5142685175, -0.0246002376,\n",
            "        -0.0759541392, -6.4747543335,  1.0405480862,  1.3090230227,\n",
            "        -6.9970712662, -6.7035560608, -6.2977070808])\n",
            "btensor.grad: tensor([-0.1301146895,  0.3814575672,  0.4331670403,  0.1446748972,\n",
            "         1.2499033213,  0.0499502420,  0.3609319329,  0.3437128067,\n",
            "         1.9824212790,  0.0689205527,  0.1654849052, -0.0288747847,\n",
            "         0.3205723763,  0.1092743874,  0.1747266054,  0.3966392279,\n",
            "         0.3081612587,  1.9571056366,  0.2436223626,  0.2313994169,\n",
            "         2.0116341114,  1.1857898235,  1.2550187111])\n",
            "ctensor.grad: tensor([-4.1524009705, -4.1524009705, -4.1524009705, 17.3720645905,\n",
            "        -0.2715938687, 15.0960998535,  0.6157650352,  3.2049524784,\n",
            "        17.3720645905, -0.2715938687, 15.0960998535,  0.6157650352,\n",
            "         3.2049524784, 17.3720645905, -0.2715938687, 15.0960998535,\n",
            "         0.6157650352,  3.2049524784])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2179.6191406250, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0655858517), tensor(1.3476742506), tensor(1.3312549591), tensor(1.6838726997), tensor(1.1821858883), tensor(2.2484791279), tensor(1.4185022116), tensor(1.4938515425), tensor(0.9884281158), tensor(1.8539344072), tensor(1.5634235144), tensor(2.6696689129), tensor(1.4261964560), tensor(1.6914906502), tensor(1.6255600452), tensor(1.3975743055), tensor(1.5631133318), tensor(0.9523748159), tensor(1.5081224442), tensor(1.5822482109), tensor(1.0515551567), tensor(1.0493258238), tensor(0.9489124417)]\n",
            "b:  [tensor(0.6773819327), tensor(0.8923285007), tensor(0.9118064642), tensor(0.9528030753), tensor(1.1465239525), tensor(0.7106383443), tensor(0.9586679935), tensor(0.9181606174), tensor(1.2223120928), tensor(0.8213348985), tensor(0.9572446346), tensor(0.6524202824), tensor(0.9718178511), tensor(0.9038224220), tensor(0.9120720625), tensor(0.9733051062), tensor(0.8391702771), tensor(1.1822719574), tensor(0.9312561154), tensor(0.9186582565), tensor(1.1823548079), tensor(1.1268105507), tensor(1.0919256210)]\n",
            "c:  [tensor(-0.0666752830), tensor(-0.0666752830), tensor(-0.0666752830), tensor(-0.0127274236), tensor(0.0014442464), tensor(-0.0017442108), tensor(0.0084710475), tensor(0.0128166592), tensor(-0.0127274236), tensor(0.0014442464), tensor(-0.0017442108), tensor(0.0084710475), tensor(0.0128166592), tensor(-0.0127274236), tensor(0.0014442464), tensor(-0.0017442108), tensor(0.0084710475), tensor(0.0128166592)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.2821254730, -0.5089036226, -0.7526645660,  1.9154576063,\n",
            "        -6.8862953186,  2.7300267220, -0.1961804628, -0.0881837606,\n",
            "        -6.2419891357,  2.3242423534,  1.5535476208,  3.3592395782,\n",
            "         0.5979359150,  1.9657433033,  1.4386988878, -0.0408078730,\n",
            "        -0.0881046057, -6.1802992821,  0.9778492451,  1.2385970354,\n",
            "        -6.7068171501, -6.4299283028, -6.0122776031])\n",
            "btensor.grad: tensor([-0.1347082108,  0.3584353924,  0.4058741033,  0.1324058175,\n",
            "         1.2066427469,  0.0402789116,  0.3376036286,  0.3233802319,\n",
            "         1.8183645010,  0.0598922372,  0.1529352665, -0.0362602472,\n",
            "         0.3007397652,  0.0993959904,  0.1616523266,  0.3716784716,\n",
            "         0.2902262211,  1.7927002907,  0.2285583019,  0.2160190344,\n",
            "         1.8474164009,  1.1486321688,  1.2034407854])\n",
            "ctensor.grad: tensor([-3.5254371166, -3.5254371166, -3.5254371166, 17.0941295624,\n",
            "        -0.2705799937, 14.7570085526,  0.5115967989,  2.8828463554,\n",
            "        17.0941295624, -0.2705799937, 14.7570085526,  0.5115967989,\n",
            "         2.8828463554, 17.0941295624, -0.2705799937, 14.7570085526,\n",
            "         0.5115967989,  2.8828463554])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2179.2270507812, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0613999367), tensor(1.3481758833), tensor(1.3319791555), tensor(1.6820529699), tensor(1.1888275146), tensor(2.2458355427), tensor(1.4187043905), tensor(1.4939578772), tensor(0.9943972826), tensor(1.8517009020), tensor(1.5619490147), tensor(2.6663973331), tensor(1.4256440401), tensor(1.6896134615), tensor(1.6241927147), tensor(1.3976292610), tensor(1.5632119179), tensor(0.9582781792), tensor(1.5072033405), tensor(1.5810757875), tensor(1.0579880476), tensor(1.0554982424), tensor(0.9546574950)]\n",
            "b:  [tensor(0.6775209904), tensor(0.8919916153), tensor(0.9114257097), tensor(0.9526817799), tensor(1.1453611851), tensor(0.7106072307), tensor(0.9583512545), tensor(0.9178556204), tensor(1.2206397057), tensor(0.8212837577), tensor(0.9571033716), tensor(0.6524634957), tensor(0.9715353847), tensor(0.9037323594), tensor(0.9119226336), tensor(0.9729560018), tensor(0.8388967514), tensor(1.1806253195), tensor(0.9310416579), tensor(0.9184565544), tensor(1.1806534529), tensor(1.1257009506), tensor(1.0907738209)]\n",
            "c:  [tensor(-0.0665288642), tensor(-0.0665288642), tensor(-0.0665288642), tensor(-0.0135689890), tensor(0.0014577288), tensor(-0.0024659336), tensor(0.0084504671), tensor(0.0126878629), tensor(-0.0135689890), tensor(0.0014577288), tensor(-0.0024659336), tensor(0.0084504671), tensor(0.0126878629), tensor(-0.0135689890), tensor(0.0014577288), tensor(-0.0024659336), tensor(0.0084504671), tensor(0.0126878629)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.1859450340, -0.5015971661, -0.7242498398,  1.8197270632,\n",
            "        -6.6416587830,  2.6435863972, -0.2021361291, -0.1062863469,\n",
            "        -5.9691905975,  2.2334794998,  1.4745587111,  3.2715070248,\n",
            "         0.5524379611,  1.8771691322,  1.3673872948, -0.0549340546,\n",
            "        -0.0985804796, -5.9033474922,  0.9191177487,  1.1724094152,\n",
            "        -6.4329175949, -6.1723604202, -5.7450647354])\n",
            "btensor.grad: tensor([-0.1390623599,  0.3368762732,  0.3807417154,  0.1212998629,\n",
            "         1.1627911329,  0.0310959816,  0.3167251050,  0.3050007820,\n",
            "         1.6723911762,  0.0511653423,  0.1412839890, -0.0432330072,\n",
            "         0.2824600339,  0.0900332928,  0.1494301558,  0.3491288424,\n",
            "         0.2735266685,  1.6465954781,  0.2144317031,  0.2017008066,\n",
            "         1.7013378143,  1.1095410585,  1.1517598629])\n",
            "ctensor.grad: tensor([-2.9284148216, -2.9284148216, -2.9284148216, 16.8313026428,\n",
            "        -0.2696470320, 14.4344530106,  0.4116139710,  2.5759315491,\n",
            "        16.8313026428, -0.2696470320, 14.4344530106,  0.4116139710,\n",
            "         2.5759315491, 16.8313026428, -0.2696470320, 14.4344530106,\n",
            "         0.4116139710,  2.5759315491])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2178.8659667969, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0573067665), tensor(1.3486695290), tensor(1.3326758146), tensor(1.6803238392), tensor(1.1952368021), tensor(2.2432746887), tensor(1.4189110994), tensor(1.4940800667), tensor(1.0001091957), tensor(1.8495539427), tensor(1.5605490208), tensor(2.6632101536), tensor(1.4251337051), tensor(1.6878203154), tensor(1.6228927374), tensor(1.3976964951), tensor(1.5633194447), tensor(0.9639206529), tensor(1.5063393116), tensor(1.5799655914), tensor(1.0641621351), tensor(1.0614277124), tensor(0.9601519108)]\n",
            "b:  [tensor(0.6776641011), tensor(0.8916749954), tensor(0.9110682607), tensor(0.9525706172), tensor(1.1442422867), tensor(0.7105847597), tensor(0.9580535293), tensor(0.9175674915), tensor(1.2190974951), tensor(0.8212409019), tensor(0.9569728971), tensor(0.6525132656), tensor(0.9712697864), tensor(0.9036511183), tensor(0.9117846489), tensor(0.9726274610), tensor(0.8386388421), tensor(1.1791088581), tensor(0.9308404922), tensor(0.9182682037), tensor(1.1790823936), tensor(1.1246316433), tensor(1.0896732807)]\n",
            "c:  [tensor(-0.0664109141), tensor(-0.0664109141), tensor(-0.0664109141), tensor(-0.0143981213), tensor(0.0014711686), tensor(-0.0031722998), tensor(0.0084346961), tensor(0.0125737237), tensor(-0.0143981213), tensor(0.0014711686), tensor(-0.0031722998), tensor(0.0084346961), tensor(0.0125737237), tensor(-0.0143981213), tensor(0.0014711686), tensor(-0.0031722998), tensor(0.0084346961), tensor(0.0125737237)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.0932488441, -0.4936725199, -0.6966841221,  1.7291618586,\n",
            "        -6.4092383385,  2.5608043671, -0.2067328095, -0.1222457290,\n",
            "        -5.7119536400,  2.1469404697,  1.3999910355,  3.1871955395,\n",
            "         0.5103049278,  1.7931135893,  1.3000295162, -0.0672382414,\n",
            "        -0.1075824499, -5.6425013542,  0.8640460372,  1.1101415157,\n",
            "        -6.1741023064, -5.9294695854, -5.4944043159])\n",
            "btensor.grad: tensor([-0.1431134641,  0.3166463375,  0.3574684858,  0.1111410856,\n",
            "         1.1189198494,  0.0224562883,  0.2977502346,  0.2881330252,\n",
            "         1.5422047377,  0.0428512096,  0.1304466724, -0.0497632623,\n",
            "         0.2655709386,  0.0812302828,  0.1380076408,  0.3285442591,\n",
            "         0.2579341233,  1.5164318085,  0.2011672258,  0.1883659363,\n",
            "         1.5710771084,  1.0693452358,  1.1005830765])\n",
            "ctensor.grad: tensor([-2.3589677811, -2.3589677811, -2.3589677811, 16.5826377869,\n",
            "        -0.2687969506, 14.1273241043,  0.3154262006,  2.2827882767,\n",
            "        16.5826377869, -0.2687969506, 14.1273241043,  0.3154262006,\n",
            "         2.2827882767, 16.5826377869, -0.2687969506, 14.1273241043,\n",
            "         0.3154262006,  2.2827882767])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2178.5258789062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0533030033), tensor(1.3491548300), tensor(1.3333457708), tensor(1.6786804199), tensor(1.2014249563), tensor(2.2407932281), tensor(1.4191212654), tensor(1.4942163229), tensor(1.0055782795), tensor(1.8474895954), tensor(1.5592194796), tensor(2.6601040363), tensor(1.4246624708), tensor(1.6861070395), tensor(1.6216564178), tensor(1.3977744579), tensor(1.5634347200), tensor(0.9693171978), tensor(1.5055269003), tensor(1.5789140463), tensor(1.0700913668), tensor(1.0671277046), tensor(0.9654107690)]\n",
            "b:  [tensor(0.6778109670), tensor(0.8913773894), tensor(0.9107324481), tensor(0.9524688125), tensor(1.1431667805), tensor(0.7105703950), tensor(0.9577732086), tensor(0.9172949791), tensor(1.2176717520), tensor(0.8212059140), tensor(0.9568525553), tensor(0.6525691152), tensor(0.9710199237), tensor(0.9035781622), tensor(0.9116573334), tensor(0.9723178744), tensor(0.8383955359), tensor(1.1777086258), tensor(0.9306517839), tensor(0.9180922508), tensor(1.1776278019), tensor(1.1236028671), tensor(1.0886229277)]\n",
            "c:  [tensor(-0.0663201660), tensor(-0.0663201660), tensor(-0.0663201660), tensor(-0.0152154854), tensor(0.0014845701), tensor(-0.0038640278), tensor(0.0084235612), tensor(0.0124736149), tensor(-0.0152154854), tensor(0.0014845701), tensor(-0.0038640278), tensor(0.0084235612), tensor(0.0124736149), tensor(-0.0152154854), tensor(0.0014845701), tensor(-0.0038640278), tensor(0.0084235612), tensor(0.0124736149)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 4.0038132668, -0.4852712750, -0.6699926853,  1.6434181929,\n",
            "        -6.1881465912,  2.4814512730, -0.2101688385, -0.1363098025,\n",
            "        -5.4690980911,  2.0643708706,  1.3295379877,  3.1060876846,\n",
            "         0.4712272882,  1.7132893801,  1.2363426685, -0.0779554844,\n",
            "        -0.1152806282, -5.3965225220,  0.8123543262,  1.0515031815,\n",
            "        -5.9292354584, -5.7000370026, -5.2588305473])\n",
            "btensor.grad: tensor([-0.1468672156,  0.2976356745,  0.3358372748,  0.1018108726,\n",
            "         1.0754470825,  0.0143685341,  0.2803408802,  0.2725325823,\n",
            "         1.4257973433,  0.0349770784,  0.1203212738, -0.0558339059,\n",
            "         0.2498335838,  0.0729633570,  0.1273130178,  0.3095762730,\n",
            "         0.2433116436,  1.4001841545,  0.1887116432,  0.1759411097,\n",
            "         1.4546447992,  1.0287461281,  1.0504058599])\n",
            "ctensor.grad: tensor([-1.8149805069, -1.8149805069, -1.8149805069, 16.3472824097,\n",
            "        -0.2680303156, 13.8345584869,  0.2226932049,  2.0021677017,\n",
            "        16.3472824097, -0.2680303156, 13.8345584869,  0.2226932049,\n",
            "         2.0021677017, 16.3472824097, -0.2680303156, 13.8345584869,\n",
            "         0.2226932049,  2.0021677017])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2178.2126464844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0493855476), tensor(1.3496313095), tensor(1.3339899778), tensor(1.6771183014), tensor(1.2074025869), tensor(2.2383878231), tensor(1.4193338156), tensor(1.4943649769), tensor(1.0108178854), tensor(1.8455040455), tensor(1.5579565763), tensor(2.6570761204), tensor(1.4242274761), tensor(1.6844695807), tensor(1.6204802990), tensor(1.3978617191), tensor(1.5635565519), tensor(0.9744815230), tensor(1.5047631264), tensor(1.5779178143), tensor(1.0757886171), tensor(1.0726107359), tensor(0.9704478383)]\n",
            "b:  [tensor(0.6779612899), tensor(0.8910976648), tensor(0.9104168415), tensor(0.9523755908), tensor(1.1421340704), tensor(0.7105636001), tensor(0.9575089812), tensor(0.9170369506), tensor(1.2163503170), tensor(0.8211783767), tensor(0.9567416906), tensor(0.6526306272), tensor(0.9707847834), tensor(0.9035129547), tensor(0.9115400314), tensor(0.9720258713), tensor(0.8381659985), tensor(1.1764125824), tensor(0.9304748178), tensor(0.9179279208), tensor(1.1762775183), tensor(1.1226146221), tensor(1.0876214504)]\n",
            "c:  [tensor(-0.0662554353), tensor(-0.0662554353), tensor(-0.0662554353), tensor(-0.0160217062), tensor(0.0014979375), tensor(-0.0045417901), tensor(0.0084169060), tensor(0.0123869665), tensor(-0.0160217062), tensor(0.0014979375), tensor(-0.0045417901), tensor(0.0084169060), tensor(0.0123869665), tensor(-0.0160217062), tensor(0.0014979375), tensor(-0.0045417901), tensor(0.0084169060), tensor(0.0123869665)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.9174320698, -0.4765090644, -0.6441969872,  1.5621743202,\n",
            "        -5.9775934219,  2.4053134918, -0.2126086354, -0.1486937404,\n",
            "        -5.2395734787,  1.9855263233,  1.2629249096,  3.0279815197,\n",
            "         0.4349524379,  1.6374366283,  1.1760779619, -0.0872795582,\n",
            "        -0.1218217015, -5.1642980576,  0.7637797594,  0.9962285757,\n",
            "        -5.6972980499, -5.4829888344, -5.0370593071])\n",
            "btensor.grad: tensor([-0.1503443718,  0.2797204256,  0.3156302273,  0.0932017565,\n",
            "         1.0327010155,  0.0068136454,  0.2642236054,  0.2580130100,\n",
            "         1.3214240074,  0.0275470614,  0.1108450890, -0.0614835322,\n",
            "         0.2351480126,  0.0652142763,  0.1172808409,  0.2919901609,\n",
            "         0.2295511514,  1.2960863113,  0.1769831181,  0.1643152237,\n",
            "         1.3503109217,  0.9882323742,  1.0015326738])\n",
            "ctensor.grad: tensor([-1.2945606709, -1.2945606709, -1.2945606709, 16.1244258881,\n",
            "        -0.2673476636, 13.5552492142,  0.1331011206,  1.7329636812,\n",
            "        16.1244258881, -0.2673476636, 13.5552492142,  0.1331011206,\n",
            "         1.7329636812, 16.1244258881, -0.2673476636, 13.5552492142,\n",
            "         0.1331011206,  1.7329636812])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2177.9160156250, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0455515385), tensor(1.3500988483), tensor(1.3346092701), tensor(1.6756331921), tensor(1.2131794691), tensor(2.2360556126), tensor(1.4195480347), tensor(1.4945245981), tensor(1.0158402920), tensor(1.8435938358), tensor(1.5567567348), tensor(2.6541235447), tensor(1.4238262177), tensor(1.6829042435), tensor(1.6193612814), tensor(1.3979570866), tensor(1.5636838675), tensor(0.9794263244), tensor(1.5040450096), tensor(1.5769736767), tensor(1.0812660456), tensor(1.0778881311), tensor(0.9752758145)]\n",
            "b:  [tensor(0.6781148314), tensor(0.8908348680), tensor(0.9101201892), tensor(0.9522904158), tensor(1.1411430836), tensor(0.7105638385), tensor(0.9572597742), tensor(0.9167925119), tensor(1.2151226997), tensor(0.8211577535), tensor(0.9566397071), tensor(0.6526973844), tensor(0.9705633521), tensor(0.9034550190), tensor(0.9114321470), tensor(0.9717502594), tensor(0.8379493952), tensor(1.1752099991), tensor(0.9303088784), tensor(0.9177744985), tensor(1.1750209332), tensor(1.1216664314), tensor(1.0866672993)]\n",
            "c:  [tensor(-0.0662156343), tensor(-0.0662156343), tensor(-0.0662156343), tensor(-0.0168173742), tensor(0.0015112750), tensor(-0.0052062143), tensor(0.0084145879), tensor(0.0123132588), tensor(-0.0168173742), tensor(0.0015112750), tensor(-0.0052062143), tensor(0.0084145879), tensor(0.0123132588), tensor(-0.0168173742), tensor(0.0015112750), tensor(-0.0052062143), tensor(0.0084145879), tensor(0.0123132588)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.8339250088, -0.4674909413, -0.6193051338,  1.4851403236,\n",
            "        -5.7768530846,  2.3322017193, -0.2141946554, -0.1595777273,\n",
            "        -5.0224289894,  1.9101998806,  1.1998984814,  2.9526944160,\n",
            "         0.4012412727,  1.5653001070,  1.1190067530, -0.0953766406,\n",
            "        -0.1273421049, -4.9448208809,  0.7181030512,  0.9440940619,\n",
            "        -5.4773755074, -5.2773599625, -4.8279547691])\n",
            "btensor.grad: tensor([-1.5355686843e-01,  2.6281714439e-01,  2.9667294025e-01,\n",
            "         8.5201740265e-02,  9.9092835188e-01, -2.3078918457e-04,\n",
            "         2.4920326471e-01,  2.4442243576e-01,  1.2275795937e+00,\n",
            "         2.0593822002e-02,  1.0197257996e-01, -6.6747307777e-02,\n",
            "         2.2141176462e-01,  5.7948231697e-02,  1.0785472393e-01,\n",
            "         2.7558350563e-01,  2.1660684049e-01,  1.2026250362e+00,\n",
            "         1.6595458984e-01,  1.5343129635e-01,  1.2565621138e+00,\n",
            "         9.4820284843e-01,  9.5421063900e-01])\n",
            "ctensor.grad: tensor([-0.7959833741, -0.7959833741, -0.7959833741, 15.9133596420,\n",
            "        -0.2667487860, 13.2884883881,  0.0463680327,  1.4741541147,\n",
            "        15.9133596420, -0.2667487860, 13.2884883881,  0.0463680327,\n",
            "         1.4741541147, 15.9133596420, -0.2667487860, 13.2884883881,\n",
            "         0.0463680327,  1.4741541147])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2177.6433105469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0417983532), tensor(1.3505572081), tensor(1.3352046013), tensor(1.6742211580), tensor(1.2187647820), tensor(2.2337937355), tensor(1.4197630882), tensor(1.4946937561), tensor(1.0206570625), tensor(1.8417556286), tensor(1.5556164980), tensor(2.6512434483), tensor(1.4234563112), tensor(1.6814075708), tensor(1.6182963848), tensor(1.3980594873), tensor(1.5638158321), tensor(0.9841635227), tensor(1.5033699274), tensor(1.5760787725), tensor(1.0865346193), tensor(1.0829703808), tensor(0.9799063206)]\n",
            "b:  [tensor(0.6782713532), tensor(0.8905879855), tensor(0.9098412991), tensor(0.9522126317), tensor(1.1401927471), tensor(0.7105706334), tensor(0.9570246339), tensor(0.9165608287), tensor(1.2139797211), tensor(0.8211437464), tensor(0.9565460682), tensor(0.6527690291), tensor(0.9703548551), tensor(0.9034038782), tensor(0.9113331437), tensor(0.9714900255), tensor(0.8377450705), tensor(1.1740915775), tensor(0.9301533103), tensor(0.9176312685), tensor(1.1738488674), tensor(1.1207574606), tensor(1.0857586861)]\n",
            "c:  [tensor(-0.0661997497), tensor(-0.0661997497), tensor(-0.0661997497), tensor(-0.0176030435), tensor(0.0015245866), tensor(-0.0058578905), tensor(0.0084164757), tensor(0.0122520160), tensor(-0.0176030435), tensor(0.0015245866), tensor(-0.0058578905), tensor(0.0084164757), tensor(0.0122520160), tensor(-0.0176030435), tensor(0.0015245866), tensor(-0.0058578905), tensor(0.0084164757), tensor(0.0122520160)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.7531192303, -0.4583002627, -0.5953234434,  1.4120528698,\n",
            "        -5.5852870941,  2.2619414330, -0.2150379717, -0.1691291928,\n",
            "        -4.8167972565,  1.8381820917,  1.1402258873,  2.8800618649,\n",
            "         0.3698812425,  1.4966632128,  1.0649198294, -0.1023886800,\n",
            "        -0.1319532990, -4.7371821404,  0.6751056314,  0.8948773146,\n",
            "        -5.2686233521, -5.0822963715, -4.6305088997])\n",
            "btensor.grad: tensor([-0.1565140188,  0.2468539476,  0.2788917422,  0.0777679086,\n",
            "         0.9502886534, -0.0068134069,  0.2351350188,  0.2316554785,\n",
            "         1.1429605484,  0.0140271783,  0.0936377048, -0.0716716647,\n",
            "         0.2085082531,  0.0511204004,  0.0989992619,  0.2602143884,\n",
            "         0.2043482661,  1.1184561253,  0.1555432677,  0.1432324648,\n",
            "         1.1720783710,  0.9089524150,  0.9085936546])\n",
            "ctensor.grad: tensor([-0.3177034557, -0.3177034557, -0.3177034557, 15.7133798599,\n",
            "        -0.2662332058, 13.0335264206, -0.0377617143,  1.2248508930,\n",
            "        15.7133798599, -0.2662332058, 13.0335264206, -0.0377617143,\n",
            "         1.2248508930, 15.7133798599, -0.2662332058, 13.0335264206,\n",
            "        -0.0377617143,  1.2248508930])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2177.3869628906, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0381233692), tensor(1.3510062695), tensor(1.3357768059), tensor(1.6728785038), tensor(1.2241671085), tensor(2.2315993309), tensor(1.4199783802), tensor(1.4948712587), tensor(1.0252789259), tensor(1.8399863243), tensor(1.5545327663), tensor(2.6484334469), tensor(1.4231156111), tensor(1.6799762249), tensor(1.6172827482), tensor(1.3981679678), tensor(1.5639516115), tensor(0.9887040854), tensor(1.5027353764), tensor(1.5752303600), tensor(1.0916049480), tensor(1.0878673792), tensor(0.9843501449)]\n",
            "b:  [tensor(0.6784306169), tensor(0.8903562427), tensor(0.9095791578), tensor(0.9521418214), tensor(1.1392818689), tensor(0.7105836272), tensor(0.9568027258), tensor(0.9163411856), tensor(1.2129132748), tensor(0.8211358786), tensor(0.9564602971), tensor(0.6528453231), tensor(0.9701584578), tensor(0.9033591747), tensor(0.9112424850), tensor(0.9712442160), tensor(0.8375522494), tensor(1.1730490923), tensor(0.9300076365), tensor(0.9174976349), tensor(1.1727530956), tensor(1.1198867559), tensor(1.0848939419)]\n",
            "c:  [tensor(-0.0662068352), tensor(-0.0662068352), tensor(-0.0662068352), tensor(-0.0183792394), tensor(0.0015378766), tensor(-0.0064973687), tensor(0.0084224520), tensor(0.0122028049), tensor(-0.0183792394), tensor(0.0015378766), tensor(-0.0064973687), tensor(0.0084224520), tensor(0.0122028049), tensor(-0.0183792394), tensor(0.0015378766), tensor(-0.0064973687), tensor(0.0084224520), tensor(0.0122028049)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.6748704910, -0.4490101337, -0.5722398758,  1.3426660299,\n",
            "        -5.4022922516,  2.1943798065, -0.2152549624, -0.1774899364,\n",
            "        -4.6218929291,  1.7692905664,  1.0836987495,  2.8099310398,\n",
            "         0.3406907022,  1.4313259125,  1.0136337280, -0.1084388494,\n",
            "        -0.1357569695, -4.5405530930,  0.6346038580,  0.8483850956,\n",
            "        -5.0702924728, -4.8970365524, -4.4438247681])\n",
            "btensor.grad: tensor([-0.1592397243,  0.2317239046,  0.2621416748,  0.0708082914,\n",
            "         0.9109202623, -0.0129714012,  0.2219319344,  0.2196310759,\n",
            "         1.0664620399,  0.0078692436,  0.0857861042, -0.0762785971,\n",
            "         0.1963706017,  0.0446797609,  0.0906651020,  0.2458104491,\n",
            "         0.1928040087,  1.0424497128,  0.1457006335,  0.1336475611,\n",
            "         1.0957556963,  0.8706834316,  0.8647501469])\n",
            "ctensor.grad: tensor([ 0.1416975260,  0.1416975260,  0.1416975260, 15.5239238739,\n",
            "        -0.2658000588, 12.7895660400, -0.1195173860,  0.9842304587,\n",
            "        15.5239238739, -0.2658000588, 12.7895660400, -0.1195173860,\n",
            "         0.9842304587, 15.5239238739, -0.2658000588, 12.7895660400,\n",
            "        -0.1195173860,  0.9842304587])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2177.1442871094, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0345244408), tensor(1.3514459133), tensor(1.3363268375), tensor(1.6716017723), tensor(1.2293944359), tensor(2.2294700146), tensor(1.4201933146), tensor(1.4950560331), tensor(1.0297158957), tensor(1.8382829428), tensor(1.5535026789), tensor(2.6456911564), tensor(1.4228020906), tensor(1.6786071062), tensor(1.6163177490), tensor(1.3982815742), tensor(1.5640904903), tensor(0.9930582643), tensor(1.5021389723), tensor(1.5744259357), tensor(1.0964866877), tensor(1.0925883055), tensor(0.9886172414)]\n",
            "b:  [tensor(0.6785923839), tensor(0.8901388645), tensor(0.9093328118), tensor(0.9520775676), tensor(1.1384088993), tensor(0.7106023431), tensor(0.9565932155), tensor(0.9161329269), tensor(1.2119162083), tensor(0.8211337924), tensor(0.9563819170), tensor(0.6529258490), tensor(0.9699735641), tensor(0.9033205509), tensor(0.9111596942), tensor(0.9710119367), tensor(0.8373704553), tensor(1.1720755100), tensor(0.9298712611), tensor(0.9173730016), tensor(1.1717264652), tensor(1.1190532446), tensor(1.0840711594)]\n",
            "c:  [tensor(-0.0662360117), tensor(-0.0662360117), tensor(-0.0662360117), tensor(-0.0191464573), tensor(0.0015511489), tensor(-0.0071251676), tensor(0.0084324079), tensor(0.0121652270), tensor(-0.0191464573), tensor(0.0015511489), tensor(-0.0071251676), tensor(0.0084324079), tensor(0.0121652270), tensor(-0.0191464573), tensor(0.0015511489), tensor(-0.0071251676), tensor(0.0084324079), tensor(0.0121652270)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.5990357399, -0.4396800995, -0.5500496626,  1.2767583132,\n",
            "        -5.2273330688,  2.1293621063, -0.2149252295, -0.1847845316,\n",
            "        -4.4369974136,  1.7033616304,  1.0301184654,  2.7421722412,\n",
            "         0.3134863079,  1.3690891266,  0.9649685621, -0.1136348248,\n",
            "        -0.1388404369, -4.3541893959,  0.5964200497,  0.8044425249,\n",
            "        -4.8816885948, -4.7208838463, -4.2670912743])\n",
            "btensor.grad: tensor([-0.1617477387,  0.2173954248,  0.2463263571,  0.0642781854,\n",
            "         0.8729103804, -0.0187240839,  0.2094837725,  0.2082852125,\n",
            "         0.9970915318,  0.0020735264,  0.0783653259, -0.0805405676,\n",
            "         0.1849146485,  0.0386083126,  0.0827989578,  0.2322663665,\n",
            "         0.1818154305,  0.9735872746,  0.1364021897,  0.1246254444,\n",
            "         1.0265836716,  0.8335503340,  0.8227568865])\n",
            "ctensor.grad: tensor([ 0.5835021734,  0.5835021734,  0.5835021734, 15.3443536758,\n",
            "        -0.2654482722, 12.5559797287, -0.1991125345,  0.7515524030,\n",
            "        15.3443536758, -0.2654482722, 12.5559797287, -0.1991125345,\n",
            "         0.7515524030, 15.3443536758, -0.2654482722, 12.5559797287,\n",
            "        -0.1991125345,  0.7515524030])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2176.9199218750, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0309989452), tensor(1.3518762589), tensor(1.3368555307), tensor(1.6703876257), tensor(1.2344543934), tensor(2.2274031639), tensor(1.4204074144), tensor(1.4952471256), tensor(1.0339773893), tensor(1.8366427422), tensor(1.5525233746), tensor(2.6430144310), tensor(1.4225139618), tensor(1.6772973537), tensor(1.6153990030), tensor(1.3983997107), tensor(1.5642317533), tensor(0.9972356558), tensor(1.5015785694), tensor(1.5736631155), tensor(1.1011888981), tensor(1.0971415043), tensor(0.9927168489)]\n",
            "b:  [tensor(0.6787564754), tensor(0.8899350762), tensor(0.9091014266), tensor(0.9520193934), tensor(1.1375726461), tensor(0.7106264830), tensor(0.9563955069), tensor(0.9159353971), tensor(1.2109822035), tensor(0.8211371899), tensor(0.9563105702), tensor(0.6530104280), tensor(0.9697994590), tensor(0.9032876492), tensor(0.9110843539), tensor(0.9707924724), tensor(0.8371990323), tensor(1.1711645126), tensor(0.9297436476), tensor(0.9172568917), tensor(1.1707627773), tensor(1.1182556152), tensor(1.0832885504)]\n",
            "c:  [tensor(-0.0662864596), tensor(-0.0662864596), tensor(-0.0662864596), tensor(-0.0199051648), tensor(0.0015644078), tensor(-0.0077417726), tensor(0.0084462445), tensor(0.0121389190), tensor(-0.0199051648), tensor(0.0015644078), tensor(-0.0077417726), tensor(0.0084462445), tensor(0.0121389190), tensor(-0.0199051648), tensor(0.0015644078), tensor(-0.0077417726), tensor(0.0084462445), tensor(0.0121389190)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.5254981518, -0.4303674698, -0.5287296772,  1.2141315937,\n",
            "        -5.0599236488,  2.0667588711, -0.2141395211, -0.1911345720,\n",
            "        -4.2614498138,  1.6402333975,  0.9793115258,  2.6766552925,\n",
            "         0.2881229818,  1.3097908497,  0.9187622070, -0.1180878580,\n",
            "        -0.1412868500, -4.1773924828,  0.5604053140,  0.7628794312,\n",
            "        -4.7021842003, -4.5532140732, -4.0995836258])\n",
            "btensor.grad: tensor([-0.1640699655,  0.2037998438,  0.2313825488,  0.0581608415,\n",
            "         0.8362629414, -0.0241117477,  0.1977095604,  0.1975543499,\n",
            "         0.9340137839, -0.0034087300,  0.0713393688, -0.0845593512,\n",
            "         0.1741107106,  0.0328953266,  0.0753586292,  0.2194604874,\n",
            "         0.1714283079,  0.9110534191,  0.1276084781,  0.1161103249,\n",
            "         0.9637269378,  0.7976791859,  0.7826120853])\n",
            "ctensor.grad: tensor([ 1.0088844299,  1.0088844299,  1.0088844299, 15.1741428375,\n",
            "        -0.2651768029, 12.3321027756, -0.2767414153,  0.5261523724,\n",
            "        15.1741428375, -0.2651768029, 12.3321027756, -0.2767414153,\n",
            "         0.5261523724, 15.1741428375, -0.2651768029, 12.3321027756,\n",
            "        -0.2767414153,  0.5261523724])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2176.7048339844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0275447369), tensor(1.3522974253), tensor(1.3373638391), tensor(1.6692330837), tensor(1.2393540144), tensor(2.2253966331), tensor(1.4206203222), tensor(1.4954438210), tensor(1.0380719900), tensor(1.8350629807), tensor(1.5515922308), tensor(2.6404011250), tensor(1.4222495556), tensor(1.6760441065), tensor(1.6145241261), tensor(1.3985215425), tensor(1.5643749237), tensor(1.0012451410), tensor(1.5010521412), tensor(1.5729395151), tensor(1.1057201624), tensor(1.1015349627), tensor(0.9966574907)]\n",
            "b:  [tensor(0.6789227128), tensor(0.8897441626), tensor(0.9088841677), tensor(0.9519670010), tensor(1.1367715597), tensor(0.7106556892), tensor(0.9562089443), tensor(0.9157480001), tensor(1.2101057768), tensor(0.8211457729), tensor(0.9562458992), tensor(0.6530987024), tensor(0.9696356058), tensor(0.9032601714), tensor(0.9110160470), tensor(0.9705851078), tensor(0.8370375037), tensor(1.1703103781), tensor(0.9296243787), tensor(0.9171488285), tensor(1.1698563099), tensor(1.1174925566), tensor(1.0825442076)]\n",
            "c:  [tensor(-0.0663574040), tensor(-0.0663574040), tensor(-0.0663574040), tensor(-0.0206558052), tensor(0.0015776571), tensor(-0.0083476407), tensor(0.0084638735), tensor(0.0121235484), tensor(-0.0206558052), tensor(0.0015776571), tensor(-0.0083476407), tensor(0.0084638735), tensor(0.0121235484), tensor(-0.0206558052), tensor(0.0015776571), tensor(-0.0083476407), tensor(0.0084638735), tensor(0.0121235484)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.4541382790, -0.4211192429, -0.5082654953,  1.1545870304,\n",
            "        -4.8996109962,  2.0064485073, -0.2129580677, -0.1966367960,\n",
            "        -4.0946512222,  1.5797675848,  0.9311081171,  2.6132700443,\n",
            "         0.2644525766,  1.2532644272,  0.8748798370, -0.1218680739,\n",
            "        -0.1431663632, -4.0095396042,  0.5264005661,  0.7235537171,\n",
            "        -4.5312056541, -4.3934659958, -3.9406547546])\n",
            "btensor.grad: tensor([-0.1662164181,  0.1908925772,  0.2172606438,  0.0523788929,\n",
            "         0.8010337949, -0.0291824341,  0.1865670383,  0.1873707771,\n",
            "         0.8764714599, -0.0085697770,  0.0646777153, -0.0882884264,\n",
            "         0.1638598442,  0.0274845362,  0.0683069229,  0.2073597312,\n",
            "         0.1615391374,  0.8540818691,  0.1192437410,  0.1080638170,\n",
            "         0.9064542651,  0.7630963326,  0.7443110943])\n",
            "ctensor.grad: tensor([ 1.4189218283,  1.4189218283,  1.4189218283, 15.0128059387,\n",
            "        -0.2649842799, 12.1173563004, -0.3525793850,  0.3074109852,\n",
            "        15.0128059387, -0.2649842799, 12.1173563004, -0.3525793850,\n",
            "         0.3074109852, 15.0128059387, -0.2649842799, 12.1173563004,\n",
            "        -0.3525793850,  0.3074109852])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2176.5031738281, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0241599083), tensor(1.3527094126), tensor(1.3378524780), tensor(1.6681351662), tensor(1.2440999746), tensor(2.2234482765), tensor(1.4208317995), tensor(1.4956451654), tensor(1.0420080423), tensor(1.8335411549), tensor(1.5507068634), tensor(2.6378490925), tensor(1.4220072031), tensor(1.6748447418), tensor(1.6136909723), tensor(1.3986465931), tensor(1.5645194054), tensor(1.0050952435), tensor(1.5005578995), tensor(1.5722532272), tensor(1.1100883484), tensor(1.1057760715), tensor(1.0004471540)]\n",
            "b:  [tensor(0.6790909171), tensor(0.8895655870), tensor(0.9086802602), tensor(0.9519200921), tensor(1.1360043287), tensor(0.7106896043), tensor(0.9560329318), tensor(0.9155703187), tensor(1.2092819214), tensor(0.8211592436), tensor(0.9561875463), tensor(0.6531904936), tensor(0.9694814682), tensor(0.9032378197), tensor(0.9109544158), tensor(0.9703891873), tensor(0.8368853927), tensor(1.1695083380), tensor(0.9295130968), tensor(0.9170483947), tensor(1.1690021753), tensor(1.1167627573), tensor(1.0818363428)]\n",
            "c:  [tensor(-0.0664481372), tensor(-0.0664481372), tensor(-0.0664481372), tensor(-0.0213987995), tensor(0.0015909005), tensor(-0.0089432001), tensor(0.0084852129), tensor(0.0121188099), tensor(-0.0213987995), tensor(0.0015909005), tensor(-0.0089432001), tensor(0.0084852129), tensor(0.0121188099), tensor(-0.0213987995), tensor(0.0015909005), tensor(-0.0089432001), tensor(0.0084852129), tensor(0.0121188099)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.3848481178, -0.4119585752, -0.4886348248,  1.0979512930,\n",
            "        -4.7459745407,  1.9483156204, -0.2114403844, -0.2013764381,\n",
            "        -3.9360566139,  1.5218205452,  0.8853610158,  2.5519165993,\n",
            "         0.2423522770,  1.1993590593,  0.8331740499, -0.1250644326,\n",
            "        -0.1445394158, -3.8500659466,  0.4942797422,  0.6863201857,\n",
            "        -4.3682169914, -4.2411274910, -3.7897138596])\n",
            "btensor.grad: tensor([-0.1682010442,  0.1785912514,  0.2038924396,  0.0469113588,\n",
            "         0.7671878338, -0.0339314938,  0.1760093570,  0.1776899099,\n",
            "         0.8238599300, -0.0134544373,  0.0583608150, -0.0918083787,\n",
            "         0.1541472673,  0.0223692656,  0.0616028309,  0.1959341168,\n",
            "         0.1521327198,  0.8020229340,  0.1112945676,  0.1004457474,\n",
            "         0.8540885448,  0.7298440337,  0.7078064680])\n",
            "ctensor.grad: tensor([ 1.8145909309,  1.8145909309,  1.8145909309, 14.8598756790,\n",
            "        -0.2648692727, 11.9111938477, -0.4267860055,  0.0947752967,\n",
            "        14.8598756790, -0.2648692727, 11.9111938477, -0.4267860055,\n",
            "         0.0947752967, 14.8598756790, -0.2648692727, 11.9111938477,\n",
            "        -0.4267860055,  0.0947752967])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2176.3115234375, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0208423138), tensor(1.3531123400), tensor(1.3383222818), tensor(1.6670911312), tensor(1.2486985922), tensor(2.2215559483), tensor(1.4210414886), tensor(1.4958505630), tensor(1.0457931757), tensor(1.8320748806), tensor(1.5498648882), tensor(2.6353566647), tensor(1.4217854738), tensor(1.6736967564), tensor(1.6128973961), tensor(1.3987743855), tensor(1.5646648407), tensor(1.0087937117), tensor(1.5000939369), tensor(1.5716022253), tensor(1.1143010855), tensor(1.1098718643), tensor(1.0040934086)]\n",
            "b:  [tensor(0.6792609692), tensor(0.8893986940), tensor(0.9084890485), tensor(0.9518783689), tensor(1.1352696419), tensor(0.7107280493), tensor(0.9558669925), tensor(0.9154018164), tensor(1.2085063457), tensor(0.8211773038), tensor(0.9561352134), tensor(0.6532856226), tensor(0.9693365693), tensor(0.9032202959), tensor(0.9108992219), tensor(0.9702041149), tensor(0.8367422223), tensor(1.1687539816), tensor(0.9294093251), tensor(0.9169551730), tensor(1.1681960821), tensor(1.1160647869), tensor(1.0811632872)]\n",
            "c:  [tensor(-0.0665579736), tensor(-0.0665579736), tensor(-0.0665579736), tensor(-0.0221345462), tensor(0.0016041420), tensor(-0.0095288567), tensor(0.0085101882), tensor(0.0121244229), tensor(-0.0221345462), tensor(0.0016041420), tensor(-0.0095288567), tensor(0.0085101882), tensor(0.0121244229), tensor(-0.0221345462), tensor(0.0016041420), tensor(-0.0095288567), tensor(0.0085101882), tensor(0.0121244229)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.3175354004, -0.4029287398, -0.4698195457,  1.0440640450,\n",
            "        -4.5986428261,  1.8922574520, -0.2096360028, -0.2054433823,\n",
            "        -3.7851495743,  1.4662789106,  0.8419213891,  2.4924890995,\n",
            "         0.2216963768,  1.1479382515,  0.7935215235, -0.1277370453,\n",
            "        -0.1454644799, -3.6984293461,  0.4639225304,  0.6510536671,\n",
            "        -4.2127428055, -4.0957388878, -3.6462318897])\n",
            "btensor.grad: tensor([-0.1700524986,  0.1668837070,  0.1911823153,  0.0417314172,\n",
            "         0.7347317934, -0.0384272337,  0.1659650505,  0.1684895754,\n",
            "         0.7755972147, -0.0180879831,  0.0523316860, -0.0951251984,\n",
            "         0.1449248791,  0.0175138712,  0.0552229881,  0.1850750446,\n",
            "         0.1431779563,  0.7543256283,  0.1037430763,  0.0932338238,\n",
            "         0.8060936928,  0.6979401708,  0.6730502844])\n",
            "ctensor.grad: tensor([ 2.1968030930,  2.1968030930,  2.1968030930, 14.7149171829,\n",
            "        -0.2648303211, 11.7131299973, -0.4995138645, -0.1122605875,\n",
            "        14.7149171829, -0.2648303211, 11.7131299973, -0.4995138645,\n",
            "        -0.1122605875, 14.7149171829, -0.2648303211, 11.7131299973,\n",
            "        -0.4995138645, -0.1122605875])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2176.1354980469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0175902843), tensor(1.3535064459), tensor(1.3387740850), tensor(1.6660983562), tensor(1.2531558275), tensor(2.2197177410), tensor(1.4212490320), tensor(1.4960594177), tensor(1.0494346619), tensor(1.8306618929), tensor(1.5490642786), tensor(2.6329216957), tensor(1.4215830564), tensor(1.6725978851), tensor(1.6121416092), tensor(1.3989043236), tensor(1.5648108721), tensor(1.0123478174), tensor(1.4996587038), tensor(1.5709846020), tensor(1.1183654070), tensor(1.1138287783), tensor(1.0076031685)]\n",
            "b:  [tensor(0.6794327497), tensor(0.8892430067), tensor(0.9083099365), tensor(0.9518415332), tensor(1.1345659494), tensor(0.7107707262), tensor(0.9557105303), tensor(0.9152420759), tensor(1.2077751160), tensor(0.8211998343), tensor(0.9560886025), tensor(0.6533838511), tensor(0.9692004323), tensor(0.9032074213), tensor(0.9108500481), tensor(0.9700293541), tensor(0.8366075754), tensor(1.1680434942), tensor(0.9293127656), tensor(0.9168688059), tensor(1.1674340963), tensor(1.1153974533), tensor(1.0805232525)]\n",
            "c:  [tensor(-0.0666862950), tensor(-0.0666862950), tensor(-0.0666862950), tensor(-0.0228634216), tensor(0.0016173853), tensor(-0.0101049906), tensor(0.0085387332), tensor(0.0121401316), tensor(-0.0228634216), tensor(0.0016173853), tensor(-0.0101049906), tensor(0.0085387332), tensor(0.0121401316), tensor(-0.0228634216), tensor(0.0016173853), tensor(-0.0101049906), tensor(0.0085387332), tensor(0.0121401316)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.2521104813, -0.3940509260, -0.4517960548,  0.9927688837,\n",
            "        -4.4572553635,  1.8381807804, -0.2075918913, -0.2089055181,\n",
            "        -3.6414685249,  1.4130146503,  0.8006667495,  2.4349083900,\n",
            "         0.2023809552,  1.0988672972,  0.7558076382, -0.1299387217,\n",
            "        -0.1459832788, -3.5541470051,  0.4352090359,  0.6176385880,\n",
            "        -4.0643334389, -3.9568610191, -3.5097117424])\n",
            "btensor.grad: tensor([-0.1717702746,  0.1556949615,  0.1791188866,  0.0368150473,\n",
            "         0.7036463022, -0.0426719189,  0.1564343870,  0.1597275734,\n",
            "         0.7312112451, -0.0225247145,  0.0465919971, -0.0982070565,\n",
            "         0.1361618638,  0.0128598213,  0.0491571426,  0.1747567058,\n",
            "         0.1346440613,  0.7105016708,  0.0965428352,  0.0863776207,\n",
            "         0.7619785070,  0.6673448682,  0.6399880648])\n",
            "ctensor.grad: tensor([ 2.5663969517,  2.5663969517,  2.5663969517, 14.5775241852,\n",
            "        -0.2648663223, 11.5226697922, -0.5708996058, -0.3141793013,\n",
            "        14.5775241852, -0.2648663223, 11.5226697922, -0.5708996058,\n",
            "        -0.3141793013, 14.5775241852, -0.2648663223, 11.5226697922,\n",
            "        -0.5708996058, -0.3141793013])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2175.9636230469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0144016743), tensor(1.3538918495), tensor(1.3392086029), tensor(1.6651544571), tensor(1.2574772835), tensor(2.2179317474), tensor(1.4214544296), tensor(1.4962712526), tensor(1.0529392958), tensor(1.8292999268), tensor(1.5483027697), tensor(2.6305425167), tensor(1.4213987589), tensor(1.6715458632), tensor(1.6114217043), tensor(1.3990360498), tensor(1.5649570227), tensor(1.0157645941), tensor(1.4992506504), tensor(1.5703986883), tensor(1.1222879887), tensor(1.1176528931), tensor(1.0109828711)]\n",
            "b:  [tensor(0.6796061397), tensor(0.8890979886), tensor(0.9081423283), tensor(0.9518094063), tensor(1.1338920593), tensor(0.7108173966), tensor(0.9555631876), tensor(0.9150906801), tensor(1.2070848942), tensor(0.8212265372), tensor(0.9560475349), tensor(0.6534849405), tensor(0.9690726399), tensor(0.9031989574), tensor(0.9108067155), tensor(0.9698643684), tensor(0.8364810944), tensor(1.1673734188), tensor(0.9292231202), tensor(0.9167889357), tensor(1.1667127609), tensor(1.1147593260), tensor(1.0799146891)]\n",
            "c:  [tensor(-0.0668325052), tensor(-0.0668325052), tensor(-0.0668325052), tensor(-0.0235857889), tensor(0.0016306341), tensor(-0.0106719602), tensor(0.0085707866), tensor(0.0121657010), tensor(-0.0235857889), tensor(0.0016306341), tensor(-0.0106719602), tensor(0.0085707866), tensor(0.0121657010), tensor(-0.0235857889), tensor(0.0016306341), tensor(-0.0106719602), tensor(0.0085707866), tensor(0.0121657010)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.1884913445, -0.3853504956, -0.4345424175,  0.9439269304,\n",
            "        -4.3214974403,  1.7859909534, -0.2053569257, -0.2118278742,\n",
            "        -3.5045802593,  1.3619282246,  0.7614721656,  2.3790893555,\n",
            "         0.1843110919,  1.0520312786,  0.7199251056, -0.1317367554,\n",
            "        -0.1461514831, -3.4167788029,  0.4080350995,  0.5859624147,\n",
            "        -3.9225709438, -3.8241124153, -3.3797116280])\n",
            "btensor.grad: tensor([-0.1733671278,  0.1450189352,  0.1676331908,  0.0321226120,\n",
            "         0.6738513112, -0.0466711521,  0.1473243535,  0.1513854265,\n",
            "         0.6902592182, -0.0267323852,  0.0410745144, -0.1011183858,\n",
            "         0.1277990937,  0.0084406137,  0.0433530807,  0.1649703979,\n",
            "         0.1264872849,  0.6701300144,  0.0896604657,  0.0798506737,\n",
            "         0.7213127613,  0.6380704641,  0.6085418463])\n",
            "ctensor.grad: tensor([ 2.9241354465,  2.9241354465,  2.9241354465, 14.4473400116,\n",
            "        -0.2649754286, 11.3393974304, -0.6410673857, -0.5113909841,\n",
            "        14.4473400116, -0.2649754286, 11.3393974304, -0.6410673857,\n",
            "        -0.5113909841, 14.4473400116, -0.2649754286, 11.3393974304,\n",
            "        -0.6410673857, -0.5113909841])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2175.8017578125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0112750530), tensor(1.3542686701), tensor(1.3396266699), tensor(1.6642570496), tensor(1.2616683245), tensor(2.2161960602), tensor(1.4216573238), tensor(1.4964854717), tensor(1.0563133955), tensor(1.8279869556), tensor(1.5475785732), tensor(2.6282174587), tensor(1.4212313890), tensor(1.6705385447), tensor(1.6107358932), tensor(1.3991692066), tensor(1.5651030540), tensor(1.0190504789), tensor(1.4988683462), tensor(1.5698428154), tensor(1.1260750294), tensor(1.1213500500), tensor(1.0142387152)]\n",
            "b:  [tensor(0.6797809601), tensor(0.8889631629), tensor(0.9079856277), tensor(0.9517817497), tensor(1.1332467794), tensor(0.7108678818), tensor(0.9554245472), tensor(0.9149472713), tensor(1.2064324617), tensor(0.8212572932), tensor(0.9560117126), tensor(0.6535888314), tensor(0.9689528346), tensor(0.9031947851), tensor(0.9107689261), tensor(0.9697087407), tensor(0.8363624215), tensor(1.1667405367), tensor(0.9291400313), tensor(0.9167153239), tensor(1.1660289764), tensor(1.1141492128), tensor(1.0793360472)]\n",
            "c:  [tensor(-0.0669960454), tensor(-0.0669960454), tensor(-0.0669960454), tensor(-0.0243019871), tensor(0.0016438919), tensor(-0.0112301046), tensor(0.0086062932), tensor(0.0122009171), tensor(-0.0243019871), tensor(0.0016438919), tensor(-0.0112301046), tensor(0.0086062932), tensor(0.0122009171), tensor(-0.0243019871), tensor(0.0016438919), tensor(-0.0112301046), tensor(0.0086062932), tensor(0.0122009171)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.1266059875, -0.3768348992, -0.4180202484,  0.8974124193,\n",
            "        -4.1910634041,  1.7356128693, -0.2029518485, -0.2142623663,\n",
            "        -3.3740830421,  1.3129160404,  0.7242198586,  2.3249514103,\n",
            "         0.1673992872,  1.0073194504,  0.6857683659, -0.1331745088,\n",
            "        -0.1459977031, -3.2859051228,  0.3823148608,  0.5559253693,\n",
            "        -3.7870769501, -3.6971383095, -3.2558414936])\n",
            "btensor.grad: tensor([-0.1748404205,  0.1348047256,  0.1567145437,  0.0276605487,\n",
            "         0.6453318596, -0.0504653454,  0.1386470497,  0.1434329748,\n",
            "         0.6524218917, -0.0307762027,  0.0358176231, -0.1038911641,\n",
            "         0.1198319197,  0.0041978359,  0.0377947092,  0.1556069255,\n",
            "         0.1186854690,  0.6328384876,  0.0831072927,  0.0736130476,\n",
            "         0.6837311983,  0.6100539565,  0.5786404610])\n",
            "ctensor.grad: tensor([ 3.2707507610,  3.2707507610,  3.2707507610, 14.3239803314,\n",
            "        -0.2651567459, 11.1628894806, -0.7101427317, -0.7043141723,\n",
            "        14.3239803314, -0.2651567459, 11.1628894806, -0.7101427317,\n",
            "        -0.7043141723, 14.3239803314, -0.2651567459, 11.1628894806,\n",
            "        -0.7101427317, -0.7043141723])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2175.6464843750, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0082087517), tensor(1.3546371460), tensor(1.3400288820), tensor(1.6634039879), tensor(1.2657339573), tensor(2.2145090103), tensor(1.4218577147), tensor(1.4967017174), tensor(1.0595630407), tensor(1.8267210722), tensor(1.5468897820), tensor(2.6259450912), tensor(1.4210798740), tensor(1.6695739031), tensor(1.6100826263), tensor(1.3993034363), tensor(1.5652486086), tensor(1.0222116709), tensor(1.4985103607), tensor(1.5693154335), tensor(1.1297324896), tensor(1.1249256134), tensor(1.0173764229)]\n",
            "b:  [tensor(0.6799572110), tensor(0.8888381720), tensor(0.9078392982), tensor(0.9517583847), tensor(1.1326286793), tensor(0.7109219432), tensor(0.9552941918), tensor(0.9148114324), tensor(1.2058150768), tensor(0.8212919235), tensor(0.9559809566), tensor(0.6536952853), tensor(0.9688405991), tensor(0.9031946063), tensor(0.9107364416), tensor(0.9695620537), tensor(0.8362511992), tensor(1.1661422253), tensor(0.9290632010), tensor(0.9166476727), tensor(1.1653801203), tensor(1.1135659218), tensor(1.0787857771)]\n",
            "c:  [tensor(-0.0671763942), tensor(-0.0671763942), tensor(-0.0671763942), tensor(-0.0250123441), tensor(0.0016571623), tensor(-0.0117797423), tensor(0.0086452048), tensor(0.0122455824), tensor(-0.0250123441), tensor(0.0016571623), tensor(-0.0117797423), tensor(0.0086452048), tensor(0.0122455824), tensor(-0.0250123441), tensor(0.0016571623), tensor(-0.0117797423), tensor(0.0086452048), tensor(0.0122455824)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.0663771629, -0.3685311675, -0.4022140503,  0.8530960083,\n",
            "        -4.0656757355,  1.6869608164, -0.2004210353, -0.2162654996,\n",
            "        -3.2496089935,  1.2658867836,  0.6888134480,  2.2724311352,\n",
            "         0.1515593380,  0.9646209478,  0.6532533169, -0.1342847049,\n",
            "        -0.1455611587, -3.1611468792,  0.3579518795,  0.5274389982,\n",
            "        -3.6574945450, -3.5756032467, -3.1377136707])\n",
            "btensor.grad: tensor([-1.7623989284e-01,  1.2501788139e-01,  1.4631852508e-01,\n",
            "         2.3386061192e-02,  6.1804991961e-01, -5.4058074951e-02,\n",
            "         1.3036751747e-01,  1.3583528996e-01,  6.1732691526e-01,\n",
            "        -3.4630894661e-02,  3.0759572983e-02, -1.0648345947e-01,\n",
            "         1.1222308874e-01,  1.6570091248e-04,  3.2479643822e-02,\n",
            "         1.4671516418e-01,  1.1122945696e-01,  5.9828805923e-01,\n",
            "         7.6816558838e-02,  6.7678451538e-02,  6.4889693260e-01,\n",
            "         5.8327835798e-01,  5.5021321774e-01])\n",
            "ctensor.grad: tensor([ 3.6069059372,  3.6069059372,  3.6069059372, 14.2071495056,\n",
            "        -0.2654083967, 10.9927558899, -0.7782310843, -0.8933030367,\n",
            "        14.2071495056, -0.2654083967, 10.9927558899, -0.7782310843,\n",
            "        -0.8933030367, 14.2071495056, -0.2654083967, 10.9927558899,\n",
            "        -0.7782310843, -0.8933030367])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2175.4997558594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0052011013), tensor(1.3549976349), tensor(1.3404159546), tensor(1.6625931263), tensor(1.2696790695), tensor(2.2128689289), tensor(1.4220554829), tensor(1.4969196320), tensor(1.0626938343), tensor(1.8255003691), tensor(1.5462346077), tensor(2.6237237453), tensor(1.4209431410), tensor(1.6686500311), tensor(1.6094603539), tensor(1.3994385004), tensor(1.5653934479), tensor(1.0252537727), tensor(1.4981755018), tensor(1.5688149929), tensor(1.1332659721), tensor(1.1283848286), tensor(1.0204013586)]\n",
            "b:  [tensor(0.6801347733), tensor(0.8887225389), tensor(0.9077029228), tensor(0.9517390728), tensor(1.1320368052), tensor(0.7109794617), tensor(0.9551717639), tensor(0.9146828651), tensor(1.2052303553), tensor(0.8213302493), tensor(0.9559550881), tensor(0.6538042426), tensor(0.9687356949), tensor(0.9031983614), tensor(0.9107090831), tensor(0.9694238305), tensor(0.8361471295), tensor(1.1655759811), tensor(0.9289923906), tensor(0.9165856838), tensor(1.1647635698), tensor(1.1130082607), tensor(1.0782625675)]\n",
            "c:  [tensor(-0.0673730522), tensor(-0.0673730522), tensor(-0.0673730522), tensor(-0.0257171709), tensor(0.0016704488), tensor(-0.0123211760), tensor(0.0086874766), tensor(0.0122995172), tensor(-0.0257171709), tensor(0.0016704488), tensor(-0.0123211760), tensor(0.0086874766), tensor(0.0122995172), tensor(-0.0257171709), tensor(0.0016704488), tensor(-0.0123211760), tensor(0.0086874766), tensor(0.0122995172)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 3.0077462196, -0.3604459167, -0.3871016502,  0.8108665943,\n",
            "        -3.9450750351,  1.6399646997, -0.1977805495, -0.2178922892,\n",
            "        -3.1308207512,  1.2207454443,  0.6551433206,  2.2214598656,\n",
            "         0.1367153227,  0.9238425493,  0.6222836971, -0.1351199746,\n",
            "        -0.1448789835, -3.0421466827,  0.3348571062,  0.5004109740,\n",
            "        -3.5335011482, -3.4592039585, -3.0249922276])\n",
            "btensor.grad: tensor([-0.1775522232,  0.1156393290,  0.1363731027,  0.0192936659,\n",
            "         0.5919145346, -0.0575052500,  0.1224507689,  0.1285499334,\n",
            "         0.5847103000, -0.0383281708,  0.0258834362, -0.1089747250,\n",
            "         0.1049184799, -0.0037366152,  0.0273528099,  0.1382343769,\n",
            "         0.1040661111,  0.5662102699,  0.0707843900,  0.0620054007,\n",
            "         0.6165472865,  0.5576966405,  0.5231804848])\n",
            "ctensor.grad: tensor([ 3.9331824780,  3.9331824780,  3.9331824780, 14.0965318680,\n",
            "        -0.2657292485, 10.8286762238, -0.8454282880, -1.0786902905,\n",
            "        14.0965318680, -0.2657292485, 10.8286762238, -0.8454282880,\n",
            "        -1.0786902905, 14.0965318680, -0.2657292485, 10.8286762238,\n",
            "        -0.8454282880, -1.0786902905])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2175.3601074219, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(3.0022504330), tensor(1.3553502560), tensor(1.3407886028), tensor(1.6618225574), tensor(1.2735080719), tensor(2.2112743855), tensor(1.4222505093), tensor(1.4971388578), tensor(1.0657112598), tensor(1.8243229389), tensor(1.5456115007), tensor(2.6215517521), tensor(1.4208203554), tensor(1.6677651405), tensor(1.6088675261), tensor(1.3995741606), tensor(1.5655374527), tensor(1.0281823874), tensor(1.4978625774), tensor(1.5683401823), tensor(1.1366807222), tensor(1.1317324638), tensor(1.0233187675)]\n",
            "b:  [tensor(0.6803135276), tensor(0.8886159062), tensor(0.9075760841), tensor(0.9517237544), tensor(1.1314698458), tensor(0.7110402584), tensor(0.9550569057), tensor(0.9145612717), tensor(1.2046760321), tensor(0.8213721514), tensor(0.9559338689), tensor(0.6539155245), tensor(0.9686377645), tensor(0.9032058716), tensor(0.9106866717), tensor(0.9692937136), tensor(0.8360499144), tensor(1.1650396585), tensor(0.9289274216), tensor(0.9165291190), tensor(1.1641771793), tensor(1.1124750376), tensor(1.0777651072)]\n",
            "c:  [tensor(-0.0675855651), tensor(-0.0675855651), tensor(-0.0675855651), tensor(-0.0264167618), tensor(0.0016837547), tensor(-0.0128546897), tensor(0.0087330686), tensor(0.0123625575), tensor(-0.0264167618), tensor(0.0016837547), tensor(-0.0128546897), tensor(0.0087330686), tensor(0.0123625575), tensor(-0.0264167618), tensor(0.0016837547), tensor(-0.0128546897), tensor(0.0087330686), tensor(0.0123625575)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.9506509304, -0.3525809646, -0.3726623058,  0.7706050873,\n",
            "        -3.8290135860,  1.5945577621, -0.1950553954, -0.2191705704,\n",
            "        -3.0174007416,  1.1774160862,  0.6231222749,  2.1719844341,\n",
            "         0.1227987558,  0.8848843575,  0.5927866697, -0.1357004344,\n",
            "        -0.1439741254, -2.9285812378,  0.3129655421,  0.4747533798,\n",
            "        -3.4147880077, -3.3476583958, -2.9173691273])\n",
            "btensor.grad: tensor([-0.1787833571,  0.1066443920,  0.1268492341,  0.0153369308,\n",
            "         0.5669206381, -0.0607713461,  0.1148824990,  0.1215782166,\n",
            "         0.5543348789, -0.0419047475,  0.0211896896, -0.1113086939,\n",
            "         0.0979495645, -0.0074865818,  0.0224343538,  0.1301274300,\n",
            "         0.0972001478,  0.5363504887,  0.0649798512,  0.0565545559,\n",
            "         0.5864090323,  0.5332592726,  0.4974648058])\n",
            "ctensor.grad: tensor([ 4.2501916885,  4.2501916885,  4.2501916885, 13.9918336868,\n",
            "        -0.2661181986, 10.6702718735, -0.9118410945, -1.2608002424,\n",
            "        13.9918336868, -0.2661181986, 10.6702718735, -0.9118410945,\n",
            "        -1.2608002424, 13.9918336868, -0.2661181986, 10.6702718735,\n",
            "        -0.9118410945, -1.2608002424])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2175.2248535156, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9993553162), tensor(1.3556952477), tensor(1.3411474228), tensor(1.6610903740), tensor(1.2772253752), tensor(2.2097237110), tensor(1.4224427938), tensor(1.4973590374), tensor(1.0686203241), tensor(1.8231871128), tensor(1.5450187922), tensor(2.6194279194), tensor(1.4207105637), tensor(1.6669174433), tensor(1.6083028316), tensor(1.3997101784), tensor(1.5656802654), tensor(1.0310025215), tensor(1.4975703955), tensor(1.5678898096), tensor(1.1399817467), tensor(1.1349731684), tensor(1.0261332989)]\n",
            "b:  [tensor(0.6804934740), tensor(0.8885179162), tensor(0.9074583054), tensor(0.9517121911), tensor(1.1309268475), tensor(0.7111041546), tensor(0.9549492598), tensor(0.9144463539), tensor(1.2041500807), tensor(0.8214175105), tensor(0.9559171796), tensor(0.6540290713), tensor(0.9685465097), tensor(0.9032169580), tensor(0.9106689692), tensor(0.9691713452), tensor(0.8359593153), tensor(1.1645311117), tensor(0.9288679957), tensor(0.9164777994), tensor(1.1636189222), tensor(1.1119651794), tensor(1.0772920847)]\n",
            "c:  [tensor(-0.0678134859), tensor(-0.0678134859), tensor(-0.0678134859), tensor(-0.0271113999), tensor(0.0016970835), tensor(-0.0133805517), tensor(0.0087819463), tensor(0.0124345534), tensor(-0.0271113999), tensor(0.0016970835), tensor(-0.0133805517), tensor(0.0087819463), tensor(0.0124345534), tensor(-0.0271113999), tensor(0.0016970835), tensor(-0.0133805517), tensor(0.0087819463), tensor(0.0124345534)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.8950355053, -0.3449482322, -0.3588566780,  0.7322274446,\n",
            "        -3.7172791958,  1.5506794453, -0.1922655702, -0.2201430202,\n",
            "        -2.9090468884,  1.1358187199,  0.5926660299,  2.1239314079,\n",
            "         0.1097438186,  0.8476697206,  0.5646802187, -0.1360646188,\n",
            "        -0.1428661942, -2.8201379776,  0.2922018170,  0.4503995478,\n",
            "        -3.3010725975, -3.2407128811, -2.8145411015])\n",
            "btensor.grad: tensor([-0.1799526364,  0.0979849100,  0.1177610457,  0.0115491748,\n",
            "         0.5429832935, -0.0638988018,  0.1076234877,  0.1149114370,\n",
            "         0.5259845853, -0.0453518033,  0.0166640282, -0.1135569811,\n",
            "         0.0912539959, -0.0110968351,  0.0176850557,  0.1223646402,\n",
            "         0.0905961618,  0.5085141659,  0.0594226122,  0.0513131618,\n",
            "         0.5582891107,  0.5099128485,  0.4730070233])\n",
            "ctensor.grad: tensor([ 4.5584478378,  4.5584478378,  4.5584478378, 13.8927783966,\n",
            "        -0.2665736675, 10.5172414780, -0.9775487781, -1.4399192333,\n",
            "        13.8927783966, -0.2665736675, 10.5172414780, -0.9775487781,\n",
            "        -1.4399192333, 13.8927783966, -0.2665736675, 10.5172414780,\n",
            "        -0.9775487781, -1.4399192333])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2175.0983886719, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9965145588), tensor(1.3560328484), tensor(1.3414931297), tensor(1.6603947878), tensor(1.2808350325), tensor(2.2082154751), tensor(1.4226322174), tensor(1.4975799322), tensor(1.0714257956), tensor(1.8220912218), tensor(1.5444550514), tensor(2.6173505783), tensor(1.4206130505), tensor(1.6661053896), tensor(1.6077649593), tensor(1.3998464346), tensor(1.5658218861), tensor(1.0337190628), tensor(1.4972978830), tensor(1.5674625635), tensor(1.1431738138), tensor(1.1381112337), tensor(1.0288496017)]\n",
            "b:  [tensor(0.6806745529), tensor(0.8884282708), tensor(0.9073492885), tensor(0.9517043233), tensor(1.1304067373), tensor(0.7111710906), tensor(0.9548485875), tensor(0.9143378735), tensor(1.2036505938), tensor(0.8214662075), tensor(0.9559049010), tensor(0.6541447639), tensor(0.9684616923), tensor(0.9032315612), tensor(0.9106558561), tensor(0.9690564275), tensor(0.8358750343), tensor(1.1640486717), tensor(0.9288139343), tensor(0.9164314866), tensor(1.1630870104), tensor(1.1114776134), tensor(1.0768423080)]\n",
            "c:  [tensor(-0.0680564120), tensor(-0.0680564120), tensor(-0.0680564120), tensor(-0.0278013553), tensor(0.0017104382), tensor(-0.0138990181), tensor(0.0088340780), tensor(0.0125153689), tensor(-0.0278013553), tensor(0.0017104382), tensor(-0.0138990181), tensor(0.0088340780), tensor(0.0125153689), tensor(-0.0278013553), tensor(0.0017104382), tensor(-0.0138990181), tensor(0.0088340780), tensor(0.0125153689)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.8408434391, -0.3375512958, -0.3456799984,  0.6956281662,\n",
            "        -3.6096558571,  1.5082653761, -0.1894359589, -0.2208513618,\n",
            "        -2.8054878712,  1.0958752632,  0.5636870861,  2.0772624016,\n",
            "         0.0974917859,  0.8121044636,  0.5378921032, -0.1362308860,\n",
            "        -0.1415949464, -2.7165462971,  0.2724972963,  0.4272832870,\n",
            "        -3.1920936108, -3.1381187439, -2.7162516117])\n",
            "btensor.grad: tensor([-0.1810595542,  0.0896536112,  0.1090344489,  0.0078677535,\n",
            "         0.5200590491, -0.0669102669,  0.1006486416,  0.1085032225,\n",
            "         0.4994575679, -0.0486896634,  0.0122649670, -0.1156976521,\n",
            "         0.0848188400, -0.0146079063,  0.0130856037,  0.1149329543,\n",
            "         0.0842700601,  0.4824640751,  0.0540680289,  0.0462977886,\n",
            "         0.5319713354,  0.4876074791,  0.4497298896])\n",
            "ctensor.grad: tensor([ 4.8584685326,  4.8584685326,  4.8584685326, 13.7991189957,\n",
            "        -0.2670945525, 10.3693189621, -1.0426371098, -1.6163116693,\n",
            "        13.7991189957, -0.2670945525, 10.3693189621, -1.0426371098,\n",
            "        -1.6163116693, 13.7991189957, -0.2670945525, 10.3693189621,\n",
            "        -1.0426371098, -1.6163116693])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.9738769531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9937264919), tensor(1.3563632965), tensor(1.3418262005), tensor(1.6597341299), tensor(1.2843409777), tensor(2.2067482471), tensor(1.4228187799), tensor(1.4978013039), tensor(1.0741323233), tensor(1.8210337162), tensor(1.5439189672), tensor(2.6153187752), tensor(1.4205271006), tensor(1.6653273106), tensor(1.6072525978), tensor(1.3999826908), tensor(1.5659620762), tensor(1.0363365412), tensor(1.4970440865), tensor(1.5670572519), tensor(1.1462614536), tensor(1.1411508322), tensor(1.0314718485)]\n",
            "b:  [tensor(0.6808566451), tensor(0.8883466125), tensor(0.9072486162), tensor(0.9517000318), tensor(1.1299086809), tensor(0.7112408876), tensor(0.9547546506), tensor(0.9142354727), tensor(1.2031760216), tensor(0.8215181231), tensor(0.9558969140), tensor(0.6542624831), tensor(0.9683830738), tensor(0.9032495618), tensor(0.9106472135), tensor(0.9689486027), tensor(0.8357968926), tensor(1.1635905504), tensor(0.9287650585), tensor(0.9163900614), tensor(1.1625796556), tensor(1.1110112667), tensor(1.0764147043)]\n",
            "c:  [tensor(-0.0683139488), tensor(-0.0683139488), tensor(-0.0683139488), tensor(-0.0284868870), tensor(0.0017238222), tensor(-0.0144103291), tensor(0.0088894367), tensor(0.0126048801), tensor(-0.0284868870), tensor(0.0017238222), tensor(-0.0144103291), tensor(0.0088894367), tensor(0.0126048801), tensor(-0.0284868870), tensor(0.0017238222), tensor(-0.0144103291), tensor(0.0088894367), tensor(0.0126048801)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.7880363464, -0.3303965032, -0.3330881596,  0.6607167721,\n",
            "        -3.5059475899,  1.4672626257, -0.1865773201, -0.2213158607,\n",
            "        -2.7064695358,  1.0575190783,  0.5361098647,  2.0319180489,\n",
            "         0.0859898105,  0.7781227827,  0.5123628378, -0.1362297535,\n",
            "        -0.1401603222, -2.6175296307,  0.2537896633,  0.4053309858,\n",
            "        -3.0876042843, -3.0396564007, -2.6222467422])\n",
            "btensor.grad: tensor([-0.1820743084,  0.0816351175,  0.1006883234,  0.0043138862,\n",
            "         0.4980889559, -0.0697906017,  0.0939481258,  0.1023739576,\n",
            "         0.4745846391, -0.0519264340,  0.0080161095, -0.1177174449,\n",
            "         0.0786131024, -0.0179948807,  0.0086395741,  0.1078191400,\n",
            "         0.0781699345,  0.4580724239,  0.0488983393,  0.0414516926,\n",
            "         0.5073007345,  0.4662874043,  0.4275559187])\n",
            "ctensor.grad: tensor([ 5.1506962776,  5.1506962776,  5.1506962776, 13.7106361389,\n",
            "        -0.2676797211, 10.2262125015, -1.1071841717, -1.7902346849,\n",
            "        13.7106361389, -0.2676797211, 10.2262125015, -1.1071841717,\n",
            "        -1.7902346849, 13.7106361389, -0.2676797211, 10.2262125015,\n",
            "        -1.1071841717, -1.7902346849])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.8566894531, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9909899235), tensor(1.3566868305), tensor(1.3421472311), tensor(1.6591067314), tensor(1.2877469063), tensor(2.2053205967), tensor(1.4230024815), tensor(1.4980229139), tensor(1.0767440796), tensor(1.8200130463), tensor(1.5434091091), tensor(2.6133308411), tensor(1.4204518795), tensor(1.6645816565), tensor(1.6067645550), tensor(1.4001188278), tensor(1.5661007166), tensor(1.0388593674), tensor(1.4968080521), tensor(1.5666728020), tensor(1.1492488384), tensor(1.1440958977), tensor(1.0340040922)]\n",
            "b:  [tensor(0.6810396910), tensor(0.8882727623), tensor(0.9071559906), tensor(0.9516991377), tensor(1.1294316053), tensor(0.7113134861), tensor(0.9546671510), tensor(0.9141390324), tensor(1.2027248144), tensor(0.8215731978), tensor(0.9558929801), tensor(0.6543821692), tensor(0.9683104157), tensor(0.9032708406), tensor(0.9106428623), tensor(0.9688476324), tensor(0.8357245922), tensor(1.1631554365), tensor(0.9287211299), tensor(0.9163532853), tensor(1.1620955467), tensor(1.1105654240), tensor(1.0760083199)]\n",
            "c:  [tensor(-0.0685857311), tensor(-0.0685857311), tensor(-0.0685857311), tensor(-0.0291682407), tensor(0.0017372386), tensor(-0.0149147110), tensor(0.0089479992), tensor(0.0127029764), tensor(-0.0291682407), tensor(0.0017372386), tensor(-0.0149147110), tensor(0.0089479992), tensor(0.0127029764), tensor(-0.0291682407), tensor(0.0017372386), tensor(-0.0149147110), tensor(0.0089479992), tensor(0.0127029764)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.7365550995, -0.3234804273, -0.3210761547,  0.6274166107,\n",
            "        -3.4059667587,  1.4276189804, -0.1837058663, -0.2215797901,\n",
            "        -2.6117486954,  1.0206830502,  0.5098737478,  1.9878587723,\n",
            "         0.0751911700,  0.7456486225,  0.4880211949, -0.1360832751,\n",
            "        -0.1385905743, -2.5228531361,  0.2360145897,  0.3844802678,\n",
            "        -2.9873752594, -2.9451220036, -2.5322988033])\n",
            "btensor.grad: tensor([-0.1830696017,  0.0738769770,  0.0926516205,  0.0008723736,\n",
            "         0.4770563543, -0.0725760460,  0.0875271559,  0.0964393616,\n",
            "         0.4512178600, -0.0550875068,  0.0039074421, -0.1196798384,\n",
            "         0.0726562142, -0.0213071108,  0.0043272972,  0.1009772420,\n",
            "         0.0722865984,  0.4351632595,  0.0439094305,  0.0367866755,\n",
            "         0.4841006100,  0.4458853602,  0.4064302146])\n",
            "ctensor.grad: tensor([ 5.4355778694,  5.4355778694,  5.4355778694, 13.6270847321,\n",
            "        -0.2683281004, 10.0876436234, -1.1712589264, -1.9619174004,\n",
            "        13.6270847321, -0.2683281004, 10.0876436234, -1.1712589264,\n",
            "        -1.9619174004, 13.6270847321, -0.2683281004, 10.0876436234,\n",
            "        -1.1712589264, -1.9619174004])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.7416992188, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9883036613), tensor(1.3570036888), tensor(1.3424568176), tensor(1.6585110426), tensor(1.2910565138), tensor(2.2039313316), tensor(1.4231833220), tensor(1.4982445240), tensor(1.0792652369), tensor(1.8190277815), tensor(1.5429241657), tensor(2.6113858223), tensor(1.4203867912), tensor(1.6638669968), tensor(1.6062997580), tensor(1.4002546072), tensor(1.5662375689), tensor(1.0412915945), tensor(1.4965889454), tensor(1.5663081408), tensor(1.1521400213), tensor(1.1469502449), tensor(1.0364502668)]\n",
            "b:  [tensor(0.6812237501), tensor(0.8882063627), tensor(0.9070710540), tensor(0.9517016411), tensor(1.1289746761), tensor(0.7113887668), tensor(0.9545857906), tensor(0.9140482545), tensor(1.2022955418), tensor(0.8216313720), tensor(0.9558930993), tensor(0.6545037627), tensor(0.9682435393), tensor(0.9032953978), tensor(0.9106427431), tensor(0.9687532187), tensor(0.8356579542), tensor(1.1627417803), tensor(0.9286820889), tensor(0.9163209796), tensor(1.1616332531), tensor(1.1101390123), tensor(1.0756220818)]\n",
            "c:  [tensor(-0.0688714087), tensor(-0.0688714087), tensor(-0.0688714087), tensor(-0.0298456550), tensor(0.0017506905), tensor(-0.0154123809), tensor(0.0090097459), tensor(0.0128095550), tensor(-0.0298456550), tensor(0.0017506905), tensor(-0.0154123809), tensor(0.0090097459), tensor(0.0128095550), tensor(-0.0298456550), tensor(0.0017506905), tensor(-0.0154123809), tensor(0.0090097459), tensor(0.0128095550)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.6863625050, -0.3168078959, -0.3096072674,  0.5956423283,\n",
            "        -3.3095486164,  1.3892828226, -0.1808310151, -0.2216550708,\n",
            "        -2.5211093426,  0.9853084087,  0.4849008322,  1.9450328350,\n",
            "         0.0650386959,  0.7146050930,  0.4648095369, -0.1358040571,\n",
            "        -0.1369065046, -2.4322843552,  0.2191282511,  0.3646780550,\n",
            "        -2.8911926746, -2.8543093204, -2.4461956024])\n",
            "btensor.grad: tensor([-1.8407580256e-01,  6.6403746605e-02,  8.4940716624e-02,\n",
            "        -2.4862289429e-03,  4.5687919855e-01, -7.5263738632e-02,\n",
            "         8.1339240074e-02,  9.0756177902e-02,  4.2921823263e-01,\n",
            "        -5.8176636696e-02, -1.1706352234e-04, -1.2157618999e-01,\n",
            "         6.6899955273e-02, -2.4532794952e-02,  1.2218952179e-04,\n",
            "         9.4390869141e-02,  6.6613398492e-02,  4.1362166405e-01,\n",
            "         3.9050817490e-02,  3.2276630402e-02,  4.6227091551e-01,\n",
            "         4.2639139295e-01,  3.8628938794e-01])\n",
            "ctensor.grad: tensor([ 5.7135133743,  5.7135133743,  5.7135133743, 13.5483007431,\n",
            "        -0.2690387070,  9.9533891678, -1.2349307537, -2.1315734386,\n",
            "        13.5483007431, -0.2690387070,  9.9533891678, -1.2349307537,\n",
            "        -2.1315734386, 13.5483007431, -0.2690387070,  9.9533891678,\n",
            "        -1.2349307537, -2.1315734386])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.6325683594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9856662750), tensor(1.3573141098), tensor(1.3427554369), tensor(1.6579457521), tensor(1.2942730188), tensor(2.2025790215), tensor(1.4233613014), tensor(1.4984661341), tensor(1.0816996098), tensor(1.8180764914), tensor(1.5424630642), tensor(2.6094825268), tensor(1.4203312397), tensor(1.6631820202), tensor(1.6058571339), tensor(1.4003900290), tensor(1.5663726330), tensor(1.0436371565), tensor(1.4963858128), tensor(1.5659623146), tensor(1.1549389362), tensor(1.1497173309), tensor(1.0388139486)]\n",
            "b:  [tensor(0.6814087629), tensor(0.8881472349), tensor(0.9069935679), tensor(0.9517074227), tensor(1.1285371780), tensor(0.7114666700), tensor(0.9545104504), tensor(0.9139630198), tensor(1.2018871307), tensor(0.8216925859), tensor(0.9558971524), tensor(0.6546271443), tensor(0.9681822062), tensor(0.9033230543), tensor(0.9106466770), tensor(0.9686651826), tensor(0.8355968595), tensor(1.1623485088), tensor(0.9286477566), tensor(0.9162930846), tensor(1.1611915827), tensor(1.1097313166), tensor(1.0752550364)]\n",
            "c:  [tensor(-0.0691706538), tensor(-0.0691706538), tensor(-0.0691706538), tensor(-0.0305193570), tensor(0.0017641811), tensor(-0.0159035437), tensor(0.0090746591), tensor(0.0129245250), tensor(-0.0305193570), tensor(0.0017641811), tensor(-0.0159035437), tensor(0.0090746591), tensor(0.0129245250), tensor(-0.0305193570), tensor(0.0017641811), tensor(-0.0159035437), tensor(0.0090746591), tensor(0.0129245250)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.6374187469, -0.3103824854, -0.2986719608,  0.5653167963,\n",
            "        -3.2165250778,  1.3522043228, -0.1779637337, -0.2215744853,\n",
            "        -2.4343373775,  0.9513319731,  0.4611268044,  1.9034088850,\n",
            "         0.0554972142,  0.6849323511,  0.4426787794, -0.1354216039,\n",
            "        -0.1351146698, -2.3456089497,  0.2030770481,  0.3458592594,\n",
            "        -2.7988572121, -2.7670400143, -2.3637332916])\n",
            "btensor.grad: tensor([-0.1849979907,  0.0591574907,  0.0775011554, -0.0057669878,\n",
            "         0.4375165105, -0.0778892040,  0.0753654838,  0.0852593184,\n",
            "         0.4084619284, -0.0612006783, -0.0040440559, -0.1233876050,\n",
            "         0.0613505244, -0.0276554823, -0.0039466619,  0.0880579352,\n",
            "         0.0611102208,  0.3933093548,  0.0343619585,  0.0278996229,\n",
            "         0.4416761398,  0.4077259600,  0.3670842648])\n",
            "ctensor.grad: tensor([ 5.9849023819,  5.9849023819,  5.9849023819, 13.4740457535,\n",
            "        -0.2698103189,  9.8232460022, -1.2982636690, -2.2994060516,\n",
            "        13.4740457535, -0.2698103189,  9.8232460022, -1.2982636690,\n",
            "        -2.2994060516, 13.4740457535, -0.2698103189,  9.8232460022,\n",
            "        -1.2982636690, -2.2994060516])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.5256347656, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9830765724), tensor(1.3576183319), tensor(1.3430436850), tensor(1.6574094296), tensor(1.2973997593), tensor(2.2012627125), tensor(1.4235364199), tensor(1.4986875057), tensor(1.0840508938), tensor(1.8171577454), tensor(1.5420246124), tensor(2.6076195240), tensor(1.4202847481), tensor(1.6625254154), tensor(1.6054356098), tensor(1.4005249739), tensor(1.5665059090), tensor(1.0458997488), tensor(1.4961980581), tensor(1.5656343699), tensor(1.1576491594), tensor(1.1524004936), tensor(1.0410987139)]\n",
            "b:  [tensor(0.6815946698), tensor(0.8880950809), tensor(0.9069231749), tensor(0.9517163634), tensor(1.1281182766), tensor(0.7115470767), tensor(0.9544408321), tensor(0.9138830304), tensor(1.2014982700), tensor(0.8217567205), tensor(0.9559050202), tensor(0.6547523141), tensor(0.9681262374), tensor(0.9033538103), tensor(0.9106546044), tensor(0.9685831666), tensor(0.8355410695), tensor(1.1619743109), tensor(0.9286179543), tensor(0.9162694216), tensor(1.1607693434), tensor(1.1093413830), tensor(1.0749063492)]\n",
            "c:  [tensor(-0.0694831610), tensor(-0.0694831610), tensor(-0.0694831610), tensor(-0.0311895646), tensor(0.0017777132), tensor(-0.0163883921), tensor(0.0091427248), tensor(0.0130478060), tensor(-0.0311895646), tensor(0.0017777132), tensor(-0.0163883921), tensor(0.0091427248), tensor(0.0130478060), tensor(-0.0311895646), tensor(0.0017777132), tensor(-0.0163883921), tensor(0.0091427248), tensor(0.0130478060)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.5896902084, -0.3041888475, -0.2882435322,  0.5363816023,\n",
            "        -3.1267452240,  1.3163423538, -0.1751081944, -0.2213439345,\n",
            "        -2.3512339592,  0.9187057018,  0.4384971261,  1.8629420996,\n",
            "         0.0465258062,  0.6565729380,  0.4215738177, -0.1349333823,\n",
            "        -0.1332355738, -2.2626218796,  0.1878106147,  0.3279903829,\n",
            "        -2.7101743221, -2.6831438541, -2.2847282887])\n",
            "btensor.grad: tensor([-0.1858906746,  0.0521731377,  0.0703715384, -0.0089508891,\n",
            "         0.4189419746, -0.0804208517,  0.0696227849,  0.0799945593,\n",
            "         0.3888822198, -0.0641458631, -0.0078675747, -0.1251581311,\n",
            "         0.0559855700, -0.0307279825, -0.0079317093,  0.0819906592,\n",
            "         0.0558173135,  0.3741395473,  0.0298298001,  0.0236645937,\n",
            "         0.4222159386,  0.3898742199,  0.3487383723])\n",
            "ctensor.grad: tensor([ 6.2501029968,  6.2501029968,  6.2501029968, 13.4041490555,\n",
            "        -0.2706422508,  9.6969556808, -1.3613233566, -2.4656217098,\n",
            "        13.4041490555, -0.2706422508,  9.6969556808, -1.3613233566,\n",
            "        -2.4656217098, 13.4041490555, -0.2706422508,  9.6969556808,\n",
            "        -1.3613233566, -2.4656217098])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.4235839844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9805333614), tensor(1.3579165936), tensor(1.3433220387), tensor(1.6569006443), tensor(1.3004398346), tensor(2.1999809742), tensor(1.4237086773), tensor(1.4989085197), tensor(1.0863225460), tensor(1.8162703514), tensor(1.5416076183), tensor(2.6057958603), tensor(1.4202467203), tensor(1.6618959904), tensor(1.6050341129), tensor(1.4006593227), tensor(1.5666371584), tensor(1.0480829477), tensor(1.4960247278), tensor(1.5653233528), tensor(1.1602741480), tensor(1.1550029516), tensor(1.0433076620)]\n",
            "b:  [tensor(0.6817814112), tensor(0.8880497217), tensor(0.9068597555), tensor(0.9517284632), tensor(1.1277171373), tensor(0.7116299868), tensor(0.9543767571), tensor(0.9138081670), tensor(1.2011278868), tensor(0.8218237758), tensor(0.9559166431), tensor(0.6548792124), tensor(0.9680754542), tensor(0.9033875465), tensor(0.9106664062), tensor(0.9685071111), tensor(0.8354904056), tensor(1.1616183519), tensor(0.9285925627), tensor(0.9162498713), tensor(1.1603655815), tensor(1.1089686155), tensor(1.0745751858)]\n",
            "c:  [tensor(-0.0698086321), tensor(-0.0698086321), tensor(-0.0698086321), tensor(-0.0318564884), tensor(0.0017912899), tensor(-0.0168671086), tensor(0.0092139328), tensor(0.0131793246), tensor(-0.0318564884), tensor(0.0017912899), tensor(-0.0168671086), tensor(0.0092139328), tensor(0.0131793246), tensor(-0.0318564884), tensor(0.0017912899), tensor(-0.0168671086), tensor(0.0092139328), tensor(0.0131793246)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.5431327820, -0.2982374132, -0.2782990932,  0.5087566376,\n",
            "        -3.0400695801,  1.2816555500, -0.1722812951, -0.2209972739,\n",
            "        -2.2716321945,  0.8873636127,  0.4169551730,  1.8235905170,\n",
            "         0.0380836502,  0.6294678450,  0.4014391303, -0.1343722641,\n",
            "        -0.1312703490, -2.1831448078,  0.1732892692,  0.3110100329,\n",
            "        -2.6249773502, -2.6024696827, -2.2090036869])\n",
            "btensor.grad: tensor([-0.1867620945,  0.0453774929,  0.0634450614, -0.0120775104,\n",
            "         0.4011117220, -0.0829049349,  0.0640646815,  0.0748844147,\n",
            "         0.3703442216, -0.0670685768, -0.0116004944, -0.1268762648,\n",
            "         0.0507784486, -0.0337209702, -0.0118261576,  0.0760850310,\n",
            "         0.0506885871,  0.3560037613,  0.0254157782,  0.0195494890,\n",
            "         0.4037992060,  0.3727509379,  0.3312152922])\n",
            "ctensor.grad: tensor([ 6.5094523430,  6.5094523430,  6.5094523430, 13.3384580612,\n",
            "        -0.2715335786,  9.5743303299, -1.4241564274, -2.6303794384,\n",
            "        13.3384580612, -0.2715335786,  9.5743303299, -1.4241564274,\n",
            "        -2.6303794384, 13.3384580612, -0.2715335786,  9.5743303299,\n",
            "        -1.4241564274, -2.6303794384])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.3242187500, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9780356884), tensor(1.3582091331), tensor(1.3435908556), tensor(1.6564182043), tensor(1.3033962250), tensor(2.1987328529), tensor(1.4238781929), tensor(1.4991290569), tensor(1.0885179043), tensor(1.8154131174), tensor(1.5412111282), tensor(2.6040105820), tensor(1.4202165604), tensor(1.6612924337), tensor(1.6046519279), tensor(1.4007930756), tensor(1.5667663813), tensor(1.0501899719), tensor(1.4958652258), tensor(1.5650284290), tensor(1.1628172398), tensor(1.1575278044), tensor(1.0454440117)]\n",
            "b:  [tensor(0.6819690466), tensor(0.8880109191), tensor(0.9068029523), tensor(0.9517436028), tensor(1.1273331642), tensor(0.7117153406), tensor(0.9543181062), tensor(0.9137381911), tensor(1.2007751465), tensor(0.8218937516), tensor(0.9559319019), tensor(0.6550077796), tensor(0.9680297375), tensor(0.9034242034), tensor(0.9106820822), tensor(0.9684367180), tensor(0.8354446888), tensor(1.1612795591), tensor(0.9285714626), tensor(0.9162343144), tensor(1.1599792242), tensor(1.1086122990), tensor(1.0742607117)]\n",
            "c:  [tensor(-0.0701467916), tensor(-0.0701467916), tensor(-0.0701467916), tensor(-0.0325203277), tensor(0.0018049141), tensor(-0.0173398703), tensor(0.0092882738), tensor(0.0133190164), tensor(-0.0325203277), tensor(0.0018049141), tensor(-0.0173398703), tensor(0.0092882738), tensor(0.0133190164), tensor(-0.0325203277), tensor(0.0018049141), tensor(-0.0173398703), tensor(0.0092882738), tensor(0.0133190164)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.4977135658, -0.2925239801, -0.2688210011,  0.4823808670,\n",
            "        -2.9563655853,  1.2481012344, -0.1694901884, -0.2205414772,\n",
            "        -2.1953489780,  0.8572618961,  0.3964423537,  1.7853240967,\n",
            "         0.0301412269,  0.6035572290,  0.3822287321, -0.1337404847,\n",
            "        -0.1292390227, -2.1069908142,  0.1594627649,  0.2948715985,\n",
            "        -2.5430991650, -2.5248627663, -2.1364080906])\n",
            "btensor.grad: tensor([-0.1876618117,  0.0387730598,  0.0567797460, -0.0151498914,\n",
            "         0.3839421570, -0.0853246450,  0.0586768985,  0.0699586868,\n",
            "         0.3527652323, -0.0699485540, -0.0152757168, -0.1285404265,\n",
            "         0.0457264185, -0.0366848707, -0.0156559944,  0.0704082847,\n",
            "         0.0457120091,  0.3388454914,  0.0211049914,  0.0155427456,\n",
            "         0.3863300383,  0.3563456237,  0.3144516945])\n",
            "ctensor.grad: tensor([ 6.7632470131,  6.7632470131,  6.7632470131, 13.2767820358,\n",
            "        -0.2724830806,  9.4552202225, -1.4868232012, -2.7938387394,\n",
            "        13.2767820358, -0.2724830806,  9.4552202225, -1.4868232012,\n",
            "        -2.7938387394, 13.2767820358, -0.2724830806,  9.4552202225,\n",
            "        -1.4868232012, -2.7938387394])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.2282714844, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9755823612), tensor(1.3584961891), tensor(1.3438506126), tensor(1.6559610367), tensor(1.3062716722), tensor(2.1975171566), tensor(1.4240449667), tensor(1.4993491173), tensor(1.0906401873), tensor(1.8145847321), tensor(1.5408341885), tensor(2.6022624969), tensor(1.4201939106), tensor(1.6607136726), tensor(1.6042879820), tensor(1.4009261131), tensor(1.5668935776), tensor(1.0522239208), tensor(1.4957189560), tensor(1.5647488832), tensor(1.1652816534), tensor(1.1599780321), tensor(1.0475107431)]\n",
            "b:  [tensor(0.6821575761), tensor(0.8879785538), tensor(0.9067526460), tensor(0.9517617822), tensor(1.1269657612), tensor(0.7118030190), tensor(0.9542646408), tensor(0.9136730433), tensor(1.2004390955), tensor(0.8219665289), tensor(0.9559507966), tensor(0.6551379561), tensor(0.9679889083), tensor(0.9034637809), tensor(0.9107014537), tensor(0.9683718085), tensor(0.8354038000), tensor(1.1609569788), tensor(0.9285545349), tensor(0.9162226915), tensor(1.1596094370), tensor(1.1082717180), tensor(1.0739623308)]\n",
            "c:  [tensor(-0.0704973862), tensor(-0.0704973862), tensor(-0.0704973862), tensor(-0.0331812799), tensor(0.0018185886), tensor(-0.0178068392), tensor(0.0093657421), tensor(0.0134668248), tensor(-0.0331812799), tensor(0.0018185886), tensor(-0.0178068392), tensor(0.0093657421), tensor(0.0134668248), tensor(-0.0331812799), tensor(0.0018185886), tensor(-0.0178068392), tensor(0.0093657421), tensor(0.0134668248)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.4534044266, -0.2870486081, -0.2597954273,  0.4571970701,\n",
            "        -2.8754975796,  1.2156465054, -0.1667315066, -0.2200008035,\n",
            "        -2.1222250462,  0.8283612132,  0.3769068718,  1.7481148243,\n",
            "         0.0226607248,  0.5787900686,  0.3639088869, -0.1330494285,\n",
            "        -0.1271534562, -2.0340049267,  0.1462962478,  0.2795386612,\n",
            "        -2.4643859863, -2.4501805305, -2.0667784214])\n",
            "btensor.grad: tensor([-0.1885329783,  0.0323481560,  0.0503146574, -0.0181686878,\n",
            "         0.3674442172, -0.0876860619,  0.0534860194,  0.0651718378,\n",
            "         0.3360886574, -0.0727790594, -0.0188965797, -0.1301789880,\n",
            "         0.0408179760, -0.0396054983, -0.0193713903,  0.0649355054,\n",
            "         0.0408777222,  0.3225631714,  0.0169131160,  0.0116391182,\n",
            "         0.3697409630,  0.3406238258,  0.2983984351])\n",
            "ctensor.grad: tensor([ 7.0118179321,  7.0118179321,  7.0118179321, 13.2190065384,\n",
            "        -0.2734905481,  9.3393831253, -1.5493720770, -2.9561741352,\n",
            "        13.2190065384, -0.2734905481,  9.3393831253, -1.5493720770,\n",
            "        -2.9561741352, 13.2190065384, -0.2734905481,  9.3393831253,\n",
            "        -1.5493720770, -2.9561741352])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.1347656250, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9731721878), tensor(1.3587779999), tensor(1.3441017866), tensor(1.6555278301), tensor(1.3090690374), tensor(2.1963329315), tensor(1.4242089987), tensor(1.4995684624), tensor(1.0926922560), tensor(1.8137841225), tensor(1.5404758453), tensor(2.6005506516), tensor(1.4201782942), tensor(1.6601585150), tensor(1.6039415598), tensor(1.4010584354), tensor(1.5670186281), tensor(1.0541880131), tensor(1.4955852032), tensor(1.5644838810), tensor(1.1676703691), tensor(1.1623563766), tensor(1.0495107174)]\n",
            "b:  [tensor(0.6823469400), tensor(0.8879524469), tensor(0.9067085385), tensor(0.9517829418), tensor(1.1266142130), tensor(0.7118930817), tensor(0.9542161822), tensor(0.9136124849), tensor(1.2001188993), tensor(0.8220421076), tensor(0.9559732676), tensor(0.6552697420), tensor(0.9679528475), tensor(0.9035062790), tensor(0.9107245207), tensor(0.9683122039), tensor(0.8353676200), tensor(1.1606498957), tensor(0.9285417199), tensor(0.9162148237), tensor(1.1592555046), tensor(1.1079461575), tensor(1.0736793280)]\n",
            "c:  [tensor(-0.0708601549), tensor(-0.0708601549), tensor(-0.0708601549), tensor(-0.0338395275), tensor(0.0018323164), tensor(-0.0182681736), tensor(0.0094463341), tensor(0.0136227012), tensor(-0.0338395275), tensor(0.0018323164), tensor(-0.0182681736), tensor(0.0094463341), tensor(0.0136227012), tensor(-0.0338395275), tensor(0.0018323164), tensor(-0.0182681736), tensor(0.0094463341), tensor(0.0136227012)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.4101758003, -0.2818017602, -0.2511997223,  0.4331544638,\n",
            "        -2.7973530293,  1.1842465401, -0.1640147865, -0.2193781734,\n",
            "        -2.0521118641,  0.8005994558,  0.3583071828,  1.7119210958,\n",
            "         0.0156128332,  0.5551191568,  0.3464211822, -0.1323156655,\n",
            "        -0.1250157356, -1.9640349150,  0.1337562501,  0.2649697065,\n",
            "        -2.3886864185, -2.3782944679, -1.9999765158])\n",
            "btensor.grad: tensor([-0.1893848181,  0.0260974169,  0.0440809280, -0.0211392045,\n",
            "         0.3515729010, -0.0900371075,  0.0484521091,  0.0605615377,\n",
            "         0.3202410936, -0.0755848289, -0.0224580765, -0.1318088472,\n",
            "         0.0360400677, -0.0424882174, -0.0230525732,  0.0596171618,\n",
            "         0.0361747518,  0.3070783615,  0.0127996206,  0.0078464746,\n",
            "         0.3539632857,  0.3255432844,  0.2830279768])\n",
            "ctensor.grad: tensor([ 7.2554345131,  7.2554345131,  7.2554345131, 13.1649570465,\n",
            "        -0.2745551467,  9.2267017365, -1.6118490696, -3.1175193787,\n",
            "        13.1649570465, -0.2745551467,  9.2267017365, -1.6118490696,\n",
            "        -3.1175193787, 13.1649570465, -0.2745551467,  9.2267017365,\n",
            "        -1.6118490696, -3.1175193787])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2174.0415039062, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9708042145), tensor(1.3590548038), tensor(1.3443448544), tensor(1.6551176310), tensor(1.3117908239), tensor(2.1951789856), tensor(1.4243702888), tensor(1.4997872114), tensor(1.0946770906), tensor(1.8130102158), tensor(1.5401352644), tensor(2.5988738537), tensor(1.4201693535), tensor(1.6596260071), tensor(1.6036118269), tensor(1.4011899233), tensor(1.5671414137), tensor(1.0560849905), tensor(1.4954633713), tensor(1.5642327070), tensor(1.1699862480), tensor(1.1646654606), tensor(1.0514465570)]\n",
            "b:  [tensor(0.6825371981), tensor(0.8879324198), tensor(0.9066705108), tensor(0.9518070221), tensor(1.1262779236), tensor(0.7119854093), tensor(0.9541726112), tensor(0.9135563970), tensor(1.1998137236), tensor(0.8221204877), tensor(0.9559991956), tensor(0.6554031372), tensor(0.9679214358), tensor(0.9035515785), tensor(0.9107511640), tensor(0.9682577252), tensor(0.8353360295), tensor(1.1603575945), tensor(0.9285329580), tensor(0.9162107110), tensor(1.1589165926), tensor(1.1076351404), tensor(1.0734109879)]\n",
            "c:  [tensor(-0.0712348744), tensor(-0.0712348744), tensor(-0.0712348744), tensor(-0.0344952568), tensor(0.0018461002), tensor(-0.0187240224), tensor(0.0095300488), tensor(0.0137866018), tensor(-0.0344952568), tensor(0.0018461002), tensor(-0.0187240224), tensor(0.0095300488), tensor(0.0137866018), tensor(-0.0344952568), tensor(0.0018461002), tensor(-0.0187240224), tensor(0.0095300488), tensor(0.0137866018)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.3679928780, -0.2767826021, -0.2430169582,  0.4101895094,\n",
            "        -2.7218155861,  1.1538779736, -0.1613477170, -0.2187051773,\n",
            "        -1.9848710299,  0.7739419937,  0.3405979872,  1.6767210960,\n",
            "         0.0089732483,  0.5324949026,  0.3297415376, -0.1315378249,\n",
            "        -0.1228357553, -1.8969290257,  0.1218013167,  0.2511273623,\n",
            "        -2.3158683777, -2.3090784550, -1.9358717203])\n",
            "btensor.grad: tensor([-0.1902552992,  0.0200159550,  0.0380007997, -0.0240624547,\n",
            "         0.3362796009, -0.0923173428,  0.0435499251,  0.0560710430,\n",
            "         0.3051612973, -0.0783871412, -0.0259418488, -0.1333944798,\n",
            "         0.0313932896, -0.0453190804, -0.0266646147,  0.0544669032,\n",
            "         0.0315809175,  0.2923533916,  0.0087888837,  0.0041077137,\n",
            "         0.3389415741,  0.3110425770,  0.2682874799])\n",
            "ctensor.grad: tensor([ 7.4943709373,  7.4943709373,  7.4943709373, 13.1145582199,\n",
            "        -0.2756760120,  9.1169939041, -1.6743021011, -3.2780125141,\n",
            "        13.1145582199, -0.2756760120,  9.1169939041, -1.6743021011,\n",
            "        -3.2780125141, 13.1145582199, -0.2756760120,  9.1169939041,\n",
            "        -1.6743021011, -3.2780125141])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.9506835938, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9684774876), tensor(1.3593268394), tensor(1.3445800543), tensor(1.6547293663), tensor(1.3144396544), tensor(2.1940546036), tensor(1.4245290756), tensor(1.5000051260), tensor(1.0965974331), tensor(1.8122618198), tensor(1.5398114920), tensor(2.5972313881), tensor(1.4201666117), tensor(1.6591150761), tensor(1.6032979488), tensor(1.4013206959), tensor(1.5672620535), tensor(1.0579175949), tensor(1.4953529835), tensor(1.5639947653), tensor(1.1722320318), tensor(1.1669079065), tensor(1.0533208847)]\n",
            "b:  [tensor(0.6827282906), tensor(0.8879183531), tensor(0.9066383839), tensor(0.9518339634), tensor(1.1259564161), tensor(0.7120800018), tensor(0.9541338086), tensor(0.9135047197), tensor(1.1995229721), tensor(0.8222016692), tensor(0.9560285807), tensor(0.6555380821), tensor(0.9678945541), tensor(0.9035997391), tensor(0.9107813835), tensor(0.9682082534), tensor(0.8353089094), tensor(1.1600792408), tensor(0.9285280704), tensor(0.9162102342), tensor(1.1585919857), tensor(1.1073380709), tensor(1.0731568336)]\n",
            "c:  [tensor(-0.0716213211), tensor(-0.0716213211), tensor(-0.0716213211), tensor(-0.0351486355), tensor(0.0018599428), tensor(-0.0191745292), tensor(0.0096168872), tensor(0.0139584914), tensor(-0.0351486355), tensor(0.0018599428), tensor(-0.0191745292), tensor(0.0096168872), tensor(0.0139584914), tensor(-0.0351486355), tensor(0.0018599428), tensor(-0.0191745292), tensor(0.0096168872), tensor(0.0139584914)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.3268322945, -0.2719846368, -0.2352261543,  0.3882580996,\n",
            "        -2.6487801075,  1.1244976521, -0.1587368250, -0.2179724574,\n",
            "        -1.9203650951,  0.7483519316,  0.3237336278,  1.6424804926,\n",
            "         0.0027133003,  0.5108773708,  0.3138267994, -0.1307393909,\n",
            "        -0.1206214428, -1.8325483799,  0.1104029566,  0.2379700541,\n",
            "        -2.2457981110, -2.2424213886, -1.8743339777])\n",
            "btensor.grad: tensor([-0.1911117882,  0.0140857697,  0.0321159437, -0.0269595385,\n",
            "         0.3215290308, -0.0945837498,  0.0387878120,  0.0517036915,\n",
            "         0.2907699645, -0.0811725855, -0.0293951035, -0.1349605024,\n",
            "         0.0268630981, -0.0481325388, -0.0302473307,  0.0494720936,\n",
            "         0.0271437988,  0.2783288956,  0.0048682094,  0.0004643202,\n",
            "         0.3246237636,  0.2971142232,  0.2541442811])\n",
            "ctensor.grad: tensor([ 7.7288851738,  7.7288851738,  7.7288851738, 13.0676059723,\n",
            "        -0.2768526077,  9.0101327896, -1.7367759943, -3.4377958775,\n",
            "        13.0676059723, -0.2768526077,  9.0101327896, -1.7367759943,\n",
            "        -3.4377958775, 13.0676059723, -0.2768526077,  9.0101327896,\n",
            "        -1.7367759943, -3.4377958775])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.8659667969, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9661908150), tensor(1.3595942259), tensor(1.3448078632), tensor(1.6543620825), tensor(1.3170177937), tensor(2.1929585934), tensor(1.4246852398), tensor(1.5002223253), tensor(1.0984559059), tensor(1.8115379810), tensor(1.5395038128), tensor(2.5956223011), tensor(1.4201698303), tensor(1.6586248875), tensor(1.6029993296), tensor(1.4014506340), tensor(1.5673804283), tensor(1.0596883297), tensor(1.4952534437), tensor(1.5637693405), tensor(1.1744103432), tensor(1.1690860987), tensor(1.0551360846)]\n",
            "b:  [tensor(0.6829202771), tensor(0.8879100680), tensor(0.9066119790), tensor(0.9518638253), tensor(1.1256490946), tensor(0.7121768594), tensor(0.9540996552), tensor(0.9134572148), tensor(1.1992459297), tensor(0.8222855926), tensor(0.9560613632), tensor(0.6556745768), tensor(0.9678721428), tensor(0.9036506414), tensor(0.9108151793), tensor(0.9681636691), tensor(0.8352860808), tensor(1.1598142385), tensor(0.9285270572), tensor(0.9162133336), tensor(1.1582810879), tensor(1.1070543528), tensor(1.0729162693)]\n",
            "c:  [tensor(-0.0720192865), tensor(-0.0720192865), tensor(-0.0720192865), tensor(-0.0357998386), tensor(0.0018738470), tensor(-0.0196198262), tensor(0.0097068530), tensor(0.0141383400), tensor(-0.0357998386), tensor(0.0018738470), tensor(-0.0196198262), tensor(0.0097068530), tensor(0.0141383400), tensor(-0.0357998386), tensor(0.0018738470), tensor(-0.0196198262), tensor(0.0097068530), tensor(0.0141383400)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.2866714001, -0.2674050331, -0.2278146744,  0.3673132658,\n",
            "        -2.5781414509,  1.0960841179, -0.1561694145, -0.2172037959,\n",
            "        -1.8584711552,  0.7237800956,  0.3076719046,  1.6091778278,\n",
            "        -0.0031917319,  0.4902132750,  0.2986397743, -0.1299029291,\n",
            "        -0.1183738112, -1.7707701921,  0.0995303243,  0.2254783511,\n",
            "        -2.1783595085, -2.1782047749, -1.8152465820])\n",
            "btensor.grad: tensor([-0.1919874549,  0.0083048344,  0.0264126435, -0.0298340321,\n",
            "         0.3073118031, -0.0968326330,  0.0341712236,  0.0475021601,\n",
            "         0.2770478725, -0.0839307308, -0.0328123569, -0.1365172267,\n",
            "         0.0224385858, -0.0508939028, -0.0337748528,  0.0446040034,\n",
            "         0.0228042603,  0.2649526596,  0.0010125637, -0.0031087399,\n",
            "         0.3109521866,  0.2837270200,  0.2405622303])\n",
            "ctensor.grad: tensor([ 7.9592385292,  7.9592385292,  7.9592385292, 13.0240488052,\n",
            "        -0.2780843377,  8.9059352875, -1.7993103266, -3.5969791412,\n",
            "        13.0240488052, -0.2780843377,  8.9059352875, -1.7993103266,\n",
            "        -3.5969791412, 13.0240488052, -0.2780843377,  8.9059352875,\n",
            "        -1.7993103266, -3.5969791412])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.7780761719, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9639432430), tensor(1.3598573208), tensor(1.3450286388), tensor(1.6540148258), tensor(1.3195276260), tensor(2.1918900013), tensor(1.4248389006), tensor(1.5004386902), tensor(1.1002550125), tensor(1.8108377457), tensor(1.5392113924), tensor(2.5940454006), tensor(1.4201785326), tensor(1.6581543684), tensor(1.6027151346), tensor(1.4015797377), tensor(1.5674965382), tensor(1.0613998175), tensor(1.4951642752), tensor(1.5635557175), tensor(1.1765238047), tensor(1.1712024212), tensor(1.0568945408)]\n",
            "b:  [tensor(0.6831131577), tensor(0.8879074454), tensor(0.9065911174), tensor(0.9518964887), tensor(1.1253554821), tensor(0.7122759223), tensor(0.9540699720), tensor(0.9134138227), tensor(1.1989820004), tensor(0.8223722577), tensor(0.9560975432), tensor(0.6558126211), tensor(0.9678540230), tensor(0.9037042856), tensor(0.9108524323), tensor(0.9681237340), tensor(0.8352675438), tensor(1.1595621109), tensor(0.9285297990), tensor(0.9162199497), tensor(1.1579831839), tensor(1.1067835093), tensor(1.0726886988)]\n",
            "c:  [tensor(-0.0724285692), tensor(-0.0724285692), tensor(-0.0724285692), tensor(-0.0364490226), tensor(0.0018878155), tensor(-0.0200600419), tensor(0.0097999498), tensor(0.0143261235), tensor(-0.0364490226), tensor(0.0018878155), tensor(-0.0200600419), tensor(0.0097999498), tensor(0.0143261235), tensor(-0.0364490226), tensor(0.0018878155), tensor(-0.0200600419), tensor(0.0097999498), tensor(0.0143261235)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.2474846840, -0.2630429268, -0.2207751274,  0.3473041058,\n",
            "        -2.5098118782,  1.0686023235, -0.1536625028, -0.2164015770,\n",
            "        -1.7990627289,  0.7001940012,  0.2923809290,  1.5767863989,\n",
            "        -0.0087618157,  0.4704712629,  0.2841527164, -0.1290580928,\n",
            "        -0.1161072850, -1.7114830017,  0.0891536623,  0.2136085331,\n",
            "        -2.1134338379, -2.1163377762, -1.7584993839])\n",
            "btensor.grad: tensor([-0.1928566545,  0.0026308298,  0.0208563060, -0.0326740146,\n",
            "         0.2935756147, -0.0990473032,  0.0296814442,  0.0433790684,\n",
            "         0.2639282346, -0.0866948962, -0.0362026691, -0.1380486190,\n",
            "         0.0181374550, -0.0536576509, -0.0372387171,  0.0399067402,\n",
            "         0.0185577795,  0.2521638870, -0.0027496219, -0.0066179037,\n",
            "         0.2978753150,  0.2708325684,  0.2275127769])\n",
            "ctensor.grad: tensor([ 8.1856374741,  8.1856374741,  8.1856374741, 12.9837093353,\n",
            "        -0.2793709338,  8.8043012619, -1.8619437218, -3.7556686401,\n",
            "        12.9837093353, -0.2793709338,  8.8043012619, -1.8619437218,\n",
            "        -3.7556686401, 12.9837093353, -0.2793709338,  8.8043012619,\n",
            "        -1.8619437218, -3.7556686401])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.6953125000, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9617340565), tensor(1.3601162434), tensor(1.3452427387), tensor(1.6536866426), tensor(1.3219712973), tensor(2.1908478737), tensor(1.4249900579), tensor(1.5006542206), tensor(1.1019970179), tensor(1.8101601601), tensor(1.5389336348), tensor(2.5925002098), tensor(1.4201925993), tensor(1.6577028036), tensor(1.6024447680), tensor(1.4017078876), tensor(1.5676103830), tensor(1.0630543232), tensor(1.4950850010), tensor(1.5633534193), tensor(1.1785746813), tensor(1.1732591391), tensor(1.0585985184)]\n",
            "b:  [tensor(0.6833069324), tensor(0.8879103661), tensor(0.9065756798), tensor(0.9519320130), tensor(1.1250752211), tensor(0.7123771906), tensor(0.9540446997), tensor(0.9133744240), tensor(1.1987305880), tensor(0.8224617243), tensor(0.9561371207), tensor(0.6559522152), tensor(0.9678401351), tensor(0.9037607312), tensor(0.9108931422), tensor(0.9680884480), tensor(0.8352531195), tensor(1.1593221426), tensor(0.9285362959), tensor(0.9162300229), tensor(1.1576977968), tensor(1.1065250635), tensor(1.0724737644)]\n",
            "c:  [tensor(-0.0728489831), tensor(-0.0728489831), tensor(-0.0728489831), tensor(-0.0370963514), tensor(0.0019018512), tensor(-0.0204952974), tensor(0.0098961852), tensor(0.0145218233), tensor(-0.0370963514), tensor(0.0019018512), tensor(-0.0204952974), tensor(0.0098961852), tensor(0.0145218233), tensor(-0.0370963514), tensor(0.0019018512), tensor(-0.0204952974), tensor(0.0098961852), tensor(0.0145218233)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.2092347145, -0.2588929534, -0.2140827179,  0.3281883001,\n",
            "        -2.4436900616,  1.0420186520, -0.1512142420, -0.2155813575,\n",
            "        -1.7420356274,  0.6775555611,  0.2778118253,  1.5452759266,\n",
            "        -0.0140298381,  0.4516067505,  0.2703213096, -0.1282013059,\n",
            "        -0.1138196588, -1.6545572281,  0.0792403817,  0.2023290694,\n",
            "        -2.0509154797, -2.0567133427, -1.7039922476])\n",
            "btensor.grad: tensor([-0.1937576383, -0.0029262304,  0.0154498667, -0.0355197191,\n",
            "         0.2802993655, -0.1012803316,  0.0252943635,  0.0393761396,\n",
            "         0.2513842583, -0.0894830823, -0.0395617485, -0.1395842731,\n",
            "         0.0138993263, -0.0564196110, -0.0407048464,  0.0352853537,\n",
            "         0.0144160837,  0.2399475574, -0.0064715743, -0.0100641251,\n",
            "         0.2853502333,  0.2584037781,  0.2149443328])\n",
            "ctensor.grad: tensor([ 8.4082860947,  8.4082860947,  8.4082860947, 12.9465656281,\n",
            "        -0.2807115912,  8.7051172256, -1.9247109890, -3.9139890671,\n",
            "        12.9465656281, -0.2807115912,  8.7051172256, -1.9247109890,\n",
            "        -3.9139890671, 12.9465656281, -0.2807115912,  8.7051172256,\n",
            "        -1.9247109890, -3.9139890671])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.6120605469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9595620632), tensor(1.3603712320), tensor(1.3454505205), tensor(1.6533766985), tensor(1.3243509531), tensor(2.1898314953), tensor(1.4251388311), tensor(1.5008689165), tensor(1.1036843061), tensor(1.8095042706), tensor(1.5386697054), tensor(2.5909855366), tensor(1.4202115536), tensor(1.6572692394), tensor(1.6021876335), tensor(1.4018352032), tensor(1.5677219629), tensor(1.0646542311), tensor(1.4950152636), tensor(1.5631618500), tensor(1.1805653572), tensor(1.1752583981), tensor(1.0602501631)]\n",
            "b:  [tensor(0.6835016012), tensor(0.8879187703), tensor(0.9065655470), tensor(0.9519703388), tensor(1.1248077154), tensor(0.7124807239), tensor(0.9540236592), tensor(0.9133389592), tensor(1.1984912157), tensor(0.8225539923), tensor(0.9561799765), tensor(0.6560932994), tensor(0.9678303599), tensor(0.9038198590), tensor(0.9109372497), tensor(0.9680576324), tensor(0.8352427483), tensor(1.1590938568), tensor(0.9285464287), tensor(0.9162434936), tensor(1.1574244499), tensor(1.1062786579), tensor(1.0722708702)]\n",
            "c:  [tensor(-0.0732803568), tensor(-0.0732803568), tensor(-0.0732803568), tensor(-0.0377419777), tensor(0.0019159565), tensor(-0.0209257081), tensor(0.0099955676), tensor(0.0147254253), tensor(-0.0377419777), tensor(0.0019159565), tensor(-0.0209257081), tensor(0.0099955676), tensor(0.0147254253), tensor(-0.0377419777), tensor(0.0019159565), tensor(-0.0209257081), tensor(0.0099955676), tensor(0.0147254253)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.1719210148, -0.2549500763, -0.2077245712,  0.3099278212,\n",
            "        -2.3796992302,  1.0163097382, -0.1488282979, -0.2147441506,\n",
            "        -1.6872887611,  0.6558333635,  0.2639546394,  1.5146329403,\n",
            "        -0.0190000907,  0.4335951805,  0.2571285367, -0.1273441315,\n",
            "        -0.1115214229, -1.5998899937,  0.0697696209,  0.1916153729,\n",
            "        -1.9907022715, -1.9992406368, -1.6516218185])\n",
            "btensor.grad: tensor([-0.1946708709, -0.0083849430,  0.0101531222, -0.0383107066,\n",
            "         0.2674711347, -0.1035065651,  0.0210281909,  0.0354702473,\n",
            "         0.2393563688, -0.0922476053, -0.0428621769, -0.1410956085,\n",
            "         0.0097543001, -0.0591331720, -0.0441274643,  0.0308000445,\n",
            "         0.0103432536,  0.2282552719, -0.0101188421, -0.0134681463,\n",
            "         0.2733440101,  0.2464219630,  0.2028521299])\n",
            "ctensor.grad: tensor([ 8.6274290085,  8.6274290085,  8.6274290085, 12.9124994278,\n",
            "        -0.2821060717,  8.6082286835, -1.9876537323, -4.0720338821,\n",
            "        12.9124994278, -0.2821060717,  8.6082286835, -1.9876537323,\n",
            "        -4.0720338821, 12.9124994278, -0.2821060717,  8.6082286835,\n",
            "        -1.9876537323, -4.0720338821])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.5273437500, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9574265480), tensor(1.3606224060), tensor(1.3456522226), tensor(1.6530842781), tensor(1.3266687393), tensor(2.1888401508), tensor(1.4252853394), tensor(1.5010827780), tensor(1.1053190231), tensor(1.8088692427), tensor(1.5384190083), tensor(2.5895006657), tensor(1.4202352762), tensor(1.6568528414), tensor(1.6019431353), tensor(1.4019616842), tensor(1.5678311586), tensor(1.0662015676), tensor(1.4949545860), tensor(1.5629804134), tensor(1.1824980974), tensor(1.1772022247), tensor(1.0618515015)]\n",
            "b:  [tensor(0.6836972237), tensor(0.8879324794), tensor(0.9065605402), tensor(0.9520114660), tensor(1.1245527267), tensor(0.7125864625), tensor(0.9540067911), tensor(0.9133073092), tensor(1.1982634068), tensor(0.8226490021), tensor(0.9562261701), tensor(0.6562359333), tensor(0.9678246975), tensor(0.9038817286), tensor(0.9109848142), tensor(0.9680312276), tensor(0.8352363706), tensor(1.1588767767), tensor(0.9285601377), tensor(0.9162603021), tensor(1.1571626663), tensor(1.1060438156), tensor(1.0720796585)]\n",
            "c:  [tensor(-0.0737225190), tensor(-0.0737225190), tensor(-0.0737225190), tensor(-0.0383860469), tensor(0.0019301341), tensor(-0.0213513859), tensor(0.0100981081), tensor(0.0149369203), tensor(-0.0383860469), tensor(0.0019301341), tensor(-0.0213513859), tensor(0.0100981081), tensor(0.0149369203), tensor(-0.0383860469), tensor(0.0019301341), tensor(-0.0213513859), tensor(0.0100981081), tensor(0.0149369203)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.1355049610, -0.2512098849, -0.2016873360,  0.2924790382,\n",
            "        -2.3177552223,  0.9914515018, -0.1465044618, -0.2139062881,\n",
            "        -1.6347155571,  0.6349921227,  0.2507532239,  1.4848260880,\n",
            "        -0.0237014070,  0.4163841009,  0.2445368618, -0.1264879405,\n",
            "        -0.1092092991, -1.5473846197,  0.0607157350,  0.1814492047,\n",
            "        -1.9326930046, -1.9438304901, -1.6012881994])\n",
            "btensor.grad: tensor([-0.1956294626, -0.0137280226,  0.0050013438, -0.0411261916,\n",
            "         0.2550432086, -0.1057174206,  0.0168443024,  0.0316584110,\n",
            "         0.2278361619, -0.0950134397, -0.0461716652, -0.1426392496,\n",
            "         0.0056757331, -0.0618789196, -0.0475448370,  0.0264229178,\n",
            "         0.0063852519,  0.2170338631, -0.0137204528, -0.0168087482,\n",
            "         0.2618328035,  0.2348722517,  0.1911968887])\n",
            "ctensor.grad: tensor([ 8.8432474136,  8.8432474136,  8.8432474136, 12.8813819885,\n",
            "        -0.2835538983,  8.5135679245, -2.0508017540, -4.2298960686,\n",
            "        12.8813819885, -0.2835538983,  8.5135679245, -2.0508017540,\n",
            "        -4.2298960686, 12.8813819885, -0.2835538983,  8.5135679245,\n",
            "        -2.0508017540, -4.2298960686])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.4494628906, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9553265572), tensor(1.3608701229), tensor(1.3458482027), tensor(1.6528084278), tensor(1.3289265633), tensor(2.1878726482), tensor(1.4254295826), tensor(1.5012958050), tensor(1.1069031954), tensor(1.8082542419), tensor(1.5381808281), tensor(2.5880448818), tensor(1.4202634096), tensor(1.6564528942), tensor(1.6017105579), tensor(1.4020873308), tensor(1.5679380894), tensor(1.0676984787), tensor(1.4949024916), tensor(1.5628086329), tensor(1.1843749285), tensor(1.1790926456), tensor(1.0634044409)]\n",
            "b:  [tensor(0.6838937998), tensor(0.8879514933), tensor(0.9065605998), tensor(0.9520553946), tensor(1.1243096590), tensor(0.7126944065), tensor(0.9539940357), tensor(0.9132793546), tensor(1.1980466843), tensor(0.8227468133), tensor(0.9562756419), tensor(0.6563801169), tensor(0.9678230286), tensor(0.9039463401), tensor(0.9110357165), tensor(0.9680090547), tensor(0.8352338672), tensor(1.1586705446), tensor(0.9285774231), tensor(0.9162804484), tensor(1.1569118500), tensor(1.1058200598), tensor(1.0718996525)]\n",
            "c:  [tensor(-0.0741753131), tensor(-0.0741753131), tensor(-0.0741753131), tensor(-0.0390287042), tensor(0.0019443869), tensor(-0.0217724349), tensor(0.0102038179), tensor(0.0151563045), tensor(-0.0390287042), tensor(0.0019443869), tensor(-0.0217724349), tensor(0.0102038179), tensor(0.0151563045), tensor(-0.0390287042), tensor(0.0019443869), tensor(-0.0217724349), tensor(0.0102038179), tensor(0.0151563045)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.0999772549, -0.2476710081, -0.1959645748,  0.2758071423,\n",
            "        -2.2577762604,  0.9674230218, -0.1442409456, -0.2130567431,\n",
            "        -1.5842269659,  0.6150035858,  0.2381848693,  1.4558348656,\n",
            "        -0.0281471834,  0.3999483585,  0.2325248569, -0.1256254315,\n",
            "        -0.1068984866, -1.4969432354,  0.0520519167,  0.1717881560,\n",
            "        -1.8767954111, -1.8903990984, -1.5529172421])\n",
            "btensor.grad: tensor([-1.9659234583e-01, -1.8996238708e-02, -6.8701803684e-05,\n",
            "        -4.3919324875e-02,  2.4302738905e-01, -1.0792207718e-01,\n",
            "         1.2749940157e-02,  2.7956604958e-02,  2.1677753329e-01,\n",
            "        -9.7803056240e-02, -4.9453258514e-02, -1.4417889714e-01,\n",
            "         1.6792416573e-03, -6.4614057541e-02, -5.0925731659e-02,\n",
            "         2.2153615952e-02,  2.4767220020e-03,  2.0627498627e-01,\n",
            "        -1.7283141613e-02, -2.0129919052e-02,  2.5079038739e-01,\n",
            "         2.2372412682e-01,  1.7995300889e-01])\n",
            "ctensor.grad: tensor([ 9.0559272766,  9.0559272766,  9.0559272766, 12.8531579971,\n",
            "        -0.2850547433,  8.4209823608, -2.1141896248, -4.3876814842,\n",
            "        12.8531579971, -0.2850547433,  8.4209823608, -2.1141896248,\n",
            "        -4.3876814842, 12.8531579971, -0.2850547433,  8.4209823608,\n",
            "        -2.1141896248, -4.3876814842])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.3710937500, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9532611370), tensor(1.3611145020), tensor(1.3460386992), tensor(1.6525485516), tensor(1.3311262131), tensor(2.1869285107), tensor(1.4255716801), tensor(1.5015079975), tensor(1.1084389687), tensor(1.8076584339), tensor(1.5379545689), tensor(2.5866172314), tensor(1.4202957153), tensor(1.6560686827), tensor(1.6014895439), tensor(1.4022121429), tensor(1.5680426359), tensor(1.0691469908), tensor(1.4948587418), tensor(1.5626460314), tensor(1.1861978769), tensor(1.1809315681), tensor(1.0649108887)]\n",
            "b:  [tensor(0.6840913892), tensor(0.8879756331), tensor(0.9065656066), tensor(0.9521021247), tensor(1.1240782738), tensor(0.7128045559), tensor(0.9539852738), tensor(0.9132550359), tensor(1.1978405714), tensor(0.8228474259), tensor(0.9563283920), tensor(0.6565258503), tensor(0.9678252935), tensor(0.9040136933), tensor(0.9110900164), tensor(0.9679911137), tensor(0.8352352381), tensor(1.1584745646), tensor(0.9285982251), tensor(0.9163038731), tensor(1.1566716433), tensor(1.1056071520), tensor(1.0717306137)]\n",
            "c:  [tensor(-0.0746385977), tensor(-0.0746385977), tensor(-0.0746385977), tensor(-0.0396700911), tensor(0.0019587174), tensor(-0.0221889541), tensor(0.0103127100), tensor(0.0153835770), tensor(-0.0396700911), tensor(0.0019587174), tensor(-0.0221889541), tensor(0.0103127100), tensor(0.0153835770), tensor(-0.0396700911), tensor(0.0019587174), tensor(-0.0221889541), tensor(0.0103127100), tensor(0.0153835770)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.0653142929, -0.2443238497, -0.1905312538,  0.2598725557,\n",
            "        -2.1996881962,  0.9441896677, -0.1420402527, -0.2122169733,\n",
            "        -1.5357278585,  0.5958333611,  0.2262163758,  1.4276430607,\n",
            "        -0.0323530287,  0.3842546940,  0.2210662812, -0.1247771978,\n",
            "        -0.1045812964, -1.4484715462,  0.0437570214,  0.1626144648,\n",
            "        -1.8229286671, -1.8388751745, -1.5064194202])\n",
            "btensor.grad: tensor([-0.1975968331, -0.0241569281, -0.0049886182, -0.0467177033,\n",
            "         0.2313584089, -0.1101528406,  0.0087510049,  0.0243221521,\n",
            "         0.2061643302, -0.1006144285, -0.0527224541, -0.1457194090,\n",
            "        -0.0022475719, -0.0673409700, -0.0543198586,  0.0179660916,\n",
            "        -0.0013544485,  0.1959619522, -0.0208067894, -0.0234100819,\n",
            "         0.2401598394,  0.2129393816,  0.1690954566])\n",
            "ctensor.grad: tensor([ 9.2656679153,  9.2656679153,  9.2656679153, 12.8277626038,\n",
            "        -0.2866084874,  8.3303909302, -2.1778485775, -4.5454549789,\n",
            "        12.8277626038, -0.2866084874,  8.3303909302, -2.1778485775,\n",
            "        -4.5454549789, 12.8277626038, -0.2866084874,  8.3303909302,\n",
            "        -2.1778485775, -4.5454549789])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.2917480469, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9512295723), tensor(1.3613556623), tensor(1.3462240696), tensor(1.6523039341), tensor(1.3332695961), tensor(2.1860067844), tensor(1.4257116318), tensor(1.5017193556), tensor(1.1099281311), tensor(1.8070809841), tensor(1.5377397537), tensor(2.5852169991), tensor(1.4203320742), tensor(1.6556993723), tensor(1.6012793779), tensor(1.4023361206), tensor(1.5681449175), tensor(1.0705488920), tensor(1.4948229790), tensor(1.5624921322), tensor(1.1879688501), tensor(1.1827207804), tensor(1.0663726330)]\n",
            "b:  [tensor(0.6842900515), tensor(0.8880048990), tensor(0.9065754414), tensor(0.9521516562), tensor(1.1238582134), tensor(0.7129169703), tensor(0.9539804459), tensor(0.9132342935), tensor(1.1976445913), tensor(0.8229508996), tensor(0.9563843608), tensor(0.6566731334), tensor(0.9678314328), tensor(0.9040837884), tensor(0.9111477137), tensor(0.9679772258), tensor(0.8352403641), tensor(1.1582884789), tensor(0.9286224842), tensor(0.9163305163), tensor(1.1564416885), tensor(1.1054046154), tensor(1.0715719461)]\n",
            "c:  [tensor(-0.0751122311), tensor(-0.0751122311), tensor(-0.0751122311), tensor(-0.0403103456), tensor(0.0019731282), tensor(-0.0226010401), tensor(0.0104248002), tensor(0.0156187424), tensor(-0.0403103456), tensor(0.0019731282), tensor(-0.0226010401), tensor(0.0104248002), tensor(0.0156187424), tensor(-0.0403103456), tensor(0.0019731282), tensor(-0.0226010401), tensor(0.0104248002), tensor(0.0156187424)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 2.0314946175, -0.2411683649, -0.1853916645,  0.2446439266,\n",
            "        -2.1434314251,  0.9217360616, -0.1399002373, -0.2113937736,\n",
            "        -1.4891407490,  0.5774546862,  0.2148201466,  1.4002283812,\n",
            "        -0.0363449678,  0.3692812920,  0.2101339847, -0.1239379644,\n",
            "        -0.1022683978, -1.4018826485,  0.0358172953,  0.1539084911,\n",
            "        -1.7710103989, -1.7891716957, -1.4617196321])\n",
            "btensor.grad: tensor([-0.1986421645, -0.0292791128, -0.0098438784, -0.0495126843,\n",
            "         0.2200383842, -0.1124022007,  0.0048296154,  0.0207591057,\n",
            "         0.1959509254, -0.1034512520, -0.0559892654, -0.1472778916,\n",
            "        -0.0061241388, -0.0700660944, -0.0576831102,  0.0138748288,\n",
            "        -0.0051119179,  0.1860296726, -0.0242843032, -0.0266412497,\n",
            "         0.2299307287,  0.2025209069,  0.1586090624])\n",
            "ctensor.grad: tensor([ 9.4726047516,  9.4726047516,  9.4726047516, 12.8050880432,\n",
            "        -0.2882145047,  8.2417221069, -2.2418098450, -4.7033104897,\n",
            "        12.8050880432, -0.2882145047,  8.2417221069, -2.2418098450,\n",
            "        -4.7033104897, 12.8050880432, -0.2882145047,  8.2417221069,\n",
            "        -2.2418098450, -4.7033104897])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.2153320312, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9492311478), tensor(1.3615938425), tensor(1.3464045525), tensor(1.6520738602), tensor(1.3353585005), tensor(2.1851067543), tensor(1.4258494377), tensor(1.5019298792), tensor(1.1113724709), tensor(1.8065211773), tensor(1.5375357866), tensor(2.5838434696), tensor(1.4203722477), tensor(1.6553443670), tensor(1.6010797024), tensor(1.4024592638), tensor(1.5682448149), tensor(1.0719059706), tensor(1.4947947264), tensor(1.5623464584), tensor(1.1896897554), tensor(1.1844620705), tensor(1.0677913427)]\n",
            "b:  [tensor(0.6844897866), tensor(0.8880392313), tensor(0.9065900445), tensor(0.9522039890), tensor(1.1236491203), tensor(0.7130316496), tensor(0.9539794922), tensor(0.9132170081), tensor(1.1974585056), tensor(0.8230571747), tensor(0.9564436674), tensor(0.6568219662), tensor(0.9678413868), tensor(0.9041566253), tensor(0.9112087488), tensor(0.9679673910), tensor(0.8352491856), tensor(1.1581120491), tensor(0.9286502004), tensor(0.9163603783), tensor(1.1562216282), tensor(1.1052122116), tensor(1.0714235306)]\n",
            "c:  [tensor(-0.0755960792), tensor(-0.0755960792), tensor(-0.0755960792), tensor(-0.0409496017), tensor(0.0019876219), tensor(-0.0230087843), tensor(0.0105401054), tensor(0.0158618074), tensor(-0.0409496017), tensor(0.0019876219), tensor(-0.0230087843), tensor(0.0105401054), tensor(0.0158618074), tensor(-0.0409496017), tensor(0.0019876219), tensor(-0.0230087843), tensor(0.0105401054), tensor(0.0158618074)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.9985010624, -0.2382078171, -0.1805303097,  0.2300944328,\n",
            "        -2.0889306068,  0.9000339508, -0.1378294826, -0.2105821967,\n",
            "        -1.4443891048,  0.5598371625,  0.2039654851,  1.3735716343,\n",
            "        -0.0401253775,  0.3549796343,  0.1996965706, -0.1231187284,\n",
            "        -0.0999568105, -1.3571033478,  0.0282016397,  0.1456410885,\n",
            "        -1.7209553719, -1.7412325144, -1.4187409878])\n",
            "btensor.grad: tensor([-0.1997170150, -0.0343325138, -0.0146136135, -0.0523135662,\n",
            "         0.2090517581, -0.1146583557,  0.0009726584,  0.0172764063,\n",
            "         0.1861017942, -0.1063035727, -0.0592887402, -0.1488548815,\n",
            "        -0.0099547505, -0.0728182793, -0.0610483885,  0.0098304749,\n",
            "        -0.0088189095,  0.1764647961, -0.0277418494, -0.0298719406,\n",
            "         0.2200713456,  0.1924371421,  0.1484535336])\n",
            "ctensor.grad: tensor([ 9.6769237518,  9.6769237518,  9.6769237518, 12.7851057053,\n",
            "        -0.2898728848,  8.1548767090, -2.3060948849, -4.8613133430,\n",
            "        12.7851057053, -0.2898728848,  8.1548767090, -2.3060948849,\n",
            "        -4.8613133430, 12.7851057053, -0.2898728848,  8.1548767090,\n",
            "        -2.3060948849, -4.8613133430])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.1396484375, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9472649097), tensor(1.3618292809), tensor(1.3465805054), tensor(1.6518577337), tensor(1.3373945951), tensor(2.1842277050), tensor(1.4259852171), tensor(1.5021396875), tensor(1.1127738953), tensor(1.8059781790), tensor(1.5373421907), tensor(2.5824959278), tensor(1.4204159975), tensor(1.6550030708), tensor(1.6008899212), tensor(1.4025815725), tensor(1.5683424473), tensor(1.0732200146), tensor(1.4947738647), tensor(1.5622086525), tensor(1.1913625002), tensor(1.1861571074), tensor(1.0691688061)]\n",
            "b:  [tensor(0.6846905947), tensor(0.8880785108), tensor(0.9066093564), tensor(0.9522591233), tensor(1.1234507561), tensor(0.7131485939), tensor(0.9539822936), tensor(0.9132031798), tensor(1.1972818375), tensor(0.8231663704), tensor(0.9565061927), tensor(0.6569724083), tensor(0.9678551555), tensor(0.9042322040), tensor(0.9112731814), tensor(0.9679614902), tensor(0.8352616429), tensor(1.1579447985), tensor(0.9286813736), tensor(0.9163934588), tensor(1.1560109854), tensor(1.1050295830), tensor(1.0712848902)]\n",
            "c:  [tensor(-0.0760900229), tensor(-0.0760900229), tensor(-0.0760900229), tensor(-0.0415879898), tensor(0.0020022010), tensor(-0.0234122723), tensor(0.0106586423), tensor(0.0161127839), tensor(-0.0415879898), tensor(0.0020022010), tensor(-0.0234122723), tensor(0.0106586423), tensor(0.0161127839), tensor(-0.0415879898), tensor(0.0020022010), tensor(-0.0234122723), tensor(0.0106586423), tensor(0.0161127839)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.9663256407, -0.2354250550, -0.1759319305,  0.2161829472,\n",
            "        -2.0361211300,  0.8790734410, -0.1358228922, -0.2097923160,\n",
            "        -1.4013895988,  0.5429574847,  0.1936387420,  1.3476581573,\n",
            "        -0.0437067002,  0.3413400650,  0.1897481978, -0.1223104298,\n",
            "        -0.0976525545, -1.3140525818,  0.0208928585,  0.1377969682,\n",
            "        -1.6727045774, -1.6949836016, -1.3774149418])\n",
            "btensor.grad: tensor([-0.2008073479, -0.0393056870, -0.0193137750, -0.0551210642,\n",
            "         0.1983726621, -0.1169261932, -0.0027993917,  0.0138514042,\n",
            "         0.1766180992, -0.1092197299, -0.0625422001, -0.1504655182,\n",
            "        -0.0137446523, -0.0755586624, -0.0644249916,  0.0058867931,\n",
            "        -0.0124801472,  0.1672787666, -0.0311822295, -0.0330733061,\n",
            "         0.2105850577,  0.1826651990,  0.1386305094])\n",
            "ctensor.grad: tensor([ 9.8788127899,  9.8788127899,  9.8788127899, 12.7677307129,\n",
            "        -0.2915834486,  8.0697593689, -2.3707385063, -5.0195469856,\n",
            "        12.7677307129, -0.2915834486,  8.0697593689, -2.3707385063,\n",
            "        -5.0195469856, 12.7677307129, -0.2915834486,  8.0697593689,\n",
            "        -2.3707385063, -5.0195469856])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2173.0629882812, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9453299046), tensor(1.3620620966), tensor(1.3467520475), tensor(1.6516548395), tensor(1.3393795490), tensor(2.1833689213), tensor(1.4261190891), tensor(1.5023486614), tensor(1.1141339540), tensor(1.8054513931), tensor(1.5371583700), tensor(2.5811734200), tensor(1.4204630852), tensor(1.6546747684), tensor(1.6007096767), tensor(1.4027030468), tensor(1.5684378147), tensor(1.0744926929), tensor(1.4947600365), tensor(1.5620783567), tensor(1.1929886341), tensor(1.1878074408), tensor(1.0705064535)]\n",
            "b:  [tensor(0.6848925352), tensor(0.8881227374), tensor(0.9066332579), tensor(0.9523170590), tensor(1.1232627630), tensor(0.7132678032), tensor(0.9539887905), tensor(0.9131926894), tensor(1.1971143484), tensor(0.8232784867), tensor(0.9565719962), tensor(0.6571245193), tensor(0.9678726196), tensor(0.9043105245), tensor(0.9113409519), tensor(0.9679595232), tensor(0.8352777362), tensor(1.1577863693), tensor(0.9287159443), tensor(0.9164296985), tensor(1.1558095217), tensor(1.1048563719), tensor(1.0711557865)]\n",
            "c:  [tensor(-0.0765939429), tensor(-0.0765939429), tensor(-0.0765939429), tensor(-0.0422256328), tensor(0.0020168684), tensor(-0.0238115862), tensor(0.0107804313), tensor(0.0163716879), tensor(-0.0422256328), tensor(0.0020168684), tensor(-0.0238115862), tensor(0.0107804313), tensor(0.0163716879), tensor(-0.0422256328), tensor(0.0020168684), tensor(-0.0238115862), tensor(0.0107804313), tensor(0.0163716879)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.9349465370, -0.2328199744, -0.1715795994,  0.2028883696,\n",
            "        -1.9849466085,  0.8588252068, -0.1338861883, -0.2090227604,\n",
            "        -1.3600749969,  0.5267899036,  0.1838085651,  1.3224654198,\n",
            "        -0.0471148342,  0.3283276558,  0.1802534312, -0.1215312481,\n",
            "        -0.0953589082, -1.2726572752,  0.0138822645,  0.1303544939,\n",
            "        -1.6261709929, -1.6503572464, -1.3376705647])\n",
            "btensor.grad: tensor([-0.2019521147, -0.0442407131, -0.0239137560, -0.0579420924,\n",
            "         0.1879889071, -0.1192172766, -0.0065150559,  0.0104990005,\n",
            "         0.1674621105, -0.1121361256, -0.0658254623, -0.1520966887,\n",
            "        -0.0174734592, -0.0783188343, -0.0677821636,  0.0019887090,\n",
            "        -0.0160862878,  0.1584107876, -0.0345690846, -0.0362490416,\n",
            "         0.2014147937,  0.1731983423,  0.1291080713])\n",
            "ctensor.grad: tensor([10.0783729553, 10.0783729553, 10.0783729553, 12.7528944016,\n",
            "        -0.2933459878,  7.9862847328, -2.4357752800, -5.1780872345,\n",
            "        12.7528944016, -0.2933459878,  7.9862847328, -2.4357752800,\n",
            "        -5.1780872345, 12.7528944016, -0.2933459878,  7.9862847328,\n",
            "        -2.4357752800, -5.1780872345])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.9853515625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9434256554), tensor(1.3622925282), tensor(1.3469195366), tensor(1.6514647007), tensor(1.3413149118), tensor(2.1825296879), tensor(1.4262510538), tensor(1.5025569201), tensor(1.1154543161), tensor(1.8049401045), tensor(1.5369839668), tensor(2.5798754692), tensor(1.4205133915), tensor(1.6543588638), tensor(1.6005384922), tensor(1.4028238058), tensor(1.5685309172), tensor(1.0757255554), tensor(1.4947528839), tensor(1.5619550943), tensor(1.1945699453), tensor(1.1894147396), tensor(1.0718059540)]\n",
            "b:  [tensor(0.6850957274), tensor(0.8881718516), tensor(0.9066617489), tensor(0.9523778558), tensor(1.1230849028), tensor(0.7133893371), tensor(0.9539989829), tensor(0.9131854773), tensor(1.1969556808), tensor(0.8233935833), tensor(0.9566411376), tensor(0.6572782397), tensor(0.9678937793), tensor(0.9043916464), tensor(0.9114121199), tensor(0.9679613709), tensor(0.8352974057), tensor(1.1576365232), tensor(0.9287539124), tensor(0.9164690971), tensor(1.1556169987), tensor(1.1046923399), tensor(1.0710359812)]\n",
            "c:  [tensor(-0.0771077350), tensor(-0.0771077350), tensor(-0.0771077350), tensor(-0.0428626612), tensor(0.0020316264), tensor(-0.0242068060), tensor(0.0109054921), tensor(0.0166385360), tensor(-0.0428626612), tensor(0.0020316264), tensor(-0.0242068060), tensor(0.0109054921), tensor(0.0166385360), tensor(-0.0428626612), tensor(0.0020316264), tensor(-0.0242068060), tensor(0.0109054921), tensor(0.0166385360)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.9043421745, -0.2303971946, -0.1674847603,  0.1901727915,\n",
            "        -1.9353464842,  0.8392685056, -0.1320108175, -0.2082865834,\n",
            "        -1.3203824759,  0.5113080740,  0.1744476557,  1.2979776859,\n",
            "        -0.0503531173,  0.3159176111,  0.1711968631, -0.1207726896,\n",
            "        -0.0930732489, -1.2328493595,  0.0071407706,  0.1232990623,\n",
            "        -1.5812983513, -1.6073054075, -1.2994530201])\n",
            "btensor.grad: tensor([-0.2031656951, -0.0491306782, -0.0284723341, -0.0607782006,\n",
            "         0.1778796613, -0.1215631962, -0.0101897418,  0.0071958303,\n",
            "         0.1586207747, -0.1151040792, -0.0691256523, -0.1537325382,\n",
            "        -0.0211779475, -0.0811207294, -0.0711671114, -0.0018310547,\n",
            "        -0.0196548402,  0.1498405933, -0.0379537344, -0.0394263268,\n",
            "         0.1925498247,  0.1640006602,  0.1198587418])\n",
            "ctensor.grad: tensor([10.2757692337, 10.2757692337, 10.2757692337, 12.7405767441,\n",
            "        -0.2951601446,  7.9044027328, -2.5012121201, -5.3369593620,\n",
            "        12.7405767441, -0.2951601446,  7.9044027328, -2.5012121201,\n",
            "        -5.3369593620, 12.7405767441, -0.2951601446,  7.9044027328,\n",
            "        -2.5012121201, -5.3369593620])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.9128417969, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9415512085), tensor(1.3625206947), tensor(1.3470830917), tensor(1.6512867212), tensor(1.3432022333), tensor(2.1817092896), tensor(1.4263812304), tensor(1.5027644634), tensor(1.1167365313), tensor(1.8044435978), tensor(1.5368183851), tensor(2.5786013603), tensor(1.4205667973), tensor(1.6540547609), tensor(1.6003758907), tensor(1.4029438496), tensor(1.5686217546), tensor(1.0769201517), tensor(1.4947521687), tensor(1.5618385077), tensor(1.1961079836), tensor(1.1909805536), tensor(1.0730686188)]\n",
            "b:  [tensor(0.6853001118), tensor(0.8882257938), tensor(0.9066947103), tensor(0.9524415135), tensor(1.1229168177), tensor(0.7135132551), tensor(0.9540128112), tensor(0.9131815434), tensor(1.1968055964), tensor(0.8235116601), tensor(0.9567135572), tensor(0.6574336886), tensor(0.9679186344), tensor(0.9044755697), tensor(0.9114866853), tensor(0.9679669738), tensor(0.8353205919), tensor(1.1574949026), tensor(0.9287952185), tensor(0.9165117145), tensor(1.1554329395), tensor(1.1045372486), tensor(1.0709251165)]\n",
            "c:  [tensor(-0.0776312947), tensor(-0.0776312947), tensor(-0.0776312947), tensor(-0.0434991941), tensor(0.0020464777), tensor(-0.0245980080), tensor(0.0110338470), tensor(0.0169133507), tensor(-0.0434991941), tensor(0.0020464777), tensor(-0.0245980080), tensor(0.0110338470), tensor(0.0169133507), tensor(-0.0434991941), tensor(0.0020464777), tensor(-0.0245980080), tensor(0.0110338470), tensor(0.0169133507)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.8745050430e+00, -2.2814005613e-01, -1.6361141205e-01,\n",
            "         1.7802071571e-01, -1.8872647285e+00,  8.2038682699e-01,\n",
            "        -1.3020703197e-01, -2.0758414268e-01, -1.2822384834e+00,\n",
            "         4.9649235606e-01,  1.6554307938e-01,  1.2741818428e+00,\n",
            "        -5.3442068398e-02,  3.0408835411e-01,  1.6256110370e-01,\n",
            "        -1.2003871799e-01, -9.0797305107e-02, -1.1945638657e+00,\n",
            "         6.6079199314e-04,  1.1660975218e-01, -1.5380197763e+00,\n",
            "        -1.5657606125e+00, -1.2626941204e+00])\n",
            "btensor.grad: tensor([-0.2044042796, -0.0539715290, -0.0329495072, -0.0636398196,\n",
            "         0.1680312753, -0.1239432096, -0.0138154626,  0.0039321184,\n",
            "         0.1500706375, -0.1181002855, -0.0724155903, -0.1554221213,\n",
            "        -0.0248593092, -0.0839322805, -0.0745579004, -0.0055986643,\n",
            "        -0.0231681243,  0.1415767670, -0.0413198471, -0.0425894260,\n",
            "         0.1840001047,  0.1550664306,  0.1108983755])\n",
            "ctensor.grad: tensor([10.4711427689, 10.4711427689, 10.4711427689, 12.7306880951,\n",
            "        -0.2970262170,  7.8240242004, -2.5670895576, -5.4962787628,\n",
            "        12.7306880951, -0.2970262170,  7.8240242004, -2.5670895576,\n",
            "        -5.4962787628, 12.7306880951, -0.2970262170,  7.8240242004,\n",
            "        -2.5670895576, -5.4962787628])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.8352050781, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9397058487), tensor(1.3627467155), tensor(1.3472430706), tensor(1.6511203051), tensor(1.3450429440), tensor(2.1809070110), tensor(1.4265097380), tensor(1.5029714108), tensor(1.1179821491), tensor(1.8039612770), tensor(1.5366612673), tensor(2.5773503780), tensor(1.4206231833), tensor(1.6537619829), tensor(1.6002215147), tensor(1.4030631781), tensor(1.5687103271), tensor(1.0780779123), tensor(1.4947577715), tensor(1.5617282391), tensor(1.1976042986), tensor(1.1925061941), tensor(1.0742959976)]\n",
            "b:  [tensor(0.6855058074), tensor(0.8882845640), tensor(0.9067320824), tensor(0.9525080323), tensor(1.1227583885), tensor(0.7136396170), tensor(0.9540302157), tensor(0.9131808281), tensor(1.1966637373), tensor(0.8236327767), tensor(0.9567892551), tensor(0.6575908065), tensor(0.9679471254), tensor(0.9045623541), tensor(0.9115646482), tensor(0.9679762721), tensor(0.8353472352), tensor(1.1573613882), tensor(0.9288399220), tensor(0.9165574312), tensor(1.1552572250), tensor(1.1043908596), tensor(1.0708229542)]\n",
            "c:  [tensor(-0.0781645253), tensor(-0.0781645253), tensor(-0.0781645253), tensor(-0.0441353545), tensor(0.0020614250), tensor(-0.0249852613), tensor(0.0111655183), tensor(0.0171961542), tensor(-0.0441353545), tensor(0.0020614250), tensor(-0.0249852613), tensor(0.0111655183), tensor(0.0171961542), tensor(-0.0441353545), tensor(0.0020614250), tensor(-0.0249852613), tensor(0.0111655183), tensor(0.0171961542)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.8454149961, -0.2260529697, -0.1599788666,  0.1664046049,\n",
            "        -1.8406510353,  0.8021682501, -0.1284610629, -0.2069137692,\n",
            "        -1.2455897331,  0.4823167920,  0.1570652127,  1.2510657310,\n",
            "        -0.0563758686,  0.2928146124,  0.1543241888, -0.1193312407,\n",
            "        -0.0885332227, -1.1577378511, -0.0055825263,  0.1102724075,\n",
            "        -1.4962766171, -1.5256677866, -1.2273435593])\n",
            "btensor.grad: tensor([-0.2057169825, -0.0587699413, -0.0373938382, -0.0665109158,\n",
            "         0.1584222615, -0.1263377666, -0.0173752010,  0.0007328987,\n",
            "         0.1418169439, -0.1211445332, -0.0757153034, -0.1571082771,\n",
            "        -0.0284996033, -0.0867594481, -0.0779716969, -0.0093210936,\n",
            "        -0.0266491324,  0.1335723400, -0.0446754694, -0.0457175970,\n",
            "         0.1757181585,  0.1463844478,  0.1021674275])\n",
            "ctensor.grad: tensor([10.6646528244, 10.6646528244, 10.6646528244, 12.7231922150,\n",
            "        -0.2989440560,  7.7450809479, -2.6334283352, -5.6560711861,\n",
            "        12.7231922150, -0.2989440560,  7.7450809479, -2.6334283352,\n",
            "        -5.6560711861, 12.7231922150, -0.2989440560,  7.7450809479,\n",
            "        -2.6334283352, -5.6560711861])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.7614746094, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9378888607), tensor(1.3629708290), tensor(1.3473995924), tensor(1.6509649754), tensor(1.3468383551), tensor(2.1801223755), tensor(1.4266364574), tensor(1.5031776428), tensor(1.1191924810), tensor(1.8034925461), tensor(1.5365122557), tensor(2.5761218071), tensor(1.4206823111), tensor(1.6534799337), tensor(1.6000750065), tensor(1.4031817913), tensor(1.5687966347), tensor(1.0792002678), tensor(1.4947693348), tensor(1.5616239309), tensor(1.1990603209), tensor(1.1939932108), tensor(1.0754892826)]\n",
            "b:  [tensor(0.6857128739), tensor(0.8883481026), tensor(0.9067738652), tensor(0.9525774717), tensor(1.1226093769), tensor(0.7137683630), tensor(0.9540510774), tensor(0.9131832719), tensor(1.1965299845), tensor(0.8237569928), tensor(0.9568682909), tensor(0.6577496529), tensor(0.9679792523), tensor(0.9046519399), tensor(0.9116460681), tensor(0.9679892659), tensor(0.8353773355), tensor(1.1572355032), tensor(0.9288879633), tensor(0.9166063070), tensor(1.1550894976), tensor(1.1042529345), tensor(1.0707292557)]\n",
            "c:  [tensor(-0.0787073448), tensor(-0.0787073448), tensor(-0.0787073448), tensor(-0.0447712578), tensor(0.0020764708), tensor(-0.0253686365), tensor(0.0113005312), tensor(0.0174869746), tensor(-0.0447712578), tensor(0.0020764708), tensor(-0.0253686365), tensor(0.0113005312), tensor(0.0174869746), tensor(-0.0447712578), tensor(0.0020764708), tensor(-0.0253686365), tensor(0.0113005312), tensor(0.0174869746)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.8170659542, -0.2241339386, -0.1565551758,  0.1552935839,\n",
            "        -1.7954512835,  0.7845847011, -0.1267784536, -0.2062799335,\n",
            "        -1.2103724480,  0.4687592685,  0.1489991546,  1.2286062241,\n",
            "        -0.0591786429,  0.2820743322,  0.1464667320, -0.1186505258,\n",
            "        -0.0862843990, -1.1223080158, -0.0115963519,  0.1042605639,\n",
            "        -1.4560092688, -1.4869790077, -1.1933414936])\n",
            "btensor.grad: tensor([-0.2070689052, -0.0635604858, -0.0417819992, -0.0694168210,\n",
            "         0.1490558088, -0.1287571192, -0.0208868086, -0.0024217367,\n",
            "         0.1338120997, -0.1242345572, -0.0790448189, -0.1588490605,\n",
            "        -0.0321261883, -0.0896022320, -0.0814031363, -0.0129983425,\n",
            "        -0.0300975069,  0.1258463860, -0.0480220914, -0.0488761663,\n",
            "         0.1677018702,  0.1379446983,  0.0936850607])\n",
            "ctensor.grad: tensor([10.8563766479, 10.8563766479, 10.8563766479, 12.7180767059,\n",
            "        -0.3009136319,  7.6675190926, -2.7002532482, -5.8163990974,\n",
            "        12.7180767059, -0.3009136319,  7.6675190926, -2.7002532482,\n",
            "        -5.8163990974, 12.7180767059, -0.3009136319,  7.6675190926,\n",
            "        -2.7002532482, -5.8163990974])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.6845703125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9360995293), tensor(1.3631931543), tensor(1.3475528955), tensor(1.6508202553), tensor(1.3485900164), tensor(2.1793546677), tensor(1.4267616272), tensor(1.5033832788), tensor(1.1203690767), tensor(1.8030366898), tensor(1.5363708735), tensor(2.5749149323), tensor(1.4207441807), tensor(1.6532081366), tensor(1.5999360085), tensor(1.4032998085), tensor(1.5688806772), tensor(1.0802885294), tensor(1.4947867393), tensor(1.5615253448), tensor(1.2004774809), tensor(1.1954429150), tensor(1.0766499043)]\n",
            "b:  [tensor(0.6859213114), tensor(0.8884164095), tensor(0.9068199396), tensor(0.9526498318), tensor(1.1224694252), tensor(0.7138996124), tensor(0.9540754557), tensor(0.9131888151), tensor(1.1964038610), tensor(0.8238843679), tensor(0.9569506645), tensor(0.6579102874), tensor(0.9680149555), tensor(0.9047444463), tensor(0.9117308855), tensor(0.9680058956), tensor(0.8354108930), tensor(1.1571171284), tensor(0.9289393425), tensor(0.9166583419), tensor(1.1549295187), tensor(1.1041232347), tensor(1.0706439018)]\n",
            "c:  [tensor(-0.0792596713), tensor(-0.0792596713), tensor(-0.0792596713), tensor(-0.0454070196), tensor(0.0020916176), tensor(-0.0257482007), tensor(0.0114389099), tensor(0.0177858397), tensor(-0.0454070196), tensor(0.0020916176), tensor(-0.0257482007), tensor(0.0114389099), tensor(0.0177858397), tensor(-0.0454070196), tensor(0.0020916176), tensor(-0.0257482007), tensor(0.0114389099), tensor(0.0177858397)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.7894397974, -0.2223787010, -0.1533465385,  0.1446690559,\n",
            "        -1.7516279221,  0.7676259875, -0.1251670420, -0.2056862712,\n",
            "        -1.1765370369,  0.4558077455,  0.1413260102,  1.2067937851,\n",
            "        -0.0618548617,  0.2718499899,  0.1389801055, -0.1180042922,\n",
            "        -0.0840562582, -1.0882248878, -0.0173978806,  0.0985656977,\n",
            "        -1.4171687365, -1.4496451616, -1.1606435776])\n",
            "btensor.grad: tensor([-0.2084606439, -0.0683225393, -0.0461006314, -0.0723418593,\n",
            "         0.1398991048, -0.1312342882, -0.0243946612, -0.0055397749,\n",
            "         0.1260675192, -0.1273639202, -0.0823922157, -0.1606347263,\n",
            "        -0.0357152820, -0.0924900770, -0.0848468542, -0.0166593790,\n",
            "        -0.0335450843,  0.1183450222, -0.0513644814, -0.0520312786,\n",
            "         0.1599232554,  0.1297247410,  0.0854063034])\n",
            "ctensor.grad: tensor([11.0464839935, 11.0464839935, 11.0464839935, 12.7152700424,\n",
            "        -0.3029347658,  7.5912828445, -2.7675814629, -5.9773063660,\n",
            "        12.7152700424, -0.3029347658,  7.5912828445, -2.7675814629,\n",
            "        -5.9773063660, 12.7152700424, -0.3029347658,  7.5912828445,\n",
            "        -2.7675814629, -5.9773063660])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.6101074219, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9343369007), tensor(1.3634139299), tensor(1.3477032185), tensor(1.6506857872), tensor(1.3502991199), tensor(2.1786034107), tensor(1.4268852472), tensor(1.5035884380), tensor(1.1215131283), tensor(1.8025932312), tensor(1.5362368822), tensor(2.5737292767), tensor(1.4208085537), tensor(1.6529459953), tensor(1.5998041630), tensor(1.4034172297), tensor(1.5689625740), tensor(1.0813440084), tensor(1.4948097467), tensor(1.5614321232), tensor(1.2018572092), tensor(1.1968564987), tensor(1.0777790546)]\n",
            "b:  [tensor(0.6861312389), tensor(0.8884894848), tensor(0.9068703651), tensor(0.9527251124), tensor(1.1223385334), tensor(0.7140333652), tensor(0.9541033506), tensor(0.9131974578), tensor(1.1962853670), tensor(0.8240149021), tensor(0.9570364356), tensor(0.6580727696), tensor(0.9680542350), tensor(0.9048398137), tensor(0.9118192196), tensor(0.9680261612), tensor(0.8354478478), tensor(1.1570060253), tensor(0.9289940596), tensor(0.9167135358), tensor(1.1547771692), tensor(1.1040015221), tensor(1.0705665350)]\n",
            "c:  [tensor(-0.0798214227), tensor(-0.0798214227), tensor(-0.0798214227), tensor(-0.0460427552), tensor(0.0021068680), tensor(-0.0261240173), tensor(0.0115806824), tensor(0.0180927832), tensor(-0.0460427552), tensor(0.0021068680), tensor(-0.0261240173), tensor(0.0115806824), tensor(0.0180927832), tensor(-0.0460427552), tensor(0.0021068680), tensor(-0.0261240173), tensor(0.0115806824), tensor(0.0180927832)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.7625232935, -0.2207832038, -0.1503422260,  0.1345037222,\n",
            "        -1.7091271877,  0.7512753606, -0.1236190498, -0.2051455379,\n",
            "        -1.1440272331,  0.4434309602,  0.1340222359,  1.1856086254,\n",
            "        -0.0644167885,  0.2621154785,  0.1318395734, -0.1173887253,\n",
            "        -0.0818445086, -1.0554326773, -0.0230007470,  0.0931760073,\n",
            "        -1.3797020912, -1.4136146307, -1.1291967630])\n",
            "btensor.grad: tensor([-0.2099367976, -0.0730763674, -0.0504278317, -0.0752974153,\n",
            "         0.1309307218, -0.1337511539, -0.0278662443, -0.0086215734,\n",
            "         0.1185443997, -0.1305474043, -0.0857758522, -0.1624747515,\n",
            "        -0.0393069983, -0.0953929424, -0.0883370638, -0.0202695131,\n",
            "        -0.0369552523,  0.1110751629, -0.0547134876, -0.0551766157,\n",
            "         0.1523793936,  0.1217288375,  0.0773491859])\n",
            "ctensor.grad: tensor([11.2350625992, 11.2350625992, 11.2350625992, 12.7147436142,\n",
            "        -0.3050078154,  7.5163145065, -2.8354399204, -6.1388549805,\n",
            "        12.7147436142, -0.3050078154,  7.5163145065, -2.8354399204,\n",
            "        -6.1388549805, 12.7147436142, -0.3050078154,  7.5163145065,\n",
            "        -2.8354399204, -6.1388549805])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.5354003906, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9326004982), tensor(1.3636332750), tensor(1.3478507996), tensor(1.6505609751), tensor(1.3519669771), tensor(2.1778678894), tensor(1.4270073175), tensor(1.5037931204), tensor(1.1226259470), tensor(1.8021615744), tensor(1.5361098051), tensor(2.5725641251), tensor(1.4208754301), tensor(1.6526931524), tensor(1.5996791124), tensor(1.4035340548), tensor(1.5690422058), tensor(1.0823678970), tensor(1.4948381186), tensor(1.5613440275), tensor(1.2032008171), tensor(1.1982353926), tensor(1.0788780451)]\n",
            "b:  [tensor(0.6863427162), tensor(0.8885672688), tensor(0.9069250822), tensor(0.9528033733), tensor(1.1222163439), tensor(0.7141696811), tensor(0.9541346431), tensor(0.9132091403), tensor(1.1961741447), tensor(0.8241487145), tensor(0.9571256042), tensor(0.6582370996), tensor(0.9680970907), tensor(0.9049381614), tensor(0.9119110703), tensor(0.9680500031), tensor(0.8354882002), tensor(1.1569019556), tensor(0.9290521145), tensor(0.9167718887), tensor(1.1546320915), tensor(1.1038875580), tensor(1.0704970360)]\n",
            "c:  [tensor(-0.0803925395), tensor(-0.0803925395), tensor(-0.0803925395), tensor(-0.0466785766), tensor(0.0021222245), tensor(-0.0264961440), tensor(0.0117258746), tensor(0.0184078366), tensor(-0.0466785766), tensor(0.0021222245), tensor(-0.0264961440), tensor(0.0117258746), tensor(0.0184078366), tensor(-0.0466785766), tensor(0.0021222245), tensor(-0.0264961440), tensor(0.0117258746), tensor(0.0184078366)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.7363047600, -0.2193438709, -0.1475307941,  0.1247880459,\n",
            "        -1.6679054499,  0.7355181575, -0.1221298873, -0.2046428323,\n",
            "        -1.1127966642,  0.4316157103,  0.1270785928,  1.1650443077,\n",
            "        -0.0668670461,  0.2528512478,  0.1250329167, -0.1168012023,\n",
            "        -0.0796434283, -1.0238810778, -0.0284148604,  0.0880751908,\n",
            "        -1.3435529470, -1.3788465261, -1.0989544392])\n",
            "btensor.grad: tensor([-0.2114564776, -0.0778034925, -0.0547065660, -0.0782783628,\n",
            "         0.1221800148, -0.1363122463, -0.0312786102, -0.0116733313,\n",
            "         0.1112527847, -0.1337922215, -0.0891795158, -0.1643364727,\n",
            "        -0.0428848863, -0.0983575583, -0.0918384790, -0.0238469839,\n",
            "        -0.0403349400,  0.1040217876, -0.0580692887, -0.0583275557,\n",
            "         0.1450679302,  0.1139125526,  0.0694688261])\n",
            "ctensor.grad: tensor([11.4222688675, 11.4222688675, 11.4222688675, 12.7164506912,\n",
            "        -0.3071326911,  7.4425282478, -2.9038507938, -6.3010826111,\n",
            "        12.7164506912, -0.3071326911,  7.4425282478, -2.9038507938,\n",
            "        -6.3010826111, 12.7164506912, -0.3071326911,  7.4425282478,\n",
            "        -2.9038507938, -6.3010826111])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.4614257812, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9308898449), tensor(1.3638513088), tensor(1.3479957581), tensor(1.6504454613), tensor(1.3535948992), tensor(2.1771476269), tensor(1.4271280766), tensor(1.5039973259), tensor(1.1237087250), tensor(1.8017412424), tensor(1.5359892845), tensor(2.5714190006), tensor(1.4209446907), tensor(1.6524491310), tensor(1.5995606184), tensor(1.4036502838), tensor(1.5691196918), tensor(1.0833613873), tensor(1.4948717356), tensor(1.5612608194), tensor(1.2045094967), tensor(1.1995806694), tensor(1.0799479485)]\n",
            "b:  [tensor(0.6865557432), tensor(0.8886498213), tensor(0.9069840312), tensor(0.9528846741), tensor(1.1221027374), tensor(0.7143086195), tensor(0.9541693330), tensor(0.9132238626), tensor(1.1960699558), tensor(0.8242858052), tensor(0.9572182298), tensor(0.6584033370), tensor(0.9681435227), tensor(0.9050394893), tensor(0.9120064378), tensor(0.9680774212), tensor(0.8355318904), tensor(1.1568048000), tensor(0.9291135669), tensor(0.9168333411), tensor(1.1544941664), tensor(1.1037812233), tensor(1.0704352856)]\n",
            "c:  [tensor(-0.0809729472), tensor(-0.0809729472), tensor(-0.0809729472), tensor(-0.0473145954), tensor(0.0021376901), tensor(-0.0268646404), tensor(0.0118745165), tensor(0.0187310372), tensor(-0.0473145954), tensor(0.0021376901), tensor(-0.0268646404), tensor(0.0118745165), tensor(0.0187310372), tensor(-0.0473145954), tensor(0.0021376901), tensor(-0.0268646404), tensor(0.0118745165), tensor(0.0187310372)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.7107715607, -0.2180561125, -0.1449096203,  0.1154893637,\n",
            "        -1.6279243231,  0.7203403115, -0.1207094789, -0.2041875124,\n",
            "        -1.0827983618,  0.4203428030,  0.1204724908,  1.1450866461,\n",
            "        -0.0692204461,  0.2440463305,  0.1185500622, -0.1162523031,\n",
            "        -0.0774673223, -0.9935234785, -0.0336662382,  0.0832528770,\n",
            "        -1.3086808920, -1.3452960253, -1.0698722601])\n",
            "btensor.grad: tensor([-0.2130361050, -0.0825262070, -0.0589563400, -0.0813120008,\n",
            "         0.1135936379, -0.1389125586, -0.0346884429, -0.0146937370,\n",
            "         0.1041601598, -0.1370743513, -0.0926001072, -0.1662443876,\n",
            "        -0.0464504361, -0.1013342142, -0.0953615904, -0.0274151564,\n",
            "        -0.0437071770,  0.0971648693, -0.0614389181, -0.0614790916,\n",
            "         0.1379529834,  0.1062789559,  0.0617747009])\n",
            "ctensor.grad: tensor([11.6081790924, 11.6081790924, 11.6081790924, 12.7203884125,\n",
            "        -0.3093094826,  7.3699250221, -2.9728357792, -6.4640278816,\n",
            "        12.7203884125, -0.3093094826,  7.3699250221, -2.9728357792,\n",
            "        -6.4640278816, 12.7203884125, -0.3093094826,  7.3699250221,\n",
            "        -2.9728357792, -6.4640278816])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.3857421875, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9292039871), tensor(1.3640682697), tensor(1.3481382132), tensor(1.6503388882), tensor(1.3551840782), tensor(2.1764419079), tensor(1.4272474051), tensor(1.5042010546), tensor(1.1247627735), tensor(1.8013316393), tensor(1.5358750820), tensor(2.5702931881), tensor(1.4210162163), tensor(1.6522134542), tensor(1.5994482040), tensor(1.4037660360), tensor(1.5691950321), tensor(1.0843256712), tensor(1.4949104786), tensor(1.5611821413), tensor(1.2057845592), tensor(1.2008936405), tensor(1.0809898376)]\n",
            "b:  [tensor(0.6867704391), tensor(0.8887370825), tensor(0.9070472121), tensor(0.9529690146), tensor(1.1219975948), tensor(0.7144501805), tensor(0.9542074203), tensor(0.9132415652), tensor(1.1959726810), tensor(0.8244262338), tensor(0.9573142529), tensor(0.6585715413), tensor(0.9681935310), tensor(0.9051438570), tensor(0.9121053815), tensor(0.9681083560), tensor(0.8355789781), tensor(1.1567143202), tensor(0.9291783571), tensor(0.9168980122), tensor(1.1543631554), tensor(1.1036823988), tensor(1.0703810453)]\n",
            "c:  [tensor(-0.0815625936), tensor(-0.0815625936), tensor(-0.0815625936), tensor(-0.0479509197), tensor(0.0021532669), tensor(-0.0272295624), tensor(0.0120266369), tensor(0.0190624241), tensor(-0.0479509197), tensor(0.0021532669), tensor(-0.0272295624), tensor(0.0120266369), tensor(0.0190624241), tensor(-0.0479509197), tensor(0.0021532669), tensor(-0.0272295624), tensor(0.0120266369), tensor(0.0190624241)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.6859183311, -0.2169224024, -0.1424720287,  0.1065946817,\n",
            "        -1.5891385078,  0.7057300806, -0.1193463802, -0.2037848830,\n",
            "        -1.0539890528,  0.4096010327,  0.1141912937,  1.1257246733,\n",
            "        -0.0714791119,  0.2356779575,  0.1123735905, -0.1157372594,\n",
            "        -0.0753089190, -0.9643119574, -0.0387477726,  0.0786921680,\n",
            "        -1.2750432491, -1.3129268885, -1.0419046879])\n",
            "btensor.grad: tensor([-0.2146865577, -0.0872439146, -0.0631853491, -0.0843678713,\n",
            "         0.1051782668, -0.1415631771, -0.0380583405, -0.0176779032,\n",
            "         0.0972544253, -0.1404224038, -0.0960438251, -0.1681966186,\n",
            "        -0.0500140786, -0.1043394804, -0.0989398956, -0.0309311152,\n",
            "        -0.0470730513,  0.0905113220, -0.0647907853, -0.0646755695,\n",
            "         0.1310400069,  0.0988247395,  0.0542681515])\n",
            "ctensor.grad: tensor([11.7929239273, 11.7929239273, 11.7929239273, 12.7265205383,\n",
            "        -0.3115383685,  7.2984242439, -3.0424141884, -6.6277365685,\n",
            "        12.7265205383, -0.3115383685,  7.2984242439, -3.0424141884,\n",
            "        -6.6277365685, 12.7265205383, -0.3115383685,  7.2984242439,\n",
            "        -3.0424141884, -6.6277365685])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.3095703125, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9275422096), tensor(1.3642841578), tensor(1.3482784033), tensor(1.6502407789), tensor(1.3567355871), tensor(2.1757502556), tensor(1.4273654222), tensor(1.5044045448), tensor(1.1257890463), tensor(1.8009322882), tensor(1.5357668400), tensor(2.5691862106), tensor(1.4210898876), tensor(1.6519857645), tensor(1.5993417501), tensor(1.4038813114), tensor(1.5692682266), tensor(1.0852618217), tensor(1.4949541092), tensor(1.5611077547), tensor(1.2070271969), tensor(1.2021753788), tensor(1.0820049047)]\n",
            "b:  [tensor(0.6869868636), tensor(0.8888290524), tensor(0.9071146250), tensor(0.9530564547), tensor(1.1219006777), tensor(0.7145944834), tensor(0.9542488456), tensor(0.9132621884), tensor(1.1958822012), tensor(0.8245700598), tensor(0.9574137926), tensor(0.6587417126), tensor(0.9682471156), tensor(0.9052512646), tensor(0.9122079611), tensor(0.9681428075), tensor(0.8356294036), tensor(1.1566302776), tensor(0.9292465448), tensor(0.9169659019), tensor(1.1542388201), tensor(1.1035908461), tensor(1.0703340769)]\n",
            "c:  [tensor(-0.0821614265), tensor(-0.0821614265), tensor(-0.0821614265), tensor(-0.0485876612), tensor(0.0021689578), tensor(-0.0275909621), tensor(0.0121822674), tensor(0.0194020364), tensor(-0.0485876612), tensor(0.0021689578), tensor(-0.0275909621), tensor(0.0121822674), tensor(0.0194020364), tensor(-0.0485876612), tensor(0.0021689578), tensor(-0.0275909621), tensor(0.0121822674), tensor(0.0194020364)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.6617288589, -0.2159329653, -0.1402130127,  0.0980850458,\n",
            "        -1.5515136719,  0.6916673183, -0.1180499196, -0.2034366131,\n",
            "        -1.0263200998,  0.3993636966,  0.1082199216,  1.1069465876,\n",
            "        -0.0736539811,  0.2277328968,  0.1064794213, -0.1152603626,\n",
            "        -0.0731675029, -0.9362009764, -0.0436790138,  0.0743820071,\n",
            "        -1.2425891161, -1.2816927433, -1.0150229931])\n",
            "btensor.grad: tensor([-0.2163976729, -0.0919588804, -0.0674049929, -0.0874678493,\n",
            "         0.0969330668, -0.1442784071, -0.0414133370, -0.0206512213,\n",
            "         0.0905295312, -0.1438454986, -0.0995440483, -0.1701973975,\n",
            "        -0.0535606146, -0.1073783636, -0.1025563478, -0.0344307423,\n",
            "        -0.0504178926,  0.0840468407, -0.0681641102, -0.0678676367,\n",
            "         0.1243149638,  0.0915388465,  0.0469130576])\n",
            "ctensor.grad: tensor([11.9765920639, 11.9765920639, 11.9765920639, 12.7348031998,\n",
            "        -0.3138195872,  7.2279987335, -3.1126034260, -6.7922558784,\n",
            "        12.7348031998, -0.3138195872,  7.2279987335, -3.1126034260,\n",
            "        -6.7922558784, 12.7348031998, -0.3138195872,  7.2279987335,\n",
            "        -3.1126034260, -6.7922558784])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.2341308594, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9259040356), tensor(1.3644992113), tensor(1.3484165668), tensor(1.6501508951), tensor(1.3582506180), tensor(2.1750721931), tensor(1.4274822474), tensor(1.5046076775), tensor(1.1267888546), tensor(1.8005427122), tensor(1.5356643200), tensor(2.5680975914), tensor(1.4211655855), tensor(1.6517655849), tensor(1.5992408991), tensor(1.4039961100), tensor(1.5693392754), tensor(1.0861710310), tensor(1.4950026274), tensor(1.5610374212), tensor(1.2082384825), tensor(1.2034269571), tensor(1.0829941034)]\n",
            "b:  [tensor(0.6872050166), tensor(0.8889257312), tensor(0.9071862102), tensor(0.9531470537), tensor(1.1218118668), tensor(0.7147415280), tensor(0.9542936087), tensor(0.9132857919), tensor(1.1957981586), tensor(0.8247174025), tensor(0.9575168490), tensor(0.6589139700), tensor(0.9683042169), tensor(0.9053617120), tensor(0.9123141766), tensor(0.9681807160), tensor(0.8356831670), tensor(1.1565525532), tensor(0.9293181300), tensor(0.9170369506), tensor(1.1541210413), tensor(1.1035064459), tensor(1.0702943802)]\n",
            "c:  [tensor(-0.0827693939), tensor(-0.0827693939), tensor(-0.0827693939), tensor(-0.0492249243), tensor(0.0021847654), tensor(-0.0279488917), tensor(0.0123414388), tensor(0.0197499171), tensor(-0.0492249243), tensor(0.0021847654), tensor(-0.0279488917), tensor(0.0123414388), tensor(0.0197499171), tensor(-0.0492249243), tensor(0.0021847654), tensor(-0.0279488917), tensor(0.0123414388), tensor(0.0197499171)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.6381949186, -0.2150987387, -0.1381292343,  0.0899389982,\n",
            "        -1.5150096416,  0.6781412363, -0.1168172956, -0.2031403780,\n",
            "        -0.9997602701,  0.3896192908,  0.1025379300,  1.0887358189,\n",
            "        -0.0757536143,  0.2201898098,  0.1008771211, -0.1148198545,\n",
            "        -0.0710486770, -0.9091553688, -0.0484767556,  0.0703130066,\n",
            "        -1.2112854719, -1.2515588999, -0.9891802073])\n",
            "btensor.grad: tensor([-0.2181721777, -0.0966999531, -0.0716036409, -0.0906037092,\n",
            "         0.0888232887, -0.1470485926, -0.0447681546, -0.0236239433,\n",
            "         0.0839853585, -0.1473432779, -0.1030774117, -0.1722641587,\n",
            "        -0.0571129918, -0.1104542017, -0.1061905622, -0.0379188061,\n",
            "        -0.0537713245,  0.0777375698, -0.0715697408, -0.0710711479,\n",
            "         0.1177495122,  0.0844024718,  0.0396946967])\n",
            "ctensor.grad: tensor([12.1592931747, 12.1592931747, 12.1592931747, 12.7452421188,\n",
            "        -0.3161531687,  7.1585955620, -3.1834237576, -6.9575943947,\n",
            "        12.7452421188, -0.3161531687,  7.1585955620, -3.1834237576,\n",
            "        -6.9575943947, 12.7452421188, -0.3161531687,  7.1585955620,\n",
            "        -3.1834237576, -6.9575943947])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Cross-entropy loss vs Poisson: tensor(2172.1572265625, grad_fn=<SubBackward0>)\n",
            "\n",
            "\n",
            "Weights:\n",
            "\n",
            "a:  [tensor(2.9242887497), tensor(1.3647136688), tensor(1.3485528231), tensor(1.6500687599), tensor(1.3597302437), tensor(2.1744070053), tensor(1.4275978804), tensor(1.5048105717), tensor(1.1277631521), tensor(1.8001623154), tensor(1.5355671644), tensor(2.5670266151), tensor(1.4212434292), tensor(1.6515525579), tensor(1.5991454124), tensor(1.4041105509), tensor(1.5694081783), tensor(1.0870541334), tensor(1.4950557947), tensor(1.5609709024), tensor(1.2094196081), tensor(1.2046494484), tensor(1.0839583874)]\n",
            "b:  [tensor(0.6874250770), tensor(0.8890271783), tensor(0.9072620273), tensor(0.9532408118), tensor(1.1217310429), tensor(0.7148914337), tensor(0.9543417096), tensor(0.9133123755), tensor(1.1957205534), tensor(0.8248683214), tensor(0.9576234818), tensor(0.6590883136), tensor(0.9683648944), tensor(0.9054753184), tensor(0.9124240875), tensor(0.9682221413), tensor(0.8357402682), tensor(1.1564809084), tensor(0.9293931127), tensor(0.9171112776), tensor(1.1540096998), tensor(1.1034290791), tensor(1.0702617168)]\n",
            "c:  [tensor(-0.0833864510), tensor(-0.0833864510), tensor(-0.0833864510), tensor(-0.0498628132), tensor(0.0022006924), tensor(-0.0283034015), tensor(0.0125041837), tensor(0.0201061070), tensor(-0.0498628132), tensor(0.0022006924), tensor(-0.0283034015), tensor(0.0125041837), tensor(0.0201061070), tensor(-0.0498628132), tensor(0.0022006924), tensor(-0.0283034015), tensor(0.0125041837), tensor(0.0201061070)]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Partial derivatives of loss w/r to hidden weights:\n",
            "\n",
            "atensor.grad: tensor([ 1.6153012514, -0.2144102752, -0.1362130642,  0.0821484327,\n",
            "        -1.4795982838,  0.6651346684, -0.1156448722, -0.2029024363,\n",
            "        -0.9742661119,  0.3803505301,  0.0971390605,  1.0710898638,\n",
            "        -0.0777877718,  0.2130322456,  0.0955373794, -0.1144191921,\n",
            "        -0.0689516068, -0.8831278086, -0.0531430542,  0.0664690435,\n",
            "        -1.1810952425, -1.2224938869, -0.9643422365])\n",
            "btensor.grad: tensor([-0.2200581282, -0.1014710665, -0.0758327767, -0.0937619209,\n",
            "         0.0808543563, -0.1498987675, -0.0481244624, -0.0265612602,\n",
            "         0.0775866508, -0.1508992314, -0.1066491604, -0.1743732691,\n",
            "        -0.0606791973, -0.1136033535, -0.1098821163, -0.0414237976,\n",
            "        -0.0571117252,  0.0715925694, -0.0749765038, -0.0743013620,\n",
            "         0.1113578677,  0.0774217844,  0.0326208174])\n",
            "ctensor.grad: tensor([12.3411254883, 12.3411254883, 12.3411254883, 12.7578115463,\n",
            "        -0.3185392320,  7.0901823044, -3.2548954487, -7.1237983704,\n",
            "        12.7578115463, -0.3185392320,  7.0901823044, -3.2548954487,\n",
            "        -7.1237983704, 12.7578115463, -0.3185392320,  7.0901823044,\n",
            "        -3.2548954487, -7.1237983704])\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "=============================================================\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.7278840542), tensor(0.8731035590)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0257787704), tensor(1.2804790735)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(1.4474463463), tensor(0.8741135597)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.4055645466), tensor(1.3802490234)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.2046388388), tensor(1.4631522894)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.9954419136), tensor(1.0109424591)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2119921446), tensor(1.3119865656)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.2530367374), tensor(1.3988512754)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(0.9163136482), tensor(1.7994464636)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0962185860), tensor(2.0272114277)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.2900168896), tensor(1.1882609129)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.7742629051), tensor(0.7909313440)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.2404593229), tensor(1.4835407734)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4690552950), tensor(1.1855643988)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0528556108), tensor(1.8936216831)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1330822706), tensor(1.2061358690)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.7619030476), tensor(0.8278465867)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.8777269125), tensor(1.6251918077)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1302598715), tensor(1.5700942278)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.9777691960), tensor(2.5866167545)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.1156797409), tensor(1.3929893970)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.3415331841), tensor(1.1823666096)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.9768123627), tensor(1.5638109446)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1385071278), tensor(1.6424727440)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.2620248795), tensor(1.3603793383)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(2.6137626171), tensor(0.9236876965)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4071328640), tensor(1.1972198486)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.4155864716), tensor(0.6780518889)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.5605460405), tensor(1.1472195387)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2947256565), tensor(1.3333151340)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(2.2273092270), tensor(0.7871593237)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.0694795847), tensor(1.6403548717)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.7543658018), tensor(0.9221651554)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3088439703), tensor(1.1323578358)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.3857028484), tensor(1.2687078714)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.9241244197), tensor(1.5376927853)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0397633314), tensor(2.5523159504)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9090080261), tensor(2.0105304718)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(2.6866796017), tensor(0.8772757649)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.0114855766), tensor(1.5323259830)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3120383024), tensor(1.3985747099)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9731820822), tensor(1.8833186626)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(2.0473296642), tensor(0.9363405704)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1893857718), tensor(1.4411653280)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5140482187), tensor(0.9427311420)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1819198132), tensor(1.3479750156)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.4276263714), tensor(1.2464209795)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.5056707859), tensor(1.1448603868)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2488348484), tensor(1.2819423676)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.3864176273), tensor(1.4666484594)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8798506260), tensor(1.5090546608)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.9146759510), tensor(1.0169764757)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4347063303), tensor(1.3044155836)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1821388006), tensor(1.1739135981)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.4138760567), tensor(1.0678119659)]\n",
            "Actual goals:  [0, 5]\n",
            "Predicted goals:  [tensor(0.6132438183), tensor(2.8654966354)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4088314772), tensor(2.0046389103)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.4047702551), tensor(1.1966192722)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(2.5725574493), tensor(1.0036586523)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1838241816), tensor(1.1505942345)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.6713720560), tensor(0.9365063906)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.2820796967), tensor(0.8816767335)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.8710473180), tensor(2.3380877972)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1848349571), tensor(1.4084759951)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1718797684), tensor(1.3834288120)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.1354151964), tensor(1.5382552147)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.5810587406), tensor(0.9737349749)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2783678770), tensor(1.3191592693)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.7348980904), tensor(1.1801151037)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9051367640), tensor(1.4944522381)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1468924284), tensor(2.3215689659)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.8275959492), tensor(0.8701111674)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.4449961185), tensor(1.1778123379)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1879802942), tensor(1.3202409744)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.4102762938), tensor(1.0783069134)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.8777429461), tensor(1.8034312725)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3264828920), tensor(1.3219279051)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9338998199), tensor(2.2593309879)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.7901804447), tensor(2.1728703976)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.8361129761), tensor(1.6733868122)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.7944629192), tensor(0.9224144816)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.7011482120), tensor(2.4949028492)]\n",
            "Actual goals:  [6, 2]\n",
            "Predicted goals:  [tensor(1.4850589037), tensor(1.2939115763)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3251165152), tensor(1.3674434423)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0745701790), tensor(1.6149599552)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1487196684), tensor(1.2095093727)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2658562660), tensor(1.2494683266)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0251625776), tensor(1.6492351294)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1166293621), tensor(1.2502368689)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.3974429369), tensor(1.4162511826)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3331730366), tensor(1.2063074112)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2544647455), tensor(1.1961857080)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.2290699482), tensor(0.9932097197)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(2.0442042351), tensor(0.8219690323)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.9725755453), tensor(1.0794624090)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.4186078310), tensor(1.1950513124)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1648622751), tensor(1.1966648102)]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [tensor(1.1046253443), tensor(1.4577449560)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.9276928306), tensor(2.5640370846)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3375246525), tensor(1.2027689219)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8331850767), tensor(2.3822786808)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.6658883095), tensor(3.3872673512)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.9757125378), tensor(0.9211627245)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.6987074614), tensor(0.8904238939)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3783634901), tensor(1.2562536001)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1683344841), tensor(1.4960167408)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4034891129), tensor(1.1839222908)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2573949099), tensor(1.6122936010)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1188566685), tensor(1.3979386091)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1264947653), tensor(1.2179024220)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.3455545902), tensor(0.9804595709)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.2638888359), tensor(1.3435119390)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3861308098), tensor(1.2439699173)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.3181601763), tensor(1.3907415867)]\n",
            "Actual goals:  [6, 1]\n",
            "Predicted goals:  [tensor(1.5884004831), tensor(0.9576637745)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8487964869), tensor(1.4606411457)]\n",
            "Actual goals:  [5, 0]\n",
            "Predicted goals:  [tensor(2.2468347549), tensor(0.9585957527)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2019593716), tensor(1.7086381912)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.7440047264), tensor(1.1226091385)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1930960417), tensor(1.1766450405)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(0.8285036683), tensor(2.4901371002)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5306251049), tensor(0.9396492243)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9695237279), tensor(1.7256604433)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1246223450), tensor(1.5707565546)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.8757570982), tensor(0.8670689464)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1933808327), tensor(1.5567345619)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.3485043049), tensor(1.4074338675)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8260691762), tensor(2.3860886097)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3484460115), tensor(1.1393271685)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4601100683), tensor(1.2462252378)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3367068768), tensor(1.1941313744)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0187745094), tensor(1.3155990839)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3821482658), tensor(1.2561749220)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.6793481112), tensor(0.9100154042)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.4857922792), tensor(1.1820586920)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8849379420), tensor(1.7424646616)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.6827979088), tensor(1.0295451880)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.7494806051), tensor(1.3474614620)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.4917565584), tensor(1.0170724392)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4156954288), tensor(1.3144586086)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(1.1201438904), tensor(1.0808911324)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.4743933678), tensor(1.3580139875)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.6343194842), tensor(2.9604701996)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8292415738), tensor(3.1747946739)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.9217157364), tensor(0.9696395397)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3826022148), tensor(1.2021687031)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2243852615), tensor(1.5478123426)]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [tensor(1.7059355974), tensor(0.7994398475)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1799311638), tensor(1.4923357964)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.1698682308), tensor(1.3444676399)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(2.3660886288), tensor(0.9199181199)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.0887832642), tensor(1.4107878208)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.6442595720), tensor(1.0595089197)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.8784577847), tensor(1.0902667046)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3161787987), tensor(1.2706035376)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.2342709303), tensor(1.3297665119)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2685940266), tensor(1.2425103188)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.5510034561), tensor(0.8249360919)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.6151729822), tensor(0.8964701891)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(0.9560015202), tensor(1.8539929390)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1394537687), tensor(1.2425254583)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.9630593061), tensor(2.7078392506)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2630612850), tensor(1.3505551815)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.4788284302), tensor(1.2012906075)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9831748009), tensor(2.3426280022)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.2295696735), tensor(1.2718002796)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0287600756), tensor(1.7240431309)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.8627455235), tensor(0.9091118574)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(3.2782135010), tensor(0.6435978413)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1234655380), tensor(1.3560260534)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1314297915), tensor(1.3108663559)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(0.8853402734), tensor(2.2380342484)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4866540432), tensor(1.2274763584)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.9283885956), tensor(1.6679222584)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1315512657), tensor(1.6847116947)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.6324676275), tensor(1.1752306223)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(2.0063054562), tensor(0.9167207479)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.0833593607), tensor(1.3880100250)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.3098493814), tensor(1.4182636738)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.6158219576), tensor(1.1954381466)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(2.3461234570), tensor(0.8372160196)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1451857090), tensor(1.2032024860)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.2676614523), tensor(1.5353505611)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0260585546), tensor(1.9043070078)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3449546099), tensor(1.1245627403)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3214491606), tensor(1.1457201242)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.1822341681), tensor(1.1866577864)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8655350804), tensor(2.7316751480)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3119865656), tensor(1.2119921446)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0109424591), tensor(1.9954419136)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(2.0272114277), tensor(1.0962185860)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8731035590), tensor(2.7278840542)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3988512754), tensor(1.2530367374)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3802490234), tensor(1.4055645466)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.7994464636), tensor(0.9163136482)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4631522894), tensor(1.2046388388)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.2804790735), tensor(1.0257787704)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(0.8741135597), tensor(1.4474463463)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(2.5866167545), tensor(0.9777691960)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2061358690), tensor(1.1330822706)]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [tensor(1.1882609129), tensor(1.2900168896)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.6251918077), tensor(0.8777269125)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.4835407734), tensor(1.2404593229)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8278465867), tensor(1.7619030476)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.5700942278), tensor(1.1302598715)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.1855643988), tensor(1.4690552950)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.7909313440), tensor(2.7742629051)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.8936216831), tensor(1.0528556108)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3603793383), tensor(1.2620248795)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1823666096), tensor(1.3415331841)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3929893970), tensor(1.1156797409)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.6424727440), tensor(1.1385071278)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5638109446), tensor(0.9768123627)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1972198486), tensor(1.4071328640)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.6780518889), tensor(2.4155864716)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9236876965), tensor(2.6137626171)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.1728703976), tensor(0.7901804447)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.4162511826), tensor(1.3974429369)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.7941428423), tensor(1.9793199301)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1323578358), tensor(1.3088439703)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.3333151340), tensor(1.2947256565)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2687078714), tensor(1.3857028484)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(0.9221651554), tensor(1.7543658018)]\n",
            "Actual goals:  [0, 5]\n",
            "Predicted goals:  [tensor(1.1472195387), tensor(1.5605460405)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6403548717), tensor(1.0694795847)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(2.0105304718), tensor(0.9090080261)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.5376927853), tensor(0.9241244197)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.5523159504), tensor(1.0397633314)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(0.7871593237), tensor(2.2273092270)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.6679222584), tensor(0.9283885956)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1752306223), tensor(1.6324676275)]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [tensor(0.9167207479), tensor(2.0063054562)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3108663559), tensor(1.1314297915)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.6847116947), tensor(1.1315512657)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2274763584), tensor(1.4866540432)]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [tensor(1.3880100250), tensor(1.0833593607)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(0.6435978413), tensor(3.2782135010)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.2380342484), tensor(0.8853402734)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3560260534), tensor(1.1234655380)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(2.7316751480), tensor(0.8655350804)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.1866577864), tensor(1.1822341681)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1954381466), tensor(1.6158219576)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.2032024860), tensor(1.1451857090)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1245627403), tensor(1.3449546099)]\n",
            "Actual goals:  [2, 4]\n",
            "Predicted goals:  [tensor(1.1457201242), tensor(1.3214491606)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.4182636738), tensor(1.3098493814)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5353505611), tensor(1.2676614523)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.9043070078), tensor(1.0260585546)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8372160196), tensor(2.3461234570)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.2939115763), tensor(1.4850589037)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.2502368689), tensor(1.1166293621)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.6733868122), tensor(1.8361129761)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(2.4949028492), tensor(0.7011482120)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3674434423), tensor(1.3251165152)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.2494683266), tensor(1.2658562660)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9224144816), tensor(1.7944629192)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.2095093727), tensor(1.1487196684)]\n",
            "Actual goals:  [4, 4]\n",
            "Predicted goals:  [tensor(1.6492351294), tensor(1.0251625776)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.5640370846), tensor(0.9276928306)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1966648102), tensor(1.1648622751)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2063074112), tensor(1.3331730366)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0794624090), tensor(1.9725755453)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4577449560), tensor(1.1046253443)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9932097197), tensor(2.2290699482)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(0.8219690323), tensor(2.0442042351)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1950513124), tensor(1.4186078310)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.1392809153), tensor(1.2754321098)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1961857080), tensor(1.2544647455)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2027689219), tensor(1.3375246525)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4107878208), tensor(1.0887832642)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8249360919), tensor(2.5510034561)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9199181199), tensor(2.3660886288)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2706035376), tensor(1.3161787987)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2425103188), tensor(1.2685940266)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.8539929390), tensor(0.9560015202)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.0902667046), tensor(1.8784577847)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.0595089197), tensor(1.6442595720)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3297665119), tensor(1.2342709303)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.8964701891), tensor(1.6151729822)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.6149599552), tensor(1.0745701790)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.7078392506), tensor(0.9630593061)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.2425254583), tensor(1.1394537687)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.3426280022), tensor(0.9831748009)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2012906075), tensor(1.4788284302)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.9793199301), tensor(0.7941428423)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.7240431309), tensor(1.0287600756)]\n",
            "Actual goals:  [5, 3]\n",
            "Predicted goals:  [tensor(1.2754321098), tensor(1.1392809153)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.3505551815), tensor(1.2630612850)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2718002796), tensor(1.2295696735)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9091118574), tensor(1.8627455235)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.5090546608), tensor(0.8798506260)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1739135981), tensor(1.1821388006)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2819423676), tensor(1.2488348484)]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [tensor(2.8654966354), tensor(0.6132438183)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1966192722), tensor(1.4047702551)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.4666484594), tensor(1.3864176273)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0678119659), tensor(1.4138760567)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(2.0046389103), tensor(1.4088314772)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3044155836), tensor(1.4347063303)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0169764757), tensor(1.9146759510)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3479750156), tensor(1.1819198132)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3985747099), tensor(1.3120383024)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.4411653280), tensor(1.1893857718)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9363405704), tensor(2.0473296642)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.9427311420), tensor(1.5140482187)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2464209795), tensor(1.4276263714)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.5323259830), tensor(1.0114855766)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.1448603868), tensor(1.5056707859)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(2.3215689659), tensor(1.1468924284)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.0783069134), tensor(1.4102762938)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.8701111674), tensor(1.8275959492)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.4944522381), tensor(0.9051367640)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.3219279051), tensor(1.3264828920)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3202409744), tensor(1.1879802942)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.2593309879), tensor(0.9338998199)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.1801151037), tensor(1.7348980904)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.8034312725), tensor(0.8777429461)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0036586523), tensor(2.5725574493)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(0.8816767335), tensor(2.2820796967)]\n",
            "Actual goals:  [3, 4]\n",
            "Predicted goals:  [tensor(1.3834288120), tensor(1.1718797684)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.5382552147), tensor(1.1354151964)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.4084759951), tensor(1.1848349571)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.1505942345), tensor(1.1838241816)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3191592693), tensor(1.2783678770)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9365063906), tensor(1.6713720560)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.9737349749), tensor(1.5810587406)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.3380877972), tensor(0.8710473180)]\n",
            "Actual goals:  [5, 0]\n",
            "Predicted goals:  [tensor(3.3872673512), tensor(0.6658883095)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3979386091), tensor(1.1188566685)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1839222908), tensor(1.4034891129)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2179024220), tensor(1.1264947653)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.3822786808), tensor(0.8331850767)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6122936010), tensor(1.2573949099)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.4960167408), tensor(1.1683344841)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9211627245), tensor(1.9757125378)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.8904238939), tensor(1.6987074614)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2562536001), tensor(1.3783634901)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.8833186626), tensor(0.9731820822)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1766450405), tensor(1.1930960417)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3907415867), tensor(1.3181601763)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1226091385), tensor(1.7440047264)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.7086381912), tensor(1.2019593716)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.3435119390), tensor(1.2638888359)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2439699173), tensor(1.3861308098)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.4606411457), tensor(0.8487964869)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9576637745), tensor(1.5884004831)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9585957527), tensor(2.2468347549)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(0.9804595709), tensor(2.3455545902)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.4901371002), tensor(0.8285036683)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.8670689464), tensor(1.8757570982)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.5707565546), tensor(1.1246223450)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(2.3860886097), tensor(0.8260691762)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.4074338675), tensor(1.3485043049)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2462252378), tensor(1.4601100683)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9396492243), tensor(1.5306251049)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1393271685), tensor(1.3484460115)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.7256604433), tensor(0.9695237279)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5567345619), tensor(1.1933808327)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8772757649), tensor(2.6866796017)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.0170724392), tensor(1.4917565584)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2561749220), tensor(1.3821482658)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(0.9100154042), tensor(1.6793481112)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.1820586920), tensor(1.4857922792)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3474614620), tensor(1.7494806051)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3144586086), tensor(1.4156954288)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1941313744), tensor(1.3367068768)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3155990839), tensor(1.0187745094)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0295451880), tensor(2.6827979088)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.7424646616), tensor(0.8849379420)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.1778123379), tensor(1.4449961185)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(3.1747946739), tensor(0.8292415738)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9696395397), tensor(1.9217157364)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0808911324), tensor(1.1201438904)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.3580139875), tensor(1.4743933678)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3444676399), tensor(1.1698682308)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.9604701996), tensor(0.6343194842)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.5478123426), tensor(1.2243852615)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(1.4923357964), tensor(1.1799311638)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(1.2021687031), tensor(1.3826022148)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.7994398475), tensor(1.7059355974)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3367068768), tensor(1.1941313744)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3261902332), tensor(0.8663200736)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.2947256565), tensor(1.3333151340)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4666484594), tensor(1.3864176273)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.3251165152), tensor(1.3674434423)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2127643824), tensor(1.2684327364)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1505942345), tensor(1.1838241816)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1972198486), tensor(1.4071328640)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.7494806051), tensor(1.3474614620)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.8731035590), tensor(2.7278840542)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.7901804447), tensor(2.1728703976)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3202409744), tensor(1.1879802942)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.0481282473), tensor(1.4498293400)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4743933678), tensor(1.3580139875)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5353505611), tensor(1.2676614523)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.4054617882), tensor(0.7607787848)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2342709303), tensor(1.3297665119)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.1648622751), tensor(1.1966648102)]\n",
            "Actual goals:  [5, 3]\n",
            "Predicted goals:  [tensor(2.5866167545), tensor(0.9777691960)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8886998892), tensor(1.4813774824)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1234655380), tensor(1.3560260534)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.6847257614), tensor(2.7232873440)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(2.5510034561), tensor(0.8249360919)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.2819423676), tensor(1.2488348484)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.4055645466), tensor(1.3802490234)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.7339831591), tensor(0.9920574427)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2439699173), tensor(1.3861308098)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.7920248508), tensor(1.5657944679)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.2562536001), tensor(1.3783634901)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8670689464), tensor(1.8757570982)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(2.3860886097), tensor(0.8260691762)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.4035565853), tensor(0.9321213961)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.3988512754), tensor(1.2530367374)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.6101169586), tensor(0.9924938679)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.7232065201), tensor(3.2664787769)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2804790735), tensor(1.0257787704)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1752306223), tensor(1.6324676275)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.0063054562), tensor(0.9167207479)]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [tensor(1.2462252378), tensor(1.4601100683)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.8816767335), tensor(2.2820796967)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.0109424591), tensor(1.9954419136)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.0295451880), tensor(2.6827979088)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.8188482523), tensor(0.7863265872)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4923357964), tensor(1.1799311638)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2754321098), tensor(1.1392809153)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.3449546099), tensor(1.1245627403)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.4566006660), tensor(1.0309948921)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6976255178), tensor(0.8972740769)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3444676399), tensor(1.1698682308)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(2.7078392506), tensor(0.9630593061)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1264947653), tensor(1.2179024220)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1739135981), tensor(1.1821388006)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9785510302), tensor(1.7101417780)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.0055742264), tensor(1.6068568230)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.9151152372), tensor(1.6478472948)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.0473296642), tensor(0.9363405704)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1966192722), tensor(1.4047702551)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1778123379), tensor(1.4449961185)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.7086381912), tensor(1.2019593716)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.5299524069), tensor(0.9998573065)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2544647455), tensor(1.1961857080)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.4866540432), tensor(1.2274763584)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.9703159332), tensor(0.8905229568)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0397633314), tensor(2.5523159504)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9338998199), tensor(2.2593309879)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.5995458364), tensor(0.8770714998)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0287600756), tensor(1.7240431309)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2685940266), tensor(1.2425103188)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2502368689), tensor(1.1166293621)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(2.5640370846), tensor(0.9276928306)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1394537687), tensor(1.2425254583)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0783069134), tensor(1.4102762938)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.0367292166), tensor(1.5299913883)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1683344841), tensor(1.4960167408)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9282406569), tensor(1.5336755514)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1323578358), tensor(1.3088439703)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.0272114277), tensor(1.0962185860)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.9043070078), tensor(1.0260585546)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.0292717218), tensor(1.7016845942)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(2.6137626171), tensor(0.9236876965)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.8701111674), tensor(1.8275959492)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(1.1314297915), tensor(1.3108663559)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.5837057829), tensor(1.0429168940)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2658562660), tensor(1.2494683266)]\n",
            "Actual goals:  [2, 4]\n",
            "Predicted goals:  [tensor(1.2573949099), tensor(1.6122936010)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9932097197), tensor(2.2290699482)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1300871372), tensor(1.1735841036)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2630612850), tensor(1.3505551815)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0833593607), tensor(1.3880100250)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3098493814), tensor(1.4182636738)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(2.2273092270), tensor(0.7871593237)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.1674234867), tensor(1.0742044449)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.5382552147), tensor(1.1354151964)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.0539684296), tensor(1.5229376554)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.4034891129), tensor(1.1839222908)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4088314772), tensor(2.0046389103)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3826022148), tensor(1.2021687031)]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [tensor(1.3485043049), tensor(1.4074338675)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1330822706), tensor(1.2061358690)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(2.7316751480), tensor(0.8655350804)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3831747770), tensor(0.9482761621)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.0114855766), tensor(1.5323259830)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2706035376), tensor(1.3161787987)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2063074112), tensor(1.3331730366)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2687078714), tensor(1.3857028484)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.9211627245), tensor(1.9757125378)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.6060040593), tensor(2.6389696598)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3044155836), tensor(1.4347063303)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.7950279713), tensor(0.9842198491)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.3119865656), tensor(1.2119921446)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.3822786808), tensor(0.8331850767)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.7185320854), tensor(3.1171708107)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4917565584), tensor(1.0170724392)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.9804974794), tensor(1.4394800663)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.3883781433), tensor(0.9351249933)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.3214491606), tensor(1.1457201242)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.8784577847), tensor(1.0902667046)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3985747099), tensor(1.3120383024)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(1.4162511826), tensor(1.3974429369)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2032024860), tensor(1.1451857090)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.3219279051), tensor(1.3264828920)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.6176478863), tensor(0.9856598377)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(0.9831748009), tensor(2.3426280022)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.8777545691), tensor(0.8850802183)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3929893970), tensor(1.1156797409)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2295696735), tensor(1.2718002796)]\n",
            "Actual goals:  [4, 5]\n",
            "Predicted goals:  [tensor(0.9804595709), tensor(2.3455545902)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.0528556108), tensor(1.8936216831)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.8826825619), tensor(1.5592194796)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(3.0210347176), tensor(0.6364206076)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(2.3660886288), tensor(0.9199181199)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9863192439), tensor(1.6517115831)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2404593229), tensor(1.4835407734)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.8833186626), tensor(0.9731820822)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1933808327), tensor(1.5567345619)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.0570080280), tensor(1.5063784122)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.1855643988), tensor(1.4690552950)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.0808911324), tensor(1.1201438904)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1393271685), tensor(1.3484460115)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4532649517), tensor(0.9535789490)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.3603793383), tensor(1.2620248795)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.8853402734), tensor(2.2380342484)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.3834288120), tensor(1.1718797684)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4276263714), tensor(1.2464209795)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.4341040850), tensor(1.0511358976)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(0.6702999473), tensor(2.2225720882)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1468924284), tensor(2.3215689659)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4411653280), tensor(1.1893857718)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3375246525), tensor(1.2027689219)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(2.4901371002), tensor(0.8285036683)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1302598715), tensor(1.5700942278)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0187745094), tensor(1.3155990839)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(2.2468347549), tensor(0.9585957527)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.2561749220), tensor(1.3821482658)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(2.0105304718), tensor(0.9090080261)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0349625349), tensor(1.7862896919)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.4101504087), tensor(0.9167138338)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9802370667), tensor(1.5404676199)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.3144586086), tensor(1.4156954288)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.9748357534), tensor(1.3699353933)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0694795847), tensor(1.6403548717)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2783678770), tensor(1.3191592693)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4186078310), tensor(1.1950513124)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.0489637852), tensor(1.6628545523)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1848349571), tensor(1.4084759951)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8708379865), tensor(1.4913862944)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.8361129761), tensor(1.6733868122)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2095093727), tensor(1.1487196684)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0794624090), tensor(1.9725755453)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.6149599552), tensor(1.0745701790)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.3388282061), tensor(0.8418958187)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1930960417), tensor(1.1766450405)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2939115763), tensor(1.4850589037)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(2.8545994759), tensor(0.6893402934)]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [tensor(2.2938866615), tensor(0.7557767034)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.3435119390), tensor(1.2638888359)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(1.4138760567), tensor(1.0678119659)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.5478123426), tensor(1.2243852615)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8772757649), tensor(2.6866796017)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3181601763), tensor(1.3907415867)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(2.6827979088), tensor(1.0295451880)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1392809153), tensor(1.2754321098)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.1245627403), tensor(1.3449546099)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.2820796967), tensor(0.8816767335)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0309948921), tensor(1.4566006660)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1799311638), tensor(1.4923357964)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.9954419136), tensor(1.0109424591)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8972740769), tensor(1.6976255178)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1698682308), tensor(1.3444676399)]\n",
            "Actual goals:  [0, 5]\n",
            "Predicted goals:  [tensor(0.7863265872), tensor(1.8188482523)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9696395397), tensor(1.9217157364)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2900168896), tensor(1.1882609129)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3415331841), tensor(1.1823666096)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.3479750156), tensor(1.1819198132)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.6442595720), tensor(1.0595089197)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.6561763287), tensor(0.8700805902)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.6904175282), tensor(0.9982143641)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.8372160196), tensor(2.3461234570)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.6169384718), tensor(0.8919819593)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0036586523), tensor(2.5725574493)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2425254583), tensor(1.1394537687)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.5336755514), tensor(0.9282406569)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(0.9276928306), tensor(2.5640370846)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.7016845942), tensor(1.0292717218)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0962185860), tensor(2.0272114277)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.4960167408), tensor(1.1683344841)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.0260585546), tensor(1.9043070078)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3088439703), tensor(1.1323578358)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.5299913883), tensor(1.0367292166)]\n",
            "Actual goals:  [5, 2]\n",
            "Predicted goals:  [tensor(1.4102762938), tensor(1.0783069134)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.1166293621), tensor(1.2502368689)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.9998573065), tensor(1.5299524069)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.8905229568), tensor(1.9703159332)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.8770714998), tensor(1.5995458364)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(2.5523159504), tensor(1.0397633314)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1961857080), tensor(1.2544647455)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(2.2593309879), tensor(0.9338998199)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2425103188), tensor(1.2685940266)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.7240431309), tensor(1.0287600756)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.2274763584), tensor(1.4866540432)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.6068568230), tensor(1.0055742264)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9630593061), tensor(2.7078392506)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.2179024220), tensor(1.1264947653)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.2019593716), tensor(1.7086381912)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.7101417780), tensor(0.9785510302)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.4449961185), tensor(1.1778123379)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1821388006), tensor(1.1739135981)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9363405704), tensor(2.0473296642)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.6478472948), tensor(0.9151152372)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4047702551), tensor(1.1966192722)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1819198132), tensor(1.3479750156)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(2.3461234570), tensor(0.8372160196)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(0.8919819593), tensor(1.6169384718)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1823666096), tensor(1.3415331841)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.9217157364), tensor(0.9696395397)]\n",
            "Actual goals:  [4, 4]\n",
            "Predicted goals:  [tensor(0.9982143641), tensor(1.6904175282)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.0595089197), tensor(1.6442595720)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.8700805902), tensor(1.6561763287)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.5725574493), tensor(1.0036586523)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1882609129), tensor(1.2900168896)]\n",
            "Actual goals:  [2, 4]\n",
            "Predicted goals:  [tensor(2.0046389103), tensor(1.4088314772)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.1839222908), tensor(1.4034891129)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.1354151964), tensor(1.5382552147)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2021687031), tensor(1.3826022148)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.0742044449), tensor(1.1674234867)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.2061358690), tensor(1.1330822706)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.7871593237), tensor(2.2273092270)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.5229376554), tensor(1.0539684296)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4182636738), tensor(1.3098493814)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4074338675), tensor(1.3485043049)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(0.9236876965), tensor(2.6137626171)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.0429168940), tensor(1.5837057829)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.6122936010), tensor(1.2573949099)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.8275959492), tensor(0.8701111674)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3108663559), tensor(1.1314297915)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3505551815), tensor(1.2630612850)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.1735841036), tensor(1.1300871372)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.3880100250), tensor(1.0833593607)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(2.2290699482), tensor(0.9932097197)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.2494683266), tensor(1.2658562660)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.0170724392), tensor(1.4917565584)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.4394800663), tensor(0.9804974794)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3120383024), tensor(1.3985747099)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8331850767), tensor(2.3822786808)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2119921446), tensor(1.3119865656)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(1.0902667046), tensor(1.8784577847)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(3.1171708107), tensor(0.7185320854)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.9351249933), tensor(1.3883781433)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.3331730366), tensor(1.2063074112)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.9482761621), tensor(1.3831747770)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.9757125378), tensor(0.9211627245)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8655350804), tensor(2.7316751480)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.3857028484), tensor(1.2687078714)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.3161787987), tensor(1.2706035376)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.4347063303), tensor(1.3044155836)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(0.9842198491), tensor(1.7950279713)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(2.6866796017), tensor(0.8772757649)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.2638888359), tensor(1.3435119390)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(0.7557767034), tensor(2.2938866615)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3907415867), tensor(1.3181601763)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2243852615), tensor(1.5478123426)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(0.6893402934), tensor(2.8545994759)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.0678119659), tensor(1.4138760567)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.1766450405), tensor(1.1930960417)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.8418958187), tensor(1.3388282061)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(1.4850589037), tensor(1.2939115763)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.6389696598), tensor(0.6060040593)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1457201242), tensor(1.3214491606)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.5323259830), tensor(1.0114855766)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.8249360919), tensor(2.5510034561)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.6628545523), tensor(1.0489637852)]\n",
            "Actual goals:  [1, 4]\n",
            "Predicted goals:  [tensor(1.4913862944), tensor(0.8708379865)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(2.7232873440), tensor(0.6847257614)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.9725755453), tensor(1.0794624090)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1950513124), tensor(1.4186078310)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.3191592693), tensor(1.2783678770)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1487196684), tensor(1.2095093727)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.6403548717), tensor(1.0694795847)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.4084759951), tensor(1.1848349571)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(3.2664787769), tensor(0.7232065201)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.0257787704), tensor(1.2804790735)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.2530367374), tensor(1.3988512754)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9924938679), tensor(1.6101169586)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.6324676275), tensor(1.1752306223)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.8757570982), tensor(0.8670689464)]\n",
            "Actual goals:  [0, 3]\n",
            "Predicted goals:  [tensor(1.3783634901), tensor(1.2562536001)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(0.8260691762), tensor(2.3860886097)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.4601100683), tensor(1.2462252378)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9321213961), tensor(1.4035565853)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.1966648102), tensor(1.1648622751)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(1.3580139875), tensor(1.4743933678)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(2.1728703976), tensor(0.7901804447)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2676614523), tensor(1.5353505611)]\n",
            "Actual goals:  [0, 1]\n",
            "Predicted goals:  [tensor(0.9777691960), tensor(2.5866167545)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.7607787848), tensor(2.4054617882)]\n",
            "Actual goals:  [4, 2]\n",
            "Predicted goals:  [tensor(1.1879802942), tensor(1.3202409744)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4813774824), tensor(0.8886998892)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.4498293400), tensor(1.0481282473)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3297665119), tensor(1.2342709303)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1838241816), tensor(1.1505942345)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.3333151340), tensor(1.2947256565)]\n",
            "Actual goals:  [3, 3]\n",
            "Predicted goals:  [tensor(1.2684327364), tensor(1.2127643824)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3864176273), tensor(1.4666484594)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.4071328640), tensor(1.1972198486)]\n",
            "Actual goals:  [2, 3]\n",
            "Predicted goals:  [tensor(1.1941313744), tensor(1.3367068768)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3861308098), tensor(1.2439699173)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3802490234), tensor(1.4055645466)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.2488348484), tensor(1.2819423676)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.3699353933), tensor(0.9748357534)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6733868122), tensor(1.8361129761)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9167207479), tensor(2.0063054562)]\n",
            "Actual goals:  [3, 0]\n",
            "Predicted goals:  [tensor(1.3560260534), tensor(1.1234655380)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.9920574427), tensor(1.7339831591)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.0745701790), tensor(1.6149599552)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.5657944679), tensor(0.7920248508)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.3264828920), tensor(1.3219279051)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.1156797409), tensor(1.3929893970)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(2.3426280022), tensor(0.9831748009)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.8850802183), tensor(1.8777545691)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.1451857090), tensor(1.2032024860)]\n",
            "Actual goals:  [4, 0]\n",
            "Predicted goals:  [tensor(1.5592194796), tensor(0.8826825619)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9856598377), tensor(1.6176478863)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.8936216831), tensor(1.0528556108)]\n",
            "Actual goals:  [4, 1]\n",
            "Predicted goals:  [tensor(1.2718002796), tensor(1.2295696735)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.3455545902), tensor(0.9804595709)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.8663200736), tensor(1.3261902332)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3674434423), tensor(1.3251165152)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.1201438904), tensor(1.0808911324)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.9199181199), tensor(2.3660886288)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4835407734), tensor(1.2404593229)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.5567345619), tensor(1.1933808327)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.4690552950), tensor(1.1855643988)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.6517115831), tensor(0.9863192439)]\n",
            "Actual goals:  [4, 3]\n",
            "Predicted goals:  [tensor(1.5063784122), tensor(1.0570080280)]\n",
            "Actual goals:  [0, 4]\n",
            "Predicted goals:  [tensor(0.9731820822), tensor(1.8833186626)]\n",
            "Actual goals:  [5, 1]\n",
            "Predicted goals:  [tensor(1.3484460115), tensor(1.1393271685)]\n",
            "Actual goals:  [0, 2]\n",
            "Predicted goals:  [tensor(0.6364206076), tensor(3.0210347176)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.3974429369), tensor(1.4162511826)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(1.2464209795), tensor(1.4276263714)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.1718797684), tensor(1.3834288120)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(2.2380342484), tensor(0.8853402734)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.0511358976), tensor(1.4341040850)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(1.1893857718), tensor(1.4411653280)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(2.2225720882), tensor(0.6702999473)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(1.2027689219), tensor(1.3375246525)]\n",
            "Actual goals:  [1, 0]\n",
            "Predicted goals:  [tensor(0.9535789490), tensor(1.4532649517)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.2620248795), tensor(1.3603793383)]\n",
            "Actual goals:  [3, 1]\n",
            "Predicted goals:  [tensor(2.3215689659), tensor(1.1468924284)]\n",
            "Actual goals:  [1, 3]\n",
            "Predicted goals:  [tensor(1.3474614620), tensor(1.7494806051)]\n",
            "Actual goals:  [7, 0]\n",
            "Predicted goals:  [tensor(2.7278840542), tensor(0.8731035590)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(0.8285036683), tensor(2.4901371002)]\n",
            "Actual goals:  [3, 2]\n",
            "Predicted goals:  [tensor(1.3821482658), tensor(1.2561749220)]\n",
            "Actual goals:  [2, 2]\n",
            "Predicted goals:  [tensor(1.5404676199), tensor(0.9802370667)]\n",
            "Actual goals:  [6, 0]\n",
            "Predicted goals:  [tensor(1.7862896919), tensor(1.0349625349)]\n",
            "Actual goals:  [2, 1]\n",
            "Predicted goals:  [tensor(1.5700942278), tensor(1.1302598715)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(1.4156954288), tensor(1.3144586086)]\n",
            "Actual goals:  [1, 1]\n",
            "Predicted goals:  [tensor(0.9585957527), tensor(2.2468347549)]\n",
            "Actual goals:  [1, 2]\n",
            "Predicted goals:  [tensor(0.9090080261), tensor(2.0105304718)]\n",
            "Actual goals:  [0, 0]\n",
            "Predicted goals:  [tensor(1.3155990839), tensor(1.0187745094)]\n",
            "Actual goals:  [2, 0]\n",
            "Predicted goals:  [tensor(0.9167138338), tensor(1.4101504087)]\n",
            "Accuracy:  0.0\n",
            "Mean Absolute Error:  tensor(1.8456426859)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Flatten\n",
        "true_goals_flat = []\n",
        "for match in actual_goals:\n",
        "    for goal in match:\n",
        "        true_goals_flat.append(goal)\n",
        "\n",
        "\n",
        "predicted_goals_flat = []\n",
        "for match in predicted_goals:\n",
        "    for goal in match:\n",
        "        predicted_goals_flat.append(round(goal.item()))\n",
        "\n",
        "\n",
        "# Histogram of Predicted Goals vs. Actual Goals\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist([true_goals_flat, predicted_goals_flat], bins=10, label=['True Goals', 'Predicted Goals'])\n",
        "plt.xlabel('Goals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Predicted Goals vs. True Goals')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hWqbgIBrS2Xn",
        "outputId": "d5567e83-ffba-4c46-dad1-50f37a7e9e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.72788405418396\n",
            "0.8731035590171814\n",
            "1.0257787704467773\n",
            "1.280479073524475\n",
            "1.447446346282959\n",
            "0.8741135597229004\n",
            "1.405564546585083\n",
            "1.3802490234375\n",
            "1.2046388387680054\n",
            "1.463152289390564\n",
            "1.9954419136047363\n",
            "1.0109424591064453\n",
            "1.2119921445846558\n",
            "1.3119865655899048\n",
            "1.2530367374420166\n",
            "1.3988512754440308\n",
            "0.916313648223877\n",
            "1.7994464635849\n",
            "1.0962185859680176\n",
            "2.0272114276885986\n",
            "1.2900168895721436\n",
            "1.1882609128952026\n",
            "2.7742629051208496\n",
            "0.7909313440322876\n",
            "1.2404593229293823\n",
            "1.4835407733917236\n",
            "1.4690552949905396\n",
            "1.185564398765564\n",
            "1.0528556108474731\n",
            "1.8936216831207275\n",
            "1.1330822706222534\n",
            "1.206135869026184\n",
            "1.7619030475616455\n",
            "0.8278465867042542\n",
            "0.8777269124984741\n",
            "1.6251918077468872\n",
            "1.1302598714828491\n",
            "1.5700942277908325\n",
            "0.9777691960334778\n",
            "2.5866167545318604\n",
            "1.1156797409057617\n",
            "1.3929893970489502\n",
            "1.3415331840515137\n",
            "1.1823666095733643\n",
            "0.9768123626708984\n",
            "1.56381094455719\n",
            "1.1385071277618408\n",
            "1.642472743988037\n",
            "1.2620248794555664\n",
            "1.3603793382644653\n",
            "2.613762617111206\n",
            "0.9236876964569092\n",
            "1.407132863998413\n",
            "1.1972198486328125\n",
            "2.415586471557617\n",
            "0.6780518889427185\n",
            "1.5605460405349731\n",
            "1.1472195386886597\n",
            "1.2947256565093994\n",
            "1.333315134048462\n",
            "2.227309226989746\n",
            "0.7871593236923218\n",
            "1.0694795846939087\n",
            "1.640354871749878\n",
            "1.7543658018112183\n",
            "0.9221651554107666\n",
            "1.308843970298767\n",
            "1.1323578357696533\n",
            "1.3857028484344482\n",
            "1.2687078714370728\n",
            "0.9241244196891785\n",
            "1.5376927852630615\n",
            "1.039763331413269\n",
            "2.5523159503936768\n",
            "0.9090080261230469\n",
            "2.010530471801758\n",
            "2.6866796016693115\n",
            "0.8772757649421692\n",
            "1.0114855766296387\n",
            "1.5323259830474854\n",
            "1.3120383024215698\n",
            "1.398574709892273\n",
            "0.9731820821762085\n",
            "1.8833186626434326\n",
            "2.0473296642303467\n",
            "0.9363405704498291\n",
            "1.1893857717514038\n",
            "1.4411653280258179\n",
            "1.5140482187271118\n",
            "0.9427311420440674\n",
            "1.181919813156128\n",
            "1.3479750156402588\n",
            "1.427626371383667\n",
            "1.246420979499817\n",
            "1.5056707859039307\n",
            "1.1448603868484497\n",
            "1.2488348484039307\n",
            "1.281942367553711\n",
            "1.3864176273345947\n",
            "1.4666484594345093\n",
            "0.8798506259918213\n",
            "1.5090546607971191\n",
            "1.9146759510040283\n",
            "1.0169764757156372\n",
            "1.4347063302993774\n",
            "1.3044155836105347\n",
            "1.1821388006210327\n",
            "1.173913598060608\n",
            "1.4138760566711426\n",
            "1.0678119659423828\n",
            "0.613243818283081\n",
            "2.8654966354370117\n",
            "1.4088314771652222\n",
            "2.004638910293579\n",
            "1.4047702550888062\n",
            "1.1966192722320557\n",
            "2.5725574493408203\n",
            "1.003658652305603\n",
            "1.1838241815567017\n",
            "1.1505942344665527\n",
            "1.6713720560073853\n",
            "0.9365063905715942\n",
            "2.2820796966552734\n",
            "0.8816767334938049\n",
            "0.87104731798172\n",
            "2.338087797164917\n",
            "1.1848349571228027\n",
            "1.4084759950637817\n",
            "1.171879768371582\n",
            "1.3834288120269775\n",
            "1.1354151964187622\n",
            "1.538255214691162\n",
            "1.5810587406158447\n",
            "0.973734974861145\n",
            "1.2783678770065308\n",
            "1.3191592693328857\n",
            "1.7348980903625488\n",
            "1.1801151037216187\n",
            "0.90513676404953\n",
            "1.4944522380828857\n",
            "1.1468924283981323\n",
            "2.3215689659118652\n",
            "1.8275959491729736\n",
            "0.8701111674308777\n",
            "1.4449961185455322\n",
            "1.1778123378753662\n",
            "1.1879802942276\n",
            "1.3202409744262695\n",
            "1.4102762937545776\n",
            "1.0783069133758545\n",
            "0.8777429461479187\n",
            "1.8034312725067139\n",
            "1.326482892036438\n",
            "1.3219279050827026\n",
            "0.9338998198509216\n",
            "2.259330987930298\n",
            "0.7901804447174072\n",
            "2.172870397567749\n",
            "1.8361129760742188\n",
            "1.673386812210083\n",
            "1.7944629192352295\n",
            "0.9224144816398621\n",
            "0.7011482119560242\n",
            "2.4949028491973877\n",
            "1.4850589036941528\n",
            "1.2939115762710571\n",
            "1.325116515159607\n",
            "1.3674434423446655\n",
            "1.0745701789855957\n",
            "1.614959955215454\n",
            "1.1487196683883667\n",
            "1.2095093727111816\n",
            "1.2658562660217285\n",
            "1.2494683265686035\n",
            "1.0251625776290894\n",
            "1.6492351293563843\n",
            "1.1166293621063232\n",
            "1.2502368688583374\n",
            "1.3974429368972778\n",
            "1.4162511825561523\n",
            "1.3331730365753174\n",
            "1.2063074111938477\n",
            "1.2544647455215454\n",
            "1.1961857080459595\n",
            "2.229069948196411\n",
            "0.993209719657898\n",
            "2.0442042350769043\n",
            "0.8219690322875977\n",
            "1.9725755453109741\n",
            "1.0794624090194702\n",
            "1.4186078310012817\n",
            "1.1950513124465942\n",
            "1.1648622751235962\n",
            "1.196664810180664\n",
            "1.1046253442764282\n",
            "1.4577449560165405\n",
            "0.9276928305625916\n",
            "2.5640370845794678\n",
            "1.337524652481079\n",
            "1.2027689218521118\n",
            "0.833185076713562\n",
            "2.3822786808013916\n",
            "0.6658883094787598\n",
            "3.3872673511505127\n",
            "1.975712537765503\n",
            "0.9211627244949341\n",
            "1.6987074613571167\n",
            "0.8904238939285278\n",
            "1.3783634901046753\n",
            "1.2562536001205444\n",
            "1.1683344841003418\n",
            "1.4960167407989502\n",
            "1.403489112854004\n",
            "1.183922290802002\n",
            "1.2573949098587036\n",
            "1.6122936010360718\n",
            "1.11885666847229\n",
            "1.39793860912323\n",
            "1.1264947652816772\n",
            "1.217902421951294\n",
            "2.3455545902252197\n",
            "0.9804595708847046\n",
            "1.2638888359069824\n",
            "1.343511939048767\n",
            "1.3861308097839355\n",
            "1.2439699172973633\n",
            "1.3181601762771606\n",
            "1.3907415866851807\n",
            "1.5884004831314087\n",
            "0.9576637744903564\n",
            "0.8487964868545532\n",
            "1.4606411457061768\n",
            "2.2468347549438477\n",
            "0.9585957527160645\n",
            "1.2019593715667725\n",
            "1.7086381912231445\n",
            "1.744004726409912\n",
            "1.1226091384887695\n",
            "1.1930960416793823\n",
            "1.176645040512085\n",
            "0.8285036683082581\n",
            "2.4901371002197266\n",
            "1.5306251049041748\n",
            "0.939649224281311\n",
            "0.9695237278938293\n",
            "1.7256604433059692\n",
            "1.1246223449707031\n",
            "1.5707565546035767\n",
            "1.875757098197937\n",
            "0.8670689463615417\n",
            "1.1933808326721191\n",
            "1.556734561920166\n",
            "1.3485043048858643\n",
            "1.4074338674545288\n",
            "0.826069176197052\n",
            "2.3860886096954346\n",
            "1.348446011543274\n",
            "1.1393271684646606\n",
            "1.460110068321228\n",
            "1.2462252378463745\n",
            "1.3367068767547607\n",
            "1.1941313743591309\n",
            "1.0187745094299316\n",
            "1.3155990839004517\n",
            "1.382148265838623\n",
            "1.256174921989441\n",
            "1.679348111152649\n",
            "0.9100154042243958\n",
            "1.4857922792434692\n",
            "1.1820586919784546\n",
            "0.8849379420280457\n",
            "1.7424646615982056\n",
            "2.682797908782959\n",
            "1.0295451879501343\n",
            "1.7494806051254272\n",
            "1.347461462020874\n",
            "1.491756558418274\n",
            "1.0170724391937256\n",
            "1.4156954288482666\n",
            "1.3144586086273193\n",
            "1.1201438903808594\n",
            "1.0808911323547363\n",
            "1.474393367767334\n",
            "1.3580139875411987\n",
            "0.6343194842338562\n",
            "2.960470199584961\n",
            "0.8292415738105774\n",
            "3.1747946739196777\n",
            "1.9217157363891602\n",
            "0.9696395397186279\n",
            "1.3826022148132324\n",
            "1.2021687030792236\n",
            "1.2243852615356445\n",
            "1.5478123426437378\n",
            "1.7059355974197388\n",
            "0.7994398474693298\n",
            "1.1799311637878418\n",
            "1.4923357963562012\n",
            "1.1698682308197021\n",
            "1.3444676399230957\n",
            "2.366088628768921\n",
            "0.9199181199073792\n",
            "1.0887832641601562\n",
            "1.41078782081604\n",
            "1.6442595720291138\n",
            "1.0595089197158813\n",
            "1.87845778465271\n",
            "1.0902667045593262\n",
            "1.316178798675537\n",
            "1.2706035375595093\n",
            "1.2342709302902222\n",
            "1.3297665119171143\n",
            "1.2685940265655518\n",
            "1.2425103187561035\n",
            "2.5510034561157227\n",
            "0.8249360918998718\n",
            "1.6151729822158813\n",
            "0.8964701890945435\n",
            "0.9560015201568604\n",
            "1.8539929389953613\n",
            "1.1394537687301636\n",
            "1.2425254583358765\n",
            "0.9630593061447144\n",
            "2.707839250564575\n",
            "1.263061285018921\n",
            "1.350555181503296\n",
            "1.4788284301757812\n",
            "1.2012906074523926\n",
            "0.9831748008728027\n",
            "2.342628002166748\n",
            "1.229569673538208\n",
            "1.2718002796173096\n",
            "1.0287600755691528\n",
            "1.7240431308746338\n",
            "1.8627455234527588\n",
            "0.9091118574142456\n",
            "3.2782135009765625\n",
            "0.6435978412628174\n",
            "1.1234655380249023\n",
            "1.35602605342865\n",
            "1.1314297914505005\n",
            "1.310866355895996\n",
            "0.8853402733802795\n",
            "2.238034248352051\n",
            "1.4866540431976318\n",
            "1.2274763584136963\n",
            "0.9283885955810547\n",
            "1.6679222583770752\n",
            "1.1315512657165527\n",
            "1.6847116947174072\n",
            "1.6324676275253296\n",
            "1.175230622291565\n",
            "2.006305456161499\n",
            "0.9167207479476929\n",
            "1.0833593606948853\n",
            "1.388010025024414\n",
            "1.3098493814468384\n",
            "1.4182636737823486\n",
            "1.6158219575881958\n",
            "1.1954381465911865\n",
            "2.346123456954956\n",
            "0.8372160196304321\n",
            "1.1451857089996338\n",
            "1.203202486038208\n",
            "1.267661452293396\n",
            "1.5353505611419678\n",
            "1.026058554649353\n",
            "1.9043070077896118\n",
            "1.3449546098709106\n",
            "1.1245627403259277\n",
            "1.3214491605758667\n",
            "1.14572012424469\n",
            "1.1822341680526733\n",
            "1.1866577863693237\n",
            "0.8655350804328918\n",
            "2.731675148010254\n",
            "1.3119865655899048\n",
            "1.2119921445846558\n",
            "1.0109424591064453\n",
            "1.9954419136047363\n",
            "2.0272114276885986\n",
            "1.0962185859680176\n",
            "0.8731035590171814\n",
            "2.72788405418396\n",
            "1.3988512754440308\n",
            "1.2530367374420166\n",
            "1.3802490234375\n",
            "1.405564546585083\n",
            "1.7994464635849\n",
            "0.916313648223877\n",
            "1.463152289390564\n",
            "1.2046388387680054\n",
            "1.280479073524475\n",
            "1.0257787704467773\n",
            "0.8741135597229004\n",
            "1.447446346282959\n",
            "2.5866167545318604\n",
            "0.9777691960334778\n",
            "1.206135869026184\n",
            "1.1330822706222534\n",
            "1.1882609128952026\n",
            "1.2900168895721436\n",
            "1.6251918077468872\n",
            "0.8777269124984741\n",
            "1.4835407733917236\n",
            "1.2404593229293823\n",
            "0.8278465867042542\n",
            "1.7619030475616455\n",
            "1.5700942277908325\n",
            "1.1302598714828491\n",
            "1.185564398765564\n",
            "1.4690552949905396\n",
            "0.7909313440322876\n",
            "2.7742629051208496\n",
            "1.8936216831207275\n",
            "1.0528556108474731\n",
            "1.3603793382644653\n",
            "1.2620248794555664\n",
            "1.1823666095733643\n",
            "1.3415331840515137\n",
            "1.3929893970489502\n",
            "1.1156797409057617\n",
            "1.642472743988037\n",
            "1.1385071277618408\n",
            "1.56381094455719\n",
            "0.9768123626708984\n",
            "1.1972198486328125\n",
            "1.407132863998413\n",
            "0.6780518889427185\n",
            "2.415586471557617\n",
            "0.9236876964569092\n",
            "2.613762617111206\n",
            "2.172870397567749\n",
            "0.7901804447174072\n",
            "1.4162511825561523\n",
            "1.3974429368972778\n",
            "0.7941428422927856\n",
            "1.9793199300765991\n",
            "1.1323578357696533\n",
            "1.308843970298767\n",
            "1.333315134048462\n",
            "1.2947256565093994\n",
            "1.2687078714370728\n",
            "1.3857028484344482\n",
            "0.9221651554107666\n",
            "1.7543658018112183\n",
            "1.1472195386886597\n",
            "1.5605460405349731\n",
            "1.640354871749878\n",
            "1.0694795846939087\n",
            "2.010530471801758\n",
            "0.9090080261230469\n",
            "1.5376927852630615\n",
            "0.9241244196891785\n",
            "2.5523159503936768\n",
            "1.039763331413269\n",
            "0.7871593236923218\n",
            "2.227309226989746\n",
            "1.6679222583770752\n",
            "0.9283885955810547\n",
            "1.175230622291565\n",
            "1.6324676275253296\n",
            "0.9167207479476929\n",
            "2.006305456161499\n",
            "1.310866355895996\n",
            "1.1314297914505005\n",
            "1.6847116947174072\n",
            "1.1315512657165527\n",
            "1.2274763584136963\n",
            "1.4866540431976318\n",
            "1.388010025024414\n",
            "1.0833593606948853\n",
            "0.6435978412628174\n",
            "3.2782135009765625\n",
            "2.238034248352051\n",
            "0.8853402733802795\n",
            "1.35602605342865\n",
            "1.1234655380249023\n",
            "2.731675148010254\n",
            "0.8655350804328918\n",
            "1.1866577863693237\n",
            "1.1822341680526733\n",
            "1.1954381465911865\n",
            "1.6158219575881958\n",
            "1.203202486038208\n",
            "1.1451857089996338\n",
            "1.1245627403259277\n",
            "1.3449546098709106\n",
            "1.14572012424469\n",
            "1.3214491605758667\n",
            "1.4182636737823486\n",
            "1.3098493814468384\n",
            "1.5353505611419678\n",
            "1.267661452293396\n",
            "1.9043070077896118\n",
            "1.026058554649353\n",
            "0.8372160196304321\n",
            "2.346123456954956\n",
            "1.2939115762710571\n",
            "1.4850589036941528\n",
            "1.2502368688583374\n",
            "1.1166293621063232\n",
            "1.673386812210083\n",
            "1.8361129760742188\n",
            "2.4949028491973877\n",
            "0.7011482119560242\n",
            "1.3674434423446655\n",
            "1.325116515159607\n",
            "1.2494683265686035\n",
            "1.2658562660217285\n",
            "0.9224144816398621\n",
            "1.7944629192352295\n",
            "1.2095093727111816\n",
            "1.1487196683883667\n",
            "1.6492351293563843\n",
            "1.0251625776290894\n",
            "2.5640370845794678\n",
            "0.9276928305625916\n",
            "1.196664810180664\n",
            "1.1648622751235962\n",
            "1.2063074111938477\n",
            "1.3331730365753174\n",
            "1.0794624090194702\n",
            "1.9725755453109741\n",
            "1.4577449560165405\n",
            "1.1046253442764282\n",
            "0.993209719657898\n",
            "2.229069948196411\n",
            "0.8219690322875977\n",
            "2.0442042350769043\n",
            "1.1950513124465942\n",
            "1.4186078310012817\n",
            "1.139280915260315\n",
            "1.2754321098327637\n",
            "1.1961857080459595\n",
            "1.2544647455215454\n",
            "1.2027689218521118\n",
            "1.337524652481079\n",
            "1.41078782081604\n",
            "1.0887832641601562\n",
            "0.8249360918998718\n",
            "2.5510034561157227\n",
            "0.9199181199073792\n",
            "2.366088628768921\n",
            "1.2706035375595093\n",
            "1.316178798675537\n",
            "1.2425103187561035\n",
            "1.2685940265655518\n",
            "1.8539929389953613\n",
            "0.9560015201568604\n",
            "1.0902667045593262\n",
            "1.87845778465271\n",
            "1.0595089197158813\n",
            "1.6442595720291138\n",
            "1.3297665119171143\n",
            "1.2342709302902222\n",
            "0.8964701890945435\n",
            "1.6151729822158813\n",
            "1.614959955215454\n",
            "1.0745701789855957\n",
            "2.707839250564575\n",
            "0.9630593061447144\n",
            "1.2425254583358765\n",
            "1.1394537687301636\n",
            "2.342628002166748\n",
            "0.9831748008728027\n",
            "1.2012906074523926\n",
            "1.4788284301757812\n",
            "1.9793199300765991\n",
            "0.7941428422927856\n",
            "1.7240431308746338\n",
            "1.0287600755691528\n",
            "1.2754321098327637\n",
            "1.139280915260315\n",
            "1.350555181503296\n",
            "1.263061285018921\n",
            "1.2718002796173096\n",
            "1.229569673538208\n",
            "0.9091118574142456\n",
            "1.8627455234527588\n",
            "1.5090546607971191\n",
            "0.8798506259918213\n",
            "1.173913598060608\n",
            "1.1821388006210327\n",
            "1.281942367553711\n",
            "1.2488348484039307\n",
            "2.8654966354370117\n",
            "0.613243818283081\n",
            "1.1966192722320557\n",
            "1.4047702550888062\n",
            "1.4666484594345093\n",
            "1.3864176273345947\n",
            "1.0678119659423828\n",
            "1.4138760566711426\n",
            "2.004638910293579\n",
            "1.4088314771652222\n",
            "1.3044155836105347\n",
            "1.4347063302993774\n",
            "1.0169764757156372\n",
            "1.9146759510040283\n",
            "1.3479750156402588\n",
            "1.181919813156128\n",
            "1.398574709892273\n",
            "1.3120383024215698\n",
            "1.4411653280258179\n",
            "1.1893857717514038\n",
            "0.9363405704498291\n",
            "2.0473296642303467\n",
            "0.9427311420440674\n",
            "1.5140482187271118\n",
            "1.246420979499817\n",
            "1.427626371383667\n",
            "1.5323259830474854\n",
            "1.0114855766296387\n",
            "1.1448603868484497\n",
            "1.5056707859039307\n",
            "2.3215689659118652\n",
            "1.1468924283981323\n",
            "1.0783069133758545\n",
            "1.4102762937545776\n",
            "0.8701111674308777\n",
            "1.8275959491729736\n",
            "1.4944522380828857\n",
            "0.90513676404953\n",
            "1.3219279050827026\n",
            "1.326482892036438\n",
            "1.3202409744262695\n",
            "1.1879802942276\n",
            "2.259330987930298\n",
            "0.9338998198509216\n",
            "1.1801151037216187\n",
            "1.7348980903625488\n",
            "1.8034312725067139\n",
            "0.8777429461479187\n",
            "1.003658652305603\n",
            "2.5725574493408203\n",
            "0.8816767334938049\n",
            "2.2820796966552734\n",
            "1.3834288120269775\n",
            "1.171879768371582\n",
            "1.538255214691162\n",
            "1.1354151964187622\n",
            "1.4084759950637817\n",
            "1.1848349571228027\n",
            "1.1505942344665527\n",
            "1.1838241815567017\n",
            "1.3191592693328857\n",
            "1.2783678770065308\n",
            "0.9365063905715942\n",
            "1.6713720560073853\n",
            "0.973734974861145\n",
            "1.5810587406158447\n",
            "2.338087797164917\n",
            "0.87104731798172\n",
            "3.3872673511505127\n",
            "0.6658883094787598\n",
            "1.39793860912323\n",
            "1.11885666847229\n",
            "1.183922290802002\n",
            "1.403489112854004\n",
            "1.217902421951294\n",
            "1.1264947652816772\n",
            "2.3822786808013916\n",
            "0.833185076713562\n",
            "1.6122936010360718\n",
            "1.2573949098587036\n",
            "1.4960167407989502\n",
            "1.1683344841003418\n",
            "0.9211627244949341\n",
            "1.975712537765503\n",
            "0.8904238939285278\n",
            "1.6987074613571167\n",
            "1.2562536001205444\n",
            "1.3783634901046753\n",
            "1.8833186626434326\n",
            "0.9731820821762085\n",
            "1.176645040512085\n",
            "1.1930960416793823\n",
            "1.3907415866851807\n",
            "1.3181601762771606\n",
            "1.1226091384887695\n",
            "1.744004726409912\n",
            "1.7086381912231445\n",
            "1.2019593715667725\n",
            "1.343511939048767\n",
            "1.2638888359069824\n",
            "1.2439699172973633\n",
            "1.3861308097839355\n",
            "1.4606411457061768\n",
            "0.8487964868545532\n",
            "0.9576637744903564\n",
            "1.5884004831314087\n",
            "0.9585957527160645\n",
            "2.2468347549438477\n",
            "0.9804595708847046\n",
            "2.3455545902252197\n",
            "2.4901371002197266\n",
            "0.8285036683082581\n",
            "0.8670689463615417\n",
            "1.875757098197937\n",
            "1.5707565546035767\n",
            "1.1246223449707031\n",
            "2.3860886096954346\n",
            "0.826069176197052\n",
            "1.4074338674545288\n",
            "1.3485043048858643\n",
            "1.2462252378463745\n",
            "1.460110068321228\n",
            "0.939649224281311\n",
            "1.5306251049041748\n",
            "1.1393271684646606\n",
            "1.348446011543274\n",
            "1.7256604433059692\n",
            "0.9695237278938293\n",
            "1.556734561920166\n",
            "1.1933808326721191\n",
            "0.8772757649421692\n",
            "2.6866796016693115\n",
            "1.0170724391937256\n",
            "1.491756558418274\n",
            "1.256174921989441\n",
            "1.382148265838623\n",
            "0.9100154042243958\n",
            "1.679348111152649\n",
            "1.1820586919784546\n",
            "1.4857922792434692\n",
            "1.347461462020874\n",
            "1.7494806051254272\n",
            "1.3144586086273193\n",
            "1.4156954288482666\n",
            "1.1941313743591309\n",
            "1.3367068767547607\n",
            "1.3155990839004517\n",
            "1.0187745094299316\n",
            "1.0295451879501343\n",
            "2.682797908782959\n",
            "1.7424646615982056\n",
            "0.8849379420280457\n",
            "1.1778123378753662\n",
            "1.4449961185455322\n",
            "3.1747946739196777\n",
            "0.8292415738105774\n",
            "0.9696395397186279\n",
            "1.9217157363891602\n",
            "1.0808911323547363\n",
            "1.1201438903808594\n",
            "1.3580139875411987\n",
            "1.474393367767334\n",
            "1.3444676399230957\n",
            "1.1698682308197021\n",
            "2.960470199584961\n",
            "0.6343194842338562\n",
            "1.5478123426437378\n",
            "1.2243852615356445\n",
            "1.4923357963562012\n",
            "1.1799311637878418\n",
            "1.2021687030792236\n",
            "1.3826022148132324\n",
            "0.7994398474693298\n",
            "1.7059355974197388\n",
            "1.3367068767547607\n",
            "1.1941313743591309\n",
            "1.3261902332305908\n",
            "0.8663200736045837\n",
            "1.2947256565093994\n",
            "1.333315134048462\n",
            "1.4666484594345093\n",
            "1.3864176273345947\n",
            "1.325116515159607\n",
            "1.3674434423446655\n",
            "1.2127643823623657\n",
            "1.2684327363967896\n",
            "1.1505942344665527\n",
            "1.1838241815567017\n",
            "1.1972198486328125\n",
            "1.407132863998413\n",
            "1.7494806051254272\n",
            "1.347461462020874\n",
            "0.8731035590171814\n",
            "2.72788405418396\n",
            "0.7901804447174072\n",
            "2.172870397567749\n",
            "1.3202409744262695\n",
            "1.1879802942276\n",
            "1.0481282472610474\n",
            "1.449829339981079\n",
            "1.474393367767334\n",
            "1.3580139875411987\n",
            "1.5353505611419678\n",
            "1.267661452293396\n",
            "2.4054617881774902\n",
            "0.7607787847518921\n",
            "1.2342709302902222\n",
            "1.3297665119171143\n",
            "1.1648622751235962\n",
            "1.196664810180664\n",
            "2.5866167545318604\n",
            "0.9777691960334778\n",
            "0.8886998891830444\n",
            "1.4813774824142456\n",
            "1.1234655380249023\n",
            "1.35602605342865\n",
            "0.6847257614135742\n",
            "2.723287343978882\n",
            "2.5510034561157227\n",
            "0.8249360918998718\n",
            "1.281942367553711\n",
            "1.2488348484039307\n",
            "1.405564546585083\n",
            "1.3802490234375\n",
            "1.7339831590652466\n",
            "0.9920574426651001\n",
            "1.2439699172973633\n",
            "1.3861308097839355\n",
            "0.7920248508453369\n",
            "1.5657944679260254\n",
            "1.2562536001205444\n",
            "1.3783634901046753\n",
            "0.8670689463615417\n",
            "1.875757098197937\n",
            "2.3860886096954346\n",
            "0.826069176197052\n",
            "1.4035565853118896\n",
            "0.9321213960647583\n",
            "1.3988512754440308\n",
            "1.2530367374420166\n",
            "1.610116958618164\n",
            "0.9924938678741455\n",
            "0.7232065200805664\n",
            "3.2664787769317627\n",
            "1.280479073524475\n",
            "1.0257787704467773\n",
            "1.175230622291565\n",
            "1.6324676275253296\n",
            "2.006305456161499\n",
            "0.9167207479476929\n",
            "1.2462252378463745\n",
            "1.460110068321228\n",
            "0.8816767334938049\n",
            "2.2820796966552734\n",
            "1.0109424591064453\n",
            "1.9954419136047363\n",
            "1.0295451879501343\n",
            "2.682797908782959\n",
            "1.8188482522964478\n",
            "0.7863265872001648\n",
            "1.4923357963562012\n",
            "1.1799311637878418\n",
            "1.2754321098327637\n",
            "1.139280915260315\n",
            "1.3449546098709106\n",
            "1.1245627403259277\n",
            "1.4566006660461426\n",
            "1.0309948921203613\n",
            "1.6976255178451538\n",
            "0.8972740769386292\n",
            "1.3444676399230957\n",
            "1.1698682308197021\n",
            "2.707839250564575\n",
            "0.9630593061447144\n",
            "1.1264947652816772\n",
            "1.217902421951294\n",
            "1.173913598060608\n",
            "1.1821388006210327\n",
            "0.9785510301589966\n",
            "1.7101417779922485\n",
            "1.0055742263793945\n",
            "1.6068568229675293\n",
            "0.915115237236023\n",
            "1.647847294807434\n",
            "2.0473296642303467\n",
            "0.9363405704498291\n",
            "1.1966192722320557\n",
            "1.4047702550888062\n",
            "1.1778123378753662\n",
            "1.4449961185455322\n",
            "1.7086381912231445\n",
            "1.2019593715667725\n",
            "1.5299524068832397\n",
            "0.9998573064804077\n",
            "1.2544647455215454\n",
            "1.1961857080459595\n",
            "1.4866540431976318\n",
            "1.2274763584136963\n",
            "1.970315933227539\n",
            "0.8905229568481445\n",
            "1.039763331413269\n",
            "2.5523159503936768\n",
            "0.9338998198509216\n",
            "2.259330987930298\n",
            "1.5995458364486694\n",
            "0.8770714998245239\n",
            "1.0287600755691528\n",
            "1.7240431308746338\n",
            "1.2685940265655518\n",
            "1.2425103187561035\n",
            "1.2502368688583374\n",
            "1.1166293621063232\n",
            "2.5640370845794678\n",
            "0.9276928305625916\n",
            "1.1394537687301636\n",
            "1.2425254583358765\n",
            "1.0783069133758545\n",
            "1.4102762937545776\n",
            "1.0367292165756226\n",
            "1.5299913883209229\n",
            "1.1683344841003418\n",
            "1.4960167407989502\n",
            "0.9282406568527222\n",
            "1.5336755514144897\n",
            "1.1323578357696533\n",
            "1.308843970298767\n",
            "2.0272114276885986\n",
            "1.0962185859680176\n",
            "1.9043070077896118\n",
            "1.026058554649353\n",
            "1.0292717218399048\n",
            "1.701684594154358\n",
            "2.613762617111206\n",
            "0.9236876964569092\n",
            "0.8701111674308777\n",
            "1.8275959491729736\n",
            "1.1314297914505005\n",
            "1.310866355895996\n",
            "1.5837057828903198\n",
            "1.0429168939590454\n",
            "1.2658562660217285\n",
            "1.2494683265686035\n",
            "1.2573949098587036\n",
            "1.6122936010360718\n",
            "0.993209719657898\n",
            "2.229069948196411\n",
            "1.13008713722229\n",
            "1.1735841035842896\n",
            "1.263061285018921\n",
            "1.350555181503296\n",
            "1.0833593606948853\n",
            "1.388010025024414\n",
            "1.3098493814468384\n",
            "1.4182636737823486\n",
            "2.227309226989746\n",
            "0.7871593236923218\n",
            "1.1674234867095947\n",
            "1.074204444885254\n",
            "1.538255214691162\n",
            "1.1354151964187622\n",
            "1.0539684295654297\n",
            "1.5229376554489136\n",
            "1.403489112854004\n",
            "1.183922290802002\n",
            "1.4088314771652222\n",
            "2.004638910293579\n",
            "1.3826022148132324\n",
            "1.2021687030792236\n",
            "1.3485043048858643\n",
            "1.4074338674545288\n",
            "1.1330822706222534\n",
            "1.206135869026184\n",
            "2.731675148010254\n",
            "0.8655350804328918\n",
            "1.3831747770309448\n",
            "0.948276162147522\n",
            "1.0114855766296387\n",
            "1.5323259830474854\n",
            "1.2706035375595093\n",
            "1.316178798675537\n",
            "1.2063074111938477\n",
            "1.3331730365753174\n",
            "1.2687078714370728\n",
            "1.3857028484344482\n",
            "0.9211627244949341\n",
            "1.975712537765503\n",
            "0.6060040593147278\n",
            "2.638969659805298\n",
            "1.3044155836105347\n",
            "1.4347063302993774\n",
            "1.7950279712677002\n",
            "0.9842198491096497\n",
            "1.3119865655899048\n",
            "1.2119921445846558\n",
            "2.3822786808013916\n",
            "0.833185076713562\n",
            "0.7185320854187012\n",
            "3.117170810699463\n",
            "1.491756558418274\n",
            "1.0170724391937256\n",
            "0.9804974794387817\n",
            "1.4394800662994385\n",
            "1.3883781433105469\n",
            "0.9351249933242798\n",
            "1.3214491605758667\n",
            "1.14572012424469\n",
            "1.87845778465271\n",
            "1.0902667045593262\n",
            "1.398574709892273\n",
            "1.3120383024215698\n",
            "1.4162511825561523\n",
            "1.3974429368972778\n",
            "1.203202486038208\n",
            "1.1451857089996338\n",
            "1.3219279050827026\n",
            "1.326482892036438\n",
            "1.6176478862762451\n",
            "0.9856598377227783\n",
            "0.9831748008728027\n",
            "2.342628002166748\n",
            "1.87775456905365\n",
            "0.8850802183151245\n",
            "1.3929893970489502\n",
            "1.1156797409057617\n",
            "1.229569673538208\n",
            "1.2718002796173096\n",
            "0.9804595708847046\n",
            "2.3455545902252197\n",
            "1.0528556108474731\n",
            "1.8936216831207275\n",
            "0.8826825618743896\n",
            "1.559219479560852\n",
            "3.0210347175598145\n",
            "0.6364206075668335\n",
            "2.366088628768921\n",
            "0.9199181199073792\n",
            "0.9863192439079285\n",
            "1.6517115831375122\n",
            "1.2404593229293823\n",
            "1.4835407733917236\n",
            "1.8833186626434326\n",
            "0.9731820821762085\n",
            "1.1933808326721191\n",
            "1.556734561920166\n",
            "1.0570080280303955\n",
            "1.506378412246704\n",
            "1.185564398765564\n",
            "1.4690552949905396\n",
            "1.0808911323547363\n",
            "1.1201438903808594\n",
            "1.1393271684646606\n",
            "1.348446011543274\n",
            "1.4532649517059326\n",
            "0.9535789489746094\n",
            "1.3603793382644653\n",
            "1.2620248794555664\n",
            "0.8853402733802795\n",
            "2.238034248352051\n",
            "1.3834288120269775\n",
            "1.171879768371582\n",
            "1.427626371383667\n",
            "1.246420979499817\n",
            "1.434104084968567\n",
            "1.0511358976364136\n",
            "0.6702999472618103\n",
            "2.222572088241577\n",
            "1.1468924283981323\n",
            "2.3215689659118652\n",
            "1.4411653280258179\n",
            "1.1893857717514038\n",
            "1.337524652481079\n",
            "1.2027689218521118\n",
            "2.4901371002197266\n",
            "0.8285036683082581\n",
            "1.1302598714828491\n",
            "1.5700942277908325\n",
            "1.0187745094299316\n",
            "1.3155990839004517\n",
            "2.2468347549438477\n",
            "0.9585957527160645\n",
            "1.256174921989441\n",
            "1.382148265838623\n",
            "2.010530471801758\n",
            "0.9090080261230469\n",
            "1.03496253490448\n",
            "1.7862896919250488\n",
            "1.410150408744812\n",
            "0.9167138338088989\n",
            "0.9802370667457581\n",
            "1.540467619895935\n",
            "1.3144586086273193\n",
            "1.4156954288482666\n",
            "0.9748357534408569\n",
            "1.369935393333435\n",
            "1.0694795846939087\n",
            "1.640354871749878\n",
            "1.2783678770065308\n",
            "1.3191592693328857\n",
            "1.4186078310012817\n",
            "1.1950513124465942\n",
            "1.0489637851715088\n",
            "1.662854552268982\n",
            "1.1848349571228027\n",
            "1.4084759950637817\n",
            "0.8708379864692688\n",
            "1.4913862943649292\n",
            "1.8361129760742188\n",
            "1.673386812210083\n",
            "1.2095093727111816\n",
            "1.1487196683883667\n",
            "1.0794624090194702\n",
            "1.9725755453109741\n",
            "1.614959955215454\n",
            "1.0745701789855957\n",
            "1.338828206062317\n",
            "0.8418958187103271\n",
            "1.1930960416793823\n",
            "1.176645040512085\n",
            "1.2939115762710571\n",
            "1.4850589036941528\n",
            "2.8545994758605957\n",
            "0.6893402934074402\n",
            "2.293886661529541\n",
            "0.7557767033576965\n",
            "1.343511939048767\n",
            "1.2638888359069824\n",
            "1.4138760566711426\n",
            "1.0678119659423828\n",
            "1.5478123426437378\n",
            "1.2243852615356445\n",
            "0.8772757649421692\n",
            "2.6866796016693115\n",
            "1.3181601762771606\n",
            "1.3907415866851807\n",
            "2.682797908782959\n",
            "1.0295451879501343\n",
            "1.139280915260315\n",
            "1.2754321098327637\n",
            "1.1245627403259277\n",
            "1.3449546098709106\n",
            "2.2820796966552734\n",
            "0.8816767334938049\n",
            "1.0309948921203613\n",
            "1.4566006660461426\n",
            "1.1799311637878418\n",
            "1.4923357963562012\n",
            "1.9954419136047363\n",
            "1.0109424591064453\n",
            "0.8972740769386292\n",
            "1.6976255178451538\n",
            "1.1698682308197021\n",
            "1.3444676399230957\n",
            "0.7863265872001648\n",
            "1.8188482522964478\n",
            "0.9696395397186279\n",
            "1.9217157363891602\n",
            "1.2900168895721436\n",
            "1.1882609128952026\n",
            "1.3415331840515137\n",
            "1.1823666095733643\n",
            "1.3479750156402588\n",
            "1.181919813156128\n",
            "1.6442595720291138\n",
            "1.0595089197158813\n",
            "1.6561763286590576\n",
            "0.8700805902481079\n",
            "1.6904175281524658\n",
            "0.9982143640518188\n",
            "0.8372160196304321\n",
            "2.346123456954956\n",
            "1.6169384717941284\n",
            "0.8919819593429565\n",
            "1.003658652305603\n",
            "2.5725574493408203\n",
            "1.2425254583358765\n",
            "1.1394537687301636\n",
            "1.5336755514144897\n",
            "0.9282406568527222\n",
            "0.9276928305625916\n",
            "2.5640370845794678\n",
            "1.701684594154358\n",
            "1.0292717218399048\n",
            "1.0962185859680176\n",
            "2.0272114276885986\n",
            "1.4960167407989502\n",
            "1.1683344841003418\n",
            "1.026058554649353\n",
            "1.9043070077896118\n",
            "1.308843970298767\n",
            "1.1323578357696533\n",
            "1.5299913883209229\n",
            "1.0367292165756226\n",
            "1.4102762937545776\n",
            "1.0783069133758545\n",
            "1.1166293621063232\n",
            "1.2502368688583374\n",
            "0.9998573064804077\n",
            "1.5299524068832397\n",
            "0.8905229568481445\n",
            "1.970315933227539\n",
            "0.8770714998245239\n",
            "1.5995458364486694\n",
            "2.5523159503936768\n",
            "1.039763331413269\n",
            "1.1961857080459595\n",
            "1.2544647455215454\n",
            "2.259330987930298\n",
            "0.9338998198509216\n",
            "1.2425103187561035\n",
            "1.2685940265655518\n",
            "1.7240431308746338\n",
            "1.0287600755691528\n",
            "1.2274763584136963\n",
            "1.4866540431976318\n",
            "1.6068568229675293\n",
            "1.0055742263793945\n",
            "0.9630593061447144\n",
            "2.707839250564575\n",
            "1.217902421951294\n",
            "1.1264947652816772\n",
            "1.2019593715667725\n",
            "1.7086381912231445\n",
            "1.7101417779922485\n",
            "0.9785510301589966\n",
            "1.4449961185455322\n",
            "1.1778123378753662\n",
            "1.1821388006210327\n",
            "1.173913598060608\n",
            "0.9363405704498291\n",
            "2.0473296642303467\n",
            "1.647847294807434\n",
            "0.915115237236023\n",
            "1.4047702550888062\n",
            "1.1966192722320557\n",
            "1.181919813156128\n",
            "1.3479750156402588\n",
            "2.346123456954956\n",
            "0.8372160196304321\n",
            "0.8919819593429565\n",
            "1.6169384717941284\n",
            "1.1823666095733643\n",
            "1.3415331840515137\n",
            "1.9217157363891602\n",
            "0.9696395397186279\n",
            "0.9982143640518188\n",
            "1.6904175281524658\n",
            "1.0595089197158813\n",
            "1.6442595720291138\n",
            "0.8700805902481079\n",
            "1.6561763286590576\n",
            "2.5725574493408203\n",
            "1.003658652305603\n",
            "1.1882609128952026\n",
            "1.2900168895721436\n",
            "2.004638910293579\n",
            "1.4088314771652222\n",
            "1.183922290802002\n",
            "1.403489112854004\n",
            "1.1354151964187622\n",
            "1.538255214691162\n",
            "1.2021687030792236\n",
            "1.3826022148132324\n",
            "1.074204444885254\n",
            "1.1674234867095947\n",
            "1.206135869026184\n",
            "1.1330822706222534\n",
            "0.7871593236923218\n",
            "2.227309226989746\n",
            "1.5229376554489136\n",
            "1.0539684295654297\n",
            "1.4182636737823486\n",
            "1.3098493814468384\n",
            "1.4074338674545288\n",
            "1.3485043048858643\n",
            "0.9236876964569092\n",
            "2.613762617111206\n",
            "1.0429168939590454\n",
            "1.5837057828903198\n",
            "1.6122936010360718\n",
            "1.2573949098587036\n",
            "1.8275959491729736\n",
            "0.8701111674308777\n",
            "1.310866355895996\n",
            "1.1314297914505005\n",
            "1.350555181503296\n",
            "1.263061285018921\n",
            "1.1735841035842896\n",
            "1.13008713722229\n",
            "1.388010025024414\n",
            "1.0833593606948853\n",
            "2.229069948196411\n",
            "0.993209719657898\n",
            "1.2494683265686035\n",
            "1.2658562660217285\n",
            "1.0170724391937256\n",
            "1.491756558418274\n",
            "1.4394800662994385\n",
            "0.9804974794387817\n",
            "1.3120383024215698\n",
            "1.398574709892273\n",
            "0.833185076713562\n",
            "2.3822786808013916\n",
            "1.2119921445846558\n",
            "1.3119865655899048\n",
            "1.0902667045593262\n",
            "1.87845778465271\n",
            "3.117170810699463\n",
            "0.7185320854187012\n",
            "0.9351249933242798\n",
            "1.3883781433105469\n",
            "1.3331730365753174\n",
            "1.2063074111938477\n",
            "0.948276162147522\n",
            "1.3831747770309448\n",
            "1.975712537765503\n",
            "0.9211627244949341\n",
            "0.8655350804328918\n",
            "2.731675148010254\n",
            "1.3857028484344482\n",
            "1.2687078714370728\n",
            "1.316178798675537\n",
            "1.2706035375595093\n",
            "1.4347063302993774\n",
            "1.3044155836105347\n",
            "0.9842198491096497\n",
            "1.7950279712677002\n",
            "2.6866796016693115\n",
            "0.8772757649421692\n",
            "1.2638888359069824\n",
            "1.343511939048767\n",
            "0.7557767033576965\n",
            "2.293886661529541\n",
            "1.3907415866851807\n",
            "1.3181601762771606\n",
            "1.2243852615356445\n",
            "1.5478123426437378\n",
            "0.6893402934074402\n",
            "2.8545994758605957\n",
            "1.0678119659423828\n",
            "1.4138760566711426\n",
            "1.176645040512085\n",
            "1.1930960416793823\n",
            "0.8418958187103271\n",
            "1.338828206062317\n",
            "1.4850589036941528\n",
            "1.2939115762710571\n",
            "2.638969659805298\n",
            "0.6060040593147278\n",
            "1.14572012424469\n",
            "1.3214491605758667\n",
            "1.5323259830474854\n",
            "1.0114855766296387\n",
            "0.8249360918998718\n",
            "2.5510034561157227\n",
            "1.662854552268982\n",
            "1.0489637851715088\n",
            "1.4913862943649292\n",
            "0.8708379864692688\n",
            "2.723287343978882\n",
            "0.6847257614135742\n",
            "1.9725755453109741\n",
            "1.0794624090194702\n",
            "1.1950513124465942\n",
            "1.4186078310012817\n",
            "1.3191592693328857\n",
            "1.2783678770065308\n",
            "1.1487196683883667\n",
            "1.2095093727111816\n",
            "1.640354871749878\n",
            "1.0694795846939087\n",
            "1.4084759950637817\n",
            "1.1848349571228027\n",
            "3.2664787769317627\n",
            "0.7232065200805664\n",
            "1.0257787704467773\n",
            "1.280479073524475\n",
            "1.2530367374420166\n",
            "1.3988512754440308\n",
            "0.9924938678741455\n",
            "1.610116958618164\n",
            "1.6324676275253296\n",
            "1.175230622291565\n",
            "1.875757098197937\n",
            "0.8670689463615417\n",
            "1.3783634901046753\n",
            "1.2562536001205444\n",
            "0.826069176197052\n",
            "2.3860886096954346\n",
            "1.460110068321228\n",
            "1.2462252378463745\n",
            "0.9321213960647583\n",
            "1.4035565853118896\n",
            "1.196664810180664\n",
            "1.1648622751235962\n",
            "1.3580139875411987\n",
            "1.474393367767334\n",
            "2.172870397567749\n",
            "0.7901804447174072\n",
            "1.267661452293396\n",
            "1.5353505611419678\n",
            "0.9777691960334778\n",
            "2.5866167545318604\n",
            "0.7607787847518921\n",
            "2.4054617881774902\n",
            "1.1879802942276\n",
            "1.3202409744262695\n",
            "1.4813774824142456\n",
            "0.8886998891830444\n",
            "1.449829339981079\n",
            "1.0481282472610474\n",
            "1.3297665119171143\n",
            "1.2342709302902222\n",
            "1.1838241815567017\n",
            "1.1505942344665527\n",
            "1.333315134048462\n",
            "1.2947256565093994\n",
            "1.2684327363967896\n",
            "1.2127643823623657\n",
            "1.3864176273345947\n",
            "1.4666484594345093\n",
            "1.407132863998413\n",
            "1.1972198486328125\n",
            "1.1941313743591309\n",
            "1.3367068767547607\n",
            "1.3861308097839355\n",
            "1.2439699172973633\n",
            "1.3802490234375\n",
            "1.405564546585083\n",
            "1.2488348484039307\n",
            "1.281942367553711\n",
            "1.369935393333435\n",
            "0.9748357534408569\n",
            "1.673386812210083\n",
            "1.8361129760742188\n",
            "0.9167207479476929\n",
            "2.006305456161499\n",
            "1.35602605342865\n",
            "1.1234655380249023\n",
            "0.9920574426651001\n",
            "1.7339831590652466\n",
            "1.0745701789855957\n",
            "1.614959955215454\n",
            "1.5657944679260254\n",
            "0.7920248508453369\n",
            "1.326482892036438\n",
            "1.3219279050827026\n",
            "1.1156797409057617\n",
            "1.3929893970489502\n",
            "2.342628002166748\n",
            "0.9831748008728027\n",
            "0.8850802183151245\n",
            "1.87775456905365\n",
            "1.1451857089996338\n",
            "1.203202486038208\n",
            "1.559219479560852\n",
            "0.8826825618743896\n",
            "0.9856598377227783\n",
            "1.6176478862762451\n",
            "1.8936216831207275\n",
            "1.0528556108474731\n",
            "1.2718002796173096\n",
            "1.229569673538208\n",
            "2.3455545902252197\n",
            "0.9804595708847046\n",
            "0.8663200736045837\n",
            "1.3261902332305908\n",
            "1.3674434423446655\n",
            "1.325116515159607\n",
            "1.1201438903808594\n",
            "1.0808911323547363\n",
            "0.9199181199073792\n",
            "2.366088628768921\n",
            "1.4835407733917236\n",
            "1.2404593229293823\n",
            "1.556734561920166\n",
            "1.1933808326721191\n",
            "1.4690552949905396\n",
            "1.185564398765564\n",
            "1.6517115831375122\n",
            "0.9863192439079285\n",
            "1.506378412246704\n",
            "1.0570080280303955\n",
            "0.9731820821762085\n",
            "1.8833186626434326\n",
            "1.348446011543274\n",
            "1.1393271684646606\n",
            "0.6364206075668335\n",
            "3.0210347175598145\n",
            "1.3974429368972778\n",
            "1.4162511825561523\n",
            "1.246420979499817\n",
            "1.427626371383667\n",
            "1.171879768371582\n",
            "1.3834288120269775\n",
            "2.238034248352051\n",
            "0.8853402733802795\n",
            "1.0511358976364136\n",
            "1.434104084968567\n",
            "1.1893857717514038\n",
            "1.4411653280258179\n",
            "2.222572088241577\n",
            "0.6702999472618103\n",
            "1.2027689218521118\n",
            "1.337524652481079\n",
            "0.9535789489746094\n",
            "1.4532649517059326\n",
            "1.2620248794555664\n",
            "1.3603793382644653\n",
            "2.3215689659118652\n",
            "1.1468924283981323\n",
            "1.347461462020874\n",
            "1.7494806051254272\n",
            "2.72788405418396\n",
            "0.8731035590171814\n",
            "0.8285036683082581\n",
            "2.4901371002197266\n",
            "1.382148265838623\n",
            "1.256174921989441\n",
            "1.540467619895935\n",
            "0.9802370667457581\n",
            "1.7862896919250488\n",
            "1.03496253490448\n",
            "1.5700942277908325\n",
            "1.1302598714828491\n",
            "1.4156954288482666\n",
            "1.3144586086273193\n",
            "0.9585957527160645\n",
            "2.2468347549438477\n",
            "0.9090080261230469\n",
            "2.010530471801758\n",
            "1.3155990839004517\n",
            "1.0187745094299316\n",
            "0.9167138338088989\n",
            "1.410150408744812\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}